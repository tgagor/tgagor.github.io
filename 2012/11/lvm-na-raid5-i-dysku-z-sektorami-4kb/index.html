<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LVM na RAID5 i dysku z sektorami 4KB | Tom's Blog</title>
<meta name=keywords content="bash,ext4,Linux,LVM,mdadm,RAID"><meta name=description content="Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sfor"><meta name=author content="timor"><link rel=canonical href=https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/><link crossorigin=anonymous href=/assets/css/stylesheet.985a7641e69c6b20f34bf2fd88dce5a50d271007390e15128242aada21b132f4.css integrity="sha256-mFp2QeacayDzS/L9iNzlpQ0nEAc5DhUSgkKq2iGxMvQ=" rel="preload stylesheet" as=style><link rel=icon href=https://gagor.pro/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gagor.pro/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gagor.pro/favicon-32x32.png><link rel=apple-touch-icon href=https://gagor.pro/apple-touch-icon.png><link rel=mask-icon href=https://gagor.pro/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/"><meta property="og:site_name" content="Tom's Blog"><meta property="og:title" content="LVM na RAID5 i dysku z sektorami 4KB"><meta property="og:description" content="Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM’a wyrównanie do chunk’a z macierzy.
Prawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 48KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART’a) że mają bloki 512B - SIC!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2012-11-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-09T19:46:18+01:00"><meta property="article:tag" content="Bash"><meta property="article:tag" content="Ext4"><meta property="article:tag" content="Linux"><meta property="article:tag" content="LVM"><meta property="article:tag" content="Mdadm"><meta property="article:tag" content="RAID"><meta name=twitter:card content="summary"><meta name=twitter:title content="LVM na RAID5 i dysku z sektorami 4KB"><meta name=twitter:description content="Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć  trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM&rsquo;a wyrównanie do chunk&rsquo;a z macierzy.
Prawidłowe wyrównanie partycji
Kupując nowy dysk (o pojemności od 500GB w górę),  mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 48KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART&rsquo;a) że mają bloki 512B - SIC!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gagor.pro/posts/"},{"@type":"ListItem","position":2,"name":"LVM na RAID5 i dysku z sektorami 4KB","item":"https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LVM na RAID5 i dysku z sektorami 4KB","name":"LVM na RAID5 i dysku z sektorami 4KB","description":"Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyrównanie do chunk\u0026rsquo;a z macierzy.\nPrawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 48KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART\u0026rsquo;a) że mają bloki 512B - SIC!\n","keywords":["bash","ext4","Linux","LVM","mdadm","RAID"],"articleBody":"Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM’a wyrównanie do chunk’a z macierzy.\nPrawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 48KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART’a) że mają bloki 512B - SIC!\nJeśli mamy fart i nasz dysk raportuje rozmiar fizycznego sektora to możemy to sprawdzić tak:\ncat /sys/block/sda/queue/physical_block_size U mnie to polecenie zwróciło 4096 czyli 4KB co jest zgodne z deklaracją producenta.\nDobre podejście do tematu wyrównania partycji zaproponował Teo Tso czyli wybranie takich parametrów głowic/sektorów na ścieżce by fdisk automatycznie wyrównywał partycje do oczekiwanej przez nas wielkości bloku. Teo proponował użycie 224 głowic i 56 sektorów - co da wyrównanie do 128KB dla wszystkich partycji z wyjątkiem pierwszej (pierwsza będzie wyrównana do 4KB o ile nie wymusimy startu z 256 sektora). Jeżeli mamy dysk z blokami 4KB lub pierwszą partycję zamierzamy wykorzystać jako np. /boot (gdzie wydajność nie ma aż takiego dużego znaczenia) to jest to ok. Ale jeśli kompatybilność z DOS’em mamy w poważaniu to możemy w fdisku utworzyć pierwszą partycję wyrównaną do 128KB lub 1MB.\nPrzeważnie wolałem cfdiska od fdiska (bo po co się męczyć z topornym interfejsem) ale nie udało mi się skubańca zmusić by tworzył pierwszą partycję w sposób nie kompatybilny z DOS’em. fdisk pomimo toporności po podaniu liczby głowic i sektorów w ścieżce podpowiedział mi poprawne wyrównanie partycji (a gdybyśmy posiadali starszą wersję, która nie jest tak sprytna to przynajmniej możemy podać ręcznie od którego sektora ma zaczynać się partycja).\nWyrównanie do 128KB fdisk -u -H 224 -S 56 /dev/sdX Opcja -u zmienia domyślną jednostkę na sektory (mamy wtedy mniejsze liczby, które łatwiej się przelicza). Dla wyrównania do 128KB pierwsza partycja powinna się zaczynać na 256 sektorze. Do wyrównania do 4KB wystarczy zacząć na 56 sektorze (wystarczające przy części nowszych dysków twardych).\nWyrównanie do 1MB Jeśli jednak dysponujemy dyskiem SSD z ciężko powiedzieć jak dużym blokiem to lepiej wykorzystać wyrównanie do 1MB - zmarnujemy trochę więcej miejsca (tych parę MB jakoś przeżyjemy) ale w tym rozmiarze na pewno zmieści się każdy sektor (a może nawet Erase Block, który obecnie przeważnie ma 512KB choć zdarzają się sztuki z 4MB). Można to osiągnąć z ustawieniem 64 głowic i 32 sektorów na ścieżkę, robimy to tak:\nfdisk -u -H 64 -S 32 /dev/sdX Pierwsza partycja powinna się zaczynać na 2048 sektorze dla wyrównania do 1MB lub 8192 sektorze jeśli chcemy zacząć od 4MB.\nPo odpaleniu fdiska z tymi parametrami wybieramy opcję n i lecimy dalej zgodnie z podpowiedziami, np.:\nfdisk -u -H 224 -S 56 /dev/sdc Polecenie (m wyświetla pomoc): m Polecenie a zmiana flagi rozruchu b modyfikacja etykiety dysku BSD c zmiana flagi kompatybilności z DOS-em d usunięcie partycji l wypisanie znanych typów partycji m wyświetlenie tego menu n dodanie nowej partycji o utworzenie nowej, pustej DOS-owej tablicy partycji p wypisanie tablicy partycji q zakończenie bez zapisywania zmian s utworzenie nowej, pustej etykiety dysku Suna t zmiana ID systemu partycji u zmiana jednostek wyświetlania/wprowadzania v weryfikacja tablicy partycji w zapis tablicy partycji na dysk i zakończenie x dodatkowe funkcje (tylko dla ekspertów) Polecenie (m wyświetla pomoc): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): Using default response p Numer partycji (1-4, domyślnie 1): Przyjęto wartość domyślną 1 Pierwszy sektor (2048-15240575, domyślnie 2048): Przyjęto wartość domyślną 2048 Ostatni sektor, +sektorów lub +rozmiar{K,M,G} (2048-15240575, domyślnie 15240575): +100M Polecenie (m wyświetla pomoc): p Dysk /dev/sdc: 7803 MB, bajtów: 7803174912 głowic: 224, sektorów/ścieżkę: 56, cylindrów: 1214, w sumie sektorów: 15240576 Jednostka = sektorów, czyli 1 * 512 = 512 bajtów Rozmiar sektora (logiczny/fizyczny) w bajtach: 512 / 512 Rozmiar we/wy (minimalny/optymalny) w bajtach: 512 / 512 Identyfikator dysku: 0xc3072e18 Urządzenie Rozruch Początek Koniec Bloków ID System /dev/sdc1 2048 206847 102400 83 Linux Polecenie (m wyświetla pomoc): w Tablica partycji została zmodyfikowana! Wywoływanie ioctl() w celu ponownego odczytu tablicy partycji. UWAGA: ponowny odczyt tablicy partycji zakończył się błędem 16: Urządzenie lub zasoby zajęte. Jądro nadal używa starej tablicy. Nowa tablica będzie używana po następnym restarcie systemu albo po uruchomieniu partprobe(8) lub kpartx(8) Synchronizacja dysków. P.S. Po co w ogóle partycjonować - można użyć całych urządzeń Co prawda w przypadku gdy zamierzam całe dyski poświęcić na RAID’a mógłbym ich nie partycjonować tylko użyć całych urządzeń i zadbać tylko by mdadm poprawnie się wyrównał (co zresztą robi automatycznie do 4KB), ale obawiałem się jak względem takich dysków zachowają się inne systemy które mam zainstalowane na komputerze - nie chciałbym aby przypadkiem uznały że to jakiś uszkodzony dysk po czym zainicjowały tablicę partycji… Może to nieuzasadniona fobia ale wiem że względem partycji RAID Autodetect nic takiego mi się nie przytrafi 😃\nTworzenie macierzy RAID Jak już utworzymy pożądane partycje na pierwszym dysku to musimy je powielić na wszystkich pozostałych dyskach, które zamierzamy włączyć do macierzy - najprościej zrobić to sfdisk’iem:\nsfdisk -d /dev/sdX | sfdisk /dev/sdY Przy czym dysk sdX to źródłowy dysk z gotowymi partycjami, a dysk sdY to każdy na którym chcemy odtworzyć partycje. Można by zrobić na to ładną pętelkę jeśli mamy więcej tych dysków ale celowo tego nie zrobię bo jak znam życie to kiedyś ktoś to skopiuje ze znakiem nowego wiersza i wrzuci do konsoli… 😀\nTeraz tworzę zdegradowaną (z dwóch dysków) macierz RAID5… Ale na dobrą sprawę bezpieczniej byłoby utworzyć macierz RAID1 z dwóch dysków (o ile miejsca byłoby wystarczająco na przeniesienie danych) a po przeniesieniu danych na macierz dodanie trzeciego dysku i powiększenie macierzy z reshapingiem do RAID5 - postanowiłem pominąć takie rozwiązanie bo nie bałem się utraty danych, miałem dokładną kopię na innej macierzy 😃\nWięc tworzymy macierz:\nmdadm --create /dev/md0 --level=5 --raid-devices=2 --chunk=512k /dev/sdX1 /dev/sdY1 Na dobrą sprawę z powyższych opcji tylko chunk nadaje się do “dopasowania” - ja mam zamiar utworzyć macierz z dość dużych zasobów (3x2TB) i przechowywać na niej raczej duże pliki więc 512KB wydaje mi się dobrą wartością. Gdybyśmy jednak potrzebowali większej liczby I/O dla małych pliczków to mniejsza wartość może być lepsza. Warto zrobić kilka benchmarków dla różnych wartości chunk’a i dopasować do przewidywanego przez nas obciążenia.\nAby macierz była widoczne już w czasie startu systemu należy dodatkowo wykonać polecenie:\nmdadm --detail --scan \u003e\u003e /etc/mdadm/mdadm.conf Warto też w pliku /etc/mdadm/mdadm.conf odkomentować i wpisać jakieś sensowne wartości dla HOMEHOST, MAILADDR.\nI teraz możemy odbudować obraz initrd:\nupdate-initramfs -u Optymalizacja macierzy RAID5 Sam proces tworzenia macierzy może zająć kilka/kilkanaście godzin - dlatego warto mu pomóc kilkoma zmianami.\nspeed_limit_xxx Na pierwszy rzut - domyślne wartości dla minimalnej i maksymalnej prędkości budowania/regenerowania/odtwarzania macierzy RAID5, można je sprawdzić:\ncat /proc/sys/dev/raid/speed_limit_max 200000 cat /proc/sys/dev/raid/speed_limit_min 1000 Domyślnie jest to minimum 1MB/s i maksimum 200MB/s. Podbijając minimalną prędkość do 10~50MB/s można kosztem większego obciążenia systemu zmusić macierz by odbudowywała się szybciej. Osobiście uważam tą optymalizację za mało znaczącą bo cały mechanizm dość elastycznie reaguje na obciążenie systemu i jeśli nic nie robimy to prędkości odbudowy są dość wysokie. Ale można to zrobić tak:\necho 50000 \u003e /proc/sys/dev/raid/speed_limit_min lub tak:\nsysctl -w dev.raid.speed_limit_min=50000 stripe_cache_size Ta optymalizacja pomogła mi dużo - ok. 30% wzrost wydajności macierzy (również w czasie odbudowy parzystości). Możemy sprawdzić wartość tego parametru, np. tak:\ncat /sys/block/md0/md/stripe_cache_size Domyślnie jest to wartość 128 - bida z nędzą, u mnie ustawienie na 32768 (maksymalna wartość tego parametru) dało największy wzrost wydajności - ale już 8192 znacznie poprawiło wydajność.\nfor i in 256 512 1024 2048 4096 8192 16384 32768; do echo \"Testowanie $i\" echo $i \u003e /sys/block/md0/md/stripe_cache_size sync echo 3 \u003e /proc/sys/vm/drop_caches dd if=/dev/zero of=file bs=1M count=10000 done Wyłączenie NCQ dla wszystkich dysków w macierzy Wydało mi się to nieco kontrowersyjne bo NCQ powinno pomagać przy losowych odczytach/zapisach - ale zapuściłem test bonnie++ i okazało się że z włączonym NCQ czasy dostępu dla niektórych obciążeń rosną nawet dziesięciokrotnie! Warto więc sprawdzić tą opcję pod przewidywanym przez nas scenariuszem obciążenia.\nNCQ dla poszczególnych dysków można wyłączyć np. tak:\nfor d in sdb sdc sdd do echo 1 \u003e /sys/block/$d/device/queue_depth done Wyłączenie cache dyskowych Wyłączenie wbudowanej w dyski pamięci cache akurat nie zwiększa wydajności ale w przypadku awarii zasilania (lub innej gwałtownej awarii systemu) zwiększa szanse macierzy na przeżycie takiego incydentu. Obecnie większość systemów plików korzysta z opóźnienia zapisu by bardziej optymalnie zapisać dane na dysku - dlatego gdy zapisujemy dany to najpierw trafiają one do cache systemowego. Dopiero po sync’u są przesyłane do cache dyskowego skąd dopiero po pewnym czasie trafiają na dysk. Wyłączenie pamięci cache na dyskach “usuwa” nam to drugie opóźnienie.\nhdparm -W0 /dev/sd* Zmiana parametrów odczytu z wyprzedzeniem Bardzo ważnym parametrem mającym wpływ na wydajność macierzy jest odpowiednie ustawienie odczytu z wyprzedzeniem. Obecnie ustawioną wartość możemy sprawdzić tak (wartość wyrażona jest w 512 bajtowych sektorach):\nblockdev --getra /dev/md0 U mnie domyślnie było 4096 (a na innym starszym systemie tylko 1536) to może być zbyt mało dla konfiguracji RAID. Większe wartości można ustawić np. tak:\nblockdev --setra 65536 /dev/md0 A tak można wykonać sprawdzanie, która wartość będzie dla nas najbardziej optymalna:\ndd if=/dev/zero of=file bs=1M count=10000 for i in 1536 4096 8192 16384 32768 65536 131072 262144 524288; do echo \"Testowanie $i\" blockdev --setra $i /dev/md0 sync echo 3 \u003e /proc/sys/vm/drop_caches dd if=file of=/dev/null bs=1M done Tworzenie zasobu LVM Mając już macierze tworzę na niej volumen LVM - najpierw przygotowanie zasobu:\npvcreate /dev/md0 Jeżeli w /etc/lvm/lvm.conf mamy ustawione opcje:\nmd_component_detection = 1 md_chunk_alignment = 1 data_alignment_detection = 1 To LVM powinien automatycznie wykryć rozmiar chunk’a z RAID’a i dostosować swoje metadane, oraz początek danych tak by wszystko było prawidłowo wyrównane względem macierzy.\nJeśli nie mamy szczęścia (bardzo stare jajko/LVM) to będziemy musieli użyć opcji -metadatasize i/lub -dataalignment:\npvcreate --metadatasize 500k /dev/md0 Teraz ciekawostka - LVM potrzebuje na 192KB na dane nagłówkowe każdego wolumenu i każdy utworzony później zasób byłby o te 192KB przesunięty, więc … trafia nasze wyrównanie do 128KB. Dlatego zmuszamy LVM’a by zaalokował nieco więcej - tutaj 256KB. OK - ale w poleceniu jest 250 - WTF? I to aby było zabawnie jest to jak najbardziej prawidłowa wartość - nie wiem jaka w tym logika, ale by metadane zajmowały 256KB podajemy 250k, by zajmowały 512KB podajemy 500k itd…\nInaczej jest z opcją -dataalignment, tutaj najbardziej optymalnie należy podać rozmiar chunk’a*ilość aktywnych dysków (dla RAID5 odejmujemy jeden) - albo minimalnie rozmiar chunka, np.:\npvcreate --dataalignment 512K /dev/md0 Poprawność możemy sprawdzić poleceniem:\npvs /dev/md0 -o+pe_start PV VG Fmt Attr PSize PFree 1st PE /dev/md0 vgraid lvm2 a- 1,82t 488,89g 512,00k U mnie 1st PE zaczyna się na 512KB, więc jest OK.\nTeraz tworzymy grupę:\nvgcreate vgraid /dev/md0 Polecenie vgcreate posiada parametr -s który pozwala określić do wielokrotności jakiej wartości będzie zaokrąglana wielkość wolumenu - wartość ta powinna być wielokrotnością chunk’a z macierzy. Domyślnie ma ona wartość 4MB więc wszystko będzie ładnie wyrównane.\nI możemy zacząć tworzyć volumeny logiczne:\nlvcreate -L1T -nsrv vgraid Formatowanie To teraz formatowanie - tutaj też czasem trzeba się wysilić by utworzony przez nas filesystem działał możliwie optymalnie na macierzy i bloku o odpowiednim rozmiarze. W przypadku filesystemu na macierzy są dwa ważne parametry: stride i stripe-width. Stride powinien odpowiadać rozmiarowi podanemu jako chunk podczas tworzenia macierzy ale wyrażonego w blokach systemu plików (domyślnie 4KB). Stripe-width powinno być ustawione na: stride * N, gdzie N to ilość aktywnych dysków w macierzy (dla RAID5 jest to ilość dysków minus 1) - przykładowo dla bloku 4KB i chunk’a 512KB, stride powinien wynosić 128 (512/4). Z kolei stripe-width dla 3 dysków to 128*(3-1)=256. Prawidłowe dobranie tych parametrów może dać wzrost wydajności rzędu 40% (według moich testów). Teoretycznie na nowszych systemach tworzone systemy plików automatycznie powinny wykryć najbardziej optymalne wyrównanie - możemy więc spróbować puścić format bez tych parametrów i później skontrolować ich wartości poleceniem:\ntune2fs -l /dev/vgraid/srv Na początek przykład dla systemu plików dla małych i średnich plików:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -m0 /dev/vgraid/srv Jeden dodatkowy parametr to resize - pozwala on zmienić domyślne ustawienie maksymalnego rozmiaru do którego możemy powiększyć dany filesystem - domyślnie jest to wartość 1000 razy większa od początkowej wielkości - ciut przekozaczone. Może nie oszczędzi to dużo miejsca na dysku (kilkadziesiąt/kilkaset MB) ale na pewno skróci czas fsck’a.\nKolejny dodatek to -m0 które wyłącza alokację 5% przestrzeni dyskowej dla root’a i usług systemowych - po prostu tutaj tego nie potrzebuję a 5% z 1TB to 50GB marnującego się miejsca!\nJeśli wiemy że będziemy przechowywać na danym zasobie tylko stosunkowo duże pliki to można użyć takich opcji:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -T largefile -m0 /dev/vgraid/srv Opcja -T to wykorzystanie szablonów opcji dla tworzenia systemów plików, które można edytować i dodawać w pliku: /etc/mke2fs.conf. Szablon largefile wykorzystuje mniejszą liczbę inodów dla nowego filesystemu, jeśli zamierzamy przechowywać głównie duże pliki to zmniejszy to czas formatowania i późniejszych fsck’ów na tym systemie plików.\nTestowanie wydajności Zalecałbym testowanie wydajności na kolejnych etapach przygotowania dysków i powtarzać te same benchmarki po każdej zmianie, tak więc testujemy:\nNa początek wszystkie dyski, z których zamierzamy zbudować RAID’a by wykluczyć ewentualne “padaki”/uszkodzone kable, itp. Np. hdparm/dd na czystym dysku i dodatkowo iozone/bonnie++ na filesystemie by sprawdzić czy nie skopaliśmy wyrównania partycji. Po zbudowaniu RAID’a powtarzamy testy - na całym urządzeniu (dd) i po sformatowaniu (iozone/bonnie++) by upewnić się że RAID jest prawidłowo wyrównany (przy czym przy RAID5 możemy się spodziewać wyników przy zapisie niższych niż przy odczycie - jest to OK). Jest to też dobry moment na sprawdzenie kilku optymalizacji dla macierzy: stripe_cache_size, wyłączenie NCQ, ustawienia odczytu z wyprzedzeniem, wyłączenie cache dysków - po każdej z tych zmian ponawiamy benchmarki by upewnić się że uzyskaliśmy poprawę/lub nie. Po przygotowaniu LVM’a - ponawiamy benchmarki na volumenie by upewnić się że nie skopaliśmy wyrównania partycji/chunk’a/itd… Dopieszczamy opcje formatowania filesystemu i jego montowania (np. noatime, commit, data, itd) - i znów posiłkujemy się benchmarkami by potwierdzić że pniemy się z wydajnością w górę. Jeśli myślicie że to dużo to zalecałbym powtórzenie części benchmarków 23 krotnie by upewnić się że wyniki nie odbiegają znacznie od siebie. Dodatkowo powinniśmy czyścić przed każdym banchmarkiem cache dyskowy aby mieć pewność że lepsze wyniki nie są skutkiem wczytania danych do pamięci. Z tego samego powodu jeśli uruchamiamy benchmarki to ilość zapisywanych/odczytywanych danych powinna być minimum 1,52 razy większa niż pamieć RAM zainstalowana w systemie by na pewnie wszystkie dane nie zmieściły się w cache’u. Oczywiście można to zlać ale później nie ma się co dziwić że system działa wolno - a na produkcyjnej maszynie dużo trudniej zaorać całą konfiguracją i utworzyć od początku z prawidłowym wyrównaniem.\nZalecane jest wykorzystanie IOZone lub Bonnie++ ponieważ testują one nie tylko prosty sekwencyjny odczyt, ale również tworzenie/kasowanie plików o różnych rozmiarach i w różnej ilości - to pozwala lepiej sprawdzić opóźnienia występujące przy przewidywanych przez nas obciążeniach oraz upewnić się że cała zabawa z wyrównywaniem zasobów miała sens. Oczywiście to tylko zalecenia 😉\nhdparm W przypadku macierzy wykorzystanie prostego hdparm’a do testów:\nhdparm -tT /dev/md0 nie zwróci realnych i sensownych wyników. Pomiar jest zbyt krótki by uzyskać sensowne wyniki - lepiej wykorzystać dd z dużą ilością danych do odczytu.\ndd Narzędzie jakże prymitywne a tak przydatne. Możemy nim zmierzyć sekwencyjny odczyt/zapis z/do macierzy i uzyskać bardziej realne wyniki niż hdparm’em. Wystarczy wymusić operację na ok. dwukrotnie większej ilości danych niż ilość pamięci RAM. Dodatkowo czyścimy cache’e przed i po mierząc całościowy czas, np. tak:\nsync; echo 3 \u003e /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026\u0026 sync) Jest to szczególnie przydatne np. przy dopasowywaniu optymalnej dla nas wartości parametru stripe_cache_size, wystarczy przygotować odpowiednią pętlę:\nfor i in 128 256 512 1024 2048 4096 8192 16384 32768; do echo \"stripe_cache_size $i\" echo $i \u003e /sys/block/md0/md/stripe_cache_size sync; echo 3 \u003e /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026\u0026 sync) done Parametr bs określa bufor wykorzystywany przy operacjach odczytu/zapisu - można go dostosować do rozmiaru chunk’a/stripe-width/itp.. count określa jak dużo takich buforów odczytać/zapisać - w powyższym przypadku jest to 10 tys. jednomegabajtowych buforów więc łącznie 10GB.\nbonnie++ Bonnie++ wymaga nieco przygotowania ale wyniki w moim przypadku bardzo odpowiadały rzeczywistym. Co pokrótce trzeba zrobić:\nmkdir /srv/test chown -R guest:guest /srv/test bonnie++ -d /srv/test -s 16g -m nazwa_maszyny -f -u guest Możemy dodać opcję -b by po każdej operacji wykonywany był sync - to byłoby coś podobnego do systemów bazodanowych lub pocztowych. Jeżeli chcemy zasymulować standardowe operacje na plikach to nie potrzebujemy tej opcji.\niozone W najprostszym wykonaniu:\niozone -a Wykona to serię pomiarów na różnych rozmiarach plików, ilości powtórzeń itd. Najbardziej interesująca opcją w konfiguracji RAID jest “Stride read”.\niozone -S 8192 -t 1 Parametr -S przekazuje rozmiar pamięci cache procesora - wykorzystywany do alokacji pamięci blokami itp (sprawdzałem czy to cokolwiek pomoże.. ale nie widziałem dużej różnicy).\nParametr -t 1 to benchmark przepustowości dysku a parametr cyfrowy określa liczbę równoczesnych wątków, które będą odczytywać/zapisywać - można w ten sposób zasymulować np. równoczesny streaming dla wielu źródeł, itp.\niozone -S 8192 -a -s 40960 Tutaj parametr -s wskazuje na jakim rozmiarze pliku w KB ma być prowadzony benchmark - tutaj 40MB.\nPodsumowanie Ogólnie zadowolony jestem że udało mi się to wszystko zebrać w jednym poście bo dotychczas miałem to zapisane w wielu różnych zakładkach i spory problem gdy potrzebowałem “właśnie tego jednego polecenia”. Ale niezadowolony jestem z tego że nie udało mi się ustalić całości tego postępowania dla dysków o sektorach/erase block’ach większych niż 4 KB. Chodzi mi szczególnie o brak jasnego wyjaśnienia czy RAID5 jest prawidłowo wyrównywany bo według jednych tak właśnie jest, a według innych tak nie jest. Stąd niektórzy zalecają by stosować format metadanych dla macierzy w wersji 0.9 lub 1.0 zamiast 1.2 ale nie ma jasnych źródeł tego rozumowania. Mam nadzieję że kiedyś uda mi się to jednoznacznie rozsądzić - na pewno zaktualizuję wtedy tego posta.\nŹródełka na których oparłem to HOWTO http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing external link http://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously external link http://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding external link http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html external link http://askubuntu.com/questions/20852/making-stripe-cache-size-permanent external link http://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56 external link https://raid.wiki.kernel.org/index.php/Performance external link http://www.mjmwired.net/kernel/Documentation/md.txt external link http://wiki.hetzner.de/index.php/Partition_Alignment/en external link http://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment external link O tym że LVM na RAID5 sam się wyrównuje:\nhttp://marc.info/?l=linux-raid\u0026m=126267824425009\u0026w=2 external link http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html external link MDADM metadata format 1.2 wyrownuje sie do 4KiB:\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html external link Kalkulator stride’a\nhttp://busybox.net/~aldot/mkfs_stride.html external link Format raid’a:\nhttps://raid.wiki.kernel.org/index.php/RAID_superblock_formats external link Wyrównanie do 4kb - choć wydaje mi się że gościu robi to na czuja i ledwie mu się udało:\nhttp://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors external link ","wordCount":"3175","inLanguage":"en","datePublished":"2012-11-07T00:00:00Z","dateModified":"2021-01-09T19:46:18+01:00","author":{"@type":"Person","name":"timor"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://gagor.pro/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gagor.pro/ accesskey=h title="Tom's Blog (Alt + H)"><img src=https://gagor.pro/favicon-36x36.avif alt="Page logo" aria-label=logo height=36 width=36>Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gagor.pro/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gagor.pro/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://gagor.pro/bookshelf/ title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://gagor.pro/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://gagor.pro/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gagor.pro/>Home</a>&nbsp;»&nbsp;<a href=https://gagor.pro/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LVM na RAID5 i dysku z sektorami 4KB</h1><div class=post-meta><span title='2012-11-07 00:00:00 +0000 UTC'>2012-11-07</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;timor</div></header><div class=post-content><p>Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć  trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM&rsquo;a wyrównanie do chunk&rsquo;a z macierzy.</p><h2 id=prawidłowe-wyrównanie-partycji>Prawidłowe wyrównanie partycji<a hidden class=anchor aria-hidden=true href=#prawidłowe-wyrównanie-partycji>#</a></h2><p>Kupując nowy dysk (o pojemności od 500GB w górę),  mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 4<del>8KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128</del>512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART&rsquo;a) że mają bloki 512B - SIC!</p><p>Jeśli mamy fart i nasz dysk raportuje rozmiar fizycznego sektora to możemy to sprawdzić tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat /sys/block/sda/queue/physical_block_size
</span></span></code></pre></div><p>U mnie to polecenie zwróciło 4096 czyli 4KB co jest zgodne z deklaracją producenta.</p><p>Dobre podejście do tematu wyrównania partycji zaproponował Teo Tso czyli wybranie takich parametrów głowic/sektorów na ścieżce by fdisk automatycznie wyrównywał partycje do oczekiwanej przez nas wielkości bloku. Teo proponował użycie 224 głowic i 56 sektorów - co da wyrównanie do 128KB dla wszystkich partycji z wyjątkiem pierwszej (pierwsza będzie wyrównana do 4KB o ile nie wymusimy startu z 256 sektora). Jeżeli mamy dysk z blokami 4KB lub pierwszą partycję zamierzamy wykorzystać jako np. /boot (gdzie wydajność nie ma aż takiego dużego znaczenia) to jest to ok. Ale jeśli kompatybilność z DOS&rsquo;em mamy w poważaniu to możemy w fdisku utworzyć pierwszą partycję wyrównaną do 128KB lub 1MB.</p><p>Przeważnie wolałem <em>cfdiska</em> od <em>fdiska</em> (bo po co się męczyć z topornym interfejsem) ale nie udało mi się skubańca zmusić by tworzył pierwszą partycję w sposób nie kompatybilny z DOS&rsquo;em. fdisk pomimo toporności po podaniu liczby głowic i sektorów w ścieżce podpowiedział mi poprawne wyrównanie partycji (a gdybyśmy posiadali starszą wersję, która nie jest tak sprytna to przynajmniej możemy podać ręcznie od którego sektora ma zaczynać się partycja).</p><h3 id=wyrównanie-do-128kb>Wyrównanie do 128KB<a hidden class=anchor aria-hidden=true href=#wyrównanie-do-128kb>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fdisk -u -H <span style=color:#ae81ff>224</span> -S <span style=color:#ae81ff>56</span> /dev/sdX
</span></span></code></pre></div><p>Opcja -u zmienia domyślną jednostkę na sektory (mamy wtedy mniejsze liczby, które łatwiej się przelicza). Dla wyrównania do 128KB pierwsza partycja powinna się zaczynać na 256 sektorze. Do wyrównania do 4KB wystarczy zacząć na 56 sektorze (wystarczające przy części nowszych dysków twardych).</p><h3 id=wyrównanie-do-1mb>Wyrównanie do 1MB<a hidden class=anchor aria-hidden=true href=#wyrównanie-do-1mb>#</a></h3><p>Jeśli jednak dysponujemy dyskiem SSD z ciężko powiedzieć jak dużym blokiem to lepiej wykorzystać wyrównanie do 1MB - zmarnujemy trochę więcej miejsca (tych parę MB jakoś przeżyjemy) ale w tym rozmiarze na pewno zmieści się każdy sektor (a może nawet <em>Erase Block</em>, który obecnie przeważnie ma 512KB choć zdarzają się sztuki z 4MB). Można to osiągnąć z ustawieniem 64 głowic i 32 sektorów na ścieżkę, robimy to tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fdisk -u -H <span style=color:#ae81ff>64</span> -S <span style=color:#ae81ff>32</span> /dev/sdX
</span></span></code></pre></div><p>Pierwsza partycja powinna się zaczynać na 2048 sektorze dla wyrównania do 1MB lub 8192 sektorze jeśli chcemy zacząć od 4MB.</p><p>Po odpaleniu fdiska z tymi parametrami wybieramy opcję <strong>n</strong> i lecimy dalej zgodnie z podpowiedziami, np.:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fdisk -u -H <span style=color:#ae81ff>224</span> -S <span style=color:#ae81ff>56</span> /dev/sdc
</span></span><span style=display:flex><span>Polecenie <span style=color:#f92672>(</span>m wyświetla pomoc<span style=color:#f92672>)</span>: &lt;strong&gt;m&lt;/strong&gt;
</span></span><span style=display:flex><span>Polecenie
</span></span><span style=display:flex><span> a zmiana flagi rozruchu
</span></span><span style=display:flex><span> b modyfikacja etykiety dysku BSD
</span></span><span style=display:flex><span> c zmiana flagi kompatybilności z DOS-em
</span></span><span style=display:flex><span> d usunięcie partycji
</span></span><span style=display:flex><span> l wypisanie znanych typów partycji
</span></span><span style=display:flex><span> m wyświetlenie tego menu
</span></span><span style=display:flex><span> &lt;strong&gt;n dodanie nowej partycji&lt;/strong&gt;
</span></span><span style=display:flex><span> o utworzenie nowej, pustej DOS-owej tablicy partycji
</span></span><span style=display:flex><span> p wypisanie tablicy partycji
</span></span><span style=display:flex><span> q zakończenie bez zapisywania zmian
</span></span><span style=display:flex><span> s utworzenie nowej, pustej etykiety dysku Suna
</span></span><span style=display:flex><span> t zmiana ID systemu partycji
</span></span><span style=display:flex><span> u zmiana jednostek wyświetlania/wprowadzania
</span></span><span style=display:flex><span> v weryfikacja tablicy partycji
</span></span><span style=display:flex><span> w zapis tablicy partycji na dysk i zakończenie
</span></span><span style=display:flex><span> x dodatkowe funkcje <span style=color:#f92672>(</span>tylko dla ekspertów<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Polecenie <span style=color:#f92672>(</span>m wyświetla pomoc<span style=color:#f92672>)</span>: &lt;strong&gt;n&lt;/strong&gt;
</span></span><span style=display:flex><span>Partition type:
</span></span><span style=display:flex><span> p primary <span style=color:#f92672>(</span><span style=color:#ae81ff>0</span> primary, <span style=color:#ae81ff>0</span> extended, <span style=color:#ae81ff>4</span> free<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span> e extended
</span></span><span style=display:flex><span>Select <span style=color:#f92672>(</span>default &lt;strong&gt;p&lt;/strong&gt;<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>Using default response p
</span></span><span style=display:flex><span>Numer partycji <span style=color:#f92672>(</span>1-4, domyślnie &lt;strong&gt;1&lt;/strong&gt;<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>Przyjęto wartość domyślną <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>Pierwszy sektor <span style=color:#f92672>(</span>2048-15240575, domyślnie &lt;strong&gt;2048&lt;/strong&gt;<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>Przyjęto wartość domyślną <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>Ostatni sektor, +sektorów lub +rozmiar<span style=color:#f92672>{</span>K,M,G<span style=color:#f92672>}</span> <span style=color:#f92672>(</span>2048-15240575, domyślnie 15240575<span style=color:#f92672>)</span>: &lt;strong&gt;+100M&lt;/strong&gt;
</span></span><span style=display:flex><span>Polecenie <span style=color:#f92672>(</span>m wyświetla pomoc<span style=color:#f92672>)</span>: &lt;strong&gt;p&lt;/strong&gt;
</span></span><span style=display:flex><span>Dysk /dev/sdc: <span style=color:#ae81ff>7803</span> MB, bajtów: <span style=color:#ae81ff>7803174912</span>
</span></span><span style=display:flex><span>głowic: 224, sektorów/ścieżkę: 56, cylindrów: 1214, w sumie sektorów: <span style=color:#ae81ff>15240576</span>
</span></span><span style=display:flex><span>Jednostka <span style=color:#f92672>=</span> sektorów, czyli <span style=color:#ae81ff>1</span> * 512 <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span> bajtów
</span></span><span style=display:flex><span>Rozmiar sektora <span style=color:#f92672>(</span>logiczny/fizyczny<span style=color:#f92672>)</span> w bajtach: <span style=color:#ae81ff>512</span> / <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>Rozmiar we/wy <span style=color:#f92672>(</span>minimalny/optymalny<span style=color:#f92672>)</span> w bajtach: <span style=color:#ae81ff>512</span> / <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>Identyfikator dysku: 0xc3072e18
</span></span><span style=display:flex><span>Urządzenie Rozruch Początek Koniec Bloków ID System
</span></span><span style=display:flex><span>/dev/sdc1 <span style=color:#ae81ff>2048</span> <span style=color:#ae81ff>206847</span> <span style=color:#ae81ff>102400</span> <span style=color:#ae81ff>83</span> Linux
</span></span><span style=display:flex><span>Polecenie <span style=color:#f92672>(</span>m wyświetla pomoc<span style=color:#f92672>)</span>: &lt;strong&gt;w&lt;/strong&gt;
</span></span><span style=display:flex><span>Tablica partycji została zmodyfikowana!
</span></span><span style=display:flex><span>Wywoływanie ioctl<span style=color:#f92672>()</span> w celu ponownego odczytu tablicy partycji.
</span></span><span style=display:flex><span>UWAGA: ponowny odczyt tablicy partycji zakończył się błędem 16: Urządzenie lub zasoby zajęte.
</span></span><span style=display:flex><span>Jądro nadal używa starej tablicy. Nowa tablica będzie używana po
</span></span><span style=display:flex><span>następnym restarcie systemu albo po uruchomieniu partprobe<span style=color:#f92672>(</span>8<span style=color:#f92672>)</span> lub kpartx<span style=color:#f92672>(</span>8<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Synchronizacja dysków.
</span></span></code></pre></div><h3 id=ps-po-co-w-ogóle-partycjonować---można-użyć-całych-urządzeń>P.S. Po co w ogóle partycjonować - można użyć całych urządzeń<a hidden class=anchor aria-hidden=true href=#ps-po-co-w-ogóle-partycjonować---można-użyć-całych-urządzeń>#</a></h3><p>Co prawda w przypadku gdy zamierzam całe dyski poświęcić na RAID&rsquo;a mógłbym ich nie partycjonować tylko użyć całych urządzeń i zadbać tylko by mdadm poprawnie się wyrównał (co zresztą robi automatycznie do 4KB), ale obawiałem się jak względem takich dysków zachowają się inne systemy które mam zainstalowane na komputerze - nie chciałbym aby przypadkiem uznały że to jakiś uszkodzony dysk po czym zainicjowały tablicę partycji&mldr; Może to nieuzasadniona fobia ale wiem że względem partycji <em>RAID Autodetect</em> nic takiego mi się nie przytrafi &#x1f603;</p><h2 id=tworzenie-macierzy-raid>Tworzenie macierzy RAID<a hidden class=anchor aria-hidden=true href=#tworzenie-macierzy-raid>#</a></h2><p>Jak już utworzymy pożądane partycje na pierwszym dysku to musimy je powielić na wszystkich pozostałych dyskach, które zamierzamy włączyć do macierzy - najprościej zrobić to sfdisk&rsquo;iem:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sfdisk -d /dev/sdX | sfdisk /dev/sdY
</span></span></code></pre></div><p>Przy czym dysk sdX to źródłowy dysk z gotowymi partycjami, a dysk sdY to każdy na którym chcemy odtworzyć partycje. Można by zrobić na to ładną pętelkę jeśli mamy więcej tych dysków ale celowo tego nie zrobię bo jak znam życie to kiedyś ktoś to skopiuje ze znakiem nowego wiersza i wrzuci do konsoli&mldr; 😀</p><p>Teraz tworzę zdegradowaną (z dwóch dysków) macierz RAID5&mldr; Ale na dobrą sprawę bezpieczniej byłoby utworzyć macierz RAID1 z dwóch dysków (o ile miejsca byłoby wystarczająco na przeniesienie danych) a po przeniesieniu danych na macierz dodanie trzeciego dysku i powiększenie macierzy z reshapingiem do RAID5 - postanowiłem pominąć takie rozwiązanie bo nie bałem się utraty danych, miałem dokładną kopię na innej macierzy &#x1f603;</p><p>Więc tworzymy macierz:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mdadm --create /dev/md0 --level<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span> --raid-devices<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> --chunk<span style=color:#f92672>=</span>512k /dev/sdX1 /dev/sdY1
</span></span></code></pre></div><p>Na dobrą sprawę z powyższych opcji tylko chunk nadaje się do &ldquo;dopasowania&rdquo; - ja mam zamiar utworzyć macierz z dość dużych zasobów (3x2TB) i przechowywać na niej raczej duże pliki więc 512KB wydaje mi się dobrą wartością. Gdybyśmy jednak potrzebowali większej liczby I/O dla małych pliczków to mniejsza wartość może być lepsza. Warto zrobić kilka benchmarków dla różnych wartości chunk&rsquo;a i dopasować do przewidywanego przez nas obciążenia.</p><p>Aby macierz była widoczne już w czasie startu systemu należy dodatkowo wykonać polecenie:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mdadm --detail --scan &gt;&gt; /etc/mdadm/mdadm.conf
</span></span></code></pre></div><p>Warto też w pliku <em>/etc/mdadm/mdadm.conf</em> odkomentować i wpisać jakieś sensowne wartości dla HOMEHOST, MAILADDR.</p><p>I teraz możemy odbudować obraz initrd:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>update-initramfs -u
</span></span></code></pre></div><h3 id=optymalizacja-macierzy-raid5>Optymalizacja macierzy RAID5<a hidden class=anchor aria-hidden=true href=#optymalizacja-macierzy-raid5>#</a></h3><p>Sam proces tworzenia macierzy może zająć kilka/kilkanaście godzin - dlatego warto mu pomóc kilkoma zmianami.</p><h4 id=speed_limit_xxx>speed_limit_xxx<a hidden class=anchor aria-hidden=true href=#speed_limit_xxx>#</a></h4><p>Na pierwszy rzut - domyślne wartości dla minimalnej i maksymalnej prędkości budowania/regenerowania/odtwarzania macierzy RAID5, można je sprawdzić:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat /proc/sys/dev/raid/speed_limit_max
</span></span><span style=display:flex><span><span style=color:#ae81ff>200000</span>
</span></span><span style=display:flex><span>cat /proc/sys/dev/raid/speed_limit_min
</span></span><span style=display:flex><span><span style=color:#ae81ff>1000</span>
</span></span></code></pre></div><p>Domyślnie jest to minimum 1MB/s i maksimum 200MB/s. Podbijając minimalną prędkość do 10~50MB/s można kosztem większego obciążenia systemu zmusić macierz by odbudowywała się szybciej. Osobiście uważam tą optymalizację za mało znaczącą bo cały mechanizm dość elastycznie reaguje na obciążenie systemu i jeśli nic nie robimy to prędkości odbudowy są dość wysokie. Ale można to zrobić tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>echo <span style=color:#ae81ff>50000</span> &gt; /proc/sys/dev/raid/speed_limit_min
</span></span></code></pre></div><p>lub tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sysctl -w dev.raid.speed_limit_min<span style=color:#f92672>=</span><span style=color:#ae81ff>50000</span>
</span></span></code></pre></div><h4 id=stripe_cache_size>stripe_cache_size<a hidden class=anchor aria-hidden=true href=#stripe_cache_size>#</a></h4><p>Ta optymalizacja pomogła mi dużo - ok. 30% wzrost wydajności macierzy (również w czasie odbudowy parzystości). Możemy sprawdzić wartość tego parametru, np. tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat /sys/block/md0/md/stripe_cache_size
</span></span></code></pre></div><p>Domyślnie jest to wartość 128 - bida z nędzą, u mnie ustawienie na 32768 (maksymalna wartość tego parametru) dało największy wzrost wydajności - ale już 8192 znacznie poprawiło wydajność.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#ae81ff>256</span> <span style=color:#ae81ff>512</span> <span style=color:#ae81ff>1024</span> <span style=color:#ae81ff>2048</span> <span style=color:#ae81ff>4096</span> <span style=color:#ae81ff>8192</span> <span style=color:#ae81ff>16384</span> 32768;
</span></span><span style=display:flex><span><span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;Testowanie </span>$i<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    echo $i &gt; /sys/block/md0/md/stripe_cache_size
</span></span><span style=display:flex><span>    sync
</span></span><span style=display:flex><span>    echo <span style=color:#ae81ff>3</span> &gt; /proc/sys/vm/drop_caches
</span></span><span style=display:flex><span>    dd <span style=color:#66d9ef>if</span><span style=color:#f92672>=</span>/dev/zero of<span style=color:#f92672>=</span>file bs<span style=color:#f92672>=</span>1M count<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><h3 id=wyłączenie-ncq-dla-wszystkich-dysków-w-macierzy>Wyłączenie NCQ dla wszystkich dysków w macierzy<a hidden class=anchor aria-hidden=true href=#wyłączenie-ncq-dla-wszystkich-dysków-w-macierzy>#</a></h3><p>Wydało mi się to nieco kontrowersyjne bo NCQ powinno pomagać przy losowych odczytach/zapisach - ale zapuściłem test bonnie++ i okazało się że z włączonym NCQ czasy dostępu dla niektórych obciążeń rosną nawet dziesięciokrotnie! Warto więc sprawdzić tą opcję pod przewidywanym przez nas scenariuszem obciążenia.</p><p>NCQ dla poszczególnych dysków można wyłączyć np. tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> d in sdb sdc sdd
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>do</span> echo <span style=color:#ae81ff>1</span> &gt; /sys/block/$d/device/queue_depth
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><h3 id=wyłączenie-cache-dyskowych>Wyłączenie cache dyskowych<a hidden class=anchor aria-hidden=true href=#wyłączenie-cache-dyskowych>#</a></h3><p>Wyłączenie wbudowanej w dyski pamięci cache akurat nie zwiększa wydajności ale w przypadku awarii zasilania (lub innej gwałtownej awarii systemu) zwiększa szanse macierzy na przeżycie takiego incydentu. Obecnie większość systemów plików korzysta z opóźnienia zapisu by bardziej optymalnie zapisać dane na dysku - dlatego gdy zapisujemy dany to najpierw trafiają one do cache systemowego. Dopiero po sync&rsquo;u są przesyłane do cache dyskowego skąd dopiero po pewnym czasie trafiają na dysk. Wyłączenie pamięci cache na dyskach &ldquo;usuwa&rdquo; nam to drugie opóźnienie.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hdparm -W0 /dev/sd*
</span></span></code></pre></div><h3 id=zmiana-parametrów-odczytu-z-wyprzedzeniem>Zmiana parametrów odczytu z wyprzedzeniem<a hidden class=anchor aria-hidden=true href=#zmiana-parametrów-odczytu-z-wyprzedzeniem>#</a></h3><p>Bardzo ważnym parametrem mającym wpływ na wydajność macierzy jest odpowiednie ustawienie odczytu z wyprzedzeniem. Obecnie ustawioną wartość możemy sprawdzić tak (wartość wyrażona jest w 512 bajtowych sektorach):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blockdev --getra /dev/md0
</span></span></code></pre></div><p>U mnie domyślnie było 4096 (a na innym starszym systemie tylko 1536) to może być zbyt mało dla konfiguracji RAID. Większe wartości można ustawić np. tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>blockdev --setra <span style=color:#ae81ff>65536</span> /dev/md0
</span></span></code></pre></div><p>A tak można wykonać sprawdzanie, która wartość będzie dla nas najbardziej optymalna:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dd <span style=color:#66d9ef>if</span><span style=color:#f92672>=</span>/dev/zero of<span style=color:#f92672>=</span>file bs<span style=color:#f92672>=</span>1M count<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#ae81ff>1536</span> <span style=color:#ae81ff>4096</span> <span style=color:#ae81ff>8192</span> <span style=color:#ae81ff>16384</span> <span style=color:#ae81ff>32768</span> <span style=color:#ae81ff>65536</span> <span style=color:#ae81ff>131072</span> <span style=color:#ae81ff>262144</span> 524288;
</span></span><span style=display:flex><span><span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;Testowanie </span>$i<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    blockdev --setra $i /dev/md0
</span></span><span style=display:flex><span>    sync
</span></span><span style=display:flex><span>    echo <span style=color:#ae81ff>3</span> &gt; /proc/sys/vm/drop_caches
</span></span><span style=display:flex><span>    dd <span style=color:#66d9ef>if</span><span style=color:#f92672>=</span>file of<span style=color:#f92672>=</span>/dev/null bs<span style=color:#f92672>=</span>1M
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><h2 id=tworzenie-zasobu-lvm>Tworzenie zasobu LVM<a hidden class=anchor aria-hidden=true href=#tworzenie-zasobu-lvm>#</a></h2><p>Mając już macierze tworzę na niej volumen LVM - najpierw przygotowanie zasobu:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pvcreate /dev/md0
</span></span></code></pre></div><p>Jeżeli w /etc/lvm/lvm.conf mamy ustawione opcje:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>md_component_detection <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>md_chunk_alignment <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>data_alignment_detection <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>To LVM powinien automatycznie wykryć rozmiar chunk&rsquo;a z RAID&rsquo;a i dostosować swoje metadane, oraz początek danych tak by wszystko było prawidłowo wyrównane względem macierzy.</p><p>Jeśli nie mamy szczęścia (bardzo stare jajko/LVM) to będziemy musieli użyć opcji -metadatasize i/lub -dataalignment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pvcreate --metadatasize 500k /dev/md0
</span></span></code></pre></div><p>Teraz ciekawostka - LVM potrzebuje na 192KB na dane nagłówkowe każdego wolumenu i każdy utworzony później zasób byłby o te 192KB przesunięty, więc &mldr; trafia nasze wyrównanie do 128KB. Dlatego zmuszamy LVM&rsquo;a by zaalokował nieco więcej - tutaj 256KB. OK - ale w poleceniu jest 250 - WTF? I to aby było zabawnie jest to jak najbardziej prawidłowa wartość - nie wiem jaka w tym logika, ale by metadane zajmowały 256KB podajemy 250k, by zajmowały 512KB podajemy 500k itd&mldr;</p><p>Inaczej jest z opcją -dataalignment, tutaj najbardziej optymalnie należy podać rozmiar chunk&rsquo;a*ilość aktywnych dysków (dla RAID5 odejmujemy jeden) - albo minimalnie rozmiar chunka, np.:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pvcreate --dataalignment 512K /dev/md0
</span></span></code></pre></div><p>Poprawność możemy sprawdzić poleceniem:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pvs /dev/md0 -o+pe_start
</span></span><span style=display:flex><span>  PV       VG     Fmt  Attr PSize PFree   1st PE
</span></span><span style=display:flex><span>  /dev/md0 vgraid lvm2 a-   1,82t 488,89g 512,00k
</span></span></code></pre></div><p>U mnie <em>1st PE</em> zaczyna się na 512KB, więc jest OK.</p><p>Teraz tworzymy grupę:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>vgcreate vgraid /dev/md0
</span></span></code></pre></div><p>Polecenie vgcreate posiada parametr -s który pozwala określić do wielokrotności jakiej wartości będzie zaokrąglana wielkość wolumenu - wartość ta powinna być wielokrotnością chunk&rsquo;a z macierzy. Domyślnie ma ona wartość 4MB więc wszystko będzie ładnie wyrównane.</p><p>I możemy zacząć tworzyć volumeny logiczne:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>lvcreate -L1T -nsrv vgraid
</span></span></code></pre></div><h2 id=formatowanie>Formatowanie<a hidden class=anchor aria-hidden=true href=#formatowanie>#</a></h2><p>To teraz formatowanie - tutaj też czasem trzeba się wysilić by utworzony przez nas filesystem działał możliwie optymalnie na macierzy i bloku o odpowiednim rozmiarze. W przypadku filesystemu na macierzy są dwa ważne parametry: stride i stripe-width. Stride powinien odpowiadać rozmiarowi podanemu jako chunk podczas tworzenia macierzy ale wyrażonego w blokach systemu plików (domyślnie 4KB). Stripe-width powinno być ustawione na: stride * N, gdzie N to ilość aktywnych dysków w macierzy (dla RAID5 jest to ilość dysków minus 1) - przykładowo dla bloku 4KB i chunk&rsquo;a 512KB, stride powinien wynosić 128 (512/4). Z kolei stripe-width dla 3 dysków to 128*(3-1)=256. Prawidłowe dobranie tych parametrów może dać wzrost wydajności rzędu 40% (według moich testów). Teoretycznie na nowszych systemach tworzone systemy plików automatycznie powinny wykryć najbardziej optymalne wyrównanie - możemy więc spróbować puścić format bez tych parametrów i później skontrolować ich wartości poleceniem:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tune2fs -l /dev/vgraid/srv
</span></span></code></pre></div><p>Na początek przykład dla systemu plików dla małych i średnich plików:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkfs.ext4 -E stride<span style=color:#f92672>=</span>128,stripe-width<span style=color:#f92672>=</span>256,resize<span style=color:#f92672>=</span>4T -m0 /dev/vgraid/srv
</span></span></code></pre></div><p>Jeden dodatkowy parametr to <em>resize</em> - pozwala on zmienić domyślne ustawienie maksymalnego rozmiaru do którego możemy powiększyć dany filesystem - domyślnie jest to wartość 1000 razy większa od początkowej wielkości - ciut przekozaczone. Może nie oszczędzi to dużo miejsca na dysku (kilkadziesiąt/kilkaset MB) ale na pewno skróci czas fsck&rsquo;a.</p><p>Kolejny dodatek to <em>-m0</em> które wyłącza alokację 5% przestrzeni dyskowej dla root&rsquo;a i usług systemowych - po prostu tutaj tego nie potrzebuję a 5% z 1TB to 50GB marnującego się miejsca!</p><p>Jeśli wiemy że będziemy przechowywać na danym zasobie tylko stosunkowo duże pliki to można użyć takich opcji:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkfs.ext4 -E stride<span style=color:#f92672>=</span>128,stripe-width<span style=color:#f92672>=</span>256,resize<span style=color:#f92672>=</span>4T -T largefile -m0 /dev/vgraid/srv
</span></span></code></pre></div><p>Opcja -T to wykorzystanie szablonów opcji dla tworzenia systemów plików, które można edytować i dodawać w pliku: /etc/mke2fs.conf. Szablon largefile wykorzystuje mniejszą liczbę inodów dla nowego filesystemu, jeśli zamierzamy przechowywać głównie duże pliki to zmniejszy to czas formatowania i późniejszych fsck&rsquo;ów na tym systemie plików.</p><h2 id=testowanie-wydajności>Testowanie wydajności<a hidden class=anchor aria-hidden=true href=#testowanie-wydajności>#</a></h2><p>Zalecałbym testowanie wydajności na kolejnych etapach przygotowania dysków i powtarzać te same benchmarki po każdej zmianie, tak więc testujemy:</p><ol><li>Na początek <strong>wszystkie</strong> dyski, z których zamierzamy zbudować RAID&rsquo;a by wykluczyć ewentualne &ldquo;padaki&rdquo;/uszkodzone kable, itp. Np. hdparm/dd na czystym dysku i dodatkowo iozone/bonnie++ na filesystemie by sprawdzić czy nie skopaliśmy wyrównania partycji.</li><li>Po zbudowaniu RAID&rsquo;a powtarzamy testy - na całym urządzeniu (dd) i po sformatowaniu (iozone/bonnie++) by upewnić się że RAID jest prawidłowo wyrównany (przy czym przy RAID5 możemy się spodziewać wyników przy zapisie niższych niż przy odczycie - jest to OK). Jest to też dobry moment na sprawdzenie kilku optymalizacji dla macierzy: stripe_cache_size, wyłączenie NCQ, ustawienia odczytu z wyprzedzeniem, wyłączenie cache dysków - po każdej z tych zmian ponawiamy benchmarki by upewnić się że uzyskaliśmy poprawę/lub nie.</li><li>Po przygotowaniu LVM&rsquo;a - ponawiamy benchmarki na volumenie by upewnić się że nie skopaliśmy wyrównania partycji/chunk&rsquo;a/itd&mldr;</li><li>Dopieszczamy opcje formatowania filesystemu i jego montowania (np. noatime, commit, data, itd) - i znów posiłkujemy się benchmarkami by potwierdzić że pniemy się z wydajnością w górę.</li></ol><p>Jeśli myślicie że to dużo to zalecałbym powtórzenie części benchmarków 2<del>3 krotnie by upewnić się że wyniki nie odbiegają znacznie od siebie. Dodatkowo powinniśmy <a title="Wymuszenie zwolnienia pamięci buforów dyskowych na Linux’ie" href=https://gagor.pl/2011/09/wymuszenie-zwolnienia-pamieci-buforow-dyskowych-na-linuxie/>czyścić przed każdym banchmarkiem cache dyskowy</a> aby mieć pewność że lepsze wyniki nie są skutkiem wczytania danych do pamięci. Z tego samego powodu jeśli uruchamiamy benchmarki to ilość zapisywanych/odczytywanych danych powinna być minimum 1,5</del>2 razy większa niż pamieć RAM zainstalowana w systemie by na pewnie wszystkie dane nie zmieściły się w cache&rsquo;u. Oczywiście można to zlać ale później nie ma się co dziwić że system działa wolno - a na produkcyjnej maszynie dużo trudniej zaorać całą konfiguracją i utworzyć od początku z prawidłowym wyrównaniem.</p><p>Zalecane jest wykorzystanie IOZone lub Bonnie++ ponieważ testują one nie tylko prosty sekwencyjny odczyt, ale również tworzenie/kasowanie plików o różnych rozmiarach i w różnej ilości - to pozwala lepiej sprawdzić opóźnienia występujące przy przewidywanych przez nas obciążeniach oraz upewnić się że cała zabawa z wyrównywaniem zasobów miała sens. Oczywiście to tylko zalecenia 😉</p><h3 id=hdparm>hdparm<a hidden class=anchor aria-hidden=true href=#hdparm>#</a></h3><p>W przypadku macierzy wykorzystanie prostego hdparm&rsquo;a do testów:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hdparm -tT /dev/md0
</span></span></code></pre></div><p>nie zwróci realnych i sensownych wyników. Pomiar jest zbyt krótki by uzyskać sensowne wyniki - lepiej wykorzystać dd z dużą ilością danych do odczytu.</p><h3 id=dd>dd<a hidden class=anchor aria-hidden=true href=#dd>#</a></h3><p>Narzędzie jakże prymitywne a tak przydatne. Możemy nim zmierzyć sekwencyjny odczyt/zapis z/do macierzy i uzyskać bardziej realne wyniki niż hdparm&rsquo;em. Wystarczy wymusić operację na ok. dwukrotnie większej ilości danych niż ilość pamięci RAM. Dodatkowo czyścimy cache&rsquo;e przed i po mierząc całościowy czas, np. tak:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sync; echo <span style=color:#ae81ff>3</span> &gt; /proc/sys/vm/drop_caches; time <span style=color:#f92672>(</span>dd <span style=color:#66d9ef>if</span><span style=color:#f92672>=</span>/dev/zero of<span style=color:#f92672>=</span>/mnt/test/test.img bs<span style=color:#f92672>=</span>1024K count<span style=color:#f92672>=</span><span style=color:#ae81ff>10240</span> <span style=color:#f92672>&amp;&amp;</span> sync<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Jest to szczególnie przydatne np. przy dopasowywaniu optymalnej dla nas wartości parametru stripe_cache_size, wystarczy przygotować odpowiednią pętlę:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#ae81ff>128</span> <span style=color:#ae81ff>256</span> <span style=color:#ae81ff>512</span> <span style=color:#ae81ff>1024</span> <span style=color:#ae81ff>2048</span> <span style=color:#ae81ff>4096</span> <span style=color:#ae81ff>8192</span> <span style=color:#ae81ff>16384</span> 32768;
</span></span><span style=display:flex><span><span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;stripe_cache_size </span>$i<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    echo $i &gt; /sys/block/md0/md/stripe_cache_size
</span></span><span style=display:flex><span>    sync; echo <span style=color:#ae81ff>3</span> &gt; /proc/sys/vm/drop_caches; time <span style=color:#f92672>(</span>dd <span style=color:#66d9ef>if</span><span style=color:#f92672>=</span>/dev/zero of<span style=color:#f92672>=</span>/mnt/test/test.img bs<span style=color:#f92672>=</span>1024K count<span style=color:#f92672>=</span><span style=color:#ae81ff>10240</span> <span style=color:#f92672>&amp;&amp;</span> sync<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>Parametr bs określa bufor wykorzystywany przy operacjach odczytu/zapisu - można go dostosować do rozmiaru chunk&rsquo;a/stripe-width/itp.. count określa jak dużo takich buforów odczytać/zapisać - w powyższym przypadku jest to 10 tys. jednomegabajtowych buforów więc łącznie 10GB.</p><h3 id=bonnie>bonnie++<a hidden class=anchor aria-hidden=true href=#bonnie>#</a></h3><p>Bonnie++ wymaga nieco przygotowania ale wyniki w moim przypadku bardzo odpowiadały rzeczywistym. Co pokrótce trzeba zrobić:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir /srv/test
</span></span><span style=display:flex><span>chown -R guest:guest /srv/test
</span></span><span style=display:flex><span>bonnie++ -d /srv/test -s 16g -m nazwa_maszyny -f -u guest
</span></span></code></pre></div><p>Możemy dodać opcję <em>-b</em> by po każdej operacji wykonywany był <em>sync</em> - to byłoby coś podobnego do systemów bazodanowych lub pocztowych. Jeżeli chcemy zasymulować standardowe operacje na plikach to nie potrzebujemy tej opcji.</p><h3 id=iozone>iozone<a hidden class=anchor aria-hidden=true href=#iozone>#</a></h3><p>W najprostszym wykonaniu:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iozone -a
</span></span></code></pre></div><p>Wykona to serię pomiarów na różnych rozmiarach plików, ilości powtórzeń itd. Najbardziej interesująca opcją w konfiguracji RAID jest &ldquo;Stride read&rdquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iozone -S <span style=color:#ae81ff>8192</span> -t <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>Parametr <em>-S</em> przekazuje rozmiar pamięci cache procesora - wykorzystywany do alokacji pamięci blokami itp (sprawdzałem czy to cokolwiek pomoże.. ale nie widziałem dużej różnicy).</p><p>Parametr -t 1 to benchmark przepustowości dysku a parametr cyfrowy określa liczbę równoczesnych wątków, które będą odczytywać/zapisywać - można w ten sposób zasymulować np. równoczesny streaming dla wielu źródeł, itp.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iozone -S <span style=color:#ae81ff>8192</span> -a -s <span style=color:#ae81ff>40960</span>
</span></span></code></pre></div><p>Tutaj parametr <em>-s</em> wskazuje na jakim rozmiarze pliku w KB ma być prowadzony benchmark - tutaj 40MB.</p><h2 id=podsumowanie>Podsumowanie<a hidden class=anchor aria-hidden=true href=#podsumowanie>#</a></h2><p>Ogólnie zadowolony jestem że udało mi się to wszystko zebrać w jednym poście bo dotychczas miałem to zapisane w wielu różnych zakładkach i spory problem gdy potrzebowałem &ldquo;właśnie tego jednego polecenia&rdquo;. Ale niezadowolony jestem z tego że nie udało mi się ustalić całości tego postępowania dla dysków o sektorach/erase block&rsquo;ach większych niż 4 KB. Chodzi mi szczególnie o brak jasnego wyjaśnienia czy RAID5 jest prawidłowo wyrównywany bo według jednych tak właśnie jest, a według innych tak nie jest. Stąd niektórzy zalecają by stosować format metadanych dla macierzy w wersji 0.9 lub 1.0 zamiast 1.2 ale nie ma jasnych źródeł tego rozumowania. Mam nadzieję że kiedyś uda mi się to jednoznacznie rozsądzić - na pewno zaktualizuję wtedy tego posta.</p><h5 id=źródełka-na-których-oparłem-to-howto>Źródełka na których oparłem to HOWTO<a hidden class=anchor aria-hidden=true href=#źródełka-na-których-oparłem-to-howto>#</a></h5><p><a href=http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing target=_blank>http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously target=_blank>http://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding target=_blank>http://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html target=_blank>http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://askubuntu.com/questions/20852/making-stripe-cache-size-permanent target=_blank>http://askubuntu.com/questions/20852/making-stripe-cache-size-permanent<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56 target=_blank>http://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=https://raid.wiki.kernel.org/index.php/Performance target=_blank>https://raid.wiki.kernel.org/index.php/Performance<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://www.mjmwired.net/kernel/Documentation/md.txt target=_blank>http://www.mjmwired.net/kernel/Documentation/md.txt<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://wiki.hetzner.de/index.php/Partition_Alignment/en target=_blank>http://wiki.hetzner.de/index.php/Partition_Alignment/en<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href="http://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment" target=_blank>http://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p><p>O tym że LVM na RAID5 sam się wyrównuje:<br><a href="http://marc.info/?l=linux-raid&amp;m=126267824425009&amp;w=2" target=_blank>http://marc.info/?l=linux-raid&m=126267824425009&w=2<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a><br><a href=http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html target=_blank>http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p><p>MDADM metadata format 1.2 wyrownuje sie do 4KiB:<br><a href=http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html target=_blank>http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p><p>Kalkulator stride&rsquo;a<br><a href=http://busybox.net/~aldot/mkfs_stride.html target=_blank>http://busybox.net/~aldot/mkfs_stride.html<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p><p>Format raid&rsquo;a:<br><a href=https://raid.wiki.kernel.org/index.php/RAID_superblock_formats target=_blank>https://raid.wiki.kernel.org/index.php/RAID_superblock_formats<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p><p>Wyrównanie do 4kb - choć wydaje mi się że gościu robi to na czuja i ledwie mu się udało:<br><a href=http://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors target=_blank>http://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors<span style=white-space:nowrap>&#8201;<svg style="height:.7em;width:.7em" focusable="false" data-prefix="fas" data-icon="external-link-alt" class="svg-inline--fa fa-external-link-alt fa-w-16" role="img" viewBox="0 0 512 512"><title>external link</title><path fill="currentcolor" d="M432 320H4e2a16 16 0 00-16 16V448H64V128H208a16 16 0 0016-16V80a16 16 0 00-16-16H48A48 48 0 000 112V464a48 48 0 0048 48H4e2a48 48 0 0048-48V336a16 16 0 00-16-16zM488 0H360c-21.37.0-32.05 25.91-17 41l35.73 35.73L135 320.37a24 24 0 000 34L157.67 377a24 24 0 0034 0L435.28 133.32 471 169c15 15 41 4.5 41-17V24A24 24 0 00488 0z"/></svg></span></a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://gagor.pro/tag/bash/>Bash</a></li><li><a href=https://gagor.pro/tag/ext4/>Ext4</a></li><li><a href=https://gagor.pro/tag/linux/>Linux</a></li><li><a href=https://gagor.pro/tag/lvm/>LVM</a></li><li><a href=https://gagor.pro/tag/mdadm/>Mdadm</a></li><li><a href=https://gagor.pro/tag/raid/>RAID</a></li></ul><nav class=paginav><a class=prev href=https://gagor.pro/2012/11/hybrid-rainbow-db/><span class=title>« Prev</span><br><span>Hybrid Rainbow DB</span>
</a><a class=next href=https://gagor.pro/2012/11/instalacja-drukarki-i-skanera-brother-dcp-130c-na-ubuntu-12-04/><span class=title>Next »</span><br><span>Instalacja drukarki i skanera Brother DCP-130C na Ubuntu 12.04</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on x" href="https://x.com/intent/tweet/?text=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB&amp;url=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f&amp;hashtags=bash%2cext4%2cLinux%2cLVM%2cmdadm%2cRAID"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f&amp;title=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB&amp;summary=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB&amp;source=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f&title=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on whatsapp" href="https://api.whatsapp.com/send?text=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB%20-%20https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on telegram" href="https://telegram.me/share/url?text=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB&amp;url=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LVM na RAID5 i dysku z sektorami 4KB on ycombinator" href="https://news.ycombinator.com/submitlink?t=LVM%20na%20RAID5%20i%20dysku%20z%20sektorami%204KB&u=https%3a%2f%2fgagor.pro%2f2012%2f11%2flvm-na-raid5-i-dysku-z-sektorami-4kb%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=tgagor/tgagor.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkzOTg4MDg1Mw==" data-category=Announcements data-category-id=DIC_kwDOAmCIlc4CcAtN data-mapping=pathname data-strict=1 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=transparent_dark data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span><a href=https://gagor.pro/>Tom's Blog</a> &copy; 2008-2025</span>
<span>Content licensed under <a rel="license noopener noreferrer" target=_blank href=https://creativecommons.org/licenses/by-sa/4.0/>(CC BY-SA 4.0)</a></span></footer><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "8d51d5292abd4c77bad90db038659671"}'></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>