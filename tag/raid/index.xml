<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RAID on Tom&#39;s Blog</title>
    <link>https://gagor.pro/tag/raid/</link>
    <description>Recent content in RAID on Tom&#39;s Blog</description>
    <generator>Hugo -- 0.155.1</generator>
    <language>en</language>
    <copyright>Content licensed under (CC BY-SA 4.0)</copyright>
    <lastBuildDate>Mon, 05 Jan 2026 20:24:09 +0100</lastBuildDate>
    <atom:link href="https://gagor.pro/tag/raid/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Creating fully encrypted ZFS pool</title>
      <link>https://gagor.pro/2021/11/creating-fully-encrypted-zfs-pool/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://gagor.pro/2021/11/creating-fully-encrypted-zfs-pool/</guid>
      <description>Learn how to create a fully encrypted ZFS pool on Linux, including steps for generating encryption keys and configuring ZFS for secure data storage.</description>
    </item>
    <item>
      <title>Re-adding failed drive in mdadm</title>
      <link>https://gagor.pro/2013/12/re-adding-failed-drive-in-mdadm/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://gagor.pro/2013/12/re-adding-failed-drive-in-mdadm/</guid>
      <description>&lt;p&gt;Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as &lt;code&gt;failed&lt;/code&gt; in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I&amp;rsquo;ve got&amp;hellip;&lt;/p&gt;
&lt;p&gt;After rebuild I&amp;rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.&lt;/p&gt;
&lt;p&gt;What I do:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LVM na RAID5 i dysku z sektorami 4KB</title>
      <link>https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/</link>
      <pubDate>Wed, 07 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/</guid>
      <description>&lt;p&gt;Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć  trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM&amp;rsquo;a wyrównanie do chunk&amp;rsquo;a z macierzy.&lt;/p&gt;
&lt;h2 id=&#34;prawidłowe-wyrównanie-partycji&#34;&gt;Prawidłowe wyrównanie partycji&lt;/h2&gt;
&lt;p&gt;Kupując nowy dysk (o pojemności od 500GB w górę),  mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 4&lt;del&gt;8KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128&lt;/del&gt;512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART&amp;rsquo;a) że mają bloki 512B - SIC!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
