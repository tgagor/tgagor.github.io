[{"content":" Praca g≈ÇƒôbokaJak odnie≈õƒá sukces w ≈õwiecie, w kt√≥rym ciƒÖgle co≈õ nas rozprasza\nAuthor: Cal Newport\namazon.plamazon.comhelion.pl Deep Work is a book written by Cal Newport that explores the concept of deep work and how it can be leveraged to achieve professional success and lead a more fulfilling life. Newport argues that in an increasingly connected and fast-paced world, the ability to focus and produce high-quality work is becoming increasingly valuable and rare. He provides practical strategies and tools for developing the discipline of deep work, including minimizing distractions, optimizing your work environment, and training your mind to focus.\nThe book is well-written and presents a compelling case for the importance of deep work in today\u0026rsquo;s economy. Newport draws on a variety of research, case studies, and personal experiences to illustrate his points, and provides a wealth of practical tips and advice for developing deep work habits.\nOverall, I would highly recommend Deep Work to anyone looking to improve their focus, productivity, and overall performance in their professional and personal life. Whether you\u0026rsquo;re an entrepreneur, a knowledge worker, or simply someone looking to lead a more fulfilling life, this book provides a wealth of practical and actionable advice that can help you get there.\n","permalink":"https://timor.site/books/2023/praca-gleboka/","summary":"Praca g≈ÇƒôbokaJak odnie≈õƒá sukces w ≈õwiecie, w kt√≥rym ciƒÖgle co≈õ nas rozprasza\nAuthor: Cal Newport\namazon.plamazon.comhelion.pl Deep Work is a book written by Cal Newport that explores the concept of deep work and how it can be leveraged to achieve professional success and lead a more fulfilling life. Newport argues that in an increasingly connected and fast-paced world, the ability to focus and produce high-quality work is becoming increasingly valuable and rare.","title":"Praca g≈Çƒôboka"},{"content":" Zaczynaj od DLACZEGOJak wielcy liderzy inspirujƒÖ innych do dzia≈Çania\nAuthor: Simon Sinek\namazon.pl ","permalink":"https://timor.site/books/2020/zaczynaj-od-dlaczego/","summary":" Zaczynaj od DLACZEGOJak wielcy liderzy inspirujƒÖ innych do dzia≈Çania\nAuthor: Simon Sinek\namazon.pl ","title":"Zaczynaj od DLACZEGO"},{"content":" Piƒôƒá dysfunkcji pracy zespo≈ÇowejOpowie≈õƒá o przyw√≥dztwie\nAuthor: Patrick M. Lencioni\namazon.plamazon.com ","permalink":"https://timor.site/books/2022/5-disfunctions-of-a-team/","summary":" Piƒôƒá dysfunkcji pracy zespo≈ÇowejOpowie≈õƒá o przyw√≥dztwie\nAuthor: Patrick M. Lencioni\namazon.plamazon.com ","title":"Piƒôƒá dysfunkcji pracy zespo≈Çowej"},{"content":" Staro≈õƒá aksolotlaHardware dreams\nAuthor: Jacek Dukaj\namazon.plamazon.com Staro≈õƒá Aksolotla is a book written by Polish author Jacek Dukaj that I recently had the pleasure of reading. The book is a science fiction novel that explores themes of longevity, immortality, and the meaning of life. The protagonist of the story is a centuries-old axolotl, a species of salamander that is capable of regenerating its limbs and organs, who embarks on a journey of self-discovery as he grapples with the consequences of his own immortality.\nI was impressed by Dukaj\u0026rsquo;s imaginative and thought-provoking writing, as well as his skillful handling of complex scientific concepts. The characters are well-drawn and the story is richly imagined, with a detailed and immersive world that kept me engaged from beginning to end.\nOverall, I would highly recommend Staro≈õƒá Aksolotla to anyone who enjoys science fiction novels that challenge the mind and explore deep philosophical ideas. It is a thought-provoking and well-written book that is sure to leave a lasting impression on its readers.\n","permalink":"https://timor.site/books/2022/starosc-aksolotla/","summary":"Staro≈õƒá aksolotlaHardware dreams\nAuthor: Jacek Dukaj\namazon.plamazon.com Staro≈õƒá Aksolotla is a book written by Polish author Jacek Dukaj that I recently had the pleasure of reading. The book is a science fiction novel that explores themes of longevity, immortality, and the meaning of life. The protagonist of the story is a centuries-old axolotl, a species of salamander that is capable of regenerating its limbs and organs, who embarks on a journey of self-discovery as he grapples with the consequences of his own immortality.","title":"Staro≈õƒá aksolotla"},{"content":" B≈ÇƒÖdzƒÖ wszyscy (ale nie ja)Dlaczego usprawiedliwiamy g≈Çupie poglƒÖdy, z≈Çe decyzje i szkodliwe dzia≈Çania?\nAuthors: Elliot Aronson, Carol Tavris\namazon.com ","permalink":"https://timor.site/books/2022/bladza-wszyscy-ale-nie-ja/","summary":" B≈ÇƒÖdzƒÖ wszyscy (ale nie ja)Dlaczego usprawiedliwiamy g≈Çupie poglƒÖdy, z≈Çe decyzje i szkodliwe dzia≈Çania?\nAuthors: Elliot Aronson, Carol Tavris\namazon.com ","title":"B≈ÇƒÖdzƒÖ wszyscy (ale nie ja)"},{"content":"IMO people don\u0026rsquo;t understand how VOLUME1 works so they don\u0026rsquo;t use it. It\u0026rsquo;s generally used far too rarely!\nIn short VOLUME means two things:\nWhatever is left in directory marked as VOLUME, stays there and can\u0026rsquo;t be changed in later layers (actually it can be changed but changes won\u0026rsquo;t be persistent). Volumes are not part of layered image FS. They\u0026rsquo;re mounted as anonymous volumes located on standard file system. This means they\u0026rsquo;re working much faster. Let me explain it a bit.\nUse VOLUME for temporary files directories That\u0026rsquo;s one of things that piss me off in the official base images. They just don\u0026rsquo;t set up volumes for temporary directories by default. That\u0026rsquo;s why in my base images I always have something like that, somewhere at beginning:\nExample Dockerfile # on Debian VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/apt\u0026#34;, \u0026#34;/var/lib/apt/lists\u0026#34;] # on CentOS VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/yum\u0026#34;, \u0026#34;/var/cache/dnf\u0026#34;] What it does? It keeps images clean. Earlier, I was installing packages like this:\nHow everybody do it - without VOLUMEs FROM debian RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y nginx \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* Debian by default saves package lists (that\u0026rsquo;s what apt-get update downloads) in /var/cache/apt/lists. Then it\u0026rsquo;s downloading deb packages to /var/cache/apt/archives and all of this rubbish will be left in image by default, unless you remove it. But by marking /var/cache/apt as a VOLUME, all content became immutable and is removed/dropped at the end of each directive (like each RUN). So I can do it right now like this:\nHow I do it - with VOLUMEs FROM debian VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/apt\u0026#34;, \u0026#34;/var/lib/apt/lists\u0026#34;] RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y nginx It just makes life easier üòÑ\nAdd clean-up task for anonymous volumes Info\nSadly, those anonymous volumes are just left after you deploy new image version or shut it down. You have to clean them from time to time!\nWhich you probably should already know, if you manage cluster of docker hosts. It might be as simple as running in cron:\nCron cleanup task docker volume prune -f https://docs.docker.com/engine/reference/builder/#volume\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2022/09/best-practices-for-writing-dockerfiles-use-volume-for-all-mutable-temporary-file-locations/","summary":"IMO people don\u0026rsquo;t understand how VOLUME1 works so they don\u0026rsquo;t use it. It\u0026rsquo;s generally used far too rarely!\nIn short VOLUME means two things:\nWhatever is left in directory marked as VOLUME, stays there and can\u0026rsquo;t be changed in later layers (actually it can be changed but changes won\u0026rsquo;t be persistent). Volumes are not part of layered image FS. They\u0026rsquo;re mounted as anonymous volumes located on standard file system. This means they\u0026rsquo;re working much faster.","title":"Best practices for writing Dockerfiles - Use VOLUME for all mutable, temporary file locations"},{"content":"People often complain, that building Docker image takes a long time. \u0026ldquo;I just added a single jar package\u0026rdquo; they say\u0026hellip; Really?\nThey often don\u0026rsquo;t remember that whole \u0026ldquo;build context\u0026rdquo;1 is uploaded to Docker daemon during build, which often means they\u0026rsquo;re not only adding \u0026ldquo;single jar\u0026rdquo;, but also all sources, test results and whatever they have in working directory.\nSolution is simple - to use .dockerignore file2. Syntax is similar to .gitignore. It excludes what shouldn\u0026rsquo;t be uploaded to Docker daemon.\nTake a look at an example file below:\n.dockerignore src tests target/*.xml # exclude temp files */temp* */*/temp* temp? # exclude things you won\u0026#39;t need anyway *~ .DS_Store *.old .vscode https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-ontext\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#exclude-with-dockerignore\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2022/09/best-practices-for-writing-dockerfiles-use-dockerignore/","summary":"People often complain, that building Docker image takes a long time. \u0026ldquo;I just added a single jar package\u0026rdquo; they say\u0026hellip; Really?\nThey often don\u0026rsquo;t remember that whole \u0026ldquo;build context\u0026rdquo;1 is uploaded to Docker daemon during build, which often means they\u0026rsquo;re not only adding \u0026ldquo;single jar\u0026rdquo;, but also all sources, test results and whatever they have in working directory.\nSolution is simple - to use .dockerignore file2. Syntax is similar to .","title":"Best practices for writing Dockerfiles - Use .dockerignore"},{"content":"I\u0026rsquo;ve been thinking for a long time about writing set of articles on the topic of: \u0026ldquo;Dockerfile writing best practices\u0026rdquo;.\nAs it\u0026rsquo;s often my daily job to prepare best in class containers, that are later used by thousands of company\u0026rsquo;s applications, I have quite good insights on the topic. Some experience and knowledge gathered is often against intuition and building it took me a while. I want to share it, with a hope that feedback I get will allow me to excel on the topic even further.\nInitially I was thinking about writing one big article, connect all the dots there and make it great\u0026hellip; I even started it. But I failed by it\u0026rsquo;s scale, so I changed strategy and now I plan to release them as series of smaller articles, that should be easier to deliver and maintain.\nTopics I want to cover Use .dockerignore Follow \u0026ldquo;Filesystem Hierarchy Standard\u0026rdquo; Don\u0026rsquo;t leave packages you don\u0026rsquo;t need in images Use VOLUME for all mutable, temporary files locations Don\u0026rsquo;t rely on Docker official images Don\u0026rsquo;t run applications as a root Use multi-stage builds Use Label Schema/OCI Image Label Spcification Image security, how to scan them and when Build cache - to use it or not? And more\u0026hellip; I will curate list of links to dedicated articles on this page as they will be arriving.\n","permalink":"https://timor.site/2022/09/dockerfile-writing-best-practices/","summary":"I\u0026rsquo;ve been thinking for a long time about writing set of articles on the topic of: \u0026ldquo;Dockerfile writing best practices\u0026rdquo;.\nAs it\u0026rsquo;s often my daily job to prepare best in class containers, that are later used by thousands of company\u0026rsquo;s applications, I have quite good insights on the topic. Some experience and knowledge gathered is often against intuition and building it took me a while. I want to share it, with a hope that feedback I get will allow me to excel on the topic even further.","title":"Dockerfile writing best practices"},{"content":" Pu≈Çapki my≈õleniaO my≈õleniu szybkim i wolnym\nAuthor: Daniel Kahneman\namazon.plamazon.com ","permalink":"https://timor.site/books/2022/thinking-fast-and-slow/","summary":" Pu≈Çapki my≈õleniaO my≈õleniu szybkim i wolnym\nAuthor: Daniel Kahneman\namazon.plamazon.com ","title":"Pu≈Çapki my≈õlenia"},{"content":" The three-body problemAuthor: Cixin Liu\namazon.plamazon.comhelion.pl The novel is set in the future and follows the discovery of an alien civilization, the Trisolarans, and humanity\u0026rsquo;s attempts to make contact and defend against them. The book is known for its complex scientific concepts, philosophical ideas, and intricate storyline.\nOverall, \u0026ldquo;The Three-Body Problem\u0026rdquo; has been highly praised by readers and critics for its unique and thought-provoking approach to science fiction.\nPersonally, I found this book to be annoying in a few places, especially when it referred to people\u0026rsquo;s commitment following government decisions. I understand that it has a Chinese background, but I found it irritating when political correctness was emphasized more than rationality. Nonetheless, it was a nice journey.\n","permalink":"https://timor.site/books/2022/three-body-problem/","summary":"The three-body problemAuthor: Cixin Liu\namazon.plamazon.comhelion.pl The novel is set in the future and follows the discovery of an alien civilization, the Trisolarans, and humanity\u0026rsquo;s attempts to make contact and defend against them. The book is known for its complex scientific concepts, philosophical ideas, and intricate storyline.\nOverall, \u0026ldquo;The Three-Body Problem\u0026rdquo; has been highly praised by readers and critics for its unique and thought-provoking approach to science fiction.\nPersonally, I found this book to be annoying in a few places, especially when it referred to people\u0026rsquo;s commitment following government decisions.","title":"The three-body problem"},{"content":"Auta siƒô zmieniajƒÖ a problemy z nimi pozostajƒÖ te same :)\nKasowanie ostrze≈ºenia wymiany oleju 1 Przekrƒôciƒá kluczyk w stacyjce do drugiej pozycji, gdy zapalajƒÖ siƒô wszystkie kontrolki (nie uruchamiamy silnika). Wciskamy r√≥wnocze≈õnie peda≈Çy hamulca i gazu do oporu, trzymamy do zako≈Ñczenia procesu. Pojawi komunikat o rozpoczƒôciu resetowania inspekcji. Mo≈ºemy zatwierdziƒá OK. Czekamy a≈º pojawi siƒô komunikat: Zatwierdzamy OK. Dopiero teraz zwalniamy peda≈Çy. https://forum.fordclubpolska.org/showthread.php?t=108532\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2022/09/ford-s-max-kasowanie-ostrzezenia-wymiany-oleju/","summary":"Auta siƒô zmieniajƒÖ a problemy z nimi pozostajƒÖ te same :)\nKasowanie ostrze≈ºenia wymiany oleju 1 Przekrƒôciƒá kluczyk w stacyjce do drugiej pozycji, gdy zapalajƒÖ siƒô wszystkie kontrolki (nie uruchamiamy silnika). Wciskamy r√≥wnocze≈õnie peda≈Çy hamulca i gazu do oporu, trzymamy do zako≈Ñczenia procesu. Pojawi komunikat o rozpoczƒôciu resetowania inspekcji. Mo≈ºemy zatwierdziƒá OK. Czekamy a≈º pojawi siƒô komunikat: Zatwierdzamy OK. Dopiero teraz zwalniamy peda≈Çy. https://forum.fordclubpolska.org/showthread.php?t=108532\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Ford S-MAX - kasowanie ostrze≈ºenia wymiany oleju"},{"content":" Czy jeste≈õ tym, kt√≥ry puka?Author: Dariusz U≈ºycki\nempik.com ","permalink":"https://timor.site/books/2022/czy-jestes-tym-ktory-puka/","summary":" Czy jeste≈õ tym, kt√≥ry puka?Author: Dariusz U≈ºycki\nempik.com ","title":"Czy jeste≈õ tym, kt√≥ry puka?"},{"content":" Kubernetes OperatorsAutomating the Container Orchestration Platform\nAuthors: Jason Dobies, Joshua Wood\namazon.plamazon.comhelion.pl ","permalink":"https://timor.site/books/2022/kubernetes-operators/","summary":" Kubernetes OperatorsAutomating the Container Orchestration Platform\nAuthors: Jason Dobies, Joshua Wood\namazon.plamazon.comhelion.pl ","title":"Kubernetes Operators"},{"content":"I\u0026rsquo;m back on the big stage!\nI haven\u0026rsquo;t attend any big conferences as presenter for some time, but this year will change it. I\u0026rsquo;m starting big, with a talk: Docker base images - Ideas how to manage them on scale on Devoxx conference in Krak√≥w, that will take place on 22-24th June 2022.\nWant to meet? Meet there üòÑ\nUpdate I uploaded slides from presentation to my Github account.\n","permalink":"https://timor.site/2022/06/back-on-the-big-stage/","summary":"I\u0026rsquo;m back on the big stage!\nI haven\u0026rsquo;t attend any big conferences as presenter for some time, but this year will change it. I\u0026rsquo;m starting big, with a talk: Docker base images - Ideas how to manage them on scale on Devoxx conference in Krak√≥w, that will take place on 22-24th June 2022.\nWant to meet? Meet there üòÑ\nUpdate I uploaded slides from presentation to my Github account.","title":"Back on the big stage!"},{"content":" DriveThe Surprising Truth About What Motivates Us\nAuthor: Daniel H. Pink\namazon.plamazon.com Drive: The Surprising Truth About What Motivates Us is a book that I recently read and found to be highly insightful. The author, Daniel H. Pink, explores the science of motivation and argues that the traditional carrot-and-stick approach is outdated and ineffective. He claims that in today\u0026rsquo;s rapidly changing and complex world, autonomy, mastery, and purpose are the most important drivers of motivation.\nI was impressed by Pink\u0026rsquo;s research and the range of real-world examples he uses to support his arguments. He presents his ideas in a clear and engaging style, making the book accessible to a wide audience.\nOverall, I found Drive to be a thought-provoking and well-written book that challenges conventional wisdom about motivation. It provides practical insights for individuals and organizations looking to create more motivating and productive work environments. I would definitely recommend this book to anyone interested in the science of motivation and how to foster a more engaged and motivated workforce.\n","permalink":"https://timor.site/books/2022/drive/","summary":"DriveThe Surprising Truth About What Motivates Us\nAuthor: Daniel H. Pink\namazon.plamazon.com Drive: The Surprising Truth About What Motivates Us is a book that I recently read and found to be highly insightful. The author, Daniel H. Pink, explores the science of motivation and argues that the traditional carrot-and-stick approach is outdated and ineffective. He claims that in today\u0026rsquo;s rapidly changing and complex world, autonomy, mastery, and purpose are the most important drivers of motivation.","title":"Drive"},{"content":" Wewnƒôtrzna graKszta≈Çtowanie psychiki gracza gie≈Çdowego\nAuthors: Robert Koppel, Howard Abell\nmaklerska.pl ","permalink":"https://timor.site/books/2022/wewnetrzna-gra/","summary":" Wewnƒôtrzna graKszta≈Çtowanie psychiki gracza gie≈Çdowego\nAuthors: Robert Koppel, Howard Abell\nmaklerska.pl ","title":"Wewnƒôtrzna gra"},{"content":" Projekt Jednoro≈ºecPowie≈õƒá o szansie w epoce przewrot√≥w cyfrowych\nAuthor: Gene Kim\nhelion.pl ","permalink":"https://timor.site/books/2021/projekt-jednorozec/","summary":" Projekt Jednoro≈ºecPowie≈õƒá o szansie w epoce przewrot√≥w cyfrowych\nAuthor: Gene Kim\nhelion.pl ","title":"Projekt Jednoro≈ºec"},{"content":"What I want to do? I use my pool to securely store backups, archive my old documents and keep huge family\u0026rsquo;s photo library.\nI have new disks. They were tortured with badblocks, so they\u0026rsquo;re ready to create ZFS pool.\nI\u0026rsquo;ve read few documents about different approaches 1 2 3. I wanted to be sure if anything changed during past years. One of articles recommends mirroring over RAIDZ. Resilvering is faster, at the same time putting IO less stress on whole pool. But pool as small as mine, relies on single drive which might die in between and data won\u0026rsquo;t be recoverable. Eventually, I decided to go for RAIDZ1 for now and in the future I rather move to RAIDZ2. For that, I have to buy one more disk - Black Friday is close, we will see.\nOne thing that is available now, but was not, when I created ZFS pool last time, is encryption. I want it. I won\u0026rsquo;t be running with my PC anywhere, but in case if disk will be damaged and I will have to return it. No worries, data is unavailable. So I want it üòÑ\nLet\u0026rsquo;s do it I use Ubuntu 21.10 but despite installation of ZFS utils, rest of commands is not Ubuntu specific and should work on any distro.\nInstall ZFS sudo apt install -y zfsutils-linux zfs-zed Encryption key First, I have to generate secure key/file, that will be used to encrypt/decrypt file system. I will keep it on my root filesystem, actually in /root directory, as it won\u0026rsquo;t be easily available for other users.\nIt\u0026rsquo;s possible to use password or raw file. On laptop, I will choose password, but on my desktop PC I prefer file. First, I was thinking to place it in /etc/zfs but it\u0026rsquo;s world readable. So I decided I will drop it in /root dir.\nGenerate encryption key sudo dd if=/dev/random of=/root/.zfs-encrypt.key bs=1 count=32 I strongly advice to make backup of this key. Multiple backups actually :)\nCreate a fully encrypted pool Now I can create pool. Most examples refer to disks as sda, sdb, etc. But I prefer to use their labels as drive model plus serial number. Just check /dev/disk/by-id/. Other way is by use of wwn-* ids, they\u0026rsquo;re also stable across restarts, cable changes, etc. I strongly recommend such ids over standard letters.\nCreate encrypted ZFS pool sudo zpool create \\ -o ashift=12 \\ -o feature@encryption=enabled \\ -O encryption=on \\ -O keylocation=file:///root/.zfs-encrypt.key \\ -O keyformat=raw \\ storage raidz1 \\ ata-WDC_WD140EDGZ-11B1PA0_9MGJK4YK \\ ata-WDC_WD140EDGZ-11B1PA0_Y6GVH40C \\ ata-WDC_WD140EDGZ-11B1PA0_Y6GWHD3C What happen here?\n-o ashift=12 - treat disks sector size as 4KB, it was detected automatically but I wanted to be sure :) -o feature@encryption=enabled -O encryption=on -O keylocation=file:///root/.zfs-encrypt.key -O keyformat=raw - enable encryption on whole pool, use raw key and default encryption method: aes-256-gcm Automatically decrypt on boot So my pool is encrypted now, but I don\u0026rsquo;t want to decrypt it manually each time my PC starts. So I added such service:\n/etc/systemd/system/zfs-load-key.service sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/zfs-load-key.service [Unit] Description=Load encryption keys DefaultDependencies=no After=zfs-import.target Before=zfs-mount.service [Service] Type=oneshot RemainAfterExit=yes ExecStart=/sbin/zfs load-key -a StandardInput=tty-force [Install] WantedBy=zfs-mount.service EOF It have to enabled it now:\nEnable key load service sudo systemctl daemon-reload sudo systemctl enable zfs-load-key In ArchLinux wiki 4, you can find example for service, that can load individual key per pool, but I had trouble getting it to work. Service above loads all keys for all pools and it\u0026rsquo;s matching my case of just two pools ;)\nLet\u0026rsquo;s configure pool There are few options worth to setup just after pool creation:\nInitial pool configuration # automatically expand pool when new disk is added sudo zpool set autoexpand=on storage # automatically replace failed disk with hot spare sudo zpool set autoreplace=on storage # enable LZ4 compression sudo zfs set compression=lz4 storage # disable access time - for better performance sudo zfs set atime=off storage For better explanation of options, check best practices on Aaron\u0026rsquo;s Toponce blog 5.\nCreate datasets (actual mount points) Now I\u0026rsquo;m ready to create few datasets, that will be actually used to store files.\nCreate ZFS datasets and configure them # photos are well compressed, so I don\u0026#39;t need to compress them again sudo zfs create storage/photos sudo zfs set compression=off storage/photos # on backups I often just copy files staring, so let use stronger compression ZSTF sudo zfs create storage/backup sudo zfs set compression=zstd storage/backup # I like to keep my downloads on pool too sudo zfs create -o mountpoint=/home/timor/Downloads storage/downloads Setting TLER on boot Having disks in RAID, it\u0026rsquo;s good to have TLER enabled to rely on RAID for error recovery instead of internal hard drive recover 6. My disks support it, but it have to be enabled\nChecking if TLER is supported sudo smartctl -l scterc /dev/sdd sudo smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org SCT Error Recovery Control: Read: Disabled Write: Disabled It\u0026rsquo;s there but disabled.\nEnabling TLER sudo smartctl -l scterc,70,70 /dev/sdd sudo smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org SCT Error Recovery Control set to: Read: 70 (7.0 seconds) Write: 70 (7.0 seconds) Warning\nExample below will overwrite /etc/rc.local. If you already use this file, please edit it on your own!\nMake it persistent between restarts:\n~/2021/11/creating-fully-encrypted-zfs-pool/ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/rc.local #!/bin/bash # I have such disks, let\u0026#39;s iterate over them # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_9MGJK4YK # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_Y6GVH40C # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_Y6GWHD3C for i in 9MGJK4YK Y6GVH40C Y6GWHD3C; do echo smartctl -l scterc,70,70 /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_\\$i \u0026gt; /dev/null; done EOF sudo chmod +x /etc/rc.local sudo systemctl enable rc-local.service That\u0026rsquo;s it. All done.\nhttps://serverfault.com/questions/972496/can-i-encrypt-a-whole-pool-with-zfsol-0-8-1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.delphix.com/blog/delphix-engineering/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://jrs-s.net/2015/02/06/zfs-you-should-use-mirror-vdevs-not-raidz/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/ZFS#Native_encryption\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pthree.org/2012/12/13/zfs-administration-part-viii-zpool-best-practices-and-caveats/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.timlinden.com/checking-tler-setting-for-linux-hard-drives/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2021/11/creating-fully-encrypted-zfs-pool/","summary":"What I want to do? I use my pool to securely store backups, archive my old documents and keep huge family\u0026rsquo;s photo library.\nI have new disks. They were tortured with badblocks, so they\u0026rsquo;re ready to create ZFS pool.\nI\u0026rsquo;ve read few documents about different approaches 1 2 3. I wanted to be sure if anything changed during past years. One of articles recommends mirroring over RAIDZ. Resilvering is faster, at the same time putting IO less stress on whole pool.","title":"Creating fully encrypted ZFS pool"},{"content":"I used to have RAID (or at least some variation of it) for my main storage. For redundancy, in case of disk failure. I started with some crazy LVM mirrors done on two disks of different size. Sync job was starting on every boot üòÑ\nThen came time for RAID5 on mdadm + LVM for volume management. It was working nice until the moment when disks became bigger. Long array rebuilds or checks, required my PC to stay turned on overnight just to validate if stuff works still.\nThen in work, I had my first contact with ZFS and it was love at first sight ‚ù§Ô∏è Biggest improvement was, that after creation of pool there\u0026rsquo;s no need to re-sync whole disk. Both resilvering or scrub were much faster than standard mdadm check.\nMy old disks are 2TB big and almost each of them is different now. I started with 3 x ST2000DL003 2TB, but after failure of one I was unable to buy exact replacement. Closest possible was: ST2000DM001. Since that moment, I had disks running at variable speed around 5400RPM and one on 7200RPM. I decided to add another drive to the pool as spare, it was Samsung HD204UI. After it failed, I added WDC Black WD2003FYYS 2TB.\nToday, oldest disk is around 10 years old, youngest are around 8. Just this is pretty awful but what pushed me to replace them, was actual lack of space üòÑ\nI was hoping to find 4 x 8TB drives which will quadruple my capacity and should provide enough space and safety for years to come, but I couldn\u0026rsquo;t find any reasonably priced offers. I even bought 4 x Toshiba X300 8TB. I started copying data to them\u0026hellip; And they started dying one by one üòû Actually, they didn\u0026rsquo;t die, but SMART was reporting \u0026ldquo;pre-fail\u0026rdquo; condition, when they were working heavily for too long. What if I have to re-sync/scrub whole pool? Will they die in the middle. I returned all of them. It was first time I gave a chance to Toshiba. It was last time üòâ\nThen I saw a movie on Youtube or article - someone was buying at bulk cheap external HDDs and \u0026ldquo;shucking\u0026rdquo; them 1. It was pretty close to how I understand building RAID as \u0026ldquo;Redundant Array of Inexpensive Disks\u0026rdquo; :simple_smile: Those disk are usually pretty good disks - either WD RED which are actually designed for NAS/RAID workload, or so called \u0026ldquo;white labels\u0026rdquo; which are just variations of HGST drives (sometimes slower, with less cache, etc).\nI noticed a promo on Amazon on WD Elements 14TB and I bought 3 of them. Wanted 4, but they were limiting orders. They were 30% cheaper than similar internal HDDs.\nWhat\u0026rsquo;s inside? I connected them as they\u0026rsquo;re provided, in those plastic coffins and started playing. First I stress tested them with badblocks 2 3.\nRun badblocks badblocks -b 4096 -wsv /dev/sde After around 1 hour, all of them were quite warm. Having between 53~55¬∞C. Theoretically those drives can operate in up to 60¬∞C, but at the end of 2nd day, two disk turned off and I have to turn them off and on again to make them discoverable, which just reminds me:\nAs SMART didn\u0026rsquo;t notice anything worrying, I was quite sure it\u0026rsquo;s just because of temperature (coldest one stayed alive). I finished my torture testing here and decided to \u0026ldquo;shuck\u0026rdquo; them.\nThere\u0026rsquo;s really good instruction on how to do that on iFixit 4, so I won\u0026rsquo;t be explaining it here.\nI ended up having 3 new WDC WD140EDGZ-11B1PA0 14TB 5400RPM disks, with average transfers around 170~200MB/s. What\u0026rsquo;s interesting SMART recognise them as 7200RPM, but I heard it\u0026rsquo;s not worth to trust it in case of WDC/HGST drives.\nsmartctl output smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Device Model: WDC WD140EDGZ-11B1PA0 LU WWN Device Id: 5 000cca 2adcc7fcb Firmware Version: 85.00A85 User Capacity: 14‚ÄØ000‚ÄØ519‚ÄØ643‚ÄØ136 bytes [14,0 TB] Sector Sizes: 512 bytes logical, 4096 bytes physical Rotation Rate: 7200 rpm Form Factor: 3.5 inches Device is: Not in smartctl database [for details use: -P showall] ATA Version is: ACS-2, ATA8-ACS T13/1699-D revision 4 SATA Version is: SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s) Local Time is: Fri Nov 12 15:59:23 2021 CET SMART support is: Available - device has SMART capability. SMART support is: Enabled === START OF READ SMART DATA SECTION === SMART overall-health self-assessment test result: PASSED General SMART Values: Offline data collection status: (0x80)\tOffline data collection activity was never started. Auto Offline Data Collection: Enabled. Self-test execution status: ( 0)\tThe previous self-test routine completed without error or no self-test has ever been run. Total time to complete Offline data collection: ( 101) seconds. Offline data collection capabilities: (0x5b) SMART execute Offline immediate. Auto Offline data collection on/off support. Suspend Offline collection upon new command. Offline surface scan supported. Self-test supported. No Conveyance Self-test supported. Selective Self-test supported. SMART capabilities: (0x0003)\tSaves SMART data before entering power-saving mode. Supports SMART auto save timer. Error logging capability: (0x01)\tError logging supported. General Purpose Logging supported. Short self-test routine recommended polling time: ( 2) minutes. Extended self-test routine recommended polling time: (1383) minutes. SCT capabilities: (0x003d)\tSCT Status supported. SCT Error Recovery Control supported. SCT Feature Control supported. SCT Data Table supported. SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 1 Raw_Read_Error_Rate 0x000b 100 100 001 Pre-fail Always - 0 2 Throughput_Performance 0x0004 100 100 054 Old_age Offline - 0 3 Spin_Up_Time 0x0007 092 092 001 Pre-fail Always - 0 (Average 328) 4 Start_Stop_Count 0x0012 100 100 000 Old_age Always - 5 5 Reallocated_Sector_Ct 0x0033 100 100 001 Pre-fail Always - 0 7 Seek_Error_Rate 0x000a 100 100 001 Old_age Always - 0 8 Seek_Time_Performance 0x0004 100 100 020 Old_age Offline - 0 9 Power_On_Hours 0x0012 100 100 000 Old_age Always - 0 10 Spin_Retry_Count 0x0012 100 100 001 Old_age Always - 0 12 Power_Cycle_Count 0x0032 100 100 000 Old_age Always - 5 22 Unknown_Attribute 0x0023 100 100 025 Pre-fail Always - 100 192 Power-Off_Retract_Count 0x0032 100 100 000 Old_age Always - 5 193 Load_Cycle_Count 0x0012 100 100 000 Old_age Always - 5 194 Temperature_Celsius 0x0002 062 062 000 Old_age Always - 25 (Min/Max 20/27) 196 Reallocated_Event_Count 0x0032 100 100 000 Old_age Always - 0 197 Current_Pending_Sector 0x0022 100 100 000 Old_age Always - 0 198 Offline_Uncorrectable 0x0008 100 100 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x000a 100 100 000 Old_age Always - 0 SMART Error Log Version: 1 No Errors Logged SMART Self-test log structure revision number 1 No self-tests have been logged. [To run self-tests, use: smartctl -t] SMART Selective self-test log data structure revision number 1 SPAN MIN_LBA MAX_LBA CURRENT_TEST_STATUS 1 0 0 Not_testing 2 0 0 Not_testing 3 0 0 Not_testing 4 0 0 Not_testing 5 0 0 Not_testing Selective self-test flags (0x0): After scanning selected spans, do NOT read-scan remainder of disk. If Selective self-test is pending on power-up, resume after 0 minute delay. That\u0026rsquo;s enough for today. Next time, I will build ZFS array with them.\nhttps://www.reddit.com/r/DataHoarder/comments/elels8/wd_my_book_14_tb_shucked_wd140edfz_us7sap140/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/badblocks\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zackreed.me/new-disk-stress-test/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.ifixit.com/Guide/How+to+Shuck+a+WD+Elements+External+Hard+Drive/137646\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2021/11/shucking-wd-elements-14tb/","summary":"I used to have RAID (or at least some variation of it) for my main storage. For redundancy, in case of disk failure. I started with some crazy LVM mirrors done on two disks of different size. Sync job was starting on every boot üòÑ\nThen came time for RAID5 on mdadm + LVM for volume management. It was working nice until the moment when disks became bigger. Long array rebuilds or checks, required my PC to stay turned on overnight just to validate if stuff works still.","title":"Shucking WD Elements 14TB"},{"content":"I don\u0026rsquo;t know how it is in your company, but in mine it\u0026rsquo;s considered a good practice to add ticket numbers to commit messages. It allows to easily determine why something was changed, etc. Makes sense, but this also means, that I should be adding this ticket to every message\u0026hellip; And this doesn\u0026rsquo;t make sense for me. I will accidentally avoid it from time to time or make a lot of typos.\nI prepared little automation to handle that. I use two git aliases. One reads ticket Id from git branch, the other automatically pre-fixes all my commit messages with it. If there\u0026rsquo;s no ticket, commit will fail.\nBranch name might start with feature/, bugfix/ or hotfix/. Then comes ticket number, I guess your tickets also have format that can be easily matched with a regular expressions. Then, I like to describe branch purpose.\nI have such aliases in my ~/.gitconfig:\n~/.gitconfig # my custom aliases [alias] jira = !\u0026#34;f() { git rev-parse --abbrev-ref HEAD | sed -n -E \u0026#39;s#^(feature|(bug|hot)-?(fix)?)/([A-Z]+-[0-9]+)[^a-zA-Z0-9].*#\\\\4#p\u0026#39; ; }; f\u0026#34; cm = !\u0026#34;f() { if echo \\\u0026#34;$1\\\u0026#34; | egrep -q \u0026#39;^[A-Z]+-[0-9]+ \u0026#39;; then git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$1\\\u0026#34;; else JIRA=$(git jira); if [ -z \u0026#34;$JIRA\u0026#34; ]; then echo \u0026gt;\u0026amp;2 \u0026#39;#### Start message with Jira ticket number! ####\u0026#39;; exit 1; else git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$JIRA $1\\\u0026#34;; fi; fi; }; f\u0026#34; How to use it? Whether I use git-flow or just simple feature branching, before any commit, I always start with fresh branch: Create new branch git checkout -b feature/ABC-123-descriptive-branch-purpose My aliases read ticket from branch, so I have to provide ticket number only once - during branch creation. Then, I commit my changes like this: Commit changes git cm \u0026#34;Descriptive message\u0026#34; Result of it is: What it does git add -A git commit -m \u0026#34;ABC-123 Descriptive message\u0026#34; With this simple trick, I have to pay attention to ticket number once in a while. Rest is magic üòé\n","permalink":"https://timor.site/2021/11/automatically-add-ticket-id-to-every-commit-message-in-git/","summary":"I don\u0026rsquo;t know how it is in your company, but in mine it\u0026rsquo;s considered a good practice to add ticket numbers to commit messages. It allows to easily determine why something was changed, etc. Makes sense, but this also means, that I should be adding this ticket to every message\u0026hellip; And this doesn\u0026rsquo;t make sense for me. I will accidentally avoid it from time to time or make a lot of typos.","title":"Automatically add ticket ID to every commit message in Git"},{"content":"I was updating my blog and needed to generate few variants of images, in different resolution.\nOption 1 - sips There\u0026rsquo;s simple, builtin tool sips, that can be used for simple resizing 1:\nResize single image sips -Z 36 orig.png --out static/favicon36x36.png -Z - maintain image aspect ratio 36 - maximum height and width It can be also used for batch image processing:\nWarning\nBeware, without \u0026ndash;out param, it will overwrite images in place!\nBatch image resizing sips -Z 1024 *.jpg Option 2 - imagemagick For more complicated use cases, imagemagick have no competition. It\u0026rsquo;s not available out of the box, you have to install it:\nInstall imagemagick brew install imagemagick Imagemagick provides additional abilities to resize and then crop images. Let use cat image below as a demo.\nSource: www.pexels.com\nJust scale keeping whole image Scale down convert demo.webp \\ -resize 300x100 \\ demo-just-resize.webp The resulting images is: Check the result identify demo-just-resize.webp demo-just-resize.webp WEBP 67x100 67x100+0+0 8-bit sRGB 4076B 0.000u 0:00.000 Scale down but keeping width Scale down keeping width convert demo.webp \\ -resize 300x100^ \\ demo-just-resize2.webp The resulting image is: Check the result identify demo-just-resize2.webp demo-just-resize2.webp WEBP 300x450 300x450+0+0 8-bit sRGB 8078B 0.000u 0:00.002 Resize and crop from top Resize and crop from top convert demo.webp \\ -resize 300x100^ \\ -extent 300x100 \\ demo-resize-crop.webp The resulting image is: Check the result identify demo-resize-crop.webp demo-resize-crop.webp WEBP 300x100 300x100+0+0 8-bit sRGB 3400B 0.000u 0:00.001 Resize and crop center Resize, then crop convert demo.webp \\ -resize 300x100^ \\ -gravity Center \\ -crop 300x100+0+0 +repage \\ demo-resize-crop2.webp The resulting image is: Check the result identify demo-resize-crop2.webp demo-resize-crop2.webp WEBP 300x100 300x100+0+0 8-bit sRGB 5202B 0.000u 0:00.001 As you might guess, there are many cases and far more options, than described in this post. Personally, I was pretty satisfied with the last one. I was able to cut most of images properly, being able to generate nice cover images with just one command.\nhttps://lifehacker.com/batch-resize-images-quickly-in-the-os-x-terminal-5962420\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2021/11/resize-images-from-command-line-on-macos/","summary":"I was updating my blog and needed to generate few variants of images, in different resolution.\nOption 1 - sips There\u0026rsquo;s simple, builtin tool sips, that can be used for simple resizing 1:\nResize single image sips -Z 36 orig.png --out static/favicon36x36.png -Z - maintain image aspect ratio 36 - maximum height and width It can be also used for batch image processing:\nWarning\nBeware, without \u0026ndash;out param, it will overwrite images in place!","title":"Resize images from command line on MacOS"},{"content":"I use brew extensively on MacOS. It\u0026rsquo;s just as convenient as many Linux package managers. What I don\u0026rsquo;t like, it leaves dependencies after removal of formula. There\u0026rsquo;s simple way to clean it up by running one command 1.\nUninstall with dependencies brew uninstall FORMULA brew autoremove Info\nIn my case running brew autoremove actually removed few packages I really wanted to have. Check the output carefully!\nhttps://stackoverflow.com/questions/7323261/uninstall-remove-a-homebrew-package-including-all-its-dependencies\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2021/11/homebrew-uninstall-formula-with-dependencies/","summary":"I use brew extensively on MacOS. It\u0026rsquo;s just as convenient as many Linux package managers. What I don\u0026rsquo;t like, it leaves dependencies after removal of formula. There\u0026rsquo;s simple way to clean it up by running one command 1.\nUninstall with dependencies brew uninstall FORMULA brew autoremove Info\nIn my case running brew autoremove actually removed few packages I really wanted to have. Check the output carefully!\nhttps://stackoverflow.com/questions/7323261/uninstall-remove-a-homebrew-package-including-all-its-dependencies\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Homebrew - uninstall formula with dependencies"},{"content":"I\u0026rsquo;ve build new PC - it\u0026rsquo;s based on Asus ROG STRIX Z590-E GAMING WIFI motherboard. Generally, I\u0026rsquo;m quite satisfied, but it have one irritating downside - after each UEFI BIOS upgrade, it\u0026rsquo;s silently resetting some of settings.\nLet me note, what I want to have there:\nAi Tweaker (use my RAM capabilities) AI Overcloack Tuner -\u0026gt; [XMP I] DRAM Frequency -\u0026gt; [DDR4-3600MHz] DRAM CAS# Latency -\u0026gt; [16] DRAM RAS# to CAS# Delay -\u0026gt; [19] DRAM RAS# ACT Time -\u0026gt; [39] DRAM Voltage -\u0026gt; [1.35000] Advanced Platform Misc Configuration (enable some power management) PCI Express Native Power Management -\u0026gt; [Enabled] Native ASPM -\u0026gt; [Auto] APM Configuration ErP Ready -\u0026gt; [Enable(S4+S5)] CPU Configuration (virtualization optimisations) Intel (VMX) Virtualization Technology -\u0026gt; [Enabled] System Agent (SA) Configuration VT-d -\u0026gt; [Enabled] Trusted Computing (TPM v2 for Windows 11) Security Device Support -\u0026gt; [Enabled] Onboard Devices Configuration USB Audio -\u0026gt; [Disabled] (don\u0026rsquo;t work well on Linux) INTEL 2.5G LAN1 -\u0026gt; [Disabled] (I don\u0026rsquo;t use them) INTEL 2.5G LAN2 -\u0026gt; [Disabled] USB power delivery in Soft Off state (S5) -\u0026gt; [Disabled] (disable mouse lightning when shut down) LED lightning (disable MB lightning when shut down) When system is in sleep, hibernate or soft off states -\u0026gt; [Stealth Mode] M.2_4 Configuration -\u0026gt; [PCIE] PRIEX16_3 Bandwidth -\u0026gt; [X2 Mode] (Xonar is only x2) CPU PCIE Configuration Mode -\u0026gt; [PCIEX16_1 + PCIEX16_2 + M.2_2] (I need that for 1st NVMe to work) Boot Boot Configuration POST Delay Time -\u0026gt; [1 sec] ","permalink":"https://timor.site/2021/10/asus-rog-strix-z590-e-gaming-wifi-my-uefi-bios-settings/","summary":"I\u0026rsquo;ve build new PC - it\u0026rsquo;s based on Asus ROG STRIX Z590-E GAMING WIFI motherboard. Generally, I\u0026rsquo;m quite satisfied, but it have one irritating downside - after each UEFI BIOS upgrade, it\u0026rsquo;s silently resetting some of settings.\nLet me note, what I want to have there:\nAi Tweaker (use my RAM capabilities) AI Overcloack Tuner -\u0026gt; [XMP I] DRAM Frequency -\u0026gt; [DDR4-3600MHz] DRAM CAS# Latency -\u0026gt; [16] DRAM RAS# to CAS# Delay -\u0026gt; [19] DRAM RAS# ACT Time -\u0026gt; [39] DRAM Voltage -\u0026gt; [1.","title":"Asus ROG STRIX Z590-E GAMING WIFI - my UEFI BIOS settings"},{"content":" Od dobrego do wielkiegoCzynniki trwa≈Çego rozwoju i zwyciƒôstwa Ô¨Årm\nAuthor: Jim Collins\namazon.plamazon.com The book is a comprehensive analysis of what makes a good company great, and how organizations can achieve long-term success. Collins and his team conducted extensive research to identify common characteristics and practices among companies that have made the leap from good to great, and the book presents the results of their findings.\nThe book is well-written and provides a wealth of practical insights and advice for organizations looking to improve their performance and achieve lasting success. I was impressed by Collins\u0026rsquo;s ability to distill complex ideas into simple and actionable steps.\nOverall, I would recommend \u0026ldquo;Good to Great\u0026rdquo; to anyone looking to improve their organizational performance, but I feel it would provide most value to the people on higher management positions. Persoanlly, I feel I\u0026rsquo;m not yet ready too take from this book fully, but is definitely worth checking out.\n","permalink":"https://timor.site/books/2021/od-dobrego-do-wielkiego/","summary":"Od dobrego do wielkiegoCzynniki trwa≈Çego rozwoju i zwyciƒôstwa Ô¨Årm\nAuthor: Jim Collins\namazon.plamazon.com The book is a comprehensive analysis of what makes a good company great, and how organizations can achieve long-term success. Collins and his team conducted extensive research to identify common characteristics and practices among companies that have made the leap from good to great, and the book presents the results of their findings.\nThe book is well-written and provides a wealth of practical insights and advice for organizations looking to improve their performance and achieve lasting success.","title":"Od dobrego do wielkiego"},{"content":" Mi≈Ço≈õƒá i matematykaIstota ukrytej rzeczywisto≈õci\nAuthor: Edward Frenkel\nempik.com ","permalink":"https://timor.site/books/2021/milosc-i-matematyka/","summary":" Mi≈Ço≈õƒá i matematykaIstota ukrytej rzeczywisto≈õci\nAuthor: Edward Frenkel\nempik.com ","title":"Mi≈Ço≈õƒá i matematyka"},{"content":"Finally, they\u0026rsquo;re available! Wait a moment.. Actually they\u0026rsquo;re available for few months, just nobody published information about moving them to quay.io and dropped poor guys using hub.docker.com without any updates! Yes, that how they did!\nI found new place accidentally, reading some news about CentOS Stream 9 on their blog. There was reference to CentOS 9 Stream dev builds of Docker images and I found \u0026ldquo;missing\u0026rdquo; stream and stream8 tags too. It\u0026rsquo;s not adding more confidence on my side to the CentOS project, when they\u0026rsquo;re not even communicating such changes publicly, sick!\nSome might remember, I was so desperate because of no official Docker images of CentOS 8 Stream available in Docker Hub, I build them on my own.\nWhat I don\u0026rsquo;t like, is that they completely stopped upgrading base images for centos8 and centos7. There\u0026rsquo;s still a lot of stuff using them, but they\u0026rsquo;re not updated for months:\nSource: https://quay.io/repository/centos/centos\nWhat next? I switched my image to use as base new location quay.io/centos/centos:*. I\u0026rsquo;ll probably stay with my builds as I upgrade them on weekly basis. And because I squash them to single layer, they\u0026rsquo;re close to size of original images. Official images with all updates can easily get above 300MB.\nREPOSITORY TAG CREATED SIZE centos 7 8 months ago 204MB quay.io/centos/centos 7 8 months ago 204MB tgagor/centos 7 2 hours ago 214MB quay.io/centos/centos 8 7 months ago 209MB centos 8 7 months ago 209MB tgagor/centos 8 2 hours ago 262MB quay.io/centos/centos stream8 2 days ago 404MB tgagor/centos stream8 2 hours ago 230MB tgagor/centos stream9 2 hours ago 164MB quay.io/centos/centos stream9-development 4 days ago 166MB Where can you get it? You can fetch docker image here:\ntgagor/centos ","permalink":"https://timor.site/2021/07/official-centos-8-stream-docker-image-finally-available/","summary":"Finally, they\u0026rsquo;re available! Wait a moment.. Actually they\u0026rsquo;re available for few months, just nobody published information about moving them to quay.io and dropped poor guys using hub.docker.com without any updates! Yes, that how they did!\nI found new place accidentally, reading some news about CentOS Stream 9 on their blog. There was reference to CentOS 9 Stream dev builds of Docker images and I found \u0026ldquo;missing\u0026rdquo; stream and stream8 tags too.","title":"Official CentOS 8 Stream Docker image finally available!"},{"content":" Inteligentny inwestorNajlepsza ksiƒÖ≈ºka o inwestowaniu warto≈õciowym\nAuthor: Benjamin Graham\nempik.com ","permalink":"https://timor.site/books/2021/inteligentny-inwestor/","summary":" Inteligentny inwestorNajlepsza ksiƒÖ≈ºka o inwestowaniu warto≈õciowym\nAuthor: Benjamin Graham\nempik.com ","title":"Inteligentny inwestor"},{"content":" Github repositories tgagor/ansible-role-docker - Installs Docker service on Ubuntu/Debian/RHEL tgagor/ansible-role-docker-compose - Simple Ansible role that will install Docker Compose tgagor/ansible-role-rpi-unifi - Installs Ubiquiti\u0026#39;s Unifi software on Raspberry Pi tgagor/ansible-role-spotify tgagor/ansible-role-template-with-molecule-tests - Template for Ansible role with Molecule and Testinfra for testing tgagor/conferences - Presentations and materials from conferences where I attended as a speaker tgagor/docker-centos - CentOS docker images, build weekly with latest security updates tgagor/docker-clamav - Simple Docker image that might be used in CI/CD tgagor/docker-grype - Docker image with anchore/grype tgagor/docker-jpegtran - Simple Docker image with jpegtran executable. Useful for CI/CD image optimization. tgagor/docker-optipng - Simple Docker image with optipng executable. Useful for CI/CD image optimization. tgagor/docker-owasp-dependency-check - Provides OWASP Dependency-Check app with pre-downloaded NVD/CVE updates tgagor/docker-packtpublishingfreelearning - Docker container to make downloads from PacktPublishing simpler tgagor/docker-unifi-controller - UniFi Controllver v7 tgagor/docker-wp-cli - Docker image with WP-CLI command line interface for Wordpress tgagor/fslint-snap - Snapcraft package of FSlint Janitor tgagor/marsennea-primes - Playing with Marsennea primes Ghists How old official Docker images are? Script that can be used to purge nexus v3 releases Docker images tgagor/clamav - ClamAV tgagor/optipng - This is simple image with optipng executable. tgagor/jpegtran - This is simple image with optipng executable. tgagor/wp-cli - Image providing wp-cli tool. tgagor/packtpublishingfreelearning - Ready to run container with code from: https://github.com/igbt6/Packt-Publishing-Free-Learning tgagor/tor - Minimal tor image tgagor/privoxy - Minimal privoxy image tgagor/owasp-dependency-check - Provides OWASP Dependency-Check app with pre-downloaded NVD/CVE updates tgagor/grype - Image with Anchore\u0026#39;s Grype binary, allows security scans of Docker images tgagor/centos-stream - There\u0026#39;s no official image for CentOS 8 Stream - so I prepared it, something for early adopters tgagor/centos - Pure, based on official CentOS images, upgraded every Monday tgagor/unifi-controller ","permalink":"https://timor.site/projects/","summary":"Github repositories tgagor/ansible-role-docker - Installs Docker service on Ubuntu/Debian/RHEL tgagor/ansible-role-docker-compose - Simple Ansible role that will install Docker Compose tgagor/ansible-role-rpi-unifi - Installs Ubiquiti\u0026#39;s Unifi software on Raspberry Pi tgagor/ansible-role-spotify tgagor/ansible-role-template-with-molecule-tests - Template for Ansible role with Molecule and Testinfra for testing tgagor/conferences - Presentations and materials from conferences where I attended as a speaker tgagor/docker-centos - CentOS docker images, build weekly with latest security updates tgagor/docker-clamav - Simple Docker image that might be used in CI/CD tgagor/docker-grype - Docker image with anchore/grype tgagor/docker-jpegtran - Simple Docker image with jpegtran executable.","title":"My projects"},{"content":"I wanted to share publicly some photos, but I performed them with navigation enabled so they contained accurate localisation of my house. I wanted to remove EXIF data GPS tags, my phone type and other irrelevant stuff.\nTL;DR You will need imagemagick installed (use apt/yum/dnf of whatever you have there):\nInstall imagemagick sudo apt install -y imagemagick To remove them just use: Strip EXIF data mogrify -strip image.jpg How to check if it\u0026rsquo;s working? First, let\u0026rsquo;s get some example images 1.\nSource: github.com/ianare/exif-samples\nTo check what tags image provides, you can use identify tool (I limited output to only GPS data because it\u0026rsquo;s just too much stuff there): Review EXIF data identify -verbose image.jpg | wc -l 156 identify -verbose image.jpg | grep GPS exif:GPSAltitudeRef: 0 exif:GPSDateStamp: 2008:10:23 exif:GPSImgDirectionRef: exif:GPSInfo: 926 exif:GPSLatitude: 43/1, 28/1, 281400000/100000000 exif:GPSLatitudeRef: N exif:GPSLongitude: 11/1, 53/1, 645599999/100000000 exif:GPSLongitudeRef: E exif:GPSMapDatum: WGS-84 exif:GPSSatellites: 06 exif:GPSTimeStamp: 14/1, 27/1, 724/100 There was 156 lines of different tags!\nAfter cleanup: Check EXIF data again identify -verbose image.jpg | wc -l 88 identify -verbose image.jpg | grep GPS you will get only generic information data, without any GPS tags.\nhttps://github.com/ianare/exif-samples/tree/master/jpg/gps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2021/03/how-to-remove-geo-localization/exif-data-from-photos/","summary":"I wanted to share publicly some photos, but I performed them with navigation enabled so they contained accurate localisation of my house. I wanted to remove EXIF data GPS tags, my phone type and other irrelevant stuff.\nTL;DR You will need imagemagick installed (use apt/yum/dnf of whatever you have there):\nInstall imagemagick sudo apt install -y imagemagick To remove them just use: Strip EXIF data mogrify -strip image.jpg How to check if it\u0026rsquo;s working?","title":"How to remove geo-localization/EXIF data from photos"},{"content":" KsiƒÖ≈ºƒôAuthor: Nicolo Machiavelli\nempik.com ","permalink":"https://timor.site/books/2021/ksiaze/","summary":" KsiƒÖ≈ºƒôAuthor: Nicolo Machiavelli\nempik.com ","title":"KsiƒÖ≈ºƒô"},{"content":"It\u0026rsquo;s sometimes useful to quickly connect to JMX console, to checkout what\u0026rsquo;s going on in your application, but the whole thing get\u0026rsquo;s tricky if you\u0026rsquo;re running your app in a container. I need it from time to time and I keep myself few times searching for set of params below:\n~/2021/02/how-to-run-jmx-monitoring-in-docker-image/ java \\ ... -Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.rmi.port=${PORT1} \\ -Dcom.sun.management.jmxremote.port=${PORT1} \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=${HOST} The whole magic here is that PORT1 in container is app\u0026rsquo;s second port. So if you expose both HTTP and HTTPS on two separate ports, you will need here PORT2. I setup both RMI and JMX ports to same value. Second thing here is a hostname, it have to be hostname which you will use to connect to the container. If you use domain name, it have to be this domain name. If you want to use IP, set it to IP. It\u0026rsquo;s because it\u0026rsquo;s actually a virtual hostname for JMX HTTP server - if you mix them, you won\u0026rsquo;t get results. Container orchestrators often automatically export variable like HOST or HOSTNAME which you can use too hook.\nAfter such configuration and mapping ports to container, you should be able to connect to container with VisualVM or Jconsole.\n","permalink":"https://timor.site/2021/02/how-to-run-jmx-monitoring-in-docker-image/","summary":"It\u0026rsquo;s sometimes useful to quickly connect to JMX console, to checkout what\u0026rsquo;s going on in your application, but the whole thing get\u0026rsquo;s tricky if you\u0026rsquo;re running your app in a container. I need it from time to time and I keep myself few times searching for set of params below:\n~/2021/02/how-to-run-jmx-monitoring-in-docker-image/ java \\ ... -Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.rmi.port=${PORT1} \\ -Dcom.sun.management.jmxremote.port=${PORT1} \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=${HOST} The whole magic here is that PORT1 in container is app\u0026rsquo;s second port.","title":"How to run JMX monitoring in Docker image?"},{"content":"We\u0026rsquo;re all divided with recent decision to focus on CentOS Stream, which essentially means that stable, professional distro will turn into rolling release now. Also CentOS board members don\u0026rsquo;t gave us more confidence for the future.\nI don\u0026rsquo;t want to be totally sceptic, I would like to test it on my own and only then, decide if it\u0026rsquo;s stable enough. But I work mostly with Docker containers and there are no official Docker images with Stream variant. I decided to create it on my own, based on official instruction.\nImages after a switch were twice times bigger than basic centos:8, so I used old school way to squash them:\nSquash image by export/import docker run --name tgagor-centos-stream tgagor/centos-stream true docker export tgagor-centos-stream | docker import - tgagor/centos-stream:squashed docker rm tgagor-centos-stream This way I\u0026rsquo;m receiving just single layer with final packages and configuration. Result was pretty impressive:\nREPOSITORY VARIANT SIZE centos 8 209MB tgagor/centos-stream latest 455MB tgagor/centos-stream squashed 297MB New image is still bigger (because upgrade install additional dependencies), but it\u0026rsquo;s acceptable. I also used Docker Hub\u0026rsquo;s hooks to customize build. Thanks to that, when you fetch image from registry, it will be already squashed ;)\nNow at least I can try it and decide on my own, if it\u0026rsquo;s stable enough for production workloads.\nWhere can you get it? You can fetch docker image here:\ntgagor/centos Update Because other CentOS images do not receive updates anymore, I renamed repo from centos-stream -\u0026gt; centos and now I\u0026rsquo;m building all the variants installing all updates on them every week (automatically).\nIf you still use tgagor/centos-stream images, please switch to tgagor/centos.\n","permalink":"https://timor.site/2021/02/centos-8-stream-docker-image/","summary":"We\u0026rsquo;re all divided with recent decision to focus on CentOS Stream, which essentially means that stable, professional distro will turn into rolling release now. Also CentOS board members don\u0026rsquo;t gave us more confidence for the future.\nI don\u0026rsquo;t want to be totally sceptic, I would like to test it on my own and only then, decide if it\u0026rsquo;s stable enough. But I work mostly with Docker containers and there are no official Docker images with Stream variant.","title":"CentOS 8 Stream Docker image"},{"content":" TL;DR\nCentOS base images sucks! They\u0026rsquo;re old, not updated for months!\nAs a professional DevOps I concern about a lot of things\u0026hellip; but security is always close to the top of the list. With Docker build environments and deployments became much more stable, which often is a result of just being stale ;/\nI\u0026rsquo;ve been talking about this for long time but it\u0026rsquo;s still hard for people to believe it. Let\u0026rsquo;s check then few, most popular images and when were they last time updated. I wrote a script for that, so feel free to check your list:\nResults show that not all images are frequently updated and if we get a little bit deeper and check how many packages require upgrade:\nImage Creation date Age (in days) Packages to upgrade centos:7 2020-11-14T00:20:04.644613188Z 75 16 centos:8 2020-12-08T00:22:53.076477777Z 51 12 debian:9 2021-01-12T00:35:06.08981705Z 16 0 debian:10 2021-01-12T00:32:37.071722022Z 16 0 ubuntu:18.04 2021-01-21T03:38:05.801776526Z 7 2 ubuntu:20.04 2021-01-21T03:38:23.37559427Z 7 4 alpine:3.11 2020-12-17T00:19:49.284211148Z 42 0 alpine:3.12 2020-12-17T00:19:42.11518025Z 42 0 alpine:3.13 2021-01-15T02:23:51.238454884Z 13 5 node:10 2021-01-27T20:32:54.257201224Z 0 13 node:12 2021-01-12T10:36:27.349274428Z 15 13 node:14 2021-01-12T10:33:48.195283512Z 15 13 node:15 2021-01-27T20:29:39.779176105Z 0 13 openjdk:8 2021-01-21T02:40:05.312239007Z 7 0 openjdk:11 2021-01-21T02:38:20.819671373Z 7 0 openjdk:15 2021-01-20T00:45:36.664060993Z 8 ? Personally, I consider running yum upgrade or apt upgrade/apt dist-upgrade in Dockerfile as anti-pattern - instead builds should be running so frequently to automatically pull all new upgrades from base images. That\u0026rsquo;s theory, but with images like CentOS, you have to do it or risk running your software on unpatched and potentially unsecure system. That\u0026rsquo;s why I don\u0026rsquo;t like CentOS images as a base in general, they just suck from this perspective.\nThere\u0026rsquo;s also another issue here - running those upgrades makes your image just bigger. Sometimes significantly bigger. That\u0026rsquo;s not what I expect from base images\n","permalink":"https://timor.site/2021/01/how-old-are-official-docker-images/","summary":"TL;DR\nCentOS base images sucks! They\u0026rsquo;re old, not updated for months!\nAs a professional DevOps I concern about a lot of things\u0026hellip; but security is always close to the top of the list. With Docker build environments and deployments became much more stable, which often is a result of just being stale ;/\nI\u0026rsquo;ve been talking about this for long time but it\u0026rsquo;s still hard for people to believe it.","title":"How old are Official Docker images?"},{"content":"I started my blog on custom (written by my) engine, but as I didn\u0026rsquo;t had enough time to enhance it I switched to Wordpress. I\u0026rsquo;ve been using Wordpress as an engine of my blog for past 8~9 years. I have small VPS with PHP + Nginx and you can find a lot of configuration examples from my config on this site üòÑ\nThere was a time, when I was really satisfied by what it provides. Not only because of features, but also beacause I was able to play with insane configuration options (check out my caching reverse proxy config). For me it was opportuninty to excel with my skills.\nBut things change. I don\u0026rsquo;t have that much time to carry this server configuration, keep it properly updated and play with new features. Actually I don\u0026rsquo;t need that anymore as I\u0026rsquo;ve been there, I saw it already\u0026hellip;\nI\u0026rsquo;ve been thinking about switching to static page generator and putting my blog in eg. github pages for really long time. I even almost completely migrated it to pelican (after playing for a while with Octopress too). Eventually I\u0026rsquo;ve found Hugo and I loved it!\nAnd here we are, I switched blog to Hugo. I will try to share why I choose this configuration soon and how I handle it.\nKeep warm, be positvie, stay negative!\n","permalink":"https://timor.site/2020/10/bye-bye-wordpress/","summary":"I started my blog on custom (written by my) engine, but as I didn\u0026rsquo;t had enough time to enhance it I switched to Wordpress. I\u0026rsquo;ve been using Wordpress as an engine of my blog for past 8~9 years. I have small VPS with PHP + Nginx and you can find a lot of configuration examples from my config on this site üòÑ\nThere was a time, when I was really satisfied by what it provides.","title":"Bye Bye Wordpress!"},{"content":" W transie inwestowaniaPodbij rynek pewno≈õciƒÖ siebie, ≈ºelaznƒÖ dyscyplinƒÖ i postawƒÖ zwyciƒôzcy\nAuthor: Mark Douglas\nempik.com ","permalink":"https://timor.site/books/2020/w-transie-investowania/","summary":" W transie inwestowaniaPodbij rynek pewno≈õciƒÖ siebie, ≈ºelaznƒÖ dyscyplinƒÖ i postawƒÖ zwyciƒôzcy\nAuthor: Mark Douglas\nempik.com ","title":"W transie inwestowania"},{"content":" Teoretyczne minimumCo musisz wiedzieƒá, ≈ºeby zaczƒÖƒá zajmowaƒá siƒô fizykƒÖ\nAuthors: Leonard Susskind, George Hrabovsky\nlegimi.pl ","permalink":"https://timor.site/books/2020/teoretyczne-minimum/","summary":" Teoretyczne minimumCo musisz wiedzieƒá, ≈ºeby zaczƒÖƒá zajmowaƒá siƒô fizykƒÖ\nAuthors: Leonard Susskind, George Hrabovsky\nlegimi.pl ","title":"Teoretyczne minimum"},{"content":" Przeciw bogomNiezwyk≈Çe dzieje ryzyka\nAuthor: Peter L. Bernstein\nempik.com ","permalink":"https://timor.site/books/2020/przeciw-bogom/","summary":" Przeciw bogomNiezwyk≈Çe dzieje ryzyka\nAuthor: Peter L. Bernstein\nempik.com ","title":"Przeciw bogom"},{"content":" Podr√≥≈º ludzi KsiƒôgiAuthor: Olga Tokarczuk\n","permalink":"https://timor.site/books/2020/tokarczuk/","summary":"Podr√≥≈º ludzi KsiƒôgiAuthor: Olga Tokarczuk","title":"Podr√≥≈º ludzi Ksiƒôgi"},{"content":" Java Performance CompanionAuthors: Charlie Hunt, Monica Beckwith, Poonam Parhar, Bengt Rutisson\namazon.pl ","permalink":"https://timor.site/books/2020/java-performance-companion/","summary":" Java Performance CompanionAuthors: Charlie Hunt, Monica Beckwith, Poonam Parhar, Bengt Rutisson\namazon.pl ","title":"Java Performance Companion"},{"content":" Troubleshooting Java PerformanceDetecting Anti-Patterns with Open Source Tools 1st ed. Edition\nAuthor: Erik Ostermueller\namazon.com ","permalink":"https://timor.site/books/2020/trubleshooting-java-performance/","summary":" Troubleshooting Java PerformanceDetecting Anti-Patterns with Open Source Tools 1st ed. Edition\nAuthor: Erik Ostermueller\namazon.com ","title":"Troubleshooting Java Performance"},{"content":"Few years ago I moved from Linux desktop to MacOS for my business, day to day work. There were 2 main reasons for that:\nCorporations don\u0026rsquo;t like Linux - they can\u0026rsquo;t manage it, they can\u0026rsquo;t support it, so they blocked it with \u0026ldquo;Security policy\u0026rdquo;, ISO20001, or other nonsense. Actually they\u0026rsquo;re partially right but in different place - many business collaboration applications don\u0026rsquo;t work well on LInux (or they don\u0026rsquo;t work at all) Skype for Business - there\u0026rsquo;s open source alternative but to get full support you have to pay for additional codecs (as far as I remember) - it\u0026rsquo;s not working stable even in paid version Outlook and calendar support - I love Thunderbird and I use it for years, but calendar invitations didn\u0026rsquo;t work nice (honestly, they didn\u0026rsquo;t work nice even between different Outlook versions\u0026hellip;) Corporate VPN apps - Christ, I always was able to get it working eventually, but\u0026hellip; why bother I\u0026rsquo;m older, maybe lazier, maybe smarter - I don\u0026rsquo;t like to spend my time resolving problems that don\u0026rsquo;t give me any value. That\u0026rsquo;s how I switched to MacOS - for business purposes only. Privately I still prefer Linux.\nAfter the switch I\u0026rsquo;ve found some differences. Annoying stuff like different behavior of home/end buttons, etc. So right now, on every Mac that I\u0026rsquo;m working with, I\u0026rsquo;m making it to work more like Linux desktop. I\u0026rsquo;ve found those information useful to few my friends too. I decided to publish this because I received too many questions about what to do, how to start?\nIf you think I\u0026rsquo;m missing something important or I did something really bad way - please comment, I will updated it.\nHow to make screnshots (full screen/partial/desktop recording)? https://support.apple.com/pl-pl/HT201361\nHow to change screenshot save localisation? By default screenshots are saved on desktop which will turn into mess quickly. It\u0026rsquo;s possible to change default save localization for created screenshots: https://discussions.apple.com/docs/DOC-9081\nKeyboard and keyboard shortcuts\u0026hellip; Polish keyboard layout is terrific, location of tilde and backslash buttons cause both Left Shift and Enter to be really far from normal hands position - in my case it\u0026rsquo;s causing pain in hands after few hours of use Another problem is location of Right Alt, it\u0026rsquo;s hidden deeply under hand during writing so it\u0026rsquo;s not convenient to write polish letters like ƒÖ≈õ≈Ç√≥ƒá≈∫, etc. Maybe it will be possible to remap few keys to make this layout more usable but right now experience is terrific.\nThis is really big issue. Because on Mac Win/CMD key is used a lot switch to normal keyboard doesn\u0026rsquo;t help. Use of most common shortcuts really overload my thumbs.\nBest solutions I\u0026rsquo;ve found is Karabiner-Elements. It allow to remap keys (ex. switch right alt/cmd) and you can define different options per device (internal/external keyboard). It\u0026rsquo;s also very useful to make standard PC keyboards to be mapped like Apple keyboard.\nSpecial function keys do not work from external keyboard (it\u0026rsquo;s not Mac compatible ) I don\u0026rsquo;t know if it\u0026rsquo;s possible to configure them. With Karabiner-Elements it\u0026rsquo;s possible to add support for some of them.\nKeyboard shortcuts are totally different than on Windows or Linux Here you could find introduction to most typical shortcuts:¬†https://www.apple.com/support/pages/shortcuts/body.html No other way - you have to learn them.\nFew of my favorites, I use everyday:\nCmd + Space - Spotlight search - think about it like ‚ÄòWin\u0026rsquo; key in Gnome 3, you can start writing app or file name to start/open it Ctrl + Left/Right - switch Desktop on specific screen (full screen apps use \u0026ldquo;whole desktop\u0026rdquo; so it\u0026rsquo;s easy way to see what you have there or start new empty desktop) Ctrl + Up - shows all active windows, desktops, etc. Useful if you\u0026rsquo;re searching specific window Problem with bash completion on linux boxes -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) I solved it by marking option in iTerm2 to always set language system variables.\nBash completion do not work well on Mac, there are no completions for hosts configured in ssh_config or /etc/hosts I initially tried this one:¬†http://davidalger.com/development/bash-completion-on-os-x-with-brew/¬†- it generally works but only for some common tools, ex, svn requires manual download of:\ncurl -L http://svn.apache.org/repos/asf/subversion/trunk/tools/client-side/bash_completion -o /usr/local/etc/bash_completion.d/svn Right now I thing that Brew makes this even easier, because it\u0026rsquo;s installing a lot of bash_completion configs.\nHow to add bash completion for docker? Those two commands will solve problem:\ncurl -L https://raw.githubusercontent.com/docker/compose/master/contrib/completion/bash/docker-compose -o /usr/local/etc/bash_completion.d/docker-compose curl -L https://raw.githubusercontent.com/docker/docker-ce/blob/master/components/cli/contrib/completion/bash/docker -o /usr/local/etc/bash_completion.d/docker SSH agent for key management does not work by default There is a Keychain application installed on MacOS by default, it\u0026rsquo;s responsible for storing keys and managing access to them. To add ssh key to Keychain you have to run:\nssh-add -K and provide password to unlock key.\nSadly this works only one time, I have to manually add key to keychain every time I login by:\nssh-add ~/.ssh/id_rsa How to automatically unlock private SSH keys on login (how to keep SSH private key password in OS X Keychain)? It\u0026rsquo;s all nice described here: https://apple.stackexchange.com/a/250572\nHow to write to multiple terminal panes in iTerm2? use cmd + shift + i to write to all panes on all tabs, or cmd + alt + i to write to all panes on current tab only\nBlurry fonts on external monitors Fonts on external monitors are really blurry - they\u0026rsquo;re badly anti-aliased or hinting is bad. I\u0026rsquo;ve found, that MacOS disable hinting on external monitors. It\u0026rsquo;s possible to enable it back. Check below and play with it to get what would work for you.\nhttps://www.howtogeek.com/358596/how-to-fix-blurry-fonts-on-macos-mojave-with-subpixel-antialiasing/\nJump word left/right (Ctrl+left/right) shortcut don\u0026rsquo;t work on console (iTerm2) You have to configure special escape sequences for Alt+left/right, described here: http://apple.stackexchange.com/a/136931\nPackages on MacOS are outdated and updates arrive later than on Linux Yes, that\u0026rsquo;s sad true. When I have Ansible 2.3 on Jenkins server for MacOS only version 2.2 was available. Version 2.3 will arrive but some time later. This is causing problems in compatibility of code (newer features/options on Jenkins cause problems during deployment and I\u0026rsquo;m not able to test this all on my workstation before real release). Another problem connected to that is that some command line tool on Mac have different switches than on Linux, ex.¬†date -rfc-3339=s is not available, causing scripts to broke on Mac when working on Linux, this also makes testing harder.\nMacOS also use quite old version of bash. As a result .bashrc won\u0026rsquo;t be parsed, you have to put everything to .bash_profile which will slow down starting of each new terminal session (ex. python virtual envs can add significant delay).\nEnd/Home keys behave differently on MacOS Generally you won\u0026rsquo;t find Home/End keys on typical MacBook keyboad - by default on Mac you have Command + Right keyboard shortcut to mimic End, and¬†Command + Left to mimic Home.\nBut\u0026hellip; by default Home/End will move you to the end of page, not line. If you want this behavior back in most of your apps you could try to change keybinding:\nOne option is to create ~/Library/KeyBindings/ and save a property list like this as ~/Library/KeyBindings/DefaultKeyBinding.dict:\n{ \u0026#34;\\UF729\u0026#34; = moveToBeginningOfLine:; \u0026#34;\\UF72B\u0026#34; = moveToEndOfLine:; \u0026#34;$\\UF729\u0026#34; = moveToBeginningOfLineAndModifySelection:; \u0026#34;$\\UF72B\u0026#34; = moveToEndOfLineAndModifySelection:; } Quit and reopen applications to apply the changes. Note that DefaultKeyBinding.dict is not supported by some applications like Xcode or Firefox.\nhttps://apple.stackexchange.com/questions/18016/can-i-change-the-behavior-of-the-home-and-end-keys-on-an-apple-keyboard-with-num\nI can\u0026rsquo;t use X forwarding with MacOS ssh client and Linux on second end There\u0026rsquo;s additional X11 server app that you can install on MacOS (it\u0026rsquo;s called XQuartz. I tried it for short time but I don\u0026rsquo;t need it anymore.\nI have problems working with¬†terminator on Mac For example:\nit\u0026rsquo;s running as python process but it\u0026rsquo;s not available in Lunchpad (not easy to switch with Cmd + Tab keyboard shortcuts are different than on Linux, so this is not making switch easier I\u0026rsquo;ve found iTerm2, which is \u0026ldquo;state of the art\u0026rdquo; terminal for MacOS. It\u0026rsquo;s popular, well supported and feature complete.\nUseful key shortcuts:\nCmd + T - new tab Cmd + D - spit vertically Cmd + Shift + D - split horizontally Cmd + Opt + Left/Right/Up/Down - move between shell windows (after split) Cmd + Left/Right - prev/next tab Ctrl + Cmd + Left/Right/Up/Down - change size of windows after split ","permalink":"https://timor.site/2020/01/moving-from-linux-to-macos-first-steps/","summary":"Few years ago I moved from Linux desktop to MacOS for my business, day to day work. There were 2 main reasons for that:\nCorporations don\u0026rsquo;t like Linux - they can\u0026rsquo;t manage it, they can\u0026rsquo;t support it, so they blocked it with \u0026ldquo;Security policy\u0026rdquo;, ISO20001, or other nonsense. Actually they\u0026rsquo;re partially right but in different place - many business collaboration applications don\u0026rsquo;t work well on LInux (or they don\u0026rsquo;t work at all) Skype for Business - there\u0026rsquo;s open source alternative but to get full support you have to pay for additional codecs (as far as I remember) - it\u0026rsquo;s not working stable even in paid version Outlook and calendar support - I love Thunderbird and I use it for years, but calendar invitations didn\u0026rsquo;t work nice (honestly, they didn\u0026rsquo;t work nice even between different Outlook versions\u0026hellip;) Corporate VPN apps - Christ, I always was able to get it working eventually, but\u0026hellip; why bother I\u0026rsquo;m older, maybe lazier, maybe smarter - I don\u0026rsquo;t like to spend my time resolving problems that don\u0026rsquo;t give me any value.","title":"Moving from Linux to MacOS ‚Äì first steps"},{"content":" Teoria kwantowa nie gryzieAuthor: Marcus Chown\nlubimyczytac.pl ","permalink":"https://timor.site/books/2019/teoria-kwanotowa-nie-gryzie/","summary":" Teoria kwantowa nie gryzieAuthor: Marcus Chown\nlubimyczytac.pl ","title":"Teoria kwantowa nie gryzie"},{"content":" Czas SpekulacjiAuthor: Gregory J. Millman\nlubimyczytac.pl ","permalink":"https://timor.site/books/2019/czas-spekulacji/","summary":" Czas SpekulacjiAuthor: Gregory J. Millman\nlubimyczytac.pl ","title":"Czas Spekulacji"},{"content":" Kr√≥tka historia czasuOd Wielkiego Wybuchu do czarnych dziur\nAuthor: Stephen Hawking\nempik.com ","permalink":"https://timor.site/books/2019/jeszcze-krotsza-historia-czasu/","summary":" Kr√≥tka historia czasuOd Wielkiego Wybuchu do czarnych dziur\nAuthor: Stephen Hawking\nempik.com ","title":"Kr√≥tka historia czasu"},{"content":" Teoria wszystkiego, czyli kr√≥tka historia wszech≈õwiataAuthor: Stephen Hawking\nempik.com ","permalink":"https://timor.site/books/2019/teoria-wszystkiego/","summary":" Teoria wszystkiego, czyli kr√≥tka historia wszech≈õwiataAuthor: Stephen Hawking\nempik.com ","title":"Teoria wszystkiego, czyli kr√≥tka historia wszech≈õwiata"},{"content":" Bezpiecze≈Ñstwo aplikacji webowychAuthors: Micha≈Ç Bentkowski, Gynvael Coldwind, Artur Czy≈º, Rafa≈Ç Janicki, Jaros≈Çaw Kami≈Ñski, Adrian Michalczyk, Mateusz Niezabitowski, Marcin Piosek, Micha≈Ç Sajdak, Grzegorz Trawi≈Ñski, Bohdan Wid≈Ça\nksiazka.sekurak.pl ","permalink":"https://timor.site/books/2019/bezpieczenstwo-aplikacji-webowych/","summary":" Bezpiecze≈Ñstwo aplikacji webowychAuthors: Micha≈Ç Bentkowski, Gynvael Coldwind, Artur Czy≈º, Rafa≈Ç Janicki, Jaros≈Çaw Kami≈Ñski, Adrian Michalczyk, Mateusz Niezabitowski, Marcin Piosek, Micha≈Ç Sajdak, Grzegorz Trawi≈Ñski, Bohdan Wid≈Ça\nksiazka.sekurak.pl ","title":"Bezpiecze≈Ñstwo aplikacji webowych"},{"content":" DevOps≈öwiatowej klasy zwinno≈õƒá, niezawodno≈õƒá i bezpiecze≈Ñstwo w Twojej organizacji\nAuthors: Gene Kim, Patrick Debois, John Willis, Jez Humble, John Allspaw\nhelion.pl ","permalink":"https://timor.site/books/2019/devops-gene-kim/","summary":" DevOps≈öwiatowej klasy zwinno≈õƒá, niezawodno≈õƒá i bezpiecze≈Ñstwo w Twojej organizacji\nAuthors: Gene Kim, Patrick Debois, John Willis, Jez Humble, John Allspaw\nhelion.pl ","title":"DevOps"},{"content":" DevOps HiringAuthor: Dave Zwieback\noreilly.com ","permalink":"https://timor.site/books/2019/devops-hiring/","summary":" DevOps HiringAuthor: Dave Zwieback\noreilly.com ","title":"DevOps Hiring"},{"content":" Book of LifeAuthor: Deborah Harkness\namazon.plamazon.com ","permalink":"https://timor.site/books/2018/book-of-life/","summary":" Book of LifeAuthor: Deborah Harkness\namazon.plamazon.com ","title":"Book of Life"},{"content":" Shadow of NightAuthor: Deborah Harkness\namazon.plamazon.com ","permalink":"https://timor.site/books/2018/shadow-of-night/","summary":" Shadow of NightAuthor: Deborah Harkness\namazon.plamazon.com ","title":"Shadow of Night"},{"content":" A Discovery of WitchesAuthor: Deborah Harkness\namazon.plamazon.com ","permalink":"https://timor.site/books/2018/a-discovery-of-witches/","summary":" A Discovery of WitchesAuthor: Deborah Harkness\namazon.plamazon.com ","title":"A Discovery of Witches"},{"content":" Projekt FeniksPowie≈õƒá o IT, modelu DevOps i o tym, jak pom√≥c firmie w odniesieniu sukcesu\nAuthors: Gene Kim, Kevin Behr, George Spafford\nhelion.pl ","permalink":"https://timor.site/books/2018/projekt-feniks/","summary":" Projekt FeniksPowie≈õƒá o IT, modelu DevOps i o tym, jak pom√≥c firmie w odniesieniu sukcesu\nAuthors: Gene Kim, Kevin Behr, George Spafford\nhelion.pl ","title":"Projekt Feniks"},{"content":" Bro≈Ñ matematycznej zag≈ÇadyJak algorytmy zwiƒôkszajƒÖ nier√≥wno≈õci i zagra≈ºajƒÖ demokracji\nAuthor: Cathy O\u0026#39;Neil\nhelion.pl ","permalink":"https://timor.site/books/2018/bron-matematycznej-zaglady/","summary":" Bro≈Ñ matematycznej zag≈ÇadyJak algorytmy zwiƒôkszajƒÖ nier√≥wno≈õci i zagra≈ºajƒÖ demokracji\nAuthor: Cathy O\u0026#39;Neil\nhelion.pl ","title":"Bro≈Ñ matematycznej zag≈Çady"},{"content":" Cisza w sieciAuthor: Micha≈Ç Zalewski\nhelion.pl ","permalink":"https://timor.site/books/2018/cisza-w-sieci/","summary":" Cisza w sieciAuthor: Micha≈Ç Zalewski\nhelion.pl ","title":"Cisza w sieci"},{"content":" Nowy domTrylogia Mrocznego Elfa. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2018/drizzt-nowy-dom/","summary":" Nowy domTrylogia Mrocznego Elfa. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Nowy dom"},{"content":" WygnanieTrylogia Mrocznego Elfa. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2018/drizzt-wygnanie/","summary":" WygnanieTrylogia Mrocznego Elfa. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Wygnanie"},{"content":" OdliczajƒÖc do dnia zeroStuxnet, czyli prawdziwa historia cyfrowej broni\nAuthor: Kim Zetter\nhelion.pl ","permalink":"https://timor.site/books/2018/odliczajac-do-dnia-zero/","summary":" OdliczajƒÖc do dnia zeroStuxnet, czyli prawdziwa historia cyfrowej broni\nAuthor: Kim Zetter\nhelion.pl ","title":"OdliczajƒÖc do dnia zero"},{"content":" OjczyznaTrylogia Mrocznego Elfa. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2018/drizzt-ojczyzna/","summary":" OjczyznaTrylogia Mrocznego Elfa. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Ojczyzna"},{"content":" Klejnot halflingaTrylogia Doliny Lodowego Wichru. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2017/tdlw-klejnot-halflinga/","summary":" Klejnot halflingaTrylogia Doliny Lodowego Wichru. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Klejnot halflinga"},{"content":" Strumienie srebraTrylogia Doliny Lodowego Wichru. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2017/tdlw-strumienie-srebra/","summary":" Strumienie srebraTrylogia Doliny Lodowego Wichru. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Strumienie srebra"},{"content":" Kryszta≈Çowy reliktTrylogia Doliny Lodowego Wichru. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://timor.site/books/2017/tdlw-krysztalowy-relikt/","summary":" Kryszta≈Çowy reliktTrylogia Doliny Lodowego Wichru. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Kryszta≈Çowy relikt"},{"content":" Czysty kodPodrƒôcznik dobrego programisty\nAuthor: Robert C. Martin\nhelion.pl ","permalink":"https://timor.site/books/2017/czysty-kod/","summary":" Czysty kodPodrƒôcznik dobrego programisty\nAuthor: Robert C. Martin\nhelion.pl ","title":"Czysty kod"},{"content":" Java. Efektywne programowanieWydanie II\nhelion.pl ","permalink":"https://timor.site/books/2017/java-efektywne-programowanie/","summary":" Java. Efektywne programowanieWydanie II\nhelion.pl ","title":"Java. Efektywne programowanie"},{"content":" Pragmatyczny programistaOd czeladnika do mistrza\nAuthors: Andrew Hunt, David Thomas\nhelion.pl ","permalink":"https://timor.site/books/2017/pragmatyczny-programista/","summary":" Pragmatyczny programistaOd czeladnika do mistrza\nAuthors: Andrew Hunt, David Thomas\nhelion.pl ","title":"Pragmatyczny programista"},{"content":" Perfekcyjna niedoskona≈Ço≈õƒáAuthor: Jacek Dukaj\nempik.com ","permalink":"https://timor.site/books/2017/perfekcyjna-niedoskonalosc/","summary":" Perfekcyjna niedoskona≈Ço≈õƒáAuthor: Jacek Dukaj\nempik.com ","title":"Perfekcyjna niedoskona≈Ço≈õƒá"},{"content":" Black Hat PythonJƒôzyk Python dla haker√≥w i pentester√≥w\nAuthor: Justin Seitz\namazon.plhelion.pl ","permalink":"https://timor.site/books/2017/black-hack-python/","summary":" Black Hat PythonJƒôzyk Python dla haker√≥w i pentester√≥w\nAuthor: Justin Seitz\namazon.plhelion.pl ","title":"Black Hat Python"},{"content":" SzpiegCzyli podstawy szpiegowskiego fachu\nAuthor: Wiktor Suworow\n","permalink":"https://timor.site/books/2017/szpieg/","summary":"SzpiegCzyli podstawy szpiegowskiego fachu\nAuthor: Wiktor Suworow","title":"Szpieg"},{"content":" Sztuka wojnyWydanie 3\nAuthors: Sun Tzu, Sun Pin\nmaklerska.pl ","permalink":"https://timor.site/books/2017/sztuka-wojny/","summary":" Sztuka wojnyWydanie 3\nAuthors: Sun Tzu, Sun Pin\nmaklerska.pl ","title":"Sztuka wojny"},{"content":" ExtensaAuthor: Jacek Dukaj\nempik.com ","permalink":"https://timor.site/books/2017/extensa/","summary":" ExtensaAuthor: Jacek Dukaj\nempik.com ","title":"Extensa"},{"content":" Wydajno≈õƒá JavyPoznaj i wykorzystaj optymalne sposoby na regulowanie wydajno≈õci oprogramowania Java!\nAuthors: Charlie Hunt, Binu John\nhelion.pl ","permalink":"https://timor.site/books/2017/wydajnosc-javy/","summary":" Wydajno≈õƒá JavyPoznaj i wykorzystaj optymalne sposoby na regulowanie wydajno≈õci oprogramowania Java!\nAuthors: Charlie Hunt, Binu John\nhelion.pl ","title":"Wydajno≈õƒá Javy"},{"content":" Gra EnderaSaga Endera. Tom 1\nAuthor: Card Orson Scott\nempik.com ","permalink":"https://timor.site/books/2016/gra-endera/","summary":" Gra EnderaSaga Endera. Tom 1\nAuthor: Card Orson Scott\nempik.com ","title":"Gra Endera"},{"content":"I had stragne statistics on one memcached servers. I had to look what it\u0026rsquo;s doing there. I found such commands that may be used to sniff, extract and make statistics from running memcached server.\nDebug GET commands tcpflow -c dst port 11211 | cut -b46- | grep ^get cut command will remove 46 bytes at beginning of every string (src, dst, port). You may need to adjust numeric parameter for cut to leave commands only. Output should look like:\nget myapp-cache-theme_registry get myapp-cache_field get myapp-cache-schema ... Debug everything except GET commands tcpflow -c dst port 11211 | cut -b46- | grep -v ^get Check transfer Add pv -r at the end to calculate transfer:\n$ tcpflow -c dst port 11211 | cut -b46- | grep ^get | pv -r \u0026gt; /dev/null tcpflow[11140]: listening on eth0 [8.25kB/s] Check command rates To check command rates not the transfer add -l param (stands for lines) to pv command:\n$ tcpflow -c dst port 11211 | cut -b46- | grep -v ^get | pv -rl \u0026gt; /dev/null tcpflow[9271]: listening on eth0 [1.51k/s] That should be enough for you to start debugging üòÑ\nSource: http://www.streppone.it/cosimo/blog/tag/memcached/\n","permalink":"https://timor.site/2016/07/debuging-commands-running-on-memcached/","summary":"I had stragne statistics on one memcached servers. I had to look what it\u0026rsquo;s doing there. I found such commands that may be used to sniff, extract and make statistics from running memcached server.\nDebug GET commands tcpflow -c dst port 11211 | cut -b46- | grep ^get cut command will remove 46 bytes at beginning of every string (src, dst, port). You may need to adjust numeric parameter for cut to leave commands only.","title":"Debuging commands running on memcached"},{"content":"It happen to me all the time that one of developers notifies me about some kind of problem that I can\u0026rsquo;t confirm from my account. Sometimes it was because of bad ssh keys configuration, other times file permissions, mostly such stuff. It\u0026rsquo;s sometimes convenient to \u0026ldquo;enter into someone\u0026rsquo;s shoes\u0026rdquo; to see what\u0026rsquo;s going on there.\nIf you\u0026rsquo;re root on machine you may do that like this:\nsu developer - Easy one but that\u0026rsquo;s not enough for all cases. When you use bastion host (or similar solutions) sometimes users have connection problems and it\u0026rsquo;s harder to check. When such user have ForwardAgent ssh option enabled you may stole this session to check login problems. After you switch to such user, you may wan\u0026rsquo;t to hide history (it\u0026rsquo;s optional üòâ ) - disable history like that:\nexport HISTFILESIZE=0 export HISTSIZE=0 unset HISTFILE Now you may stole ssh session, but first check if you have your dev is logged on:\n$ ls -la /tmp/ | grep ssh drwx------ 2 root root 4096 Apr 27 20:56 ssh-crYKv29798 drwx------ 2 developer developer 4096 Apr 27 18:03 ssh-cVXFo28108 Export SSH_AUTH_SOCK with path to developer\u0026rsquo;s agent socket:\nSSH_AUTH_SOCK=/tmp/ssh-cVXFo28108/agent.28108 Finally you may try to login via ssh as developer and see with his eyes what\u0026rsquo;s now working.\n","permalink":"https://timor.site/2016/04/how-to-stole-ssh-session-when-youre-root/","summary":"It happen to me all the time that one of developers notifies me about some kind of problem that I can\u0026rsquo;t confirm from my account. Sometimes it was because of bad ssh keys configuration, other times file permissions, mostly such stuff. It\u0026rsquo;s sometimes convenient to \u0026ldquo;enter into someone\u0026rsquo;s shoes\u0026rdquo; to see what\u0026rsquo;s going on there.\nIf you\u0026rsquo;re root on machine you may do that like this:\nsu developer - Easy one but that\u0026rsquo;s not enough for all cases.","title":"How to stole ssh session when you‚Äôre root"},{"content":"Virtualenvs in python are cheap but from time to time you will install something with pip on your system and when time comes removing all this crap could be difficult. I found this bash snippet that will uninstall package with all dependencies:\nfor dep in $(pip show python-neutronclient | grep Requires | sed \u0026#39;s/Requires: //g; s/,//g\u0026#39;) ; do sudo pip uninstall -y $dep ; done pip uninstall -y python-neutronclient Source: http://stackoverflow.com/a/32698209/4828478\n","permalink":"https://timor.site/2016/04/pip-uninstall-package-with-dependencies/","summary":"Virtualenvs in python are cheap but from time to time you will install something with pip on your system and when time comes removing all this crap could be difficult. I found this bash snippet that will uninstall package with all dependencies:\nfor dep in $(pip show python-neutronclient | grep Requires | sed \u0026#39;s/Requires: //g; s/,//g\u0026#39;) ; do sudo pip uninstall -y $dep ; done pip uninstall -y python-neutronclient Source: http://stackoverflow.","title":"pip - uninstall package with dependencies"},{"content":"I\u0026rsquo;ve been using standard MySQL dumps as backup technique on my VPS for few years. It works fine and backups were usable few times when I needed them. But in other places I\u0026rsquo;m using xtrabackup. It\u0026rsquo;s faster when crating backups and a lot faster when restoring them - they\u0026rsquo;re binary so there is no need to reevaluate all SQL create tables/inserts/etc. Backups also include my.cnf config file so restoring on other machine should be easy.\nAfter I switched from MariaDB to Percona I have Percona repos configured, so I will use latest version of xtrabackup.\napt-get install -y percona-xtrabackup Prerequisities xtrabackup requires configured user to be able to make backups. One way is to write user and password in plaintext in ~/.my.cnf. Another is using mysql_config_editor to generate ~/.mylogin.cnf file with encrypted credentials. To be honest I didn\u0026rsquo;t check what kind of security provides this encryption but it feels better than keeping password in plaintext.\nI do not want to create new user for this task - I just used debian-sys-maint user. Check password for this user like this:\ngrep password /etc/mysql/debian.cnf Now create encrypted file:\nmysql_config_editor set --login-path=client --host=localhost --user=debian-sys-maint --password Hit enter and copy/paste password. File .mylogin.cnf should be created with binary content. We may check this with:\n# mysql_config_editor print [client] user = debian-sys-maint password = ***** host = localhost Looks OK.\nBackuping Now backup script. I placed it directly in cron.daily dir ex. /etc/cron.daily/zz-percona-backup with content:\n#!/bin/bash DATE=`date +%F-%H%M%S` DIR=/backup/xtrabackup DST=$DIR/${DATE}.tar.xz # this will produce directories with compresses files # mkdir -p $DST # xtrabackup --backup --compress --target-dir=$DST # this will produce tar.xz archives xtrabackup --backup --stream=tar | xz -9 \u0026gt; $DST # delete files older than 30 days find $DIR -type f -mtime +30 -delete I prefer to have single archive with backup because I\u0026rsquo;m transferring those files to my NAS (for security). But for local backups directories are more convenient and faster when restoring. Also tar archives have to be decompressed with -ioption.\nRestoring First time I saw it it scared me a little but after all worked fine and without problems\u0026hellip;\nservice mysql stop rm -rf /var/lib/mysql mkdir /var/lib/mysql Now prepare backup, if you used directory backups it\u0026rsquo;s easy:\nxtrabackup --decompress --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --prepare --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --copy-back --target-dir=/backup/xtrabackup/2016-03-14-214233 But if you used tar archives it\u0026rsquo;s little more messy\u0026hellip; You have to create temporary dir and extract archive there:\nmkdir /tmp/restore tar -xvif /backup/xtrabackup/2016-03-14-214233.tar.xz -C /tmp/restore xtrabackup --prepare --target-dir=/tmp/restore xtrabackup --copy-back --target-dir=/tmp/restore We have to fix ownership of restored files and db may be started:\nchown -R mysql:mysql /var/lib/mysql service mysql start If your backup is huge you should reorder commands to shutdown database after backup decompression.\nSource: https://www.percona.com/doc/percona-xtrabackup/2.3/xtrabackup_bin/xtrabackup_binary.html http://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html https://www.percona.com/doc/percona-xtrabackup/2.1/innobackupex/streaming_backups_innobackupex.html\n","permalink":"https://timor.site/2016/04/daily-mysql-backups-with-xtrabackup/","summary":"I\u0026rsquo;ve been using standard MySQL dumps as backup technique on my VPS for few years. It works fine and backups were usable few times when I needed them. But in other places I\u0026rsquo;m using xtrabackup. It\u0026rsquo;s faster when crating backups and a lot faster when restoring them - they\u0026rsquo;re binary so there is no need to reevaluate all SQL create tables/inserts/etc. Backups also include my.cnf config file so restoring on other machine should be easy.","title":"Daily MySQL backups with xtrabackup"},{"content":"When you deploy your application in cloud you don\u0026rsquo;t need and don\u0026rsquo;t want your hosts exposed via SSH to the world. Malware scans whole network for easy SSH access and when find something will try some brute force attacks, overloading such machines. It\u0026rsquo;s easier to have one exposed, but secured host, that doesn\u0026rsquo;t host anything and is used as proxy/gateway to access our infrastructure- it\u0026rsquo;s called bastion host.\nAnsible is quite easy to integrate with bastion host configuration. We will need custom ansible.cfg and ssh_config file. So let\u0026rsquo;s start with ssh_config:\nHost bastion Hostname ip.xxx.xxx.xxx.xxx.or.host.name User ubuntu IdentityFile ~/.ssh/id_rsa PasswordAuthentication no ForwardAgent yes ServerAliveInterval 60 TCPKeepAlive yes ControlMaster auto ControlPath ~/.ssh/ansible-%r@%h:%p ControlPersist 15m ProxyCommand none LogLevel QUIET Host * User ubuntu IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 TCPKeepAlive yes ProxyCommand ssh -q -A ubuntu@bastion nc %h %p LogLevel QUIET StrictHostKeyChecking no Now I will describe what most important options mean. For bastion:\nUser - I\u0026rsquo;m using Ubuntu kickstarted on cloud as bastion host with it\u0026rsquo;s default user. Never use root here - you don\u0026rsquo;t need that ForwardAgent yes - we want to forward our ssh keys through bastion to destination hosts, ServerAliveInterval 60 - this is like keepalive connection, ssh will send small ping/pong packets every 60 seconds so your connection won\u0026rsquo;t hung/terminate after long time, ControlMaster auto - we will open one connection to bastion host and multiplex other ssh connections through it, connection will be opened for ControlPersist time, ControlPath - this have to be configured same way like in ansible.cfg, ProxyCommand none - we\u0026rsquo;re setting ProxyCommand for all hosts but we need it disabled for bastion,`` Default hosts configuration:\nProxyCommand ssh -q -A ubuntu@bastion nc %h %p - this is what makes all magic, it will pipe your ssh connection via bastion to destination host, StrictHostKeyChecking no - this options shouldn\u0026rsquo;t be there for production but it\u0026rsquo;s useful at beginning when you create and destroy machines few times before you test everything. Normally this will cause notifications about ssh key changes, but you\u0026rsquo;re aware of that - you just recreated those machines. I\u0026rsquo;ve found examples without netcat but was unable to get them working - this one worked for me really well.\nTo test if connections work fine use this configuration like:\nssh -F ssh_config bastion ssh -F ssh_config other.host.behind.bastion And now ansible.cfg:\n[defaults] forks=20 [ssh_connection] ssh_args = -F ./ssh_config -o ControlMaster=auto -o ControlPersist=5m -o LogLevel=QUIET control_path = ~/.ssh/ansible-%%r@%%h:%%p pipelining=True Most important section here is in ssh_args where we\u0026rsquo;re pointing to ssh_config file in current dir with -F option. I also have to reenter configuration for multiplexing here - it wasn\u0026rsquo;t working with ssh only configuration. control_path option have to use same paths like ssh_config (% signs are escaped with %%).\nYou should be able to run ansible/ansible-playbook commands normally now - all traffic will be forwarded through bastion.\nIt\u0026rsquo;s good time now to install fail2ban on bastion and maybe reconfigure it to run ssh on crazy high port üôÇ\nSources http://alexbilbie.com/2014/07/using-ansible-with-a-bastion-host/ http://blog.scottlowe.org/2015/12/24/running-ansible-through-ssh-bastion-host/ https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\n","permalink":"https://timor.site/2016/04/use-bastion-host-with-ansible/","summary":"When you deploy your application in cloud you don\u0026rsquo;t need and don\u0026rsquo;t want your hosts exposed via SSH to the world. Malware scans whole network for easy SSH access and when find something will try some brute force attacks, overloading such machines. It\u0026rsquo;s easier to have one exposed, but secured host, that doesn\u0026rsquo;t host anything and is used as proxy/gateway to access our infrastructure- it\u0026rsquo;s called bastion host.\nAnsible is quite easy to integrate with bastion host configuration.","title":"Use bastion host with Ansible"},{"content":"Lately I was searching for mobile notebook that I could use for remote work. I checked f ThinkPad series but they were huge bricks that have nothing in common with \u0026lsquo;mobile\u0026rsquo; word. Then I saw ASUS Zenbook that I didn\u0026rsquo;t take into account before and it was exactly what I was searching for.\nConfiguration of Skylake based notebook right now is not straightforward - there are still glitches and small bugs that are waiting to be fixed. I want to sum up what I\u0026rsquo;ve done after installation. I started with fresh¬†Ubuntu 16.04 to get Debian based distro with possibly latest¬†kernel and patches.\nSome SSD tweaks Change mount options for filesystems on SSD from:\n/dev/mapper/ubuntu--vg-root / ext4 errors=remount-ro 0 1 to:\n/dev/mapper/ubuntu--vg-root / ext4 discard,noatime,errors=remount-ro 0 1 And move /tmp to RAM with this additional line in /etc/fstab:\ntmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0 Now add deadline scheduler for root disk - edit /etc/rc.local and add this line before exit 0:\necho deadline \u0026gt; /sys/block/sda/queue/scheduler echo 1 \u0026gt; /sys/block/sda/queue/iosched/fifo_batch I have configured swap but I don\u0026rsquo;t want to use it too much and setting low swappines sysctl option will help. Run this as root:\necho \u0026#34;vm.swappiness = 1\u0026#34; \u0026gt; /etc/sysctl.conf.d/90-swappines.conf sysctl -p /etc/sysctl.conf.d/90-swappines.conf Power usage tweaks I\u0026rsquo;ve installed laptop-mode-tools to achieve lower power usage on battery. So:\napt-get install -y laptop-mode-tools By default it\u0026rsquo;s cutting hard CPU performance on battery (half performance, no turbo) so I fixed this by changing /etc/laptop-mode/conf.d/intel_pstate.conf section On battery:\n#On battery BATT_INTEL_PSTATE_PERF_MIN_PCT=0 # Minimum performance, in percent BATT_INTEL_PSTATE_PERF_MAX_PCT=100 # Maximum performance, in percent BATT_INTEL_PSTATE_NO_TURBO=0 # Disable \u0026#34;Turbo Boost\u0026#34;? Laptop mode tools won\u0026rsquo;t start automatically so we may integrate them with pm-utils (that are already installed on Ubuntu) to get it running when needed. We have to create new config file:\nsudo touch /etc/pm/sleep.d/10-laptop-mode-tools sudo chmod a+x /etc/pm/sleep.d/10-laptop-mode-tools with content like this:\ncase $1 in hibernate) /etc/init.d/laptop-mode stop ;; suspend) /etc/init.d/laptop-mode stop ;; thaw)d /etc/init.d/laptop-mode start ;; resume) /etc/init.d/laptop-mode start ;; *) echo Something is not right. ;; esac Now I will enable ALPM for SATA in AHCI mode optimizations:\necho SATA_ALPM_ENABLE=true | sudo tee /etc/pm/config.d/sata_alpm And some kernel parameters in /etc/default/grub:\nGRUB_CMDLINE_LINUX=\u0026#34;pcie_aspm=force drm.vblankoffdelay=1 i915.semaphores=1\u0026#34; and update-grub with:\nupdate-grub You may use powertop to nail power heavy processes. There is also powerstat to benchmark power usage through time - I have:\nsudo pm-powersave true powerstat ...... Summary: System: 4.49 Watts on average with standard deviation 0.46 It\u0026rsquo;s really nice. I should be able to run about 8~9h! Sweet!\nSources https://www.reddit.com/r/linux/comments/3ia8ta/review_of_ubuntu_on_asus_ux305fa/\nhttps://help.ubuntu.com/community/PowerManagement/ReducedPower#Using_less_power_with_laptop-mode-tools\nhttps://help.ubuntu.com/community/AsusZenbook\nhttps://wiki.ubuntu.com/Kernel/PowerManagementALPM\nDisable touchpad when writing It\u0026rsquo;s crazy annoying when you tap touchpad during writing text and lose focus on editor window. There is solution for that, it\u0026rsquo;s even installed by default on Ubuntu and it\u0026rsquo;s called: syndaemon. It\u0026rsquo;s started by default like this:\nsyndaemon -i 1.0 -t -K -R 1 second feels too small for me. I will adjust it to 2s. There is no easy way to do this. I created script to run on login:\n#!/bin/bash killall syndaemon syndaemon -d -i 2.0 -t -K -R Now better üôÇ\nVD-PAU I installed vdpauinfo tool to see if it\u0026rsquo;s working:\napt-get install -y vdpauinfo It wasn\u0026rsquo;t:\nvdpauinfo display: :0 screen: 0 Failed to open VDPAU backend libvdpau_va_gl.so: cannot open shared object file: No such file or directory Error creating VDPAU device: 1 I checked this library and couldn\u0026rsquo;t find it - it wasn\u0026rsquo;t installed. Easy fix:\napt-get install -y libvdpau-va-gl1 Check again:\nvdpauinfo display: :0 screen: 0 libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 API version: 1 Information string: OpenGL/VAAPI/libswscale backend for VDPAU Video surface: name width height types ------------------------------------------- 420 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 422 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 444 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 Decoder capabilities: name level macbs width height ---------------------------------------------------- MPEG1 --- not supported --- MPEG2_SIMPLE --- not supported --- MPEG2_MAIN --- not supported --- H264_BASELINE 51 16384 2048 2048 H264_MAIN 51 16384 2048 2048 H264_HIGH 51 16384 2048 2048 VC1_SIMPLE --- not supported --- VC1_MAIN --- not supported --- VC1_ADVANCED --- not supported --- MPEG4_PART2_SP --- not supported --- MPEG4_PART2_ASP --- not supported --- DIVX4_QMOBILE --- not supported --- DIVX4_MOBILE --- not supported --- DIVX4_HOME_THEATER --- not supported --- DIVX4_HD_1080P --- not supported --- DIVX5_QMOBILE --- not supported --- DIVX5_MOBILE --- not supported --- DIVX5_HOME_THEATER --- not supported --- DIVX5_HD_1080P --- not supported --- H264_CONSTRAINED_BASELINE 51 16384 2048 2048 H264_EXTENDED --- not supported --- H264_PROGRESSIVE_HIGH --- not supported --- H264_CONSTRAINED_HIGH --- not supported --- H264_HIGH_444_PREDICTIVE --- not supported --- HEVC_MAIN --- not supported --- HEVC_MAIN_10 --- not supported --- HEVC_MAIN_STILL --- not supported --- HEVC_MAIN_12 --- not supported --- HEVC_MAIN_444 --- not supported --- Output surface: name width height nat types ---------------------------------------------------- B8G8R8A8 8192 8192 y R8G8B8A8 8192 8192 y R10G10B10A2 8192 8192 y B10G10R10A2 8192 8192 y A8 8192 8192 y Bitmap surface: name width height ------------------------------ B8G8R8A8 8192 8192 R8G8B8A8 8192 8192 R10G10B10A2 8192 8192 B10G10R10A2 8192 8192 A8 8192 8192 Video mixer: feature name sup ------------------------------------ DEINTERLACE_TEMPORAL - DEINTERLACE_TEMPORAL_SPATIAL - INVERSE_TELECINE - NOISE_REDUCTION - SHARPNESS - LUMA_KEY - HIGH QUALITY SCALING - L1 - HIGH QUALITY SCALING - L2 - HIGH QUALITY SCALING - L3 - HIGH QUALITY SCALING - L4 - HIGH QUALITY SCALING - L5 - HIGH QUALITY SCALING - L6 - HIGH QUALITY SCALING - L7 - HIGH QUALITY SCALING - L8 - HIGH QUALITY SCALING - L9 - parameter name sup min max ----------------------------------------------------- VIDEO_SURFACE_WIDTH - VIDEO_SURFACE_HEIGHT - CHROMA_TYPE - LAYERS - attribute name sup min max ----------------------------------------------------- BACKGROUND_COLOR - CSC_MATRIX - NOISE_REDUCTION_LEVEL - SHARPNESS_LEVEL - LUMA_KEY_MIN_LUMA - LUMA_KEY_MAX_LUMA - Looks better now\u0026hellip; But not impressive, there\u0026rsquo;s only H264 support.\nI\u0026rsquo;ve tried it in VLC but it was crashing from time to time the whole VLC (leaving it running in background). Time to test VA-API üôÇ\nVA-API Like earlier I have to install one tool to see what we have: vainfo\napt-get install -y vainfo Checking what we have on system:\nvainfo libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 vainfo: VA-API version: 0.39 (libva 1.7.0) vainfo: Driver version: Intel i965 driver for Intel(R) Skylake - 1.7.0 vainfo: Supported profile and entrypoints VAProfileMPEG2Simple :\tVAEntrypointVLD VAProfileMPEG2Simple :\tVAEntrypointEncSlice VAProfileMPEG2Main :\tVAEntrypointVLD VAProfileMPEG2Main :\tVAEntrypointEncSlice VAProfileH264ConstrainedBaseline:\tVAEntrypointVLD VAProfileH264ConstrainedBaseline:\tVAEntrypointEncSlice VAProfileH264Main :\tVAEntrypointVLD VAProfileH264Main :\tVAEntrypointEncSlice VAProfileH264High :\tVAEntrypointVLD VAProfileH264High :\tVAEntrypointEncSlice VAProfileH264MultiviewHigh :\tVAEntrypointVLD VAProfileH264MultiviewHigh :\tVAEntrypointEncSlice VAProfileH264StereoHigh :\tVAEntrypointVLD VAProfileH264StereoHigh :\tVAEntrypointEncSlice VAProfileVC1Simple :\tVAEntrypointVLD VAProfileVC1Main :\tVAEntrypointVLD VAProfileVC1Advanced :\tVAEntrypointVLD VAProfileNone :\tVAEntrypointVideoProc VAProfileJPEGBaseline :\tVAEntrypointVLD VAProfileJPEGBaseline :\tVAEntrypointEncPicture VAProfileVP8Version0_3 :\tVAEntrypointVLD VAProfileVP8Version0_3 :\tVAEntrypointEncSlice VAProfileHEVCMain :\tVAEntrypointVLD VAProfileHEVCMain :\tVAEntrypointEncSlice It requires package i965-va-driver to work but on my system it was installed (probably during VDPAU installation as dependency).\nIt was working almost fine\u0026hellip; In VLC on my machine VA-API on X11 was drawing through all desktops. VA-API DRM was working better\u0026hellip; But crashed my X11 server after few minutes of watching ;/\nOpenCL You may thing: for what the hell you need OpenCL on such tiny machine? I doesn\u0026rsquo;t care - I want it üôÇ\nFirst install clinfo package:\napt-get install -y clinfo And run it:\nclinfo Number of platforms 0 Not too much üòÄ\nFor Intel GPU/CPU OpenCL support we will need beignet package:\napt-get install -y beignet clinfo Number of platforms 1 Platform Name Intel Gen OCL Driver Platform Vendor Intel Platform Version OpenCL 1.2 beignet 1.1.1 Platform Profile FULL_PROFILE Platform Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_spir cl_khr_icd Platform Extensions function suffix Intel Platform Name Intel Gen OCL Driver Number of devices 1 Device Name Intel(R) HD Graphics Skylake ULX GT2 Device Vendor Intel Device Vendor ID 0x8086 Device Version OpenCL 1.2 beignet 1.1.1 Driver Version 1.1.1 Device OpenCL C Version OpenCL C 1.2 beignet 1.1.1 Device Type GPU Device Profile FULL_PROFILE Max compute units 24 Max clock frequency 1000MHz Device Partition (core) Max number of sub-devices 1 Supported partition types None, None, None Max work item dimensions 3 Max work item sizes 512x512x512 Max work group size 512 Preferred work group size multiple 16 Preferred / native vector sizes char 16 / 8 short 8 / 8 int 4 / 4 long 2 / 2 half 0 / 8 (cl_khr_fp16) float 4 / 4 double 0 / 2 (n/a) Half-precision Floating-point support (cl_khr_fp16) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Single-precision Floating-point support (core) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Double-precision Floating-point support (n/a) Address bits 32, Little-Endian Global memory size 2147483648 (2GiB) Error Correction support No Max memory allocation 1073741824 (1024MiB) Unified memory for Host and Device Yes Minimum alignment for any data type 128 bytes Alignment of base address 1024 bits (128 bytes) Global Memory cache type Read/Write Global Memory cache size 8192 Global Memory cache line 64 bytes Image support Yes Max number of samplers per kernel 16 Max size for 1D images from buffer 65536 pixels Max 1D or 2D image array size 2048 images Max 2D image size 8192x8192 pixels Max 3D image size 8192x8192x2048 pixels Max number of read image args 128 Max number of write image args 8 Local memory type Global Local memory size 65536 (64KiB) Max constant buffer size 134217728 (128MiB) Max number of constant args 8 Max size of kernel argument 1024 Queue properties Out-of-order execution No Profiling Yes Prefer user sync for interop Yes Profiling timer resolution 80ns Execution capabilities Run OpenCL kernels Yes Run native kernels Yes SPIR versions printf() buffer size 1048576 (1024KiB) Built-in kernels __cl_copy_region_align4;__cl_copy_region_align16;__cl_cpy_region_unalign_same_offset;__cl_copy_region_unalign_dst_offset;__cl_copy_region_unalign_src_offset;__cl_copy_buffer_rect;__cl_copy_image_1d_to_1d;__cl_copy_image_2d_to_2d;__cl_copy_image_3d_to_2d;__cl_copy_image_2d_to_3d;__cl_copy_image_3d_to_3d;__cl_copy_image_2d_to_buffer;__cl_copy_image_3d_to_buffer;__cl_copy_buffer_to_image_2d;__cl_copy_buffer_to_image_3d;__cl_fill_region_unalign;__cl_fill_region_align2;__cl_fill_region_align4;__cl_fill_region_align8_2;__cl_fill_region_align8_4;__cl_fill_region_align8_8;__cl_fill_region_align8_16;__cl_fill_region_align128;__cl_fill_image_1d;__cl_fill_image_1d_array;__cl_fill_image_2d;__cl_fill_image_2d_array;__cl_fill_image_3d; Device Available Yes Compiler Available Yes Linker Available Yes Device Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_fp16 cl_khr_spir cl_khr_icd NULL platform behavior clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...) Intel Gen OCL Driver clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...) Success [Intel] clCreateContext(NULL, ...) [default] Success [Intel] clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version 2.2.8 ICD loader Profile OpenCL 1.2 NOTE:\tyour OpenCL library declares to support OpenCL 1.2, but it seems to support up to OpenCL 2.1 too. Sources https://wiki.archlinux.org/index.php/GPGPU#Intel\nSuspend/Hibernate on lid close Default configuration of Ubuntu 16.04 was that after I close lid screen was blocked and LCD disabled. But system was still working normally - I strongly prefer to hibernate in such case and use no battery at all.\nI achieved that with systemd-logind. Edit /etc/systemd/logind.conf and uncomment line with HandleLidSwitch:\n[Login] HandleLidSwitch=suspend HandleLidSwitchDocked=ignore Now restart systemd-logind service with:\nsystemctl restart systemd-logind.service Problem with function keys Function keys were mostly working but not always like I expected. For example when I disable touchpad - it\u0026rsquo;s not disabled üôÇ\nI found that module asus-nb-wmi is responssible for that and it\u0026rsquo;s still buggy. So I disabled it at all with:\necho \u0026#34;blacklist asus-nb-wmi\u0026#34; \u0026gt; /etc/modprobe.d/blacklist-ux305.conf Volume UP/DOWN/MUTE are still working fine - that\u0026rsquo;s enough for me. Rest could be configured with some keyboard shortcuts - more info here.\nTODO/Issues I still face some bugs:\nI could see occasional flickering from time to time. Rather after running notebook for some time than overheating/overloading it. This may be driver issue or maybe SNA acceleration method - I have to experiment a little to get this solved.\nLooks like disabling Virtualization support and VT-d in BIOS helped. It\u0026rsquo;s not final solution but for now I don\u0026rsquo;t need it\u0026hellip; A lot üòâ Tapping sometimes behave strange, for ex. tap to click stops to work and I have to use touchpad buttons for that. I think this may be related to syndaemon configuration because it started after I tuned it.\nIt was that. My hack for syndaemon broke touchpad. I will play with this a little more later. I like to use copy by selection and paste by middle click on my desktop - I\u0026rsquo;m addicted to this option but it\u0026rsquo;s not working on my laptop. I\u0026rsquo;m not sure if this will be convenient enough on touchpad to use.\nTo right click just tap with two fingers, to middle click (third button) tap with three fingers. Copy/paste is again easy like before. If you found errors in my text of know better solutions for described problems, please let me know in comments.\n","permalink":"https://timor.site/2016/04/tweaking-asus-zenbook-ux305ca-on-linux/","summary":"Lately I was searching for mobile notebook that I could use for remote work. I checked f ThinkPad series but they were huge bricks that have nothing in common with \u0026lsquo;mobile\u0026rsquo; word. Then I saw ASUS Zenbook that I didn\u0026rsquo;t take into account before and it was exactly what I was searching for.\nConfiguration of Skylake based notebook right now is not straightforward - there are still glitches and small bugs that are waiting to be fixed.","title":"Tweaking ASUS Zenbook UX305CA on Linux"},{"content":"I try to use IPv6 where it\u0026rsquo;s available but it\u0026rsquo;s sometimes so hard\u0026hellip; It happen quite often that I can\u0026rsquo;t download packages from repos because they weren\u0026rsquo;t configured on IPv6 vhosts even when host is available via IPv6 address. For APT you may use this trick to force IPv4 connections only:\necho \u0026#39;Acquire::ForceIPv4 \u0026#34;true\u0026#34;;\u0026#39; \u0026gt; /etc/apt/apt.conf.d/99force-ipv4 If you need more than that, then gai.conf will allow you to filter where you will be connecting via IPv4 and where via IPv6 - in example bellow you will prefer IPv4 whenever it\u0026rsquo;s available:\necho \u0026#39;precedence ::ffff:0:0/96 100\u0026#39; \u0026gt;\u0026gt; /etc/gai.conf ","permalink":"https://timor.site/2016/03/prefer-ipv4-over-ipv6/","summary":"I try to use IPv6 where it\u0026rsquo;s available but it\u0026rsquo;s sometimes so hard\u0026hellip; It happen quite often that I can\u0026rsquo;t download packages from repos because they weren\u0026rsquo;t configured on IPv6 vhosts even when host is available via IPv6 address. For APT you may use this trick to force IPv4 connections only:\necho \u0026#39;Acquire::ForceIPv4 \u0026#34;true\u0026#34;;\u0026#39; \u0026gt; /etc/apt/apt.conf.d/99force-ipv4 If you need more than that, then gai.conf will allow you to filter where you will be connecting via IPv4 and where via IPv6 - in example bellow you will prefer IPv4 whenever it\u0026rsquo;s available:","title":"Prefer IPv4 over IPv6"},{"content":"Sometimes it\u0026rsquo;s easier to use octal file permissions but they\u0026rsquo;re not so easy to list. I caught myself few times that I didn\u0026rsquo;t remember how to list them - so this is a reason for that note.\nstat -c \u0026#34;%a %n\u0026#34; * 755 bin 755 games 755 include Yes, it\u0026rsquo;s that easy üòÉ\nAnd here also with human readable attributes:\nstat -c \u0026#39;%A %a %n\u0026#39; * drwxr-xr-x 755 bin drwxr-xr-x 755 games drwxr-xr-x 755 include ","permalink":"https://timor.site/2016/02/list-octal-file-permissions-in-bash/","summary":"Sometimes it\u0026rsquo;s easier to use octal file permissions but they\u0026rsquo;re not so easy to list. I caught myself few times that I didn\u0026rsquo;t remember how to list them - so this is a reason for that note.\nstat -c \u0026#34;%a %n\u0026#34; * 755 bin 755 games 755 include Yes, it\u0026rsquo;s that easy üòÉ\nAnd here also with human readable attributes:\nstat -c \u0026#39;%A %a %n\u0026#39; * drwxr-xr-x 755 bin drwxr-xr-x 755 games drwxr-xr-x 755 include ","title":"List octal file permissions in bash"},{"content":"I was configuring WordPress with HyperDB plugin on PHP 7.0 but the only I get were constant 500 errors. As I found here PHP 7.0 is not supported by HyperDB for now - it\u0026rsquo;s rely on mysql php extension but in PHP 7.0 there is only mysqli extension. But few folks fixed it and it\u0026rsquo;s possible to use it.\ncurl -O https://raw.githubusercontent.com/soulseekah/hyperdb-mysqli/master/db.php mv db.php /var/www/wordpress/wp-content/ And configure it ex. like this:\ncat \u0026lt;\u0026lt;DBCONFIG \u0026gt; /var/www/wordpress/db-config.php \u0026lt;?php \\$wpdb-\u0026gt;save_queries = false; \\$wpdb-\u0026gt;persistent = false; \\$wpdb-\u0026gt;max_connections = 10; \\$wpdb-\u0026gt;check_tcp_responsiveness = true; \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;master.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 1, \u0026#39;read\u0026#39; =\u0026gt; 1, )); \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;slave.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 0, \u0026#39;read\u0026#39; =\u0026gt; 1, )); DBCONFIG Now WordPress could handle crash of master database.\nSources https://www.digitalocean.com/community/tutorials/how-to-optimize-wordpress-performance-with-mysql-replication-on-ubuntu-14-04\n","permalink":"https://timor.site/2016/02/wordpress-with-hyperdb-on-php-7-0/","summary":"I was configuring WordPress with HyperDB plugin on PHP 7.0 but the only I get were constant 500 errors. As I found here PHP 7.0 is not supported by HyperDB for now - it\u0026rsquo;s rely on mysql php extension but in PHP 7.0 there is only mysqli extension. But few folks fixed it and it\u0026rsquo;s possible to use it.\ncurl -O https://raw.githubusercontent.com/soulseekah/hyperdb-mysqli/master/db.php mv db.php /var/www/wordpress/wp-content/ And configure it ex. like this:","title":"WordPress with HyperDB on PHP 7.0"},{"content":"I\u0026rsquo;m playing a lot with Docker lately. Building images, and then rebuilding, and then building again\u0026hellip; It\u0026rsquo;s pretty boring. To automate this task a little I used inotify to build automatically after I changed any file. This trick could be used in many different situations.\nYou will need inotify-tools package:\nsudo apt-get install -y inotify-tools Then run something like this:\nwhile inotifywait -e modify -r .; do docker-compose build; done This commands will rebuild my Docker images after any file change in current directory. Use Ctrl+c to exit from loop.\n","permalink":"https://timor.site/2016/02/automatically-build-after-file-change/","summary":"I\u0026rsquo;m playing a lot with Docker lately. Building images, and then rebuilding, and then building again\u0026hellip; It\u0026rsquo;s pretty boring. To automate this task a little I used inotify to build automatically after I changed any file. This trick could be used in many different situations.\nYou will need inotify-tools package:\nsudo apt-get install -y inotify-tools Then run something like this:\nwhile inotifywait -e modify -r .; do docker-compose build; done This commands will rebuild my Docker images after any file change in current directory.","title":"Automatically build after file change"},{"content":"I never tried it before but today I needed to install WordPress\u0026hellip; From command line only. And there is a way to do this with wp-cli.\nWP-CLI installation First some requirements (as root):\napt-get install php5-cli php5-mysql mysql-client curl And now installation of wp-cli (as root too):\ncurl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar mv wp-cli.phar /usr/local/bin/wp Check if it\u0026rsquo;s working:\n$ wp --version WP-CLI 0.22.0 WordPress installation Now you should switch to user of your web application, ex. like this:\nsu - www-data -s /bin/bash And install WP:\nwp core download --path=/var/www/wordpress wp core config --path=/var/www/wordpress \\ --dbname=wordpress \\ --dbuser=wordpress \\ --dbpass=wordpresspass \\ --dbhost=localhost \\ --locale=pl_PL wp core install --path=/var/www/wordpress \\ --url=\u0026#34;http://example.com\u0026#34; \\ --title=\u0026#34;Example blog\u0026#34; \\ --admin_user=never_use_admin_here \\ --admin_password=admin_pass \\ --admin_email=admin@example.com \\ --skip-email Here you may find more about wp-cli configuration and commands.\n","permalink":"https://timor.site/2016/02/install-wordpress-from-command-line/","summary":"I never tried it before but today I needed to install WordPress\u0026hellip; From command line only. And there is a way to do this with wp-cli.\nWP-CLI installation First some requirements (as root):\napt-get install php5-cli php5-mysql mysql-client curl And now installation of wp-cli (as root too):\ncurl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar mv wp-cli.phar /usr/local/bin/wp Check if it\u0026rsquo;s working:\n$ wp --version WP-CLI 0.22.0 WordPress installation Now you should switch to user of your web application, ex.","title":"Install WordPress from command-line"},{"content":"When I started playing with Docker I was running a lot of commands to build image, delete containers running on old image, run containers based on new image, etc\u0026hellip; A lot of log commands with links, volumes, etc\u0026hellip;\nThen I started searching for something to automate this task and here I get to docker-compse command, this is how you may install it:\npip install docker-compose And install additional bash completions (run as root):\ncurl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/bash/docker-compose \u0026gt; /etc/bash_completion.d/docker-compose Then you may prepare docker-compose.yml file like:\nweb: build: . command: php -S 0.0.0.0:8000 -t /code ports: - \u0026#34;8000:8000\u0026#34; links: - db volumes: - .:/code db: image: orchardup/mysql environment: MYSQL_DATABASE: wordpress More informations about syntax may be found here: https://docs.docker.com/compose/compose-file/\nAnd run such environment with:\ndocker-compose up Or to run this in background:\ndocker-compose up -d To stop and cleanup it use:\ndocker-compose stop \u0026amp;\u0026amp; docker-compose rm -f -v Other usable commands are:\ndocker-compose build --force-rm # to rebuild images and clean after docker-compose ps # to list containers I\u0026rsquo;m still playing with volumes in this but don\u0026rsquo;t have anything interesting enough to paste here - maybe later.\nSources https://docs.docker.com/compose/install/\nhttps://docs.docker.com/compose/completion/\n","permalink":"https://timor.site/2016/02/install-docker-compose/","summary":"When I started playing with Docker I was running a lot of commands to build image, delete containers running on old image, run containers based on new image, etc\u0026hellip; A lot of log commands with links, volumes, etc\u0026hellip;\nThen I started searching for something to automate this task and here I get to docker-compse command, this is how you may install it:\npip install docker-compose And install additional bash completions (run as root):","title":"Install Docker Compose"},{"content":"I\u0026rsquo;ve played with Docker a little in it early days but didn\u0026rsquo;t stick for longer with it. It\u0026rsquo;s stable now so I wanted to check how it\u0026rsquo;s running now.\nI really can\u0026rsquo;t accept this method of installation:\ncurl -fsSL https://get.docker.com/ | sh I think that world is going to it\u0026rsquo;s end when I see such scritps\u0026hellip; I prefer to do this manually, knowing exactly what I have to do.\nInstall prerequisites:\napt-get update apt-get install -y apt-transport-https ca-certificates Purge old packages if you used them:\napt-get purge lxc-docker* apt-get purge docker.io* Add GPG key:\napt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add repo - use ONLY ONE repo appropriate for your system (lsb_release -a to check):\necho \u0026#34;deb https://apt.dockerproject.org/repo debian-wheezy main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-jessie main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-stretch main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-precise main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-trusty main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list Refresh repos and install Docker:\napt-get update apt-get install -y docker-engine Start service if it\u0026rsquo;s not running:\nservice docker start Grant access to docker service for non-root user I don\u0026rsquo;t like to use apps that require me to use root account. Docker even advice not to do so - service is running as root and you should add docker group and user to it to grant access to service socket:\ngroupadd docker gpasswd -a ${USER} docker service docker restart Now logout, login again and and you should be able to use docker command:\ndocker version docker info docker run hello-world Have fun üòÉ\nSources https://docs.docker.com/linux/\nhttps://docs.docker.com/engine/installation/\n","permalink":"https://timor.site/2016/02/manual-installation-of-docker-on-debian-ubuntu/","summary":"I\u0026rsquo;ve played with Docker a little in it early days but didn\u0026rsquo;t stick for longer with it. It\u0026rsquo;s stable now so I wanted to check how it\u0026rsquo;s running now.\nI really can\u0026rsquo;t accept this method of installation:\ncurl -fsSL https://get.docker.com/ | sh I think that world is going to it\u0026rsquo;s end when I see such scritps\u0026hellip; I prefer to do this manually, knowing exactly what I have to do.\nInstall prerequisites:","title":"Manual installation of Docker on Debian/Ubuntu"},{"content":"I started playing with Docker and here I will write some commands that where not so obvious at beginning üòÉ\nList running containers:\ndocker ps List also not running containers:\ndocker ps -a Remove all containers (be careful with that):\ndocker rm $(docker ps -a -q) Remove all images:\ndocker rmi $(docker images -q) Docker won\u0026rsquo;t remove any old volumes used by containers, so after some time you may be interested in deleting them all:\ndocker volume rm $(docker volume ls -q) Run container and enter bash:\ndocker run --name deb -t -i debian:jessie /bin/bash Show build logs from container:\ndocker logs deb Enter bash into running container:\ndocker exec -it deb /bin/bash Build image from Dockerfile in current directory:\ndocker build -t my_web . After playing a little with this command I started searching for something to automate my tasks a little, and found docker-compose - you may be interested in it too.\nSources https://techoverflow.net/blog/2013/10/22/docker-remove-all-images-and-containers/\n","permalink":"https://timor.site/2016/02/some-useful-commands-in-docker/","summary":"I started playing with Docker and here I will write some commands that where not so obvious at beginning üòÉ\nList running containers:\ndocker ps List also not running containers:\ndocker ps -a Remove all containers (be careful with that):\ndocker rm $(docker ps -a -q) Remove all images:\ndocker rmi $(docker images -q) Docker won\u0026rsquo;t remove any old volumes used by containers, so after some time you may be interested in deleting them all:","title":"Some useful commands in Docker"},{"content":" RubyWzorce projektowe\nAuthor: Russ Olsen\nhelion.pl ","permalink":"https://timor.site/books/2016/ruby-wzorce-projektowe/","summary":" RubyWzorce projektowe\nAuthor: Russ Olsen\nhelion.pl ","title":"Ruby"},{"content":" Sztuka podstƒôpu≈Åama≈Çem ludzi, nie has≈Ça\nAuthors: Kevin Mitnick, William L. Simon\nhelion.pl ","permalink":"https://timor.site/books/2016/sztuka-podstepu/","summary":" Sztuka podstƒôpu≈Åama≈Çem ludzi, nie has≈Ça\nAuthors: Kevin Mitnick, William L. Simon\nhelion.pl ","title":"Sztuka podstƒôpu"},{"content":"I was doing a lot of changes to my old posts, switched to HTTPS, etc. Sometimes it was useful to change some particular text in all my old posts at a time, but there is no such feature in WordPress. But WordPress runs on MySQL and I could use SQL query to update such posts.\nMake backup - it\u0026rsquo;s not required but strongly advised üòÉ\nNow use this query as template to replace in place whatever you need:\ne\u003eYou should see something like:\nQuery OK, 157 rows affected (0.04 sec) Rows matched: 455 Changed: 157 Warnings: 0 This will remove \u0026lt;!--more--\u0026gt;from all posts (it\u0026rsquo;s used by WordPress to span article when showed on tag/category pages).\nAnother example to update all URLs to HTTPS:\nUPDATE wp_posts SET post_content = REPLACE(post_content, \u0026#34;http://gagor.pl\u0026#34;, \u0026#34;https://gagor.pl\u0026#34;); Be careful with that and make DB backup before you start.\n","permalink":"https://timor.site/2016/02/mass-replace-in-wordpress-posts-via-mysql-query/","summary":"\u003cp\u003eI was doing a lot of changes to my old posts, switched to HTTPS, etc. Sometimes it was useful to change some particular text in all my old posts at a time, but there is no such feature in WordPress. But WordPress runs on MySQL and I could use SQL query to update such posts.\u003c/p\u003e\n\u003cp\u003eMake backup - it\u0026rsquo;s not required but strongly advised üòÉ\u003c/p\u003e\n\u003cp\u003eNow use this query as template to replace in place whatever you need:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e","title":"Mass replace in WordPress posts via MySQL query"},{"content":"From few days I have access to UPC\u0026rsquo;s www.horizon.tv platform - until now it was useless on Linux. But there is Pipelight that will use Wine to emulate Silverlight on Linux and it\u0026rsquo;s working pretty well - you\u0026rsquo;re just few commands away from achieving that:\n# stop browser killall firefox # remove old version if you have it sudo apt-get remove pipelight Now configure repos and install packages:\nsudo apt-add-repository ppa:pipelight/stable sudo apt-get update sudo apt-get install --install-recommends pipelight-multi sudo pipelight-plugin --update Enable plugin (run it with sudo for system wide installation):\npipelight-plugin --enable silverlight Start Firefox and test if plugin is working here: http://bubblemark.com/silverlight2.html\nNow enter www.horizon.tv and try it yourself.\nP.S. It works only on Firefox because Chrome do not support NPAPI plugins anymore üòÉ\nSources http://www.webupd8.org/2013/08/pipelight-use-silverlight-in-your-linux.html\n","permalink":"https://timor.site/2016/02/use-www-horizon-tv-with-pipelight-silverlight-on-linux-ubuntu/","summary":"From few days I have access to UPC\u0026rsquo;s www.horizon.tv platform - until now it was useless on Linux. But there is Pipelight that will use Wine to emulate Silverlight on Linux and it\u0026rsquo;s working pretty well - you\u0026rsquo;re just few commands away from achieving that:\n# stop browser killall firefox # remove old version if you have it sudo apt-get remove pipelight Now configure repos and install packages:\nsudo apt-add-repository ppa:pipelight/stable sudo apt-get update sudo apt-get install --install-recommends pipelight-multi sudo pipelight-plugin --update Enable plugin (run it with sudo for system wide installation):","title":"Use www.horizon.tv with Pipelight/Silverlight on Linux/Ubuntu"},{"content":"I just bought new wifi card for my desktop computer. Like in topic, it\u0026rsquo;s Intel Dual Band Wireless-AC 7260 for Desktop.\nI was searching for card that:\nsupport AC standard have 5GHz network support (2,4GHz channels are cluttered heavily in my neighborhood have PCI/PCIx or USB3 connector is Linux friendly (no modules compilation by hand, support for aircrack-ng, kismet) This one is the only I found that comply my expectations.\nI found time to play with kismet and aircrack-ng and it was working fine. Card works without problems on kernel 4.2.0. Highest transfer on my net I could get from my NAS - about 23 MB/s (megabytes per second) - much better than on my old N router (approx 6,5 MB/s).\nHere\u0026rsquo;s information from lspci -vvv:\n05:00.0 Network controller: Intel Corporation Wireless 7260 (rev 73) Subsystem: Intel Corporation Dual Band Wireless-AC 7260 Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast \u0026gt;TAbort- \u0026lt;TAbort- \u0026lt;MAbort- \u0026gt;SERR- \u0026lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 39 Region 0: Memory at f7c00000 (64-bit, non-prefetchable) [size=8K] Capabilities: [c8] Power Management version 3 Flags: PMEClk- DSI+ D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold+) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [d0] MSI: Enable+ Count=1/1 Maskable- 64bit+ Address: 00000000fee0400c Data: 4123 Capabilities: [40] Express (v2) Endpoint, MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0, Latency L0s \u0026lt;512ns, L1 unlimited ExtTag- AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd- ExtTag- PhantFunc- AuxPwr+ NoSnoop+ FLReset- MaxPayload 128 bytes, MaxReadReq 128 bytes DevSta: CorrErr+ UncorrErr- FatalErr- UnsuppReq- AuxPwr+ TransPend- LnkCap: Port #0, Speed 2.5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s \u0026lt;4us, L1 \u0026lt;32us ClockPM+ Surprise- LLActRep- BwNot- ASPMOptComp- LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+ ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed 2.5GT/s, Width x1, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt- DevCap2: Completion Timeout: Range B, TimeoutDis+, LTR+, OBFF Via WAKE# DevCtl2: Completion Timeout: 16ms to 55ms, TimeoutDis-, LTR-, OBFF Disabled LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis- Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS- Compliance De-emphasis: -6dB LnkSta2: Current De-emphasis Level: -3.5dB, EqualizationComplete-, EqualizationPhase1- EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest- Capabilities: [100 v1] Advanced Error Reporting UESta: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UEMsk: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol- CESta: RxErr- BadTLP+ BadDLLP- Rollover- Timeout- NonFatalErr+ CEMsk: RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr+ AERCap: First Error Pointer: 00, GenCap- CGenEn- ChkCap- ChkEn- Capabilities: [140 v1] Device Serial Number 7c-5c-f8-ff-xx-xx-xx-xx Capabilities: [14c v1] Latency Tolerance Reporting Max snoop latency: 0ns Max no snoop latency: 0ns Capabilities: [154 v1] Vendor Specific Information: ID=cafe Rev=1 Len=014 \u0026lt;?\u0026gt; Kernel driver in use: iwlwifi And iwconfig:\nwlp5s0 IEEE 802.11abgn ESSID:\u0026#34;cis5\u0026#34; Mode:Managed Frequency:5.5 GHz Access Point: 34:7A:60:XX:XX:XX Bit Rate=780 Mb/s Tx-Power=22 dBm Retry short limit:7 RTS thr:off Fragment thr:off Power Management:off Link Quality=60/70 Signal level=-50 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:7 Invalid misc:184 Missed beacon:0 If you\u0026rsquo;re thinking about buying any Linux friendly AC wifi card - this one is worth it\u0026rsquo;s price.\n","permalink":"https://timor.site/2016/02/intel-dual-band-wireless-ac-7260-for-desktop-on-linux/","summary":"I just bought new wifi card for my desktop computer. Like in topic, it\u0026rsquo;s Intel Dual Band Wireless-AC 7260 for Desktop.\nI was searching for card that:\nsupport AC standard have 5GHz network support (2,4GHz channels are cluttered heavily in my neighborhood have PCI/PCIx or USB3 connector is Linux friendly (no modules compilation by hand, support for aircrack-ng, kismet) This one is the only I found that comply my expectations.","title":"Intel Dual Band Wireless-AC 7260 for Desktop on Linux"},{"content":"I watched nice presentation about how Cloudflare protects itself against DoS. Most of us are not able to do that exactly like them but some of tips were general enough to be used on typical web front server.\nI took notes from this presentation and presented here. Thanks to Marek agreement I also reposted all examples (in easier to copy paste way).\nHowto prepare against ACK/FIN/RST/X-mas flood Use conntrack rule:\niptables -A INPUT --dst 1.2.3.4 -m conntrack --ctstate INVALID -j DROP which will only work with disabled tcp_loose setting (it\u0026rsquo;s by default enabled) with addition to sysctl:\nsysctl -w net.netfilter.nf_conntrack_tcp_loose=0 Howto prepare against SYN floods SYN flood is hard case - because when you use conntrack it will make your performance worst validating state for every new single packet.\nThe only way to get around this is to enable syncookies:\nsysctl -w net.ipv4.tcp_syncookies=1 sysctl -w net.ipv4.tcp_timestamps=1 Enabling syncookies will cause loose of some of connection informations, that are pretty useful like:\nwindow scaling factor ECN bit (Explicit Congestion Notification) For that we will use tcp_timestamp option, that will use few bits from timestamp field to store some of this informations.\nThis still may be not efficient enough, but in kernel 4.4 there will be some update to how syncookies are served that should make it few times faster than with older one.\nRelated docs: https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\nHowto prepare against botnet attack Symptoms:\nconcurrent connection count going up many sockets in orphaned state many sockets in time wait state Solutions:\nEnable connlimit feature on conntrack to limit amount of concurrent connections to our service Use hashlimits to rate limit SYN packets per IP Use ipsets to efficiently block many IP/subnet addresses manual blacklisting - feed IP blacklist from HTTP server logs supports subnets, timeouts automatic blacklisting hashlimits Disable HTTP keep-alives to make this attack look more like SYN flood This may still not work against DDoS because huge amount of bots won\u0026rsquo;t allow you to block them efficiently enough.\nSome exciting system tweaks and examples from this presentation I hope to find some time to merge them into template/script that could be used much easier - but first I have to play with these rules a little and test what will be most useful.\nNIC: Discard with flow steering ethtool -N eth3 flow-type udp4 dst-ip 129.168.254.30 dst-port 53 action -1 Flow steering for priority ethtool -X eth3 weight 0 1 1 1 1 1 1 1 1 1 1 ethtool -N eth3 flow-type tcp4 dst-port 22 action 0 SYN backlog size sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.tcp_max_syn_backlog=65535 It\u0026rsquo;s rounded to next power of two (in this case to 65536).\nSYN backlog decay sysctl -w net.ipv4.tcp_synack_retries=1 L7 connection count sysctl -w net.ipv4.tcp_max_orphans=262144 sysctl -w net.ipv4.tcp_orphan_retries=1 sysctl -w net.ipv4.tcp_max_tw_buckets=360000 sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w net.ipv4.tcp_fin_timeout=5 L3: u32 iptables -A INPUT \\ --dst 1.2.3.4 \\ --p udp -m udp --dport 53 \\ -m u32 --u32 \u0026#34;6\u0026amp;0xFF=0x6 \u0026amp;\u0026amp; 4\u0026amp;0x1FFF=0 \u0026amp;\u0026amp; 0\u0026gt;\u0026gt;22\u0026amp;0x3C@4=0x29\u0026#34; \\ -j DROP L4: Conntrack iptables -t raw -A PREROUTING \\ -i eth2 \\ --dst 1.2.3.4 \\ -j ACCEPT iptables -t raw -A PREROUTING \\ -i eth2 \\ -j NOTRACK iptables -A INPUT \\ --dst 1.2.3.4 \\ -m conntrack --ctstate INVALID \\ -j DROP Tuning conntrack sysctl -w net.netfilter.nf_conntrack_tcp_loose=0 sysctl -w net.netfilter.nf_conntrack_helper=0 sysctl -w net.nf_conntrack_max=2000000 echo 2500000 \u0026gt; /sys/module/nf_conntrack/parameters/hashsize More info about conntrack sysctl options: https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt\nL7: Connlimit iptables -t raw -A PREROUTING \\ -i eth2 \\ --dst 1.2.4.5 \\ -j ACCEPT iptables -A INPUT \\ --dst 1.2.3.4 \\ -p tcp -m tcp --dport 80 \\ -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN \\ -m connlimit \\ --connlimit-above 10 \\ --connlimit-mask 32 \\ --connlimit-saddr \\ -j DROP L7: ipset for blacklisting ipset -exist create ta_d335c5 hash:net family inet ipset add ta_d335c5 192.168.0.0/16 ipset add ta_d335c5 10.0.0/8 iptables -A INPUT \\ -m set --match-set ta_d335c5 src \\ -j DROP L7: being evil - TARPIT iptables -A INPUT \\ -m set --match-set ta_d335c5 src \\ -j TARPIT TARPIT target will imitate successful connection for the client (bot in this case) but without responding to it\u0026rsquo;s queries. It will cost that bot a lot more resources and time to timeout and drop this connection than when using DROP or REJECT here.\nL7: hashlimit for rate limiting iptables -A INPUT \\ --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\ --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\ -m hashlimit \\ --hashlimi-above 123/sec \\ --hashlimit-burst 5 \\ --hashlimit-mode srcip \\ --hashlimit-srcmask 24 \\ --hashlimit-name 341654b1d4af9bf \\ -j DROP L7: auto-blacklist ipset -exist create blacklist hash:net timeout 60 iptables -A INPUT \\ --dst 1.2.3.4 \\ -m set --match-set blaclist src \\ -j DROP iptables -A INPUT \\ --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\ --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\ -m hashlimit \\ --hashlimit-above 100/sec \\ --hashlimit-mode srcip \\ --hashlimit-srcmask 24 \\ --hashlimit-name hl_blacklist \\ -j SET --add-set blacklist src L7+: payload in TCP - string iptables -A INPUT \\ --dst 1.2.3.4 \\ -p tcp --dport 80 \\ -m string \\ --hex-string 486f737777777777... \\ --from 231 --to 300 \\ -j DROP For more informations and explanations watch this great presentation:\nAnd here is the whole presentation with additional examples:\nhttps://speakerdeck.com/majek04/lessons-from-defending-the-indefensible\n","permalink":"https://timor.site/2016/02/prepare-for-dos-like-cloudflare-do/","summary":"I watched nice presentation about how Cloudflare protects itself against DoS. Most of us are not able to do that exactly like them but some of tips were general enough to be used on typical web front server.\nI took notes from this presentation and presented here. Thanks to Marek agreement I also reposted all examples (in easier to copy paste way).\nHowto prepare against ACK/FIN/RST/X-mas flood Use conntrack rule:","title":"Prepare for DoS like Cloudflare do"},{"content":"I was looking at backup task running on my desk and saw that it\u0026rsquo;s spending a lot of time on ~/.local/share/zeitgeist directory. I checked and it had 4.6GB:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,4G activity.sqlite-wal 311M fts.index WTF? Fortunately I found here that I could easily delete some of this:\nzeitgeist-daemon --quit Now check that it\u0026rsquo;s not running:\nps axu | grep zeitgeist-daemon timor 9105 0.0 0.0 10756 2140 pts/5 S+ 19:01 0:00 grep --color=auto zeitgeist-daemon Nothing there so we may start deleting:\ncd ~/.local/share/zeitgeist rm -r fts.index rm activity.sqlite-wal rm activity.sqlite-shm Start zeitgeist-daemon again:\nzeitgeist-daemon --replace\u0026amp; Now check files sizes again:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,0K activity.sqlite-wal 640K fts.index Much better üòÉ\n","permalink":"https://timor.site/2016/02/zeitgeist-activity-sqlite-wal-getting-huge/","summary":"I was looking at backup task running on my desk and saw that it\u0026rsquo;s spending a lot of time on ~/.local/share/zeitgeist directory. I checked and it had 4.6GB:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,4G activity.sqlite-wal 311M fts.index WTF? Fortunately I found here that I could easily delete some of this:\nzeitgeist-daemon --quit Now check that it\u0026rsquo;s not running:\nps axu | grep zeitgeist-daemon timor 9105 0.0 0.","title":"Zeitgeist activity.sqlite-wal getting huge"},{"content":"There are many possible real life cases and not all optimization technics will be suitable for you but I hope it will be a good starting place.\nAlso you shouldn\u0026rsquo;t copy paste examples with faith that they will make your server fly üòÉ You have to support your decisions with excessive tests and help of monitoring system (ex. Grafana).\nCache static and dynamic content Setting caching static and dynamic content strategy may offload your server from additional load from repetitive downloads of same, rarely updated files. This will make your site to load faster for frequent visitors.\nExample configuration:\nlocation ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf|m4v)$ { access_log off; log_not_found off; tcp_nodelay off; open_file_cache max=500 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; expires max; } For additional performance gain, you may:\ndisable logging for static files, disable tcp_nodelay option - it\u0026rsquo;s useful to send a lot of small files (ideally smaller than single TCP packet - 1,5Kb), but images are rather big files and sending them all together will gain better performance, play with open_file_cache - it will take off some IO load, add long long expires. Caching dynamic content is harder case. There are articles that are rarely updated and they may lay in cache forever but other pages are pretty dynamic and shouldn\u0026rsquo;t be cached for long. Even if caching dynamic content sounds scary for you it\u0026rsquo;s not. So called micro caching (caching for short period of time, like 1s) - is great solution for digg effect or slashdotting.\nLet say your page gets ten views per second and you will cache ever site for 1s, then you will be able to server 90% of requests from cache. Leaving precious CPU cycles for other tasks.\nCompress data On your page you should use filetypes that are efficiently compressed like: JPEG, PNG, MP3, etc. But all HTML, CSS, JS may be compressed too on the fly by web server, just enable options like that globally:\ngzip on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml image/svg+xml; You may also precompress these files stronger during build/deploy process and use gzip_static module to serve them without additional overhead for compression. Ex.:\ngzip_static on; Then use script like this to compress files:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; Files have to had same timestamp like original (not compressed) file to be used by Nginx.\nOptimize SSL/TLS New optimized versions of HTTP protocols like HTTP/2 or SPDY require HTTPS configuration (at least in browsers implementation). Then SSL/TLS high cost of every new HTTPS connection became crucial case for further optimizations.\nThere are few steps required for improved SSL/TLS performance.\nEnable SSL session caching Use ssl_session_cache directive to cache parameters used when securing each new connection, ex.:\nssl_session_cache builtin:1000 shared:SSL:10m; Enable SSL session tickets Tickets store information about specific SSL/TLS connection so connection may be reused without new handshake, ex.:\nssl_session_tickets on; Configure OCSP stapling for SSL This will lower handshaking time by caching SSL/TLS certificate informations. This is per site/certificate configuration, ex.:\nssl_stapling on; ssl_stapling_verify on; ssl_certificate /etc/ssl/certs/my_site_cert.crt; ssl_certificate_key /etc/ssl/private/my_site_key.key; ssl_trusted_certificate /etc/ssl/certs/authority_cert.pem; A ssl_trusted_certificate file have to point to trusted certificate chain file - root + intermediate certificates (this can be downloaded from your certificate provider site (sometimes you have to merge by yourself those files).\nExcessive article in this topic could be found here: https://raymii.org/s/tutorials/OCSP_Stapling_on_nginx.html\nImplement HTTP/2 or SPDY If you have HTTPS configured the only thing you have to do is to add two options on listen directive, ex.:\nlisten 443 ssl http2; # currently http2 is preferred against spdy; # on SSL enabled vhost ssl on; You may also advertise for HTTP connection that you have newer protocol available, for that on HTTP connections use this header:\nadd_header Alternate-Protocol 443:npn-spdy/3; SPDY and HTTP/2 protocols use:\nheaders compression, single, multiplexed connection (carrying pieces of multiple requests and responses at the same time) rather than multiple connection for every piece of web page. After SPDY or HTTP/2 implementation you no longer need typical HTTP/1.1 optimizations like:\ndomain sharding, resource (JS/CSS) merging, image sprites. Tune other nginx performance options Access logs Disable access logs were you don\u0026rsquo;t need them, ex.: for static files. You may also use buffer and flush options with access_log directive, ex.:\naccess_log /var/log/nginx/access.log buffer=1m flush=10s; With buffer Nginx will hold that much data in memory before writing it to disk. flush tells Nginx how often it should write gathered logs to disk.\nProxy buffering Turning proxy buffering may impact performance of your reverse proxy.\nNormally when buffering is disabled, Nginx will pass response directly to client synchronously.\nWhen buffering is enable it will store response in memory set by proxy_buffer_size option and if response is too big it will be stored in temporary file.\nproxy_buffering on; proxy_buffer_size 16k; Keepalive for client and upstream connections] Every new connection costs some time for handshake and will add latency to requests. By using keepalive connections will be reused without this overhead.\nFor client connections:\nkeepalive_timeout = 120s; For upstream connections:\nupstream web_backend { server 127.0.0.1:80; server 10.0.0.2:80; keepalive 32; } Limit connections to some resources Some time users/bots overload your service by querying it to fast. You may limit allowed connections to protect your service in such case, ex.:\nlimit_conn_zone $binary_remote_addr zone=owncloud:1m; server { # ... limit_conn owncloud 10; # ... } Adjust woker count Normally Nginx will start with only 1 worker process, you should adjust this variable to at the number of CPU\u0026rsquo;s, in case of quad core CPU use in main section:\nworker_processes 4; Use socket sharding In latest kernel and Nginx versions (at least 1.9.1) there is new feature of sockets sharding. This will offload management of new connections to kernel. Each worker will create a socket listener and kernel will assign new connections to them as they become available.\nlisten 80 reuseport; Thread pools Thread pools are solution for mostly long blocking IO operations that may block whole Nginx event queue (ex. when used with big files or slow storage).\nlocation / { root /storage; aio threads; } This will help a lot if you see many Nginx processes in D state, with high IO wait times.\nTune Linux for performance Backlog queue If you could see on your system connection that appear to be staling then you have to increase net.core.somaxconn. This system parameter describes the maximum number of backlogged sockets. Default is 128 so setting this to 1024 should be no big deal on any decent machine.\necho \u0026#34;net.core.somaxconn=1024\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf File descriptors If your system is serving a lot of connections you may get reach system wide open descriptor limit. Nginx uses up to two descriptors for each connection. Then you have to increase sys.fs.fs_max.\necho \u0026#34;sys.fs.fs_max=3191256\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf Ephemeral ports Nginx used as a proxy creates temporary (ephemeral) ports for each upstream server. On busy proxy servers this will result in many connection in TIME_WAIT state.\nSolution for that is to increase range of available ports by setting net.ipv4.ip_local_port_range. You may also benefit from lowering net.ipv4.tcp_fin_timeout setting (connection will be released faster, but be careful with that).\nUse reverse-proxy This with microcaching technic is worth separate article, I will add link here when it will be ready.\nSource: http://www.fromdual.com/huge-amount-of-time-wait-connections https://www.nginx.com/blog/10-tips-for-10x-application-performance/ https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/ https://www.nginx.com/blog/thread-pools-boost-performance-9x/ https://tweaked.io/guide/kernel/ https://t37.net/nginx-optimization-understanding-sendfile-tcp_nodelay-and-tcp_nopush.html ","permalink":"https://timor.site/2016/01/optimize-nginx-for-performance/","summary":"There are many possible real life cases and not all optimization technics will be suitable for you but I hope it will be a good starting place.\nAlso you shouldn\u0026rsquo;t copy paste examples with faith that they will make your server fly üòÉ You have to support your decisions with excessive tests and help of monitoring system (ex. Grafana).\nCache static and dynamic content Setting caching static and dynamic content strategy may offload your server from additional load from repetitive downloads of same, rarely updated files.","title":"Optimize Nginx for performance"},{"content":"Sometime you need to make quick and dirty image backup of VM running on XenServer and this post is about such case üòÉ\nList machines:\nxl list Name ID Mem VCPUs State Time(s) Domain-0 0 4066 8 r----- 3526567.3 webfront1.example.com 1 4096 4 r----- 3186487.2 webfront2.example.com 2 2048 2 -b---- 920408.2 Now you may export one:\nxe vm-export vm=webfront1.example.com filename=/srv/backup/webfront.xva Export succeeded You may also use uuid for that - list machines with xe vm-list (best with less) and then:\nxe vm-export uuid=1234a43d-c5af-f1ef-b3c1-12347f63d84c filename=/srv/backup/webfront.xva That\u0026rsquo;s all!\n","permalink":"https://timor.site/2016/01/xenserver-export-vm-to-file/","summary":"Sometime you need to make quick and dirty image backup of VM running on XenServer and this post is about such case üòÉ\nList machines:\nxl list Name ID Mem VCPUs State Time(s) Domain-0 0 4066 8 r----- 3526567.3 webfront1.example.com 1 4096 4 r----- 3186487.2 webfront2.example.com 2 2048 2 -b---- 920408.2 Now you may export one:\nxe vm-export vm=webfront1.example.com filename=/srv/backup/webfront.xva Export succeeded You may also use uuid for that - list machines with xe vm-list (best with less) and then:","title":"XenServer - export VM to file"},{"content":"Sometimes deployment process or other havy task may cause some Nagios checks to rise below normal levels and bother admin. If this is expected and you want to add downtime on host/service during this task you may use this script:\n#!/bin/bash function die { echo $1; exit 1; } if [[ $# -eq 0 ]] ; then die \u0026#34;Give hostname and time in minutes as parameter!\u0026#34; fi if [[ $# -eq 1 ]] ; then MINUTES=15 else MINUTES=$2 fi HOST=$1 NAGURL=http://nagios.example.com/nagios/cgi-bin/cmd.cgi USER=nagiosuser PASS=nagiospassword SERVICENAME=someservice COMMENT=\u0026#34;Deploying new code\u0026#34; export MINUTES echo \u0026#34;Scheduling downtime on $HOST for $MINUTES minutes...\u0026#34; # The following is urlencoded already STARTDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34;` # This gives us the date/time X minutes from now ENDDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34; -d \u0026#34;$MINUTES min\u0026#34;` curl --silent --show-error \\ --data cmd_typ=56 \\ --data cmd_mod=2 \\ --data host=$HOST \\ --data-urlencode \u0026#34;service=$SERVICENAME\u0026#34; \\ --data-urlencode \u0026#34;com_data=$COMMENT\u0026#34; \\ --data trigger=0 \\ --data-urlencode \u0026#34;start_time=$STARTDATE\u0026#34; \\ --data-urlencode \u0026#34;end_time=$ENDDATE\u0026#34; \\ --data fixed=1 \\ --data hours=2 \\ --data minutes=0 \\ --data btnSubmit=Commit \\ --insecure \\ $NAGURL -u \u0026#34;$USER:$PASS\u0026#34;| grep -q \u0026#34;Your command request was successfully submitted to Nagios for processing.\u0026#34; || die \u0026#34;Failed to con tact nagios\u0026#34;; echo Scheduled downtime on nagios from $STARTDATE to $ENDDATE Threat this script as template with some tips:\nI you want to add downtime on service, then provide SERVICENAME and --data cmd_typ=56 \\. If you want downtime on whole host, just remove this line: --data-urlencode \u0026quot;service=$SERVICENAME\u0026quot; \\ and --data cmd_typ=86 \\ Another thing that in my example nagios page use basic auth for security, if your don\u0026rsquo;t use it, you may remove -u \u0026quot;$USER:$PASS\u0026quot; from parameters. If you get Start or end time not valid, then you have to adapt dates to your formats of dates accepted by Nagios (probably this depends on Nagios version or timezone configuration). Sources http://stackoverflow.com/questions/6842683/how-to-set-downtime-for-any-specific-nagios-host-for-certain-time-from-commandli\n","permalink":"https://timor.site/2016/01/nagios-downtime-on-hostservice-from-command-line-with-curl/","summary":"Sometimes deployment process or other havy task may cause some Nagios checks to rise below normal levels and bother admin. If this is expected and you want to add downtime on host/service during this task you may use this script:\n#!/bin/bash function die { echo $1; exit 1; } if [[ $# -eq 0 ]] ; then die \u0026#34;Give hostname and time in minutes as parameter!\u0026#34; fi if [[ $# -eq 1 ]] ; then MINUTES=15 else MINUTES=$2 fi HOST=$1 NAGURL=http://nagios.","title":"Nagios - downtime on host/service from command line with curl"},{"content":"Now when you have CollectD and InfluxDB installed you may configure Grafana üòÉ\nFirst configure repo with current Grafana version (select your distro):\ncurl https://packagecloud.io/gpg.key | sudo apt-key add - deb https://packagecloud.io/grafana/testing/debian/ wheezy main Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get update apt-get install -y apt-transport-https apt-get install -y grafana By default Grafana will use sqlite database to keep information about users, etc:\n[database] # Either \u0026#34;mysql\u0026#34;, \u0026#34;postgres\u0026#34; or \u0026#34;sqlite3\u0026#34;, it\u0026#39;s your choice ;type = sqlite3 ;host = 127.0.0.1:3306 ;name = grafana ;user = root ;password = If that\u0026rsquo;s ok for you, you may leave it as is. I prefer to configure MySQL database (create user, database, grant permissions to user):\n[database] type = mysql host = 127.0.0.1:3306 name = grafana user = grafana password = mydbpassword So Grafana should be running on port 3000 by default, now it\u0026rsquo;s time to connect ex.: http://localhost:3000 (use your host). Now click Data sources on left panel, then Add new on top panel and fill source data like below:\nBecause we didn\u0026rsquo;t set authorization for InfluxDB you may just type whatever login/password there. Now Test Connection and Save and you should be ready to play with Grafana.\nI also used scripted dashboard for Grafana to add easily statistics for my hosts, you may find it here: https://github.com/anryko/grafana-influx-dashboard\nSources http://docs.grafana.org/installation/debian/\n","permalink":"https://timor.site/2016/01/grafana-installation-and-configuraton-with-influxdb-and-collectd-on-debian-ubuntu/","summary":"Now when you have CollectD and InfluxDB installed you may configure Grafana üòÉ\nFirst configure repo with current Grafana version (select your distro):\ncurl https://packagecloud.io/gpg.key | sudo apt-key add - deb https://packagecloud.io/grafana/testing/debian/ wheezy main Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get update apt-get install -y apt-transport-https apt-get install -y grafana By default Grafana will use sqlite database to keep information about users, etc:","title":"Grafana - installation and configuraton with InfluxDB and CollectD on Debian/Ubuntu"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then‚Ä¶ I found InfluxDB. Project is young but looks promising.\nLet\u0026rsquo;s start! On project page there is no info about repo but it\u0026rsquo;s available, configure it:\ncurl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/debian wheezy stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf for Ubuntu use url like (of course selecting your version):\necho \u0026#34;deb https://repos.influxdata.com/ubuntu wily stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get install -y apt-transport-https apt-get install -y influxdb Now edit /etc/influxdb/influxdb.conf and uncoment/fill [collectd] section like this:\n[collectd] enabled = true bind-address = \u0026#34;:8096\u0026#34; database = \u0026#34;collectd_db\u0026#34; typesdb = \u0026#34;/usr/share/collectd/types.db\u0026#34; You may adjust port to whatever suits you best. database sets InfluxDB database used to store collectd data, and typesdb is file from collectd package defining collectd metrics structure (this is location for Debian) - so you have collectd service installed earlier.\nNow you may check if InfluxDB is working fine by connecting to web admin panel, by standard on port 8083.\nSources https://github.com/influxdata/influxdb/issues/585\u0026quot;\nhttps://anomaly.io/collectd-metrics-to-influxdb/\n","permalink":"https://timor.site/2016/01/influxdb-installation-and-configuration-on-debianubuntu/","summary":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then‚Ä¶ I found InfluxDB. Project is young but looks promising.\nLet\u0026rsquo;s start! On project page there is no info about repo but it\u0026rsquo;s available, configure it:\ncurl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/debian wheezy stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf for Ubuntu use url like (of course selecting your version):","title":"InfluxDB - installation and configuration on Debian/Ubuntu"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then‚Ä¶ I found InfluxDB. Project is young but looks promising.\nInstallation of collectd is easy on Debian because packages are in default repo. One problem is that packages may be old, ex. on wheezy it version 5.1. But in backports/backports-sloppy you may find current 5.5, so enable backports first:\necho \u0026#34;deb http://http.debian.net/debian wheezy-backports main contrib non-free\u0026#34; \u0026gt; /etc/apt/sources.list.d/backports.list echo \u0026#34;deb http://http.debian.net/debian wheezy-backports-sloppy main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/backports.list Install package:\napt-get update apt-get install -y -t backports-sloppy collectd collectd-utils # or on recent system just apt-get install -y collectd collectd-utils Now edit configuration /etc/collectd/collectd.conf and add network section:\nLoadPlugin network \u0026lt;Plugin \u0026#34;network\u0026#34;\u0026gt; Server \u0026#34;localhost\u0026#34; \u0026#34;8096\u0026#34; \u0026lt;/Plugin\u0026gt; Use your InfluxDB hostname:port.\nNow select and add enable some plugins - list here and restart service:\nservice collectd restart That\u0026rsquo;s all - now install InfluxDB.\nSources https://anomaly.io/collectd-metrics-to-influxdb/\nhttp://backports.debian.org/Instructions/\n","permalink":"https://timor.site/2016/01/collectd-installation-and-configuration-with-influxdb-on-debianubuntu/","summary":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then‚Ä¶ I found InfluxDB. Project is young but looks promising.\nInstallation of collectd is easy on Debian because packages are in default repo. One problem is that packages may be old, ex. on wheezy it version 5.1. But in backports/backports-sloppy you may find current 5.","title":"CollectD - installation and configuration with InfluxDB on Debian/Ubuntu"},{"content":"From the first moment I heard about Let\u0026rsquo;s Encrypt I liked it and wanted to use it as fast as possible. But the more I read how they want to implement it, the more I dislike it.\nCurrent project with automatic configuration is not what I want to use at all. I have many very complicated configs and I do not trust such tools enough to use them. I like UNIX\u0026rsquo;s single purpose principle, tools should do one thing and do it well - nothing more.\nBut there is one neet tool that use Let\u0026rsquo;s Encrypt API only leaving all configuration for me, it\u0026rsquo;s acme-tiny python based script. I won\u0026rsquo;t copy/paste examples - documentation is written pretty well.\n","permalink":"https://timor.site/2016/01/lets-encrypt-without-auto-configuration/","summary":"From the first moment I heard about Let\u0026rsquo;s Encrypt I liked it and wanted to use it as fast as possible. But the more I read how they want to implement it, the more I dislike it.\nCurrent project with automatic configuration is not what I want to use at all. I have many very complicated configs and I do not trust such tools enough to use them. I like UNIX\u0026rsquo;s single purpose principle, tools should do one thing and do it well - nothing more.","title":"Let‚Äôs Encrypt - without auto configuration"},{"content":"Lately I had a lot of brute force attacks on my WordPress blog. I used basic auth to /wp-admin part in nginx configuration to block this and as a better solution I wan\u0026rsquo;t to block source IPs at all on firewall.\nTo do this, place this filter code in /etc/fail2ban/filter.d/wp-login.conf:\n# WordPress brute force wp-login.php filter: # # Block IPs trying to authenticate in WordPress blog # # Matches e.g. # 178.218.54.109 - - [31/Dec/2015:10:39:34 +0100] \u0026#34;POST /wp-login.php HTTP/1.1\u0026#34; 401 188 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0\u0026#34; # [Definition] failregex = ^\u0026lt;HOST\u0026gt; .* \u0026#34;POST /wp-login.php ignoreregex = Then edit your /etc/fail2ban/jail.local and add:\n[wp-login] enabled = true port = http,https filter = wp-login logpath = /var/log/nginx/access.log maxretry = 3 Now restart fail2ban:\nservice fail2ban restart All done üòÑ\n","permalink":"https://timor.site/2015/12/fail2ban-block-wp-login-php-brute-force-attacks/","summary":"Lately I had a lot of brute force attacks on my WordPress blog. I used basic auth to /wp-admin part in nginx configuration to block this and as a better solution I wan\u0026rsquo;t to block source IPs at all on firewall.\nTo do this, place this filter code in /etc/fail2ban/filter.d/wp-login.conf:\n# WordPress brute force wp-login.php filter: # # Block IPs trying to authenticate in WordPress blog # # Matches e.g. # 178.","title":"fail2ban - block wp-login.php brute force attacks"},{"content":"I have some Ansible roles to configure my vps, Raspberry Pi, etc. I like to test them before I broke something on my real, not clustered machines - I use Vagrant for that.\nBut with it I had one problem - in playbooks I define hosts as groups of severs ex. web for my vps:\nExample Ansible playbook - hosts: web gather_facts: True sudo: True ... But testing machine wasn\u0026rsquo;t in this group and when I run vagrant I could only see:\nAnsible run $ vagrant provision ==\u0026gt; default: Running provisioner: fix-no-tty (shell)... default: Running: inline script ==\u0026gt; default: Running provisioner: ansible... PLAY [web] ******************************************************************** skipping: no hosts matched PLAY RECAP ******************************************************************** To get rid of this I have to add default vagrant machine to my default group in Vagrantfile:\nVagrantfile config.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.groups = { \u0026#34;web\u0026#34; =\u0026gt; [\u0026#34;default\u0026#34;] } ansible.sudo = true ansible.limit = \u0026#34;all\u0026#34; ansible.playbook = \u0026#34;web.yml\u0026#34; end And that solved my problem üòÑ\n","permalink":"https://timor.site/2015/12/ansible-on-vagrant-skipping-no-hosts-matched/","summary":"I have some Ansible roles to configure my vps, Raspberry Pi, etc. I like to test them before I broke something on my real, not clustered machines - I use Vagrant for that.\nBut with it I had one problem - in playbooks I define hosts as groups of severs ex. web for my vps:\nExample Ansible playbook - hosts: web gather_facts: True sudo: True ... But testing machine wasn\u0026rsquo;t in this group and when I run vagrant I could only see:","title":"Ansible on Vagrant - skipping: no hosts matched"},{"content":"Normally you want dynamic content to be fresh and not catchable. But sometimes it may be useful to cache it, like when you have website behind reverse proxy. To do this try something like this:\n\u0026lt;filesmatch \u0026#34;\\.(php|cgi|pl)$\u0026#34;\u0026gt; Header unset Pragma Header unset Expires Header set Cache-Control \u0026#34;max-age=3600, public\u0026#34; \u0026lt;/filesmatch\u0026gt; Sources http://www.askapache.com/htaccess/speed-up-your-site-with-caching-and-cache-control.html\n","permalink":"https://timor.site/2015/12/apache-force-caching-dynamic-php-content-with-mod_headers/","summary":"Normally you want dynamic content to be fresh and not catchable. But sometimes it may be useful to cache it, like when you have website behind reverse proxy. To do this try something like this:\n\u0026lt;filesmatch \u0026#34;\\.(php|cgi|pl)$\u0026#34;\u0026gt; Header unset Pragma Header unset Expires Header set Cache-Control \u0026#34;max-age=3600, public\u0026#34; \u0026lt;/filesmatch\u0026gt; Sources http://www.askapache.com/htaccess/speed-up-your-site-with-caching-and-cache-control.html","title":"Apache - Force caching dynamic PHP content with mod_headers"},{"content":"It will happen from time to time, that you\u0026rsquo;re on alien machine and have to brutally update things in db without knowing credentials. Example is for root (quite secure candidate to change because it shouldn\u0026rsquo;t be used in app üòÉ ) but will work for any user.\nshutdown db service mysql stop create text file with command like this (update user accordingly) ex. in /tmp/pwchange.txt SET PASSWORD FOR \u0026#34;root\u0026#34;@\u0026#34;localhost\u0026#34; = PASSWORD(\u0026#34;HereYourNewPassword\u0026#34;); start mysqld with --init-file param mysqld_safe --init-file=/tmp/pwchange.txt sometimes you may require to point configuration file ex. --defaults-file=/etc/mysql/my.cnf\nwait until it loads and kill mysql (ex. Ctrl+C / kill / etc) start mysql service mysql start delete file with password rm -f /tmp/pwchange.txt You should be able to login with updated password.\nSources https://dev.mysql.com/doc/refman/5.5/en/resetting-permissions.html\n","permalink":"https://timor.site/2015/12/mysql-reset-root-password/","summary":"It will happen from time to time, that you\u0026rsquo;re on alien machine and have to brutally update things in db without knowing credentials. Example is for root (quite secure candidate to change because it shouldn\u0026rsquo;t be used in app üòÉ ) but will work for any user.\nshutdown db service mysql stop create text file with command like this (update user accordingly) ex. in /tmp/pwchange.txt SET PASSWORD FOR \u0026#34;root\u0026#34;@\u0026#34;localhost\u0026#34; = PASSWORD(\u0026#34;HereYourNewPassword\u0026#34;); start mysqld with --init-file param mysqld_safe --init-file=/tmp/pwchange.","title":"MySQL - reset root password"},{"content":"I hate movies recorded on phone in vertical position. This just short tip how I dealt with with it last time:\nfor m in *.mp4 do avconv -i $m -vf \u0026#34;transpose=1\u0026#34; -codec:a copy -codec:v libx264 -preset slow -crf 23 rotated-$m done Other examples:\nhttp://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg\nhttp://superuser.com/questions/578321/how-to-flip-a-video-180¬∞-vertical-upside-down-with-ffmpeg\n","permalink":"https://timor.site/2015/12/rotate-movies/","summary":"I hate movies recorded on phone in vertical position. This just short tip how I dealt with with it last time:\nfor m in *.mp4 do avconv -i $m -vf \u0026#34;transpose=1\u0026#34; -codec:a copy -codec:v libx264 -preset slow -crf 23 rotated-$m done Other examples:\nhttp://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg\nhttp://superuser.com/questions/578321/how-to-flip-a-video-180¬∞-vertical-upside-down-with-ffmpeg","title":"Rotate movies"},{"content":"I had some passwords saved in remmina but like it always happen, I wasn\u0026rsquo;t been able to remember them when needed. Trying to restore them I found that they\u0026rsquo;re encrypted in .remmina directory.\nThen I used this script to the decrypt them 1:\nExtract script import base64 from Crypto.Cipher import DES3 secret = base64.decodestring(\u0026#34;\u0026lt;STRING FROM remmina.prefs\u0026gt;\u0026#34;) password = base64.decodestring(\u0026#34;\u0026lt;STRING FROM XXXXXXX.remmina\u0026gt;\u0026#34;) print DES3.new(secret[:24], DES3.MODE_CBC, secret[24:]).decrypt(password) http://askubuntu.com/questions/290824/how-to-extract-saved-password-from-remmina\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://timor.site/2015/12/extract-password-saved-in-remmina/","summary":"I had some passwords saved in remmina but like it always happen, I wasn\u0026rsquo;t been able to remember them when needed. Trying to restore them I found that they\u0026rsquo;re encrypted in .remmina directory.\nThen I used this script to the decrypt them 1:\nExtract script import base64 from Crypto.Cipher import DES3 secret = base64.decodestring(\u0026#34;\u0026lt;STRING FROM remmina.prefs\u0026gt;\u0026#34;) password = base64.decodestring(\u0026#34;\u0026lt;STRING FROM XXXXXXX.remmina\u0026gt;\u0026#34;) print DES3.new(secret[:24], DES3.MODE_CBC, secret[24:]).decrypt(password) http://askubuntu.com/questions/290824/how-to-extract-saved-password-from-remmina\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Extract password saved in remmina"},{"content":"After long break I\u0026rsquo;m thinking about writing more on my blog. I was reviewing my favorites/bookmarks and half of them was broken, so I can\u0026rsquo;t rely on them in case of knowledge management.\nI think I will write shorter, less descriptive articles just to be pointers to useful solutions from past.\n","permalink":"https://timor.site/2015/12/im-back/","summary":"After long break I\u0026rsquo;m thinking about writing more on my blog. I was reviewing my favorites/bookmarks and half of them was broken, so I can\u0026rsquo;t rely on them in case of knowledge management.\nI think I will write shorter, less descriptive articles just to be pointers to useful solutions from past.","title":"I‚Äôm back"},{"content":"Allow from IP without password prompt, and also allow from any address with password prompt\nOrder deny,allow Deny from all AuthName \u0026#34;htaccess password prompt\u0026#34; AuthUserFile /web/askapache.com/.htpasswd AuthType Basic Require valid-user Allow from 172.17.10.1 Satisfy Any Sources http://www.askapache.com/htaccess/apache-authentication-in-htaccess.html\n","permalink":"https://timor.site/2015/12/apache-authbasic-but-excluding-ip/","summary":"Allow from IP without password prompt, and also allow from any address with password prompt\nOrder deny,allow Deny from all AuthName \u0026#34;htaccess password prompt\u0026#34; AuthUserFile /web/askapache.com/.htpasswd AuthType Basic Require valid-user Allow from 172.17.10.1 Satisfy Any Sources http://www.askapache.com/htaccess/apache-authentication-in-htaccess.html","title":"Apache AuthBasic but excluding IP"},{"content":" SplƒÖtana sieƒáPrzewodnik po bezpiecze≈Ñstwie nowoczesnych aplikacji WWW\nAuthor: Micha≈Ç Zalewski\nhelion.pl ","permalink":"https://timor.site/books/2015/splatana-siec/","summary":" SplƒÖtana sieƒáPrzewodnik po bezpiecze≈Ñstwie nowoczesnych aplikacji WWW\nAuthor: Micha≈Ç Zalewski\nhelion.pl ","title":"SplƒÖtana sieƒá"},{"content":" RubyProgramowanie\nAuthors: David Flanagan, Yukihiro Matsumoto\nhelion.pl ","permalink":"https://timor.site/books/2015/ruby-programowanie/","summary":" RubyProgramowanie\nAuthors: David Flanagan, Yukihiro Matsumoto\nhelion.pl ","title":"Ruby"},{"content":" GITRozproszony system kontroli wersji\nAuthor: W≈Çodzimierz Gajda\nhelion.pl ","permalink":"https://timor.site/books/2015/git/","summary":" GITRozproszony system kontroli wersji\nAuthor: W≈Çodzimierz Gajda\nhelion.pl ","title":"GIT"},{"content":"When configuring RAID it\u0026rsquo;s quite important to have the same partition tables on every disk. I\u0026rsquo;v done this many times on msdos partition tables like this:\nsfdisk -d /dev/sda | sfdisk /dev/sdb but it\u0026rsquo;s not working any more on GPT partition tables. Hopefully it still can be done but with different toolstack üòÑ\nInstall gdisk:\napt-get install -y gdisk Then use sgdisk like this:\nsgdisk -R /dev/sd_dest /dev/sd_src sgdisk -G /dev/sd_dest First command will copy partition from /dev/sd_src to /dev/sd_dest. Second will randomize partition UUID\u0026rsquo;s - needed only if you want to use disks in same machine (this is my case).\n","permalink":"https://timor.site/2014/07/copy-gtp-partiotion-table-between-disks/","summary":"When configuring RAID it\u0026rsquo;s quite important to have the same partition tables on every disk. I\u0026rsquo;v done this many times on msdos partition tables like this:\nsfdisk -d /dev/sda | sfdisk /dev/sdb but it\u0026rsquo;s not working any more on GPT partition tables. Hopefully it still can be done but with different toolstack üòÑ\nInstall gdisk:\napt-get install -y gdisk Then use sgdisk like this:\nsgdisk -R /dev/sd_dest /dev/sd_src sgdisk -G /dev/sd_dest First command will copy partition from /dev/sd_src to /dev/sd_dest.","title":"Copy GTP partiotion table between disks"},{"content":"There is need plugin for Django, named django-debug-toolbar but it needs some time to configure. So when I need simple way to debug SQL queries I use small hack. Add to your settings.py:\nLOGGING = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;handlers\u0026#39;: { \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, } }, \u0026#39;loggers\u0026#39;: { \u0026#39;django.db.backends\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;], \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, }, } } To get this working DEBUG option have to be set to True:\nDEBUG = True After this setup, when you run you app in development mode:\n./manage.py runserver you will see SQL queries in console output.\nSources https://docs.djangoproject.com/en/1.6/topics/logging/#examples\n","permalink":"https://timor.site/2014/05/quickly-setup-sql-query-logging-on-console-in-django/","summary":"There is need plugin for Django, named django-debug-toolbar but it needs some time to configure. So when I need simple way to debug SQL queries I use small hack. Add to your settings.py:\nLOGGING = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;handlers\u0026#39;: { \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, } }, \u0026#39;loggers\u0026#39;: { \u0026#39;django.db.backends\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;], \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, }, } } To get this working DEBUG option have to be set to True:","title":"Quickly setup SQL query logging on console in Django"},{"content":"On Debian in default installation you have different configuration files for PHP in Apache, FPM, CLI, etc. But on CentOS you have only one php.ini for all of them. In case I have, I need to have different configuration file for scripts running in CLI mode (more memory, etc). I could run it like this:\nphp -c /etc/php-cli.ini script.php But this a little burdensome. So I do it like this:\ncat \u0026gt; /etc/profile.d/php-cli-ini.sh \u0026lt;\u0026lt;SCRIPT #!/bin/bash alias php=\u0026#34;php -c /etc/php-cli.ini\u0026#34; SCRIPT cp /etc/php.ini /etc/php-cli.ini Logout, login and now every user can run PHP scripts in CLI with different configuration - exactly what I need :)\n","permalink":"https://timor.site/2014/05/changing-default-php-ini-file-for-php-cli-on-centos/","summary":"On Debian in default installation you have different configuration files for PHP in Apache, FPM, CLI, etc. But on CentOS you have only one php.ini for all of them. In case I have, I need to have different configuration file for scripts running in CLI mode (more memory, etc). I could run it like this:\nphp -c /etc/php-cli.ini script.php But this a little burdensome. So I do it like this:","title":"Changing default php.ini file for PHP-CLI on CentOS"},{"content":"Everybody knows passwd command but it\u0026rsquo;s useless when you need to change ex. root password from command line without waiting for input. In such case oneliner below could help:\necho \u0026#34;root:new_password\u0026#34; | chpasswd ","permalink":"https://timor.site/2014/05/command-to-change-root-password/","summary":"Everybody knows passwd command but it\u0026rsquo;s useless when you need to change ex. root password from command line without waiting for input. In such case oneliner below could help:\necho \u0026#34;root:new_password\u0026#34; | chpasswd ","title":"Command to change root password"},{"content":"These are few steps to get Steam running on Ubuntu:\nwget -c media.steampowered.com/client/installer/steam.deb dpkg -i steam.deb apt-get install -f apt-get update Solutions for some issues Some time ago I needed 32 bit flash even on 64 bit system - I don\u0026rsquo;t need it currently but I\u0026rsquo;m living this as a tip.\napt-get install adobe-flashplugin:i386 After Ubuntu upgrade I was unable to run Steam anymore - It shouted on me with strange \u0026ldquo;networking problem\u0026rdquo;. I have to clean Steam configuration with:\nsteam --reset Sources http://linuxg.net/how-to-install-the-latest-steam-client-available-on-ubuntu-13-10-13-04-12-10-12-04-and-linux-mint-15-14-13/\nhttp://askubuntu.com/questions/353522/why-is-steam-not-able-to-connect-steam-network\n","permalink":"https://timor.site/2014/04/install-steam-on-debian-ubuntu/","summary":"These are few steps to get Steam running on Ubuntu:\nwget -c media.steampowered.com/client/installer/steam.deb dpkg -i steam.deb apt-get install -f apt-get update Solutions for some issues Some time ago I needed 32 bit flash even on 64 bit system - I don\u0026rsquo;t need it currently but I\u0026rsquo;m living this as a tip.\napt-get install adobe-flashplugin:i386 After Ubuntu upgrade I was unable to run Steam anymore - It shouted on me with strange \u0026ldquo;networking problem\u0026rdquo;.","title":"Install Steam on Debian/Ubuntu"},{"content":"When I was trying to update packages on one host I\u0026rsquo;ve stuck with yum hung on update. I run strace and see:\nstrace -p 43734 Process 43734 attached - interrupt to quit futex(0x807c938, FUTEX_WAIT, 1, NULL \u0026lt;unfinished ...\u0026gt; Process 43734 detached It looks like yum database was corrupted, to repair this run:\nrm -f /var/lib/rpm/__db* rpm --rebuilddb yum clean all yum update Instead rm on db-files you could use gzip to have backup of these files.\n","permalink":"https://timor.site/2014/04/rebuild-yum-rpm-database/","summary":"When I was trying to update packages on one host I\u0026rsquo;ve stuck with yum hung on update. I run strace and see:\nstrace -p 43734 Process 43734 attached - interrupt to quit futex(0x807c938, FUTEX_WAIT, 1, NULL \u0026lt;unfinished ...\u0026gt; Process 43734 detached It looks like yum database was corrupted, to repair this run:\nrm -f /var/lib/rpm/__db* rpm --rebuilddb yum clean all yum update Instead rm on db-files you could use gzip to have backup of these files.","title":"Rebuild yum/rpm database"},{"content":"I\u0026rsquo;ve few Nagios checks that require root privileges but running nrpe as root user is not acceptable. I prefer to use sudo for only these few commands.\nRun visudo and coment out this line:\n#Defaults requiretty This change is crucial to get scripts working.\nThen add at the end of file:\n%nrpe ALL=(ALL) NOPASSWD: /usr/lib64/nagios/plugins/ I\u0026rsquo;ve used nrpe group, but you have to add exactly group that your nrpe process uses.\nNow you should be able to run checks as root - edit /etc/nagios/nrpe.cfg and add check like this:\ncommand[check_as_root]=/usr/bin/sudo /usr/lib64/nagios/plugins/check_with_root_privileges ","permalink":"https://timor.site/2014/03/nagios-run-checks-as-root-with-nrpe/","summary":"I\u0026rsquo;ve few Nagios checks that require root privileges but running nrpe as root user is not acceptable. I prefer to use sudo for only these few commands.\nRun visudo and coment out this line:\n#Defaults requiretty This change is crucial to get scripts working.\nThen add at the end of file:\n%nrpe ALL=(ALL) NOPASSWD: /usr/lib64/nagios/plugins/ I\u0026rsquo;ve used nrpe group, but you have to add exactly group that your nrpe process uses.","title":"Nagios - run checks as root with NRPE"},{"content":"After reading some SEO stuff I wanted to add some meta tags to my WordPress blog. I found this site: codex.wordpress.org/Meta_Tags_in_WordPress.\nSo WordPress thinks that it\u0026rsquo;s not necessary to have this meta tags any more\u0026hellip; But I want it! üòÉ Next funny thing is how they suggest to add meta tags: copy header.php - what about theme updates?\nI prefer to use functions.php file - just create it in your courrent theme directory with such content:\n\u0026lt;?php // this will remove WordPress version from header remove_action(\u0026#39;wp_head\u0026#39;, \u0026#39;wp_generator\u0026#39;); // handler function for adding custom tags function custom_header_meta() { ?\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;Tomasz GƒÖgor\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;contact\u0026#34; content=\u0026#34;my@mail.com\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;\u0026lt;?php wp_title( \u0026#39;|\u0026#39;, true, \u0026#39;right\u0026#39; ); ?\u0026gt;\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;some keywords\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;verify-v1\u0026#34; content=\u0026#34;google webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;bing webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;fb:admins\u0026#34; content=\u0026#34;facebook identificatio\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;copyright\u0026#34; content=\u0026#34;Copyright (c)1997-2014 Tomasz GƒÖgor. All Rights Reserved.\u0026#34; /\u0026gt; \u0026lt;?php } // running handler function add_action(\u0026#39;wp_head\u0026#39;, \u0026#39;custom_header_meta\u0026#39;); ","permalink":"https://timor.site/2014/03/wordpress-add-meta-tags-author-description-keywords-etc/","summary":"After reading some SEO stuff I wanted to add some meta tags to my WordPress blog. I found this site: codex.wordpress.org/Meta_Tags_in_WordPress.\nSo WordPress thinks that it\u0026rsquo;s not necessary to have this meta tags any more\u0026hellip; But I want it! üòÉ Next funny thing is how they suggest to add meta tags: copy header.php - what about theme updates?\nI prefer to use functions.php file - just create it in your courrent theme directory with such content:","title":"WordPress - add meta tags: author, description, keywords, etc"},{"content":"Let say you have MediaWiki installation but you lost admin credentials. If you have other account or if you could create one without any rights we\u0026rsquo;re in home üòâ\nWe have few options to do this.\nReset admin password We have to connect to database and use this SQL:\nUPDATE `user` SET user_password = CONCAT( SUBSTRING(user_password, 1, 3), SUBSTRING(MD5(user_name), 1, 8), \u0026#39;:\u0026#39;, MD5(CONCAT(SUBSTRING(MD5(user_name), 1, 8), \u0026#39;-\u0026#39;, MD5(\u0026#39;new password\u0026#39;)))) WHERE user_name = \u0026#39;Admin\u0026#39;; Just replace Admin with your username and new password with your password.\nRaise another user rights If we don\u0026rsquo;t want to mess with admin account we could raise permissions for other user, ex. Tom:\nINSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;bureaucrat\u0026#39;); INSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;sysop\u0026#39;); Sources http://www.mediawiki.org/wiki/Manual_talk:Resetting_passwords\nhttp://www.mediawiki.org/wiki/Manual_talk:AdminSettings.php\n","permalink":"https://timor.site/2014/03/mediawiki-recover-admin-rights/","summary":"Let say you have MediaWiki installation but you lost admin credentials. If you have other account or if you could create one without any rights we\u0026rsquo;re in home üòâ\nWe have few options to do this.\nReset admin password We have to connect to database and use this SQL:\nUPDATE `user` SET user_password = CONCAT( SUBSTRING(user_password, 1, 3), SUBSTRING(MD5(user_name), 1, 8), \u0026#39;:\u0026#39;, MD5(CONCAT(SUBSTRING(MD5(user_name), 1, 8), \u0026#39;-\u0026#39;, MD5(\u0026#39;new password\u0026#39;)))) WHERE user_name = \u0026#39;Admin\u0026#39;; Just replace Admin with your username and new password with your password.","title":"Mediawiki - recover admin rights"},{"content":"I need to check memory usage of memcached server so I used:\necho stats | nc 127.0.0.1 11211 STAT pid 2743 STAT uptime 263 STAT time 1395438951 STAT version 1.4.13 STAT pointer_size 64 STAT rusage_user 0.482926 STAT rusage_system 2.675593 STAT curr_items 8667 STAT total_items 10742 STAT bytes 23802513 STAT curr_connections 296 STAT total_connections 399 STAT connection_structures 297 STAT cmd_flush 0 STAT cmd_get 52578 STAT cmd_set 10792 STAT get_hits 28692 STAT get_misses 23886 STAT evictions 0 STAT bytes_read 35984361 STAT bytes_written 192647437 STAT limit_maxbytes 536870912 STAT threads 2 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT replication MASTER STAT repcached_qi_free 8189 STAT repcached_wdata 0 STAT repcached_wsize 1026048 END For me, bytes value was important but you could find more about all statistics here.\n","permalink":"https://timor.site/2014/03/checking-memcached-status/","summary":"I need to check memory usage of memcached server so I used:\necho stats | nc 127.0.0.1 11211 STAT pid 2743 STAT uptime 263 STAT time 1395438951 STAT version 1.4.13 STAT pointer_size 64 STAT rusage_user 0.482926 STAT rusage_system 2.675593 STAT curr_items 8667 STAT total_items 10742 STAT bytes 23802513 STAT curr_connections 296 STAT total_connections 399 STAT connection_structures 297 STAT cmd_flush 0 STAT cmd_get 52578 STAT cmd_set 10792 STAT get_hits 28692 STAT get_misses 23886 STAT evictions 0 STAT bytes_read 35984361 STAT bytes_written 192647437 STAT limit_maxbytes 536870912 STAT threads 2 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT replication MASTER STAT repcached_qi_free 8189 STAT repcached_wdata 0 STAT repcached_wsize 1026048 END For me, bytes value was important but you could find more about all statistics here.","title":"Checking memcached status"},{"content":"I have development server with postfix - I wanted to allow outbound traffic to one domain but cut off all the rest. I definitely do not want that test mail or any debug info goes to service users.\nI have to add something like that to /etc/postfix/transport:\nallowed.domain.com : * discard: Then run:\npostmap /etc/postfix/transport At end, add these to /etc/postfix/main.cf:\ntransport_maps = hash:/etc/postfix/transport Reload postfix:\npostfix reload Test if it works:\necho test | mail -s test whatever@whatever.com You should see in logs that message was dropped:\nMar 18 21:14:28 devmx1 postfix/cleanup[29968]: 1E77654391: message-id=20140318211428.1E77280521@domain.com\u0026gt; Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: from=\u0026lt;root@domain.com\u0026gt;, size=431, nrcpt=1 (queue active) Mar 18 21:14:28 devmx1 postfix/discard[29970]: 1E77654391: to=\u0026lt;whatever@whatever.com\u0026gt;, relay=none, delay=0.1, delays=0.09/0.01/0/0, dsn=2.0.0, status=sent (whatever.com) Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: removed ","permalink":"https://timor.site/2014/03/postfix-automatically-drop-outbound-mail/","summary":"I have development server with postfix - I wanted to allow outbound traffic to one domain but cut off all the rest. I definitely do not want that test mail or any debug info goes to service users.\nI have to add something like that to /etc/postfix/transport:\nallowed.domain.com : * discard: Then run:\npostmap /etc/postfix/transport At end, add these to /etc/postfix/main.cf:\ntransport_maps = hash:/etc/postfix/transport Reload postfix:\npostfix reload Test if it works:","title":"Postfix - automatically drop outbound mail"},{"content":"In recent Ansible update to 1.5 version there is really nice feature ssh pipelining. This option is serious alternative to accelerated mode.\nJust add to you config file (ex. ~/.ansible.cfg):\n[ssh_connection] pipelining=True Now run any playbook - you will see the difference üòÑ\nSource (and extended info about):\nhttp://blog.ansibleworks.com/2014/01/15/ssh-connection-upgrades-coming-in-ansible-1-5/\n","permalink":"https://timor.site/2014/03/ansible-ssh-pipelining/","summary":"In recent Ansible update to 1.5 version there is really nice feature ssh pipelining. This option is serious alternative to accelerated mode.\nJust add to you config file (ex. ~/.ansible.cfg):\n[ssh_connection] pipelining=True Now run any playbook - you will see the difference üòÑ\nSource (and extended info about):\nhttp://blog.ansibleworks.com/2014/01/15/ssh-connection-upgrades-coming-in-ansible-1-5/","title":"Ansible - ssh pipelining"},{"content":"To najlepszy przepis na chrusty jaki znam - wychodzƒÖ bardzo kruche i delikatne.\nSk≈Çadniki 4 z√≥≈Çtka, 4 ≈Çy≈ºki wina bia≈Çego lub czerwonego, 4 ≈Çy≈ºki mƒÖki. Spos√≥b przygotowania Wszystkie sk≈Çadniki wymieszaƒá i wyrobiƒá. Ciasto powinno byƒá mniej wiecej takie jak na pierogi. Biƒá pa≈ÇkƒÖ/wa≈Çkiem, sk≈Çadaƒá na p√≥≈Ç i tak kilka razy przez ok 5 minut. Potem ciasto rozwa≈Çkowaƒá bardzo cieniutko i wykrawaƒá chrusty, ma≈Çe bo mocno rosnƒÖ. Nastƒôpnie wrzucaƒá na rozgrzany olej/smalec.\n","permalink":"https://timor.site/2014/02/chrusty-faworki/","summary":"To najlepszy przepis na chrusty jaki znam - wychodzƒÖ bardzo kruche i delikatne.\nSk≈Çadniki 4 z√≥≈Çtka, 4 ≈Çy≈ºki wina bia≈Çego lub czerwonego, 4 ≈Çy≈ºki mƒÖki. Spos√≥b przygotowania Wszystkie sk≈Çadniki wymieszaƒá i wyrobiƒá. Ciasto powinno byƒá mniej wiecej takie jak na pierogi. Biƒá pa≈ÇkƒÖ/wa≈Çkiem, sk≈Çadaƒá na p√≥≈Ç i tak kilka razy przez ok 5 minut. Potem ciasto rozwa≈Çkowaƒá bardzo cieniutko i wykrawaƒá chrusty, ma≈Çe bo mocno rosnƒÖ. Nastƒôpnie wrzucaƒá na rozgrzany olej/smalec.","title":"Chrusty, faworki"},{"content":"I had quite simple task - compare two lists of hosts and check if hosts from first one are also on the second one. I started with diff:\ndiff -u biglist.txt hosts_to_check.txt | grep -E \u0026#34;^\\+\u0026#34; It was fine but output needs some filtering to get what I want.\nI\u0026rsquo;ve found another example with grep:\ngrep -Fxv -f biglist.txt hosts_to_check.txt | sort -n This will search for all lines in hosts_to_check.txt which don\u0026rsquo;t match any line in biglist.txt. So after this I\u0026rsquo;ve got list of hosts that I have to check. That\u0026rsquo;s exactly what I need üòÑ\n","permalink":"https://timor.site/2014/02/comparing-two-lists-in-bash/","summary":"I had quite simple task - compare two lists of hosts and check if hosts from first one are also on the second one. I started with diff:\ndiff -u biglist.txt hosts_to_check.txt | grep -E \u0026#34;^\\+\u0026#34; It was fine but output needs some filtering to get what I want.\nI\u0026rsquo;ve found another example with grep:\ngrep -Fxv -f biglist.txt hosts_to_check.txt | sort -n This will search for all lines in hosts_to_check.","title":"Comparing two lists in bash"},{"content":"After WSUS installing on Windows Server 2012 I discovered that it\u0026rsquo;s running on port 8530, different than on older version of Windows (it was using port 80 from beginning). But what\u0026rsquo;s more interesting it was running ONLY on IPv6 interface! Switching binding configuration in IIS doesn\u0026rsquo;t help.\nI could stand switching port - it\u0026rsquo;s nothing hard with GPO, but IPv6 only configuration was not acceptable.\nAfter googling for some time I found one command that solved my problems by switching WSUS to older behavior and run it on port 80 (on default website).\nJust run on elevated command line:\nC:\\Program Files\\Update Services\\Tools\\WSUSutil usecustomwebsite false After half a minute WSUS was working like a charm üòÉ\nSources http://social.technet.microsoft.com/Forums/windowsserver/en-US/88514e56-1179-4af7-9f5e-5339d3e750a5/how-to-change-wsus-2012-port-to-80?forum=winserverwsus\nhttp://community.spiceworks.com/topic/160971-how-do-you-change-the-port-number-for-your-wsus\n","permalink":"https://timor.site/2014/01/change-default-wsus-port-from-8530-to-80-on-windows-server-2012/","summary":"After WSUS installing on Windows Server 2012 I discovered that it\u0026rsquo;s running on port 8530, different than on older version of Windows (it was using port 80 from beginning). But what\u0026rsquo;s more interesting it was running ONLY on IPv6 interface! Switching binding configuration in IIS doesn\u0026rsquo;t help.\nI could stand switching port - it\u0026rsquo;s nothing hard with GPO, but IPv6 only configuration was not acceptable.\nAfter googling for some time I found one command that solved my problems by switching WSUS to older behavior and run it on port 80 (on default website).","title":"Change default WSUS port from 8530 to 80 on Windows Server 2012"},{"content":"After reading some good opinions about MariaDB I wanted to give it a try. Upgrade looks quite straight forward but I found some issues a little tricky.\nInstallation Add repo and key:\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main deb-src http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main SRC (find more repositories here)\nNow install MariaDB:\nsudo apt-get update sudo apt-get install mariadb-server It could be better to install mariadb-server-5.5 and mariadb-client-5.5 package instead, because of this error.\nMariaDB repo pinning Some time after installation I have problem with newer packages from Debian repositories that upgraded my MariaDB installation back to MySQL - it\u0026rsquo;s described here, so I used pinning to resolve that.\ncat \u0026gt; /etc/apt/preferences.d/mariadb.pref \u0026lt;\u0026lt;PIN Package: * Pin: origin mirrors.supportex.net Pin-Priority: 1000 PIN Results Before migration to MariaDB, front page of my blog needs about 650 ms to generate. After switch, it was only about 550ms. So it\u0026rsquo;s about 15% - absolutely for free üòÑ\n","permalink":"https://timor.site/2014/01/debian-upgrade-mysql-to-mariadb/","summary":"After reading some good opinions about MariaDB I wanted to give it a try. Upgrade looks quite straight forward but I found some issues a little tricky.\nInstallation Add repo and key:\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main deb-src http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main SRC (find more repositories here)\nNow install MariaDB:\nsudo apt-get update sudo apt-get install mariadb-server It could be better to install mariadb-server-5.5 and mariadb-client-5.5 package instead, because of this error.","title":"Debian - Upgrade MySQL to MariaDB"},{"content":"I was thinking about allowing access to my website using SPDY protocol for better performance and security (and for fun of course üòÉ ). But SPDY have one disadvantage - you need SSL certificate signed by known authority that will verfiy in common browsers. So you can\u0026rsquo;t use self signed certificates because everyone will see a warning entering your site. Certs are quite expensive so I started searching for free one and to my surprise I found such!\nI found these two sites where you can generate freeware certificates for your website:\nhttps://www.startssl.com/ (I prefer this one because it better recognized) https://www.cacert.org I wouldn\u0026rsquo;t trust these certification authorities enough to use it for: access my mail or other private data. But I\u0026rsquo;m fine with using it for my public websites (like my blog) to gain speed from SPDY.\nConfiguring cert Fetch the Root CA and Class 1 Intermediate Server CA certificates:\nwget http://www.startssl.com/certs/ca.pem wget http://www.startssl.com/certs/sub.class1.server.ca.pem Create a unified certificate from your certificate and the CA certificates:\ncat ssl.crt sub.class1.server.ca.pem ca.pem \u0026gt; /etc/nginx/conf/ssl-unified.crt Enable SPDY Configure your nginx server to use the new key and certificate (in the global settings or a server section):\nssl on; ssl_certificate /etc/nginx/conf/ssl-unified.crt; ssl_certificate_key /etc/nginx/conf/ssl.key; Then enable SPDY like that:\nserver { listen your_ip:80; listen your_id:443 default_server ssl spdy; # other stuff } Advertise SPDY protocol Now advertise SPDY with Alternate-Protocol header - add this clause in main location:\nadd_header Alternate-Protocol \u0026#34;443:npn-spdy/2\u0026#34;; Have fun with SPDY on your site üòÑ\n","permalink":"https://timor.site/2014/01/nginx-enabling-spdy-with-freeware-certificate/","summary":"I was thinking about allowing access to my website using SPDY protocol for better performance and security (and for fun of course üòÉ ). But SPDY have one disadvantage - you need SSL certificate signed by known authority that will verfiy in common browsers. So you can\u0026rsquo;t use self signed certificates because everyone will see a warning entering your site. Certs are quite expensive so I started searching for free one and to my surprise I found such!","title":"Nginx - enabling SPDY with freeware certificate"},{"content":"I\u0026rsquo;ve been using different code editors for different purposes. Gedit was fine for small scripts but not for bigger projects. It lacks intelligent code completion (function/class names, etc.). I was searching for convenient editor for Python, Perl, Ruby with support for frameworks like Django, Rails, etc. I know Sublime Text - but it\u0026rsquo;s paid. There is LimeText - open source clone, but it\u0026rsquo;s not ready to be used on daily basics.\nI found Brackets - open source editor designed by Adobe. I\u0026rsquo;m testing it right now.\nBrackets installation on Ubuntu sudo add-apt-repository ppa:webupd8team/brackets sudo apt-get update sudo apt-get install brackets Source http://www.webupd8.org/2013/11/install-brackets-in-ubuntu-via-ppa-open.html\nI was using Brackets for some time and while it\u0026rsquo;s really nice editor - it\u0026rsquo;s mostly designed for webmasters (writing web apps). But I also need to write in Ruby, Python, Perl and many other - then Github announced Atom editor. I switched to Atom - it\u0026rsquo;s similar to Brackets but covers my interests better üôÇ\nAtom installation on Ubuntu Similarly to Brackets installation. Add ppa and install:\nsudo add-apt-repository ppa:webupd8team/atom sudo apt-get update sudo apt-get install -y atom Atom plugins I use Atom is great because of all plugins available - my favorite are:\nlanguage-docker (Dockerfile syntax highlighting) language-terraform (Terraform files syntax highlighting) language-python autocomplete-python (with Kite) linter-foodcritic linter-python-pep8 linter-rubocop minimap tabs-to-spaces Atom themes I use Default dark theme in Atom is really nice - but there are better themes. My favorite are:\nUI Theme: seti-ui Syntax theme: Monokai ","permalink":"https://timor.site/2014/01/searching-for-better-code-editor/","summary":"I\u0026rsquo;ve been using different code editors for different purposes. Gedit was fine for small scripts but not for bigger projects. It lacks intelligent code completion (function/class names, etc.). I was searching for convenient editor for Python, Perl, Ruby with support for frameworks like Django, Rails, etc. I know Sublime Text - but it\u0026rsquo;s paid. There is LimeText - open source clone, but it\u0026rsquo;s not ready to be used on daily basics.","title":"Searching for better code editor"},{"content":"After connecting few computers with Windows 8.1 to domain we found that these computers are not recognized or recognized as Windows 6.3 (which is true) on WSUS 3.0 running on Windows Server 2008. The bad thing was that they can\u0026rsquo;t properly report to WSUS and get updates from it.\nI found that there are two updates that have to be installed (but they\u0026rsquo;re not working without additional steps):\nhttp://support.microsoft.com/kb/2720211 http://support.microsoft.com/kb/2734608 After installation of second update there are two additional steps that have to be performed to get WSUS working:\nReindex the WSUS Database Use the Server Cleanup Wizard - this one is trivial, so I hope you get this right Reindex the WSUS Database To do this perform these steps:\nCopy sript from this site to file named WsusDBMaintenance.sql Install sqlcmd from this site - search for file named like \u0026ldquo;SQLServer2005_SQLCMD\u0026rdquo; with proper architecture (x86/amd64/ia64) run: sqlcmd -S np:\\\\.\\pipe\\MSSQL$MICROSOFT##SSEE\\sql\\query -i C:\\path to script saved in first point\\WsusDBMaintenance.sql Use WSUS Server Cleanup Wizard Done. Your WSUS will not recognize 8.1 clients but will work with them and serve updates.\nSources http://social.technet.microsoft.com/Forums/en-US/559fe878-e2a2-4ec6-9d91-55ea1b67caef/manage-windows-81-windows-server-2012-r2-on-wsus-30?forum=winserverwsus\n","permalink":"https://timor.site/2014/01/manage-windows-8-1-and-windows-server-2012-r2-in-wsus-3-0/","summary":"After connecting few computers with Windows 8.1 to domain we found that these computers are not recognized or recognized as Windows 6.3 (which is true) on WSUS 3.0 running on Windows Server 2008. The bad thing was that they can\u0026rsquo;t properly report to WSUS and get updates from it.\nI found that there are two updates that have to be installed (but they\u0026rsquo;re not working without additional steps):\nhttp://support.microsoft.com/kb/2720211 http://support.microsoft.com/kb/2734608 After installation of second update there are two additional steps that have to be performed to get WSUS working:","title":"Manage Windows 8.1 and Windows Server 2012 R2 in WSUS 3.0"},{"content":"I love Shotwell for it\u0026rsquo;s simplicity and easy export to Piwigo. After Christmas I added new photos to my library but after that I made some modifications to them (red eye reduction, etc\u0026hellip;). Because Shotwell generate thumbnails only on import, all my modifications were not visible on preview.\nI\u0026rsquo;ve started searching how to regenerate thumbs and found this info. There were two issues with this method:\nthis howto was for old version (with old paths) and only for 128px thumbs I definitely don\u0026rsquo;t want to regenerate thumbnails for 40k photos! After some tweaking this will do work for thumbnails from last month (enough for me):\nsqlite3 ~/.local/share/shotwell/data/photo.db \\ \u0026#34;select id||\u0026#39; \u0026#39;||filename from PhotoTable where date(timestamp,\u0026#39;unixepoch\u0026#39;,\u0026#39;localtime\u0026#39;) \u0026gt; date(\u0026#39;now\u0026#39;,\u0026#39;start of month\u0026#39;,\u0026#39;-1 month\u0026#39;) order by timestamp desc\u0026#34; | while read id filename; do tf1=$(printf ~/.local/share/shotwell/thumbs/thumbs128/thumb%016x.jpg $id); tf2=$(printf ~/.local/share/shotwell/thumbs/thumbs360/thumb%016x.jpg $id); test -e \u0026#34;$tf\u0026#34; || { echo -n \u0026#34;Generating thumb for $filename\u0026#34;; convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 128x128 $tf1 convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 360x360 $tf2 echo } done Remember to install imagemagick:\napt-get install imagemagick ","permalink":"https://timor.site/2014/01/regenerate-thumbnails-in-shotwell-for-last-month/","summary":"I love Shotwell for it\u0026rsquo;s simplicity and easy export to Piwigo. After Christmas I added new photos to my library but after that I made some modifications to them (red eye reduction, etc\u0026hellip;). Because Shotwell generate thumbnails only on import, all my modifications were not visible on preview.\nI\u0026rsquo;ve started searching how to regenerate thumbs and found this info. There were two issues with this method:\nthis howto was for old version (with old paths) and only for 128px thumbs I definitely don\u0026rsquo;t want to regenerate thumbnails for 40k photos!","title":"Regenerate thumbnails in Shotwell 0.15 (for last month)"},{"content":"Few days ago I\u0026rsquo;ve read a book ‚ÄòEven Faster Web Sites‚Äò about websites optimisation and I found one thing usefuluseful, not only on websites. There was a small tip about looploop unlooping. I want to quote them for later use.\nFirst - with switch statement var iterations = Math.ceil(values.length / 8); var startAt = values.length % 8; var i = 0; do { switch(startAt) { case 0: process(values[i++]); case 7: process(values[i++]); case 6: process(values[i++]); case 5: process(values[i++]); case 4: process(values[i++]); case 3: process(values[i++]); case 2: process(values[i++]); case 1: process(values[i++]); } startAt = 0; } while(--iterations \u0026gt; 0); Second - without switch var iterations = Math.floor(values.length / 8); var leftover = values.length % 8; var i = 0; if(leftover \u0026gt; 0) { do { process(values[i++]); } while(--leftover \u0026gt; 0); } do { process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); } while (--iterations \u0026gt; 0); I found second example more readable and I prefer it.\nThese examples after translation could be easily used in other scripting languages.\n","permalink":"https://timor.site/2014/01/loop-unlooping-in-javascript/","summary":"Few days ago I\u0026rsquo;ve read a book ‚ÄòEven Faster Web Sites‚Äò about websites optimisation and I found one thing usefuluseful, not only on websites. There was a small tip about looploop unlooping. I want to quote them for later use.\nFirst - with switch statement var iterations = Math.ceil(values.length / 8); var startAt = values.length % 8; var i = 0; do { switch(startAt) { case 0: process(values[i++]); case 7: process(values[i++]); case 6: process(values[i++]); case 5: process(values[i++]); case 4: process(values[i++]); case 3: process(values[i++]); case 2: process(values[i++]); case 1: process(values[i++]); } startAt = 0; } while(--iterations \u0026gt; 0); Second - without switch var iterations = Math.","title":"Loop unlooping in Javascript"},{"content":"Some time ago I write article about tracking nicknames of users (from comments) on a WordPress blog with Piwik. This time I\u0026rsquo;m doing same but for Google Analytics.\nI\u0026rsquo;m using Google Analytics plugin for WordPress so I\u0026rsquo;ve edited googleanalytics.php file to add some additional code for user tracking:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _gaq = _gaq || []; _gaq.push([\u0026#39;_setAccount\u0026#39;, \u0026#39;UA-YOUR-UNIQ-NUMBER\u0026#39;]); _gaq.push([\u0026#39;_setCustomVar\u0026#39;, 1, \u0026#39;Nickname\u0026#39;, comment_author, 1]); _gaq.push([\u0026#39;_trackPageview\u0026#39;]); (function() { var ga = document.createElement(\u0026#39;script\u0026#39;); ga.type = \u0026#39;text/javascript\u0026#39;; ga.async = true; ga.src = (\u0026#39;https:\u0026#39; == document.location.protocol ? \u0026#39;https://ssl\u0026#39; : \u0026#39;http://www\u0026#39;) + \u0026#39;.google-analytics.com/ga.js\u0026#39;; var s = document.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; s.parentNode.insertBefore(ga, s); })(); \u0026lt;/script\u0026gt; Source https://developers.google.com/analytics/devguides/collection/gajs/gaTrackingCustomVariables?hl=pl\n","permalink":"https://timor.site/2014/01/tracking-users-by-nickname-on-wordpress-using-google-analytics/","summary":"Some time ago I write article about tracking nicknames of users (from comments) on a WordPress blog with Piwik. This time I\u0026rsquo;m doing same but for Google Analytics.\nI\u0026rsquo;m using Google Analytics plugin for WordPress so I\u0026rsquo;ve edited googleanalytics.php file to add some additional code for user tracking:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.","title":"Tracking users by nickname on WordPress using Google Analytics"},{"content":"Some time ago I\u0026rsquo;ve show how to precompress js and css file with gzip to be available for Nginx\u0026rsquo;s mod_gzip. In default configuration Apache don\u0026rsquo;t have such module but similar functionality could be achieved with few custom rewirtes.\nBasically we will start with these rewrites to serve gzipped CSS/JS files if they exist and the client accepts gzip compression:\nRewriteEngine on RewriteCond %{HTTP:Accept-encoding} gzip RewriteCond %{REQUEST_FILENAME}\\.gz -s RewriteRule ^(.*)\\.(js|css)$ $1\\.$2\\.gz [QSA] Then we need to setup proper content types for such compressed files - I know how to do this in two ways:\npure rewrites with mod_header - witch should serve correct content type and prevent mod_deflate to gzip files that are already gzipped RewriteRule \\.css\\.gz$ - [T=text/css,E=no-gzip:1,E=manualgzip:1] RewriteRule \\.js\\.gz$ - [T=text/javascript,E=no-gzip:1,E=manualgzip:1] \u0026lt;ifmodule mod_headers.c\u0026gt; # setup this header only if rewrites above were used Header set Content-Encoding \u0026#34;gzip\u0026#34; env=manualgzip \u0026lt;/ifmodule\u0026gt; by using Files clause (we could add this globally in httpd.conf) \u0026lt;files *.css.gz\u0026gt; ForceType text/css Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt; \u0026lt;files *.js.gz\u0026gt; #ForceType text/javascript # lately this one is more popular ForceType application/javascript Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt; Both ways work fine. First one sets no-gzip variable to bypass second time compression. Second one rely on such option in my mod_deflate\u0026rsquo;s config:\nSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|bz2|sit|rar|gz)$ no-gzip dont-vary which won\u0026rsquo;t compress any gz file, and this is why I have to setup Content-Encoding to gzip manually.\nIn both cases you will end with javacript and CSS files served from earlier prepared precomressed versions, with proper content type without engaging mod_deflate regardless you use js/css or js.gz/css.gz extension. But I strongly suggest to use extensions without gz - you will be able to disable this mechanism without any change in website code.\nIf you don\u0026rsquo;t know how to prepare files just look here.\nP.S.\nI found another similar but BAD example - it\u0026rsquo;s using AddEncoding clause to add gzip content type to ALL gzip files - this will cause problems with other compressed files with gz extension ex. tar.gz. Don\u0026rsquo;t do this. My rules above are more selective.\nSources http://stackoverflow.com/questions/7947906/add-expiry-headers-using-apache-for-paths-which-dont-exist-in-the-filesystem http://stackoverflow.com/questions/9076752/how-to-force-apache-to-use-manually-pre-compressed-gz-file-of-css-and-js-files\n","permalink":"https://timor.site/2013/12/apache-precompressing-static-files-with-gzip/","summary":"Some time ago I\u0026rsquo;ve show how to precompress js and css file with gzip to be available for Nginx\u0026rsquo;s mod_gzip. In default configuration Apache don\u0026rsquo;t have such module but similar functionality could be achieved with few custom rewirtes.\nBasically we will start with these rewrites to serve gzipped CSS/JS files if they exist and the client accepts gzip compression:\nRewriteEngine on RewriteCond %{HTTP:Accept-encoding} gzip RewriteCond %{REQUEST_FILENAME}\\.gz -s RewriteRule ^(.*)\\.(js|css)$ $1\\.$2\\.gz [QSA] Then we need to setup proper content types for such compressed files - I know how to do this in two ways:","title":"Apache - precompressing static files with gzip"},{"content":"I\u0026rsquo;m happy owner of Galaxy Nexus 7 and lately I updated my tablet to Android 4.4 Kitkat. One of features I most expected was ability to block some permissions of some applications. Such setting was available in 4.4 version but was removed in latest 4.4.2 - Google didn\u0026rsquo;t explain it exactly why. I don\u0026rsquo;t like when for ex. game need: camera or GPS access - for what I asked?\nBut there is new app so called App Ops that unhides build-in interface allowing edit of application permissions. I strongly suggest to install it.\nRequirements You will need rooted device with Android 4.3 or 4.4 version.\nThis instruction could brick your device - use it on your own responsibility.\nInstall Xposed Read instructions here: http://forum.xda-developers.com/showthread.php?t=1574401 (because installation of this package have in history some bricked devices).\ndownload the Xposed Installer APK and install it launch the Xposed Installer and go to the \u0026ldquo;Framework\u0026rdquo; section, then click on \u0026ldquo;Install/Update\u0026rdquo; reboot your device Install App Ops Read instructions here: http://forum.xda-developers.com/showthread.php?t=2564865\ndownload newest version from here, for now it will be AppOpsXposed-1.5 install it search for new App ops option in Settings under PERSONAL section ","permalink":"https://timor.site/2013/12/android-xposed-appops-reclaim-control-over-installed-applications-permissions/","summary":"I\u0026rsquo;m happy owner of Galaxy Nexus 7 and lately I updated my tablet to Android 4.4 Kitkat. One of features I most expected was ability to block some permissions of some applications. Such setting was available in 4.4 version but was removed in latest 4.4.2 - Google didn\u0026rsquo;t explain it exactly why. I don\u0026rsquo;t like when for ex. game need: camera or GPS access - for what I asked?\nBut there is new app so called App Ops that unhides build-in interface allowing edit of application permissions.","title":"Android: Xposed + AppOps - reclaim control over installed applications permissions"},{"content":"After the last NSA scandal I\u0026rsquo;ve found some time to read some texts about PFS and ECDSA keys lately. I always used RSA keys but wanted to give a try to ECDSA so I wanted to give it a try (test performance, etc). Here is how I\u0026rsquo;ve done it.\nFirstly find your favorite curve. A short tip about bit length and complexity could be found here. From it you will now that using 256 bit ECDSA key should be enough for next 10-20 years.\n$ openssl ecparam -list_curves secp112r1 : SECG/WTLS curve over a 112 bit prime field secp112r2 : SECG curve over a 112 bit prime field secp128r1 : SECG curve over a 128 bit prime field secp128r2 : SECG curve over a 128 bit prime field secp160k1 : SECG curve over a 160 bit prime field secp160r1 : SECG curve over a 160 bit prime field secp160r2 : SECG/WTLS curve over a 160 bit prime field secp192k1 : SECG curve over a 192 bit prime field secp224k1 : SECG curve over a 224 bit prime field secp224r1 : NIST/SECG curve over a 224 bit prime field secp256k1 : SECG curve over a 256 bit prime field secp384r1 : NIST/SECG curve over a 384 bit prime field secp521r1 : NIST/SECG curve over a 521 bit prime field prime192v1: NIST/X9.62/SECG curve over a 192 bit prime field prime192v2: X9.62 curve over a 192 bit prime field prime192v3: X9.62 curve over a 192 bit prime field prime239v1: X9.62 curve over a 239 bit prime field prime239v2: X9.62 curve over a 239 bit prime field prime239v3: X9.62 curve over a 239 bit prime field prime256v1: X9.62/SECG curve over a 256 bit prime field sect113r1 : SECG curve over a 113 bit binary field sect113r2 : SECG curve over a 113 bit binary field sect131r1 : SECG/WTLS curve over a 131 bit binary field sect131r2 : SECG curve over a 131 bit binary field sect163k1 : NIST/SECG/WTLS curve over a 163 bit binary field sect163r1 : SECG curve over a 163 bit binary field sect163r2 : NIST/SECG curve over a 163 bit binary field sect193r1 : SECG curve over a 193 bit binary field sect193r2 : SECG curve over a 193 bit binary field sect233k1 : NIST/SECG/WTLS curve over a 233 bit binary field sect233r1 : NIST/SECG/WTLS curve over a 233 bit binary field sect239k1 : SECG curve over a 239 bit binary field sect283k1 : NIST/SECG curve over a 283 bit binary field sect283r1 : NIST/SECG curve over a 283 bit binary field sect409k1 : NIST/SECG curve over a 409 bit binary field sect409r1 : NIST/SECG curve over a 409 bit binary field sect571k1 : NIST/SECG curve over a 571 bit binary field sect571r1 : NIST/SECG curve over a 571 bit binary field c2pnb163v1: X9.62 curve over a 163 bit binary field c2pnb163v2: X9.62 curve over a 163 bit binary field c2pnb163v3: X9.62 curve over a 163 bit binary field c2pnb176v1: X9.62 curve over a 176 bit binary field c2tnb191v1: X9.62 curve over a 191 bit binary field c2tnb191v2: X9.62 curve over a 191 bit binary field c2tnb191v3: X9.62 curve over a 191 bit binary field c2pnb208w1: X9.62 curve over a 208 bit binary field c2tnb239v1: X9.62 curve over a 239 bit binary field c2tnb239v2: X9.62 curve over a 239 bit binary field c2tnb239v3: X9.62 curve over a 239 bit binary field c2pnb272w1: X9.62 curve over a 272 bit binary field c2pnb304w1: X9.62 curve over a 304 bit binary field c2tnb359v1: X9.62 curve over a 359 bit binary field c2pnb368w1: X9.62 curve over a 368 bit binary field c2tnb431r1: X9.62 curve over a 431 bit binary field wap-wsg-idm-ecid-wtls1: WTLS curve over a 113 bit binary field wap-wsg-idm-ecid-wtls3: NIST/SECG/WTLS curve over a 163 bit binary field wap-wsg-idm-ecid-wtls4: SECG curve over a 113 bit binary field wap-wsg-idm-ecid-wtls5: X9.62 curve over a 163 bit binary field wap-wsg-idm-ecid-wtls6: SECG/WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls7: SECG/WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls8: WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls9: WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls10: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls11: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls12: WTLS curvs over a 224 bit prime field Oakley-EC2N-3: IPSec/IKE/Oakley curve #3 over a 155 bit binary field. Not suitable for ECDSA. Questionable extension field! Oakley-EC2N-4: IPSec/IKE/Oakley curve #4 over a 185 bit binary field. Not suitable for ECDSA. Questionable extension field! Now generate new private key with chosen curve (prime256v1 looks fine, like: c2pnb272w1, sect283k1, sect283r1 or secp256k1, etc)\n$ openssl ecparam -out ec_key.pem -name prime256v1 -genkey And generate self-signed certificate that could be directly used:\n$ openssl req -new -key ec_key.pem -x509 -nodes -days 365 -out cert.pem You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [AU]:PL State or Province Name (full name) [Some-State]:example.pl Locality Name (eg, city) []:example.pl Organization Name (eg, company) [Internet Widgits Pty Ltd]:example.pl Organizational Unit Name (eg, section) []:example.pl Common Name (e.g. server FQDN or YOUR name) []:example.pl Email Address []:hostmaster@example.pl ","permalink":"https://timor.site/2013/12/generate-ecdsa-key-with-openssl/","summary":"After the last NSA scandal I\u0026rsquo;ve found some time to read some texts about PFS and ECDSA keys lately. I always used RSA keys but wanted to give a try to ECDSA so I wanted to give it a try (test performance, etc). Here is how I\u0026rsquo;ve done it.\nFirstly find your favorite curve. A short tip about bit length and complexity could be found here. From it you will now that using 256 bit ECDSA key should be enough for next 10-20 years.","title":"Generate ECDSA key with OpenSSL"},{"content":"Lately I tried to remove some streams from MKV file - I wanted: video, audio in my language and no subtitles. I achieved it with mkvtoolnix utils.\nFirstly I have to identify streams in file:\n$ mkvmerge -i input_file.mkv File \u0026#39;test.mkv\u0026#39;: container: Matroska Track ID 0: video (V_MPEG4/ISO/AVC) Track ID 1: audio (A_DTS) Track ID 2: audio (A_AC3) Track ID 3: audio (A_DTS) Track ID 4: audio (A_AC3) Track ID 5: subtitles (S_TEXT/UTF8) Track ID 6: subtitles (S_TEXT/UTF8) Chapters: 16 entries You could use more verbose tool mkvinfo for that purpose too.\nNow we know what to do next:\n$ mkvmerge -o out.mkv -d 0 --audio-tracks 2 --no-subtitles input_file.mkv mkvmerge v6.3.0 (\u0026#39;You can\u0026#39;t stop me!\u0026#39;) built on Jun 29 2013 11:48:33 \u0026#39;test.mkv\u0026#39;: Using the demultiplexer for the format \u0026#39;Matroska\u0026#39;. \u0026#39;test.mkv\u0026#39; track 0: Using the output module for the format \u0026#39;AVC/h.264\u0026#39;. \u0026#39;test.mkv\u0026#39; track 2: Using the output module for the format \u0026#39;AC3\u0026#39;. The file \u0026#39;out.mkv\u0026#39; has been opened for writing. Progress: 100% The cue entries (the index) are being written... Muxing took 3 minutes 10 seconds. This is the fastest way - no need for conversion of any stream.\nSource: http://bunin.livejournal.com/357913.html\n","permalink":"https://timor.site/2013/12/delete-audio-track-from-mkv-file/","summary":"Lately I tried to remove some streams from MKV file - I wanted: video, audio in my language and no subtitles. I achieved it with mkvtoolnix utils.\nFirstly I have to identify streams in file:\n$ mkvmerge -i input_file.mkv File \u0026#39;test.mkv\u0026#39;: container: Matroska Track ID 0: video (V_MPEG4/ISO/AVC) Track ID 1: audio (A_DTS) Track ID 2: audio (A_AC3) Track ID 3: audio (A_DTS) Track ID 4: audio (A_AC3) Track ID 5: subtitles (S_TEXT/UTF8) Track ID 6: subtitles (S_TEXT/UTF8) Chapters: 16 entries You could use more verbose tool mkvinfo for that purpose too.","title":"Delete audio track from mkv file"},{"content":"Some time ago I prepared a PC that was responsible for batch encoding of movies to formats suitable for web players (such as. Video.js, JW Player, Flowplayer, etc.)\nI used HandBrake for conversion to MP4 format (becase this soft was the fastest one) and ffmpeg (aka avconv in new version) for two pass encoding to WEBM.\nBelow are commands used by me for that conversion:\nMP4 HandBrakeCLI -e x264 -q 20.0 -a 1 -E faac -B 64 -6 mono -R 44.1 -D 0.0 -f mp4 --strict-anamorphic -m -x ref=1:weightp=1:subq=2:rc-lookahead=10:trellis=0:8x8dct=0 -O -i \u0026#34;input_file.avi\u0026#34; -o \u0026#34;output_file.mp4\u0026#34; WEBM avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -an -pass 1 -f webm /dev/null avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -codec:a libvorbis -b:a 96k -pass 2 -f webm \u0026#34;output_file.webm\u0026#34; Nginx configuration for MP4 I used configuration similar to that below for MP4 pseudostreaming and to protect direct urls to videos from linking on other sites (links will expire after sometime). There is also example usage of limit_rate clause that will slow down downloading of a file (it\u0026rsquo;s still two times bigger than video streaming speed so should be enough).\nlocation ~ \\.m(p4|4v)$ { ## This must match the URI part related to the MD5 hash and expiration time. secure_link $arg_ticket,$arg_e; ## The MD5 hash is built from our secret token, the URI($path in PHP) and our expiration time. secure_link_md5 somerandomtext$uri$arg_e; ## If the hash is incorrect then $secure_link is a null string. if ($secure_link = \u0026#34;\u0026#34;) { return 403; } ## The current local time is greater than the specified expiration time. if ($secure_link = \u0026#34;0\u0026#34;) { return 403; } ## If everything is ok $secure_link is 1. mp4; mp4_buffer_size 10m; mp4_max_buffer_size 1024m; limit_rate 1024k; limit_rate_after 5m; } Sources http://nginx.org/en/docs/http/ngx_http_mp4_module.html\nhttp://wiki.nginx.org/HttpSecureLinkModule\n","permalink":"https://timor.site/2013/12/preparing-video-files-for-streaming-on-website-in-mp4-and-webm-format/","summary":"Some time ago I prepared a PC that was responsible for batch encoding of movies to formats suitable for web players (such as. Video.js, JW Player, Flowplayer, etc.)\nI used HandBrake for conversion to MP4 format (becase this soft was the fastest one) and ffmpeg (aka avconv in new version) for two pass encoding to WEBM.\nBelow are commands used by me for that conversion:\nMP4 HandBrakeCLI -e x264 -q 20.","title":"Preparing video files for streaming on website in MP4 and WEBM format"},{"content":"SPDY is new protocol proposed by Google as an alternative for HTTP(S). Currently Chrome and Firefox browsers are using it as default if available on server. It is faster in most cases by few to several percent. The side effect of using mod_spdy is that it\u0026rsquo;s working well only with thread safe Apache\u0026rsquo;s modules. PHP module for Apache is not thread safe so we need to use PHP as CGI or FastCGI service. CGI is slow - so running mod_spdy for performance gain with CGI is simply pointless. FastCGI is better but it\u0026rsquo;s not possible to share APC cache in FastCGI mode (ex. using spawn-fcgi), so it\u0026rsquo;s poor too. Best for PHP is PHP-FPM which is FastCGI service with dynamic process manager and could use full advantages of APC. In such configuration I could switch from apache prefork to worker which should use less resources and be more¬†predictable.\nInstallation On Squeeze we need to install dot.deb repository - instructions are here:¬†http://www.dotdeb.org/instructions/\nThen we could install:\napt-get install apache2-mpm-worker php5-fpm libapache2-mod-fastcgi Now, mod_spdy - packages are available here:¬†https://developers.google.com/speed/spdy/mod_spdy/ Choose your architecture.\nwget¬†https://dl-ssl.google.com/dl/linux/direct/mod-spdy-beta_current_i386.deb dpkg -i¬†mod-spdy-beta_current_i386.deb Installation of this package will add automatically a new apt repository for mod_spdy.\nIf you have Apache\u0026rsquo;s module for PHP still installed you should remove it (you won\u0026rsquo;t need in anymore):\napt-get purge libapache2-mod-php5 Configuring PHP-FPM First I\u0026rsquo;m changing php-fpm default pool configuration file - edit /etc/php5/fpm/pool.d/www.conf\n; I want it to listen on socket, not on port listen = /var/run/php5-fpm/site1.socket ;uncomment to set proper permission for socket listen.owner = www-data listen.group = www-data listen.mode = 0660 ;uncomment and change to - PHP leaks, so kill child after 100 requests pm.max_requests = 100 ; for proper chroot handling we will need also php_admin_value[doc_root] = /var/www/site1 php_admin_value[cgi.fix_pathinfo] = 0 Now restart php-fpm:\nservice php5-fpm restart Connecting Apache with PHP-FPM In VirtualHost paste this:\n\u0026lt;IfModule mod_fastcgi.c\u0026gt; Alias /php5.fcgi /var/www/site1/php5.fcgi FastCGIExternalServer /var/www/site1/php5.fcgi -socket /var/lib/apache2/fastcgi/site1.socket AddType application/x-httpd-fastphp5 .php Action application/x-httpd-fastphp5 /php5.fcgi \u0026lt;Directory \u0026#34;/var/www/site1/\u0026#34;\u0026gt; Order deny,allow Deny from all \u0026lt;Files \u0026#34;php5.fcgi\u0026#34;\u0026gt; Order allow,deny Allow from all \u0026lt;/Files\u0026gt; \u0026lt;/Directory\u0026gt; \u0026lt;/IfModule\u0026gt; Enable needed modules and restart Apache:\na2enmod actions a2enmod fastcgi service apache2 restart SSL SPDY requires encrypted connection so you need configured SSL (virtualhost running on port 443). Typical configuration for SSL looks similar to this:\n\u0026lt;virtualhost *:443\u0026gt; # some random stuff - exactly like in you NON SSL configuration üòÑ SSLEngine on SSLCertificateFile /etc/ssl/certs/example.com.crt SSLCertificateKeyFile /etc/ssl/private/example.com.priv.key SSLCACertificateFile /etc/ssl/private/ca.crt\u0026lt;/virtualhost\u0026gt; Testing Should work now üòÉ\nSo, use Chromium, enter the site you just configured and then on second tab go to: chrome://net-internals/#spdy. You should see your site there if it\u0026rsquo;s running on SPDY.\nYou could also use plugins for Firefox or Chromium to test if site is running on SPDY.\nAdvertise SPDY on HTTP When you test if SPDY is working fine (and is faster in your configuration) you could advertise availability of SPDY protocol on your HTTP VirtualHost. Thanks to that when browser supports SPDY it will use it for faster access. To do this just add header in configuration:\nHeader set Alternate-Protocol \u0026#34;443:spdy/2\u0026#34; There are more options that could be used, if you need just check docs here.\n","permalink":"https://timor.site/2013/12/running-apache-with-mod_spdy-and-php-fpm/","summary":"SPDY is new protocol proposed by Google as an alternative for HTTP(S). Currently Chrome and Firefox browsers are using it as default if available on server. It is faster in most cases by few to several percent. The side effect of using mod_spdy is that it\u0026rsquo;s working well only with thread safe Apache\u0026rsquo;s modules. PHP module for Apache is not thread safe so we need to use PHP as CGI or FastCGI service.","title":"Running Apache with mod_spdy and PHP-FPM"},{"content":"Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as failed in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I\u0026rsquo;ve got\u0026hellip;\nAfter rebuild I\u0026rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.\nWhat I do:\nI\u0026rsquo;ve added checked disk to array as spare mdadm /dev/md0 -a /dev/sdb1 I\u0026rsquo;ve¬†set this \u0026ldquo;slow\u0026rdquo; drive as failed (and rebuild started) mdadm /dev/md0 -f /dev/sdf1 I removed this drive echo 1 \u0026gt; /sys/block/sdf/device/delete I run dmesg to see what was scsi host of this drive dmesg [ 1302.433419] sd 8:0:0:0: [sdf] Synchronizing SCSI cache [ 1302.433468] sd 8:0:0:0: [sdf] Stopping disk Now I could readd this drive echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host8/scan I run dmesg¬†again to see if disk was detected dmesg [ 1489.013270] scsi 8:0:0:0: Direct-Access ATA SEAGATE ST2000DM001 1AQ1 PQ: 0 ANSI: 5 [ 1489.013375] sd 8:0:0:0: [sdf] 3907029168 512-byte logical blocks: (2.00 TB/1.81 TiB) [ 1489.013397] sd 8:0:0:0: Attached scsi generic sg6 type 0 [ 1489.013445] sd 8:0:0:0: [sdf] Write Protect is off [ 1489.013447] sd 8:0:0:0: [sdf] Mode Sense: 00 3a 00 00 [ 1489.013466] sd 8:0:0:0: [sdf] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [ 1498.318159] sdf: sdf1 [ 1498.318391] sd 8:0:0:0: [sdf] Attached SCSI disk Now I could re-add this drive to the array as spare mdadm /dev/md0 -a /dev/sdf1 ","permalink":"https://timor.site/2013/12/re-adding-failed-drive-in-mdadm/","summary":"Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as failed in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I\u0026rsquo;ve got\u0026hellip;\nAfter rebuild I\u0026rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.\nWhat I do:","title":"Re-adding failed drive in mdadm"},{"content":"I was configuring GlusterFS on few servers using Ansible and have a need to update /etc/hosts with hostnames for easier configuration. I found this one working:\n- name: Update /etc/hosts lineinfile: dest=/etc/hosts regexp=\u0026#39;.*{{item}}$\u0026#39; line=\u0026#39;{{hostvars.{{item}}.ansible_default_ipv4.address}} {{item}}\u0026#39; state=present with_items: \u0026#39;{{groups.somegroup}}\u0026#39; Source: http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html\n","permalink":"https://timor.site/2013/12/ansible-dynamicaly-update-etc-hosts-files-on-target-servers/","summary":"I was configuring GlusterFS on few servers using Ansible and have a need to update /etc/hosts with hostnames for easier configuration. I found this one working:\n- name: Update /etc/hosts lineinfile: dest=/etc/hosts regexp=\u0026#39;.*{{item}}$\u0026#39; line=\u0026#39;{{hostvars.{{item}}.ansible_default_ipv4.address}} {{item}}\u0026#39; state=present with_items: \u0026#39;{{groups.somegroup}}\u0026#39; Source: http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html","title":"Ansible - Dynamicaly update /etc/hosts files on target servers"},{"content":"I\u0026rsquo;ve started testing new Ghost blogging platform for a while on a virtual machine before I take decision about switching to it (or maybe won\u0026rsquo;t)\u0026hellip; After few days, I wanted to go forward with more testing and stuck on \u0026ldquo;e-mail and password\u0026rdquo; login prompt üòÉ\nI\u0026rsquo;ve started looking into files and found ghost_dir/content/data/ghost-dev.db SQLite database. It can be opened like that:\nsqlite3 content/data/ghost-dev.db Then you could see whats your mail (and other info):\nsqlite\u0026gt; select * from users Now password - after searching a little I found: ghost_dir/core/server/models/user.js file. There is a tip in it:\n// Hash the provided password with bcrypt return nodefn.call(bcrypt.hash, _user.password, null, null); So I used this site: http://bcrypthashgenerator.apphb.com/ to generate bcrypt hash and updated it in DB:\nsqlite\u0026gt; update users set password=\u0026#34;$2a$10$f29LDrB8S1JMfdF40Vmf1.h2OyhtlcefaMrFQVpHeX9XQ7Xiq17KC\u0026#34; where id = 1; sqlite\u0026gt; .quit Additionally as suggested by henshao: if the account has been locked, you can set status to active to unlock the account, like that:\nsqlite\u0026gt; update users set status = ‚Äúactive‚Äù; Now try to log with updated credentials.\nP.S. I strongly suggest to change password after successful login.\n","permalink":"https://timor.site/2013/11/reset-user-password-in-your-own-ghost-blog/","summary":"I\u0026rsquo;ve started testing new Ghost blogging platform for a while on a virtual machine before I take decision about switching to it (or maybe won\u0026rsquo;t)\u0026hellip; After few days, I wanted to go forward with more testing and stuck on \u0026ldquo;e-mail and password\u0026rdquo; login prompt üòÉ\nI\u0026rsquo;ve started looking into files and found ghost_dir/content/data/ghost-dev.db SQLite database. It can be opened like that:\nsqlite3 content/data/ghost-dev.db Then you could see whats your mail (and other info):","title":"Reset user password in your own Ghost Blog"},{"content":"It\u0026rsquo;s quite rare to have problems with XFS and inodes exhaustion. Mostly because XFS doesn\u0026rsquo;t have inode limit in a manner known from other filesystems - it\u0026rsquo;s using some percentage of whole filesystem as a limit and in most distributions it\u0026rsquo;s 25%. So it\u0026rsquo;s really huge amount of inodes. But some tools and distributions lowered limit ex. 5% or 10% and there you could have problems more often.\nYou could check what is you limit by issuing xfs_info with drive and searching for imaxpct value:\nxfs_info root@zombi:~# xfs_info /srv/backup/ metadane=/dev/mapper/slow-backup isize=256 agcount=17, agsize=2621440 blks = sectsz=512 attr=2 data = bsize=4096 blocks=44564480, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal bsize=4096 blocks=20480, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime=brak extsz=4096 blocks=0, rtextents=0 In this case I have 25% and it could be changed dynamically with xfs_growfs -m XX where XX is new percentage of volume capacity.\nIt\u0026rsquo;s also possible to change imaxpct on creation time by adding option -i maxpct=XX.\n","permalink":"https://timor.site/2013/11/inodes-on-xfs/","summary":"It\u0026rsquo;s quite rare to have problems with XFS and inodes exhaustion. Mostly because XFS doesn\u0026rsquo;t have inode limit in a manner known from other filesystems - it\u0026rsquo;s using some percentage of whole filesystem as a limit and in most distributions it\u0026rsquo;s 25%. So it\u0026rsquo;s really huge amount of inodes. But some tools and distributions lowered limit ex. 5% or 10% and there you could have problems more often.\nYou could check what is you limit by issuing xfs_info with drive and searching for imaxpct value:","title":"Inodes on XFS"},{"content":"I\u0026rsquo;ve bought a NAS and customized it a little. But there was one thing which make my nights sleepless. NAS was seeking disks every 5~10 seconds - these was really irritating - especially when it was silent in room. I found that part of firmware was indexing or logging something so I wanted it dead! kill -9 was unsuccessful - process restarted after a while\u0026hellip;. wrrr\u0026hellip;\nI googled a little and found another signal I could use SIGSTOP, which will freeze process until I send SIGCONT to it - that was exactly what I need (because I normally use NFS/Samba and don\u0026rsquo;t need nothing more running on this device).\nkill -SIGSTOP `pgrep svcd` Because with this process paused Web GUI is not working, I need from time to time run it again:\nkill -SIGCONT `pgrep svcd` Sources http://major.io/2009/06/15/two-great-signals-sigstop-and-sigcont/\n","permalink":"https://timor.site/2013/11/kill-with-sigstop-and-sigcont/","summary":"I\u0026rsquo;ve bought a NAS and customized it a little. But there was one thing which make my nights sleepless. NAS was seeking disks every 5~10 seconds - these was really irritating - especially when it was silent in room. I found that part of firmware was indexing or logging something so I wanted it dead! kill -9 was unsuccessful - process restarted after a while\u0026hellip;. wrrr\u0026hellip;\nI googled a little and found another signal I could use SIGSTOP, which will freeze process until I send SIGCONT to it - that was exactly what I need (because I normally use NFS/Samba and don\u0026rsquo;t need nothing more running on this device).","title":"Kill with SIGSTOP and SIGCONT"},{"content":"I\u0026rsquo;ve just bought new toy - Iomega StorCenter ix2-200 Cloud Edition. I have to play with few options before I could start using it. First thing - Firmware upgrade.\nFirmware upgrade I\u0026rsquo;ve started searching for latest firmware for ix2-200 Cloud and found that I have to register on Lenovo site to get firmware\u0026hellip; I don\u0026rsquo;t like such sites where they force me to give all private data, but after few clicks on \u0026ldquo;Recommended articles\u0026rdquo; on that site I landed here:\nhttps://lenovo-eu-en.custhelp.com/app/answers/detail/a_id/26790\nSo it looks that I don\u0026rsquo;t need to register - point for me.\nSSH access I like to be root on my devices so I want SSH access with root privileges - nothing easier. Login to admin panel and then go to URL:\nhttp://[your-nas-ip]/diagnostics.html On this site you have to enable \u0026ldquo;remote access for technical support\u0026rdquo;, then you could login to your device via SSH using credentials:\nlogin: root password: soho+your_admin_password That\u0026rsquo;s all I need to start üòÉ\nOther customization This device is running Debian Lenny for ARM but changed a little by producer. This HOWTO shows few tricks about installing custom software and changing default behavior. Based on: http://techmonks.net/installing-transmission-and-dnsmasq-on-a-nas/\n","permalink":"https://timor.site/2013/11/my-new-toy-iomega-storcenter-ix2-200-cloud-edition/","summary":"I\u0026rsquo;ve just bought new toy - Iomega StorCenter ix2-200 Cloud Edition. I have to play with few options before I could start using it. First thing - Firmware upgrade.\nFirmware upgrade I\u0026rsquo;ve started searching for latest firmware for ix2-200 Cloud and found that I have to register on Lenovo site to get firmware\u0026hellip; I don\u0026rsquo;t like such sites where they force me to give all private data, but after few clicks on \u0026ldquo;Recommended articles\u0026rdquo; on that site I landed here:","title":"My new toy - Iomega StorCenter ix2-200 Cloud Edition"},{"content":"After some configuration changes I\u0026rsquo;ve stuck with VBP not listening nor on HTTP, nor on SSH port. Last resort was to use CLI to reenable HTTP access. Connect with parameters:\nBaud rate: 9600 Parity: none Bits: 8 Stopbits: 1 Flow control: none Then in login prompt you have to use login credentials (yes - they\u0026rsquo;re the same on every box (WTF?)):\nUser: - root Pass: - @#$%^\u0026amp;*!() Password is shift + 2345678190 - there is 1 before 9!\nAfter logging on, there are three commands that should reenable HTTP access:\n/etc/conf/bin/ep_mfg cfg_commit /etc/conf/bin/config_network Works for me!\nSources http://community.polycom.com/t5/Management-Security-and-Rich/VBP-ST-Factory-Reset-Problems/td-p/8974 http://community.polycom.com/t5/Management-Security-and-Rich/I-can-t-access-to-a-Polycom-VBP-5300-ST-by-HTTP/td-p/18698 http://blogs.scansource.com/polycom-vbp-e-series-reset-procedures/\n","permalink":"https://timor.site/2013/11/reenable-web-interface-on-polycom-vbp-5300-st-from-cli/","summary":"After some configuration changes I\u0026rsquo;ve stuck with VBP not listening nor on HTTP, nor on SSH port. Last resort was to use CLI to reenable HTTP access. Connect with parameters:\nBaud rate: 9600 Parity: none Bits: 8 Stopbits: 1 Flow control: none Then in login prompt you have to use login credentials (yes - they\u0026rsquo;re the same on every box (WTF?)):\nUser: - root Pass: - @#$%^\u0026amp;*!() Password is shift + 2345678190 - there is 1 before 9!","title":"Reenable web interface on Polycom VBP 5300 ST from CLI"},{"content":"I\u0026rsquo;ve crated this blog to get feedback from other IT guys about what I\u0026rsquo;m doing wrong (or not good enough). But this idea failed\u0026hellip;\nI have only few comments on my blog (and about thousand and a half spams per month) - so, no feedback in my national language at all.\nSwitching to English should make my audience bigger and I hope to have more attention thanks to that. This will be also a good practice of my English skill. I hope to find enough free time to translate some of older articles, but for now I\u0026rsquo;m thinking rather about changing engine of blog - to something more convenient. Maybe Octopress or Ghost.\nI hope you enjoy a little more.\n","permalink":"https://timor.site/2013/11/changing-language-of-articles-on-my-blog-to-english/","summary":"I\u0026rsquo;ve crated this blog to get feedback from other IT guys about what I\u0026rsquo;m doing wrong (or not good enough). But this idea failed\u0026hellip;\nI have only few comments on my blog (and about thousand and a half spams per month) - so, no feedback in my national language at all.\nSwitching to English should make my audience bigger and I hope to have more attention thanks to that. This will be also a good practice of my English skill.","title":"Changing language of articles on my blog to English"},{"content":"Niedawno zainteresowa≈Çem siƒô us≈ÇugƒÖ Gearman i jedynej rzeczy kt√≥rej mi brakowa≈Ço to jakiego≈õ ≈Çatwego mechanizmu zarzƒÖdzajƒÖcego workerami. Ale jak zwykle okaza≈Ço siƒô ≈ºe inni mieli ju≈º ten problem i odpowiednie narzƒôdzie istnieje - mowa o GearmanManagerze.\nInstalacja GearmanManagera Aby zainstalowaƒá GeramanManagera na serwerze gdzie ju≈º mamy Gearmana trzeba wykonaƒá kilka krok√≥w (wcze≈õniej powinni≈õmy te≈º zainstalowaƒá modu≈Ç gearmana do php\u0026rsquo;a):\napt-get install git -y git clone https://github.com/brianlmoon/GearmanManager.git cd GearmanManager/install chmod +x install.sh ./install.sh Detecting linux distro as redhat- or debian-compatible Where is your php executable? (usually /usr/bin) /usr/bin Which PHP library to use, pecl/gearman or PEAR::Net_Gearman? 1) pecl 2) pear #? 1 Installing to /usr/local/share/gearman-manager Installing executable to /usr/local/bin/gearman-manager Installing configs to /etc/gearman-manager Installing init script to /etc/init.d/gearman-manager Install ok! Run /etc/init.d/gearman-manager to start and stop Worker scripts can be installed in /etc/gearman-manager/workers, configuration can be edited in /etc/gearman-manager/config.ini Mamy dzia≈ÇajƒÖcego GearmanManagera. Zalecam przyglƒÖdniƒôcie siƒô plikowi config-advanced.ini bo jest tam kilka opcji, kt√≥re warto dodatkowo ustawiƒá.\nSprawdzenie dzia≈Çania Geramana i GearmanManagera Przyk≈Çad workera mo≈ºna znale≈∫ƒá tutaj http://brian.moonspot.net/GearmanManager. Po pobraniu go i zapisaniu w pliku /etc/gearman-manager/workers/fetch_url.php mo≈ºemy rƒôcznie zakolejkowaƒá zadanie dla Geramana:\ngearman -f fetch_url -- http://google.pl/robots.txt ≈πr√≥d≈Ço:\nhttps://github.com/brianlmoon/GearmanManager\nhttp://brian.moonspot.net/GearmanManager\n","permalink":"https://timor.site/2013/11/gearmanmanager-wygodne-zarzadzanie-workerami/","summary":"Niedawno zainteresowa≈Çem siƒô us≈ÇugƒÖ Gearman i jedynej rzeczy kt√≥rej mi brakowa≈Ço to jakiego≈õ ≈Çatwego mechanizmu zarzƒÖdzajƒÖcego workerami. Ale jak zwykle okaza≈Ço siƒô ≈ºe inni mieli ju≈º ten problem i odpowiednie narzƒôdzie istnieje - mowa o GearmanManagerze.\nInstalacja GearmanManagera Aby zainstalowaƒá GeramanManagera na serwerze gdzie ju≈º mamy Gearmana trzeba wykonaƒá kilka krok√≥w (wcze≈õniej powinni≈õmy te≈º zainstalowaƒá modu≈Ç gearmana do php\u0026rsquo;a):\napt-get install git -y git clone https://github.com/brianlmoon/GearmanManager.git cd GearmanManager/install chmod +x install.","title":"GearmanManager: wygodne zarzƒÖdzanie workerami"},{"content":"W teorii nie powinno siƒô blokowaƒá aktualizacji pakiet√≥w bo ≈ÇatajƒÖ dziury itd\u0026hellip;. Ale! Zdarzy≈Çy mi siƒô ostatnio dwie sytuacje, kt√≥re do tego mnie zmusi≈Çy:\naktualizacja hudsona ko≈Ñczy≈Ça siƒô b≈Çƒôdem przy starcie us≈Çugi, aktualizacja domU Xen sko≈Ñczy≈Ça siƒô problemem z kompatybilno≈õciƒÖ mechanizmu udev w systemie i jƒÖdrze (hypervisor mia≈Ç starsze jƒÖdro ni≈º spodziewa≈Ço siƒô DomU). W takich sytuacjach bardzo przydaje siƒô mo≈ºliwo≈õƒá zablokowania aktualizacji jednej \u0026ldquo;psujƒÖcej\u0026rdquo; paczki na pewien okres czasu by nie op√≥≈∫niaƒá innych aktualizacji a sobie daƒá czas na rozpracowanie problemu.\nWstrzymywanie aktualizacji pakietu Aktualizacjƒô wstrzymujemy o tak:\necho \u0026#34;paczka hold\u0026#34; | dpkg --set-selections Odblokowanie aktualizacji pakietu By ponownie zezwoliƒá na aktualizacjƒô wystarczy:\necho \u0026#34;paczka install\u0026#34; | dpkg --set-selections Sprawdzenie listy wstrzymanych paczek dpkg --get-selections | grep hold ≈πr√≥d≈Ço:\nhttp://www.debianadmin.com/how-to-prevent-a-package-from-being-updated-in-debian.html\n","permalink":"https://timor.site/2013/11/debian-zablokowanie-aktualizacji-pakietu/","summary":"W teorii nie powinno siƒô blokowaƒá aktualizacji pakiet√≥w bo ≈ÇatajƒÖ dziury itd\u0026hellip;. Ale! Zdarzy≈Çy mi siƒô ostatnio dwie sytuacje, kt√≥re do tego mnie zmusi≈Çy:\naktualizacja hudsona ko≈Ñczy≈Ça siƒô b≈Çƒôdem przy starcie us≈Çugi, aktualizacja domU Xen sko≈Ñczy≈Ça siƒô problemem z kompatybilno≈õciƒÖ mechanizmu udev w systemie i jƒÖdrze (hypervisor mia≈Ç starsze jƒÖdro ni≈º spodziewa≈Ço siƒô DomU). W takich sytuacjach bardzo przydaje siƒô mo≈ºliwo≈õƒá zablokowania aktualizacji jednej \u0026ldquo;psujƒÖcej\u0026rdquo; paczki na pewien okres czasu by nie op√≥≈∫niaƒá innych aktualizacji a sobie daƒá czas na rozpracowanie problemu.","title":"Debian - zablokowanie aktualizacji pakietu"},{"content":"Ostatnio trafi≈Çem na ciekawƒÖ us≈Çugƒô, kt√≥ra pozwala oddelegowaƒá d≈Çugo trwajƒÖce zadania z us≈Çugi webowej. Mowa o Gearman\u0026rsquo;ie. Us≈Çuga jest o tyle ciekawa ≈ºe nie narzuca ani jƒôzyka dla klienta (wiƒôkszo≈õƒá popularnych ma gotowe biblioteki), ani jƒôzyk dla skrypt√≥w w tej us≈Çudze nie jest narzucany. Mo≈ºna tƒô us≈Çugƒô wykorzystaƒá jako most pomiƒôdzy PHP a np. JavƒÖ/Pythonem lub do zlecenia zada≈Ñ z serwera na Linux\u0026rsquo;ie do wykonania na serwerze Windowsowym (bo np. narzƒôdzia dostƒôpne sƒÖ tylko dla Windowsa). O innych zaletach mo≈ºna poczytaƒá na stronce wiƒôc nie bƒôdƒô przynudzaƒá.\nStandardowo zainstalowa≈Çem paczkƒô z repo Debiania i rozbi≈Çem siƒô przy kompilacji modu≈Çu z PECL\u0026rsquo;a - w repo by≈Ça jaka≈õ prehistoryczna wersja. Postanowi≈Çem uruchomiƒá aktualnƒÖ wersje 1.0.6 z ga≈Çƒôzi testowej przekompilowujƒÖc jƒÖ na Wheezym (by uniknƒÖƒá zale≈ºno≈õci z wersji testowej).\nInstalacja gearman\u0026rsquo;a Dorzucamy ≈∫r√≥d≈Ça z testing - dziƒôki temu nie aktualizujemy systemu ale bƒôdziemy mogli pobraƒá ≈õwie≈ºe paczki ≈∫r√≥d≈Çowe:\necho \u0026#34;deb-src http://ftp.pl.debian.org/debian jessie main non-free contrib\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list apt-get update Przygotowujemy katalog roboczy, pobieramy paczki i kompilujemy:\nmkdir gearman cd gearman apt-get build-dep gearman-job-server -y apt-get install bzr apt-get source gearman-job-server cd gearmand-1.0.6 ./debian/rules clean ./debian/rules binary cd .. dpkg -i gearman-job-server_1.0.6-2_i386.deb gearman-tools_1.0.6-2_i386.deb libgearman-dev_1.0.6-2_i386.deb libgearman7_1.0.6-2_i386.deb apt-get install -f -y Musia≈Çem rƒôcznie doinstalowaƒá bazar (paczka bzr), bo w czasie kompilacji pojawia≈Çy siƒô b≈Çƒôdy z tym poleceniem - nie jestem pewien na ile jest potrzebne ale oczywi≈õcie mo≈ºemy to posprzƒÖtaƒá po skompilowaniu paczek.\nP.S. Jestem przekonany ≈ºe zamiast \u0026ldquo;rules clean/binary\u0026rdquo; jest jakie≈õ polecenie, kt√≥rego powinno siƒô u≈ºyƒá ale nie mog≈Çem sobie go przypomnieƒá\u0026hellip;\nInstalacja modu≈Çu dla PHP'a Poniewa≈º wcze≈õniej zainstalowali≈õmy aktualne biblioteki libgearman-dev to instalacja modu≈Çu dla PHP powinna byƒá bardzo prosta:\npecl install gearman echo \u0026#34;extension=gearman.so\u0026#34; \u0026gt; /etc/php5/conf.d/gearman.ini P.S. W paczkach PHP 5.3 z dotdeb\u0026rsquo;a mo≈ºna znale≈∫ƒá ju≈º skompilowany modu≈Ç dla gearman\u0026rsquo;a.\nNa razie tyle - muszƒô teraz poszukaƒá jak w wygodny, zautomatyzowany spos√≥b zarzƒÖdzaƒá skryptami zleconymi do gearman\u0026rsquo;a.\n","permalink":"https://timor.site/2013/10/instalacja-gearman-job-server-1-0-6-na-debianie-wheezy/","summary":"Ostatnio trafi≈Çem na ciekawƒÖ us≈Çugƒô, kt√≥ra pozwala oddelegowaƒá d≈Çugo trwajƒÖce zadania z us≈Çugi webowej. Mowa o Gearman\u0026rsquo;ie. Us≈Çuga jest o tyle ciekawa ≈ºe nie narzuca ani jƒôzyka dla klienta (wiƒôkszo≈õƒá popularnych ma gotowe biblioteki), ani jƒôzyk dla skrypt√≥w w tej us≈Çudze nie jest narzucany. Mo≈ºna tƒô us≈Çugƒô wykorzystaƒá jako most pomiƒôdzy PHP a np. JavƒÖ/Pythonem lub do zlecenia zada≈Ñ z serwera na Linux\u0026rsquo;ie do wykonania na serwerze Windowsowym (bo np. narzƒôdzia dostƒôpne sƒÖ tylko dla Windowsa).","title":"Instalacja gearman-job-server 1.0.6 na Debianie Wheezy"},{"content":"Kolejna zabawna sytuacja - pewna aplikacja dotNET\u0026rsquo;owa dzia≈Ça≈Ça dziwnie na 64-bitowym systemie, a tymczasem na 32-bitowej maszynie ta sama aplikacja dzia≈Ça≈Ça bez problem√≥w. Jedyna r√≥≈ºnica to inne wersje klient√≥w ODBC na tych systemach, kt√≥re po kilku testach okaza≈Çy siƒô byƒá przyczynƒÖ ca≈Çego z≈Ça.\nPojawi≈Ç siƒô pomys≈Ç by odpaliƒá aplikacje na 64 bitowym systemie ale w trybie 32 bit - poni≈ºej kr√≥tkie HOWTO jak to osiƒÖgnƒÖƒá:\npotrzebujemy narzƒôdzia corflags.exe kt√≥re pozwoli oznaczyƒá nam binarkƒô jako 32-bitowƒÖ, do pobrania tutaj a instrukcja jej u≈ºycia tutaj. Instalujemy Windows SDK i zaznaczamy wy≈ÇƒÖcznie .NET Development Tools w kategorii Developer Tools / Windows Development Tools Odpalamy CMD i w nim CorFlags z lokalizacji: C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin (przynajmniej u mnie): cd C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\ CorFlags.exe c:\\sciezka\\do\\pliku.exe /32BIT+ Tyle - aplikacja uruchomi≈Ça siƒô bez problemu jako 32 bitowa i korzysta≈Ça z 32 bitowego ODBC.\n≈πr√≥d≈Ça http://stackoverflow.com/questions/10945664/run-anycpu-as-32-bit-on-64-bit-systems\nhttp://stackoverflow.com/questions/242304/where-should-i-download-corflags-exe-from\n","permalink":"https://timor.site/2013/10/uruchamiania-aplikacji-net-jako-32-bitowej-w-64-bitowym-systemie/","summary":"Kolejna zabawna sytuacja - pewna aplikacja dotNET\u0026rsquo;owa dzia≈Ça≈Ça dziwnie na 64-bitowym systemie, a tymczasem na 32-bitowej maszynie ta sama aplikacja dzia≈Ça≈Ça bez problem√≥w. Jedyna r√≥≈ºnica to inne wersje klient√≥w ODBC na tych systemach, kt√≥re po kilku testach okaza≈Çy siƒô byƒá przyczynƒÖ ca≈Çego z≈Ça.\nPojawi≈Ç siƒô pomys≈Ç by odpaliƒá aplikacje na 64 bitowym systemie ale w trybie 32 bit - poni≈ºej kr√≥tkie HOWTO jak to osiƒÖgnƒÖƒá:\npotrzebujemy narzƒôdzia corflags.exe kt√≥re pozwoli oznaczyƒá nam binarkƒô jako 32-bitowƒÖ, do pobrania tutaj a instrukcja jej u≈ºycia tutaj.","title":"Uruchamianie aplikacji .NET jako 32-bitowej w 64-bitowym systemie"},{"content":"Od jakiego≈õ czasu mo≈ºna kupiƒá w NetArcie certyfikaty SSL, a niedawno zrobili na nie promocjƒô - 15z≈Ç za pierwszy rok (za certyfikat na jednƒÖ stronkƒô). Tzw. tanie i dobre. Po wyrobieniu certyfikatu i zapisaniu z panelu klienta mam pliczki: stonka.crt i netart_rootca.crt, kt√≥re wrzucamy do Apachego, powiedzmy tak:\nSSLCertificateFile /etc/ssl/certs/stonka.crt SSLCertificateKeyFile /etc/ssl/private/priv.key SSLCACertificateFile /etc/ssl/certs/netart_rootca.crt Certyfikat dzia≈Ça w Chromie ale nie weryfikuje siƒô w Firefoxie i Internet Explorerze. FF wy≈õwietla b≈ÇƒÖd: sec_error_unknown_issuer - co oznacza brak certyfikatu wystawcy gdzie≈õ w ≈Ça≈Ñcuchu certyfikat√≥w. W FAQ zero jak chodzi o konfiguracjƒô certyfikat√≥w na serwerze poza NetArt\u0026rsquo;em\u0026hellip;\nPrzeglƒÖdnƒÖ≈Çem informacje certyfikatu rootca:\nopenssl x509 -in netart_rootca.crt -text -noout\u0026lt;/pre\u0026gt; Certificate: Data: Version: 3 (0x2) Serial Number: 46:53:b1:a6:1e:ba:2d:c7:a3:2e:f9:39:5a:4e:f8:8c Signature Algorithm: sha1WithRSAEncryption Issuer: C=PL, O=Unizeto Technologies S.A., OU=Certum Certification Authority, CN=Certum Global Services CA Validity Not Before: Jul 6 10:31:40 2012 GMT Not After : Jul 4 10:31:40 2022 GMT Subject: C=PL, O=NetArt Sp\\xC3\\xB3\\xC5\\x82ka Akcyjna S.K.A., OU=http://nazwa.pl, CN=nazwaSSL Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public Key: (2048 bit) Modulus (2048 bit): 00:cc:91:f5:f7:01:09:4f:75:c8:09:c7:14:8f:e4: 1a:99:78:20:99:40:59:6f:10:2f:ff:fe:d0:10:ff: 06:a3:39:3d:c4:f1:4b:07:cf:22:39:20:80:43:50: c1:af:b4:01:71:a0:a3:30:11:52:d3:d2:98:d9:c2: 69:f7:e3:00:d9:19:3f:3d:b3:3b:52:75:e3:d3:0c: ab:ff:57:01:3a:83:5c:f5:02:bb:28:fe:90:38:8e: a2:84:cf:61:48:e7:99:e0:72:24:b6:11:58:4a:18: 57:0d:34:18:5e:35:c8:b3:ac:04:5f:8d:38:2f:a2: cf:d2:dc:74:d8:41:02:ec:e0:db:0c:54:81:a4:7a: c5:34:d5:19:86:b6:1e:65:f7:3c:f6:b2:dd:3a:b5: b7:91:61:18:fd:81:2c:8a:68:d7:d6:a8:33:b7:47: b8:f9:48:ad:35:ee:11:93:f9:c2:a9:fa:94:8e:4f: bb:d1:1e:a7:64:74:b4:f9:0f:88:a7:11:a7:33:1a: c2:b1:14:0c:12:a8:6b:82:44:78:4e:d5:79:8f:5c: 60:29:47:4c:36:35:52:c7:ad:6c:c0:20:39:93:f1: c8:b3:3b:d9:c6:ec:dd:22:45:27:a2:50:12:07:f8: fe:38:79:24:89:b9:f7üá©üá™e0:c6:e9:64:e3:f4:0b: fa:c7 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 CRL Distribution Points: URI:http://crl.certum.pl/gsca.crl Authority Information Access: CA Issuers - URI:http://repository.certum.pl/gsca.cer X509v3 Authority Key Identifier: keyid:45:C5:B2:86:4E:CC:DD:29:97:E4:DD:14:C4:6E:AE:4D:B8:C1:77:F8 X509v3 Subject Key Identifier: 9D:CE:F0:5A:B4:CB:25:CF:36:A5:82:5D:8F:F7:7F:98:46:19:37:2E X509v3 Key Usage: critical Certificate Sign, CRL Sign X509v3 Certificate Policies: Policy: X509v3 Any Policy CPS: https://www.certum.pl/CPS Signature Algorithm: sha1WithRSAEncryption 53:01:c7:87:ad:ac:d7:52:32:1f:79:5d:87:f0:01:88:8e:99: 3f:07:d8:e4:bc:84:0a:8d:5f:d5:d5:62:c2:9b:79:33:46:f9: 8a:d9:b2:96:ed:35:8a:29:3b:5f:38:7a:6a:70:1d:8b:84:1a: a3:90:81:f7:2e:60:77:78:f0:d0:84:a3:e9:8a:3c:ef:8a:34: 6b:b1:9c:e8:e1:76:f4:87:1e:7b:3c:18:6f:98:70:2c:2a:8a: 22:f5:ba:96:52:7e:26:62:8b:96:03:32:22:f9:80:d7:f1:dd: 9e:c2:79:b4:17:0d:40:ff:50:6a:28:6f:e8:6f:11:8a:f9:b4: 65:2b:52:86:31:50:c7:4d:e6:f3:beüá©üá™6a:d1:89:90:27:61: 6c:1c:7d:90:1f:9a:ed:02:d4:01:22:5e:8b:0b:c9:99:34:f1: 1d:04:f4:d6:d0:71:7c:8f:0c:31:a3:2f:20:ad:35:c8:d3:b4: 0b:38:74:89:a5:d3:55:72:e9:af:b0:b8:9f:02:c9:85:69:01: d8:7e:00:44:25:91:2c:5e:5b:9f:ed:52:a8:bb:5d:94:20:f4: c4:82:35üá©üá™e5:d3:05:3c:14:d5:08:80:e4:74:47:e3:fa:f7: 8c:73:40:a8:2d:ea:1f:96:c8:e3:03:2c:62:08:cc:44:02:46: a5:81:c2:0a CA NetArtu nie jest domy≈õlnie zainstalowane w ≈ºadnej przeglƒÖdarce wiƒôc nic dziwnego - ale sƒÖ tam klucze Unizeto/Certum - dorzucƒô wiƒôc klucz CA (Chrome najwidoczniej sam potrafi to zrobiƒá):\nwget http://repository.certum.pl/gsca.cer openssl x509 -inform der -in gsca.cer -out gsca.pem cat gsca.pem \u0026gt;\u0026gt; netart_rootca.crt Restart Apachego i przeglƒÖdarki ju≈º nie krzyczƒÖ. Mogliby siƒô tylko wysiliƒá na jakƒÖ≈õ instrukcjƒô albo udostƒôpnienie od razu cabudle.crt z wszystkimi potrzebnymi certami.\n","permalink":"https://timor.site/2013/10/certyfikaty-nazwassl-na-wlasnym-serwerze/","summary":"Od jakiego≈õ czasu mo≈ºna kupiƒá w NetArcie certyfikaty SSL, a niedawno zrobili na nie promocjƒô - 15z≈Ç za pierwszy rok (za certyfikat na jednƒÖ stronkƒô). Tzw. tanie i dobre. Po wyrobieniu certyfikatu i zapisaniu z panelu klienta mam pliczki: stonka.crt i netart_rootca.crt, kt√≥re wrzucamy do Apachego, powiedzmy tak:\nSSLCertificateFile /etc/ssl/certs/stonka.crt SSLCertificateKeyFile /etc/ssl/private/priv.key SSLCACertificateFile /etc/ssl/certs/netart_rootca.crt Certyfikat dzia≈Ça w Chromie ale nie weryfikuje siƒô w Firefoxie i Internet Explorerze. FF wy≈õwietla b≈ÇƒÖd: sec_error_unknown_issuer - co oznacza brak certyfikatu wystawcy gdzie≈õ w ≈Ça≈Ñcuchu certyfikat√≥w.","title":"Certyfikaty nazwaSSL na w≈Çasnym serwerze"},{"content":"Trafi≈Ç mi siƒô ostatnio ciekawy problem - ot√≥≈º standardowo przed ko≈Ñcem roku poprawia≈Çem filtry antyspamowe i optymalizowa≈Çem konfiguracjƒô Postfix\u0026rsquo;a. Chcia≈Çem zmieniƒá domy≈õlnƒÖ warto≈õƒá smtpd_delay_reject=yes na smtpd_delay_reject=no by odrzucaƒá spamer√≥w najwcze≈õniej jak to mo≈ºliwe. I ciekawe kuku, kt√≥re sobie zrobi≈Çem polega≈Ço na tym ≈ºe sam nie mog≈Çem wysy≈Çaƒá poczty po logowaniu SSL\u0026rsquo;em\u0026hellip;\nDostawa≈Çem przy tym bardzo wymownƒÖ odpowied≈∫:\nOct 8 16:30:39 tyr postfix/smtpd[21039]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP\u0026lt;/unknown\u0026gt; Wiƒôc wrzuci≈Çem debug_peer_list = 67.x.x.x do main.cf by zobaczyƒá dok≈Çadniej o co biega:\nOct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt;\u0026gt;\u0026gt; START Client host RESTRICTIONS \u0026lt; \u0026lt;\u0026lt; Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated status=0 Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject Oct 8 16:47:49 tyr postfix/smtpd[23899]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown[67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject status=2 Oct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied Oct 8 16:47:49 tyr postfix/smtpd[23899]: watchdog_pat: 0xb82c67f8 Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026lt; unknown[67.x.x.x]: QUIT Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 221 2.0.0 Bye\u0026lt;/unknown\u0026gt; generic_checks: name=permit_sasl_authenticated status=0 sugeruje ≈ºe autoryzacja jest ok, a chwile p√≥≈∫niej reject. Sprawdzi≈Çem konfiguracjƒô SASL\u0026rsquo;a (z dovecot\u0026rsquo;a) i zaczyna≈Çem bezskutecznie komentowaƒá kolejne linie w main.cf. Jaja polega≈Çy na tym ≈ºe po ustawieniu smtpd_delay_reject=yes wszystko wraca≈Ço do normy\u0026hellip; Ale nie chcia≈Çem tego tak zostawiƒá.\nOl≈õni≈Ço mnie dopiero po chwili - przecie≈º po≈ÇƒÖczenia SSL SMTP odbywajƒÖ siƒô na inny port - zdefiniowany w master.cf - mo≈ºe co≈õ tam bru≈∫dzi. A tutaj od razu rzuci≈Ça mi siƒô w oczy r√≥≈ºnica w konfiguracji dla us≈Çugi submission i ssmtp:\nsubmission inet n - - - - smtpd ... -o smtpd_sender_restrictions=reject_sender_login_mismatch,permit_sasl_authenticated,reject ... smtps inet n - - - - smtpd ... -o smtpd_client_restrictions=permit_sasl_authenticated,reject ... Submission ustawia≈Çem niedawno i widocznie trafi≈Çem na lepszego FAQ\u0026rsquo;a üòÉ WyglƒÖda na to ≈ºe sprawdzanie autoryzacji SASL w smtpd_client_restrictions odbywa≈Ço siƒô w tym przypadku zanim klient siƒô autoryzowa≈Ç (albo by≈Ço jakie≈õ lekkie op√≥≈∫nienie). Zamiana smtpd_client_restrictions na smtpd_sender_restrictions za≈Çatwi≈Ço sprawƒô. Przy okazji zauwa≈ºy≈Çem ≈ºe po SSL\u0026rsquo;u mo≈ºna by≈Ço spooflowaƒá innych u≈ºytkownik√≥w co r√≥wnie≈º postanowi≈Çem szybko naprawiƒá. A wszystko dlatego ≈ºe zachcia≈Ço mi siƒô \u0026ldquo;wczesnych optymalizacji\u0026rdquo; i chcia≈Çem w tych us≈Çugach pominƒÖƒá czƒô≈õƒá check√≥w, kt√≥re mam w main.cf.\nW minimalnej wersji konfiguracja w master.cf powinna wyglƒÖdaƒá tak:\nsubmission inet n - - - - smtpd -o smtpd_tls_security_level=encrypt -o smtpd_tls_auth_only = yes -o smtpd_sasl_auth_enable=yes smtps inet n - - - - smtpd -o smtpd_tls_wrappermode=yes -o smtpd_sasl_auth_enable=yes P.S. Znalaz≈Çem potencjalnego winnego - w dokumentacji dovecot\u0026rsquo;a wykorzystano smtpd_client_restrictions: http://wiki2.dovecot.org/HowTo/PostfixAndDovecotSASL\n","permalink":"https://timor.site/2013/10/postfix-ciekawy-problem-z-smtpd_delay_reject-i-permit_sasl_authenticated/","summary":"Trafi≈Ç mi siƒô ostatnio ciekawy problem - ot√≥≈º standardowo przed ko≈Ñcem roku poprawia≈Çem filtry antyspamowe i optymalizowa≈Çem konfiguracjƒô Postfix\u0026rsquo;a. Chcia≈Çem zmieniƒá domy≈õlnƒÖ warto≈õƒá smtpd_delay_reject=yes na smtpd_delay_reject=no by odrzucaƒá spamer√≥w najwcze≈õniej jak to mo≈ºliwe. I ciekawe kuku, kt√≥re sobie zrobi≈Çem polega≈Ço na tym ≈ºe sam nie mog≈Çem wysy≈Çaƒá poczty po logowaniu SSL\u0026rsquo;em\u0026hellip;\nDostawa≈Çem przy tym bardzo wymownƒÖ odpowied≈∫:\nOct 8 16:30:39 tyr postfix/smtpd[21039]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.","title":"Postfix: ciekawy problem z smtpd_delay_reject i permit_sasl_authenticated"},{"content":"Pomimo ≈ºe Python du≈ºo czƒô≈õciej wykorzystywany jest w ≈õrodowiskach UNIX\u0026rsquo;owcy/Linux\u0026rsquo;owych to znajdzie siƒô kilka fajnych zastosowa≈Ñ dla tego jƒôzyka na Windowsie. Mo≈ºliwo≈õci na instalacjƒô jest kilka, a najprostsza to wykorzystanie instalatora ActiveState. Wersja ta ma w sobie wszystko co potrzebne:\nrozszerzenia dla API Windows menad≈ºera pakiet√≥w PyPM dokumentacjƒô Niestety jaki≈õ czas temu zmieni≈Çy siƒô zasady licencjonowania w ActiveState i aktualne wersje dla zastosowa≈Ñ produkcyjnych wymagajƒÖ zakupu licencji (1000$/rok - a≈º chce siƒô zacytowaƒá z Dnia ≈öwira: czizys k\u0026hellip;wa\u0026hellip;). Wiem jak lepiej wydaƒá takƒÖ kasƒô wiƒôc spr√≥bujƒô uzyskaƒá podobnƒÖ funkcjonalno≈õƒá na tym co mo≈ºna pobraƒá za darmo z sieci.\nInstalatory Python\u0026rsquo;a dla Windows mo≈ºna znale≈∫ƒá tutaj: http://www.python.org/download/releases/\nTeraz pakiety z obs≈ÇugƒÖ API Windows (wybieramy stosownie do wcze≈õniej pobranej wersji Pythona): http://sourceforge.net/projects/pywin32/files/pywin32/\nI na koniec setuptools by m√≥c doinstalowaƒá dodatkowe modu≈Çy. Wybieramy interesujƒÖcƒÖ nas wersjƒô tutaj: https://pypi.python.org/pypi/setuptools/\nObecnie jest to 1.1.6 - zgodnie z opisem z tej strony: https://pypi.python.org/pypi/setuptools/1.1.6#windows pobieramy ez_setup.py i uruchamiamy.\nNa koniec odpalamy CLI i instalujemy inne przydatne nam paczki, np.:\neasy_install couchdb easy_install cx-oracle P.S. I tutaj ma≈Çy kruczek - instalacja cx-Oracle z pomocƒÖ easy_install uda siƒô tylko na 32-bitowych Windowsach, na 64-bitowych konieczne jest zainstalowanie Visual Studio Express by mo≈ºliwe by≈Ço skompilowanie paczek\u0026hellip; (tak mnie te≈º siƒô w tej chwili ju≈º odechciewa≈Ço\u0026hellip;)\nAle na szczƒô≈õcie w przypadku tej paczki da siƒô inaczej, wystarczy pobraƒá ju≈º skompilowanƒÖ paczkƒô ze strony: http://cx-oracle.sourceforge.net dopasowanƒÖ do wybranej wcze≈õniej wersji Pythona. P.S. 2. Mo≈ºna siƒô obyƒá bez tej paczki i wykorzystaƒá pyodbc razem z kontrolerem ODBC z klienta Oracle, ale pyodbc nie obs≈Çuguje wywo≈Ça≈Ñ procedur ze zmiennymi wiƒÖzanymi in/out lub out - a ja akurat tego potrzebowa≈Çem, je≈õli to nie tw√≥j problem to pyodbc bƒôdzie prostsze üòÑ ","permalink":"https://timor.site/2013/09/instalacja-pythona-na-windowsie/","summary":"Pomimo ≈ºe Python du≈ºo czƒô≈õciej wykorzystywany jest w ≈õrodowiskach UNIX\u0026rsquo;owcy/Linux\u0026rsquo;owych to znajdzie siƒô kilka fajnych zastosowa≈Ñ dla tego jƒôzyka na Windowsie. Mo≈ºliwo≈õci na instalacjƒô jest kilka, a najprostsza to wykorzystanie instalatora ActiveState. Wersja ta ma w sobie wszystko co potrzebne:\nrozszerzenia dla API Windows menad≈ºera pakiet√≥w PyPM dokumentacjƒô Niestety jaki≈õ czas temu zmieni≈Çy siƒô zasady licencjonowania w ActiveState i aktualne wersje dla zastosowa≈Ñ produkcyjnych wymagajƒÖ zakupu licencji (1000$/rok - a≈º chce siƒô zacytowaƒá z Dnia ≈öwira: czizys k\u0026hellip;wa\u0026hellip;).","title":"Instalacja Python‚Äôa na Windowsie"},{"content":"Ten one liner za≈Çatwia sprawƒô:\npython -c \u0026#39;import django; print \u0026#34;.\u0026#34;.join([str(s) for s in django.VERSION]);\u0026#39; ","permalink":"https://timor.site/2013/09/sprawdzanie-zainstalowanej-wersji-django/","summary":"Ten one liner za≈Çatwia sprawƒô:\npython -c \u0026#39;import django; print \u0026#34;.\u0026#34;.join([str(s) for s in django.VERSION]);\u0026#39; ","title":"Sprawdzanie zainstalowanej wersji Django"},{"content":"I\u0026rsquo;ve just received a comment on my blog with text attached below. It looks like spam message template. I think it could be easily used for creation of banning rules, etc. Use it in a way that will make this dumbass spammer look even more stupid ;-)\n{ {I have|I‚Äôve} been {surfing|browsing} online more than {three|3|2|4} hours today, yet I never found any interesting article like yours. {It‚Äôs|It is} pretty worth enough for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made good content as you did, the {internet|net|web} will be {much more|a lot more} useful than ever before.| I {couldn‚Äôt|could not} {resist|refrain from} commenting. {Very well|Perfectly|Well|Exceptionally well} written!| {I will|I‚Äôll} {right away|immediately} {take hold of|grab|clutch|grasp|seize|snatch} your {rss|rss feed} as I {can not|can‚Äôt} {in finding|find|to find} your {email|e-mail} subscription {link|hyperlink} or {newsletter|e-newsletter} service. Do {you have|you‚Äôve} any? {Please|Kindly} {allow|permit|let} me {realize|recognize|understand|recognise|know} {so that|in order that} I {may just|may|could} subscribe. Thanks.| {It is|It‚Äôs} {appropriate|perfect|the best} time to make some plans for the future and {it is|it‚Äôs} time to be happy. {I have|I‚Äôve} read this post and if I could I {want to|wish to|desire to} suggest you {few|some} interesting things or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write next articles referring to this article. I {want to|wish to|desire to} read {more|even more} things about it!| {It is|It‚Äôs} {appropriate|perfect|the best} time to make {a few|some} plans for {the future|the longer term|the long run} and {it is|it‚Äôs} time to be happy. {I have|I‚Äôve} {read|learn} this {post|submit|publish|put up} and if I {may just|may|could} I {want to|wish to|desire to} {suggest|recommend|counsel} you {few|some} {interesting|fascinating|attention-grabbing} {things|issues} or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write {next|subsequent} articles {relating to|referring to|regarding} this article. I {want to|wish to|desire to} {read|learn} {more|even more} {things|issues} {approximately|about} it!| {I have|I‚Äôve} been {surfing|browsing} {online|on-line} {more than|greater than} {three|3} hours {these days|nowadays|today|lately|as of late}, {yet|but} I {never|by no means} {found|discovered} any {interesting|fascinating|attention-grabbing} article like yours. {It‚Äôs|It is} {lovely|pretty|beautiful} {worth|value|price} {enough|sufficient} for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made {just right|good|excellent} {content|content material} as {you did|you probably did}, the {internet|net|web} {will be|shall be|might be|will probably be|can be|will likely be} {much more|a lot more} {useful|helpful} than ever before.| Ahaa, its {nice|pleasant|good|fastidious} {discussion|conversation|dialogue} {regarding|concerning|about|on the topic of} this {article|post|piece of writing|paragraph} {here|at this place} at this {blog|weblog|webpage|website|web site}, I have read all that, so {now|at this time} me also commenting {here|at this place}.| I am sure this {article|post|piece of writing|paragraph} has touched all the internet {users|people|viewers|visitors}, its really really {nice|pleasant|good|fastidious} {article|post|piece of writing|paragraph} on building up new {blog|weblog|webpage|website|web site}.| Wow, this {article|post|piece of writing|paragraph} is {nice|pleasant|good|fastidious}, my {sister|younger sister} is analyzing {such|these|these kinds of} things, {so|thus|therefore} I am going to {tell|inform|let know|convey} her.| {Saved as a favorite|bookmarked!!}, {I really like|I like|I love} {your blog|your site|your web site|your website}!| Way cool! Some {very|extremely} valid points! I appreciate you {writing this|penning this} {article|post|write-up} {and the|and also the|plus the} rest of the {site is|website is} {also very|extremely|very|also really|really} good.| Hi, {I do believe|I do think} {this is an excellent|this is a great} {blog|website|web site|site}. I stumbledupon it ;) {I will|I am going to|I‚Äôm going to|I may} {come back|return|revisit} {once again|yet again} {since I|since i have} {bookmarked|book marked|book-marked|saved as a favorite} it. Money and freedom {is the best|is the greatest} way to change, may you be rich and continue to {help|guide} {other people|others}.| Woah! I‚Äôm really {loving|enjoying|digging} the template/theme of this {site|website|blog}. It‚Äôs simple, yet effective. A lot of times it‚Äôs {very hard|very difficult|challenging|tough|difficult|hard} to get that ‚Äûperfect balance‚Äù between {superb usability|user friendliness|usability} and {visual appearance|visual appeal|appearance}. I must say {that you‚Äôve|you have|you‚Äôve} done a {awesome|amazing|very good|superb|fantastic|excellent|great} job with this. {In addition|Additionally|Also}, the blog loads {very|extremely|super} {fast|quick} for me on {Safari|Internet explorer|Chrome|Opera|Firefox}. {Superb|Exceptional|Outstanding|Excellent} Blog!| These are {really|actually|in fact|truly|genuinely} {great|enormous|impressive|wonderful|fantastic} ideas in {regarding|concerning|about|on the topic of} blogging. You have touched some {nice|pleasant|good|fastidious} {points|factors|things} here. Any way keep up wrinting.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I‚Äôve {incorporated||added|included} you guys to {|my|our||my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey}! Someone in my {Myspace|Facebook} group shared this {site|website} with us so I came to {give it a look|look it over|take a look|check it out}. I‚Äôm definitely {enjoying|loving} the information. I‚Äôm {book-marking|bookmarking} and will be tweeting this to my followers! {Terrific|Wonderful|Great|Fantastic|Outstanding|Exceptional|Superb|Excellent} blog and {wonderful|terrific|brilliant|amazing|great|excellent|fantastic|outstanding|superb} {style and design|design and style|design}.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I‚Äôve {incorporated|added|included} you guys to {|my|our|my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey} would you mind {stating|sharing} which blog platform you‚Äôre {working with|using}? I‚Äôm {looking|planning|going} to start my own blog {in the near future|soon} but I‚Äôm having a {tough|difficult|hard} time {making a decision|selecting|choosing|deciding} between BlogEngine/Wordpress/B2evolution and Drupal. The reason I ask is because your {design and style|design|layout} seems different then most blogs and I‚Äôm looking for something {completely unique|unique}. P.S {My apologies|Apologies|Sorry} for {getting|being} off-topic but I had to ask!| {Howdy|Hi there|Hi|Hey there|Hello|Hey} would you mind letting me know which {webhost|hosting company|web host} you‚Äôre {utilizing|working with|using}? I‚Äôve loaded your blog in 3 {completely different|different} {internet browsers|web browsers|browsers} and I must say this blog loads a lot {quicker|faster} then most. Can you {suggest|recommend} a good {internet hosting|web hosting|hosting} provider at a {honest|reasonable|fair} price? {Thanks a lot|Kudos|Cheers|Thank you|Many thanks|Thanks}, I appreciate it!| {I love|I really like|I like|Everyone loves} it {when people|when individuals|when folks|whenever people} {come together|get together} and share {opinions|thoughts|views|ideas}. Great {blog|website|site}, {keep it up|continue the good work|stick with it}!| Thank you for the {auspicious|good} writeup. It in fact was a amusement account it. Look advanced to {far|more} added agreeable from you! {By the way|However}, how {can|could} we communicate?| {Howdy|Hi there|Hey there|Hello|Hey} just wanted to give you a quick heads up. The {text|words} in your {content|post|article} seem to be running off the screen in {Ie|Internet explorer|Chrome|Firefox|Safari|Opera}. I‚Äôm not sure if this is a {format|formatting} issue or something to do with {web browser|internet browser|browser} compatibility but I {thought|figured} I‚Äôd post to let you know. The {style and design|design and style|layout|design} look great though! Hope you get the {problem|issue} {solved|resolved|fixed} soon. {Kudos|Cheers|Many thanks|Thanks}| This is a topic {that is|that‚Äôs|which is} {close to|near to} my heart‚Ä¶ {Cheers|Many thanks|Best wishes|Take care|Thank you}! {Where|Exactly where} are your contact details though?| It‚Äôs very {easy|simple|trouble-free|straightforward|effortless} to find out any {topic|matter} on {net|web} as compared to {books|textbooks}, as I found this {article|post|piece of writing|paragraph} at this {website|web site|site|web page}.| Does your {site|website|blog} have a contact page? I‚Äôm having {a tough time|problems|trouble} locating it but, I‚Äôd like to {send|shoot} you an {e-mail|email}. I‚Äôve got some {creative ideas|recommendations|suggestions|ideas} for your blog you might be interested in hearing. Either way, great {site|website|blog} and I look forward to seeing it {develop|improve|expand|grow} over time.| {Hola|Hey there|Hi|Hello|Greetings}! I‚Äôve been {following|reading} your {site|web site|website|weblog|blog} for {a long time|a while|some time} now and finally got the {bravery|courage} to go ahead and give you a shout out from {New Caney|Kingwood|Huffman|Porter|Houston|Dallas|Austin|Lubbock|Humble|Atascocita} {Tx|Texas}! Just wanted to {tell you|mention|say} keep up the {fantastic|excellent|great|good} {job|work}!| Greetings from {Idaho|Carolina|Ohio|Colorado|Florida|Los angeles|California}! I‚Äôm {bored to tears|bored to death|bored} at work so I decided to {check out|browse} your {site|website|blog} on my iphone during lunch break. I {enjoy|really like|love} the {knowledge|info|information} you {present|provide} here and can‚Äôt wait to take a look when I get home. I‚Äôm {shocked|amazed|surprised} at how {quick|fast} your blog loaded on my {mobile|cell phone|phone} .. I‚Äôm not even using WIFI, just 3G .. {Anyhow|Anyways}, {awesome|amazing|very good|superb|good|wonderful|fantastic|excellent|great} {site|blog}!| Its {like you|such as you} {read|learn} my {mind|thoughts}! You {seem|appear} {to understand|to know|to grasp} {so much|a lot} {approximately|about} this, {like you|such as you} wrote the {book|e-book|guide|ebook|e book} in it or something. {I think|I feel|I believe} {that you|that you simply|that you just} {could|can} do with {some|a few} {%|p.c.|percent} to {force|pressure|drive|power} the message {house|home} {a bit|a little bit}, {however|but} {other than|instead of} that, {this is|that is} {great|wonderful|fantastic|magnificent|excellent} blog. {A great|An excellent|A fantastic} read. {I‚Äôll|I will} {definitely|certainly} be back.| I visited {multiple|many|several|various} {websites|sites|web sites|web pages|blogs} {but|except|however} the audio {quality|feature} for audio songs {current|present|existing} at this {website|web site|site|web page} is {really|actually|in fact|truly|genuinely} {marvelous|wonderful|excellent|fabulous|superb}.| {Howdy|Hi there|Hi|Hello}, i read your blog {occasionally|from time to time} and i own a similar one and i was just {wondering|curious} if you get a lot of spam {comments|responses|feedback|remarks}? If so how do you {prevent|reduce|stop|protect against} it, any plugin or anything you can {advise|suggest|recommend}? I get so much lately it‚Äôs driving me {mad|insane|crazy} so any {assistance|help|support} is very much appreciated.| Greetings! {Very helpful|Very useful} advice {within this|in this particular} {article|post}! {It is the|It‚Äôs the} little changes {that make|which will make|that produce|that will make} {the biggest|the largest|the greatest|the most important|the most significant} changes. {Thanks a lot|Thanks|Many thanks} for sharing!| {I really|I truly|I seriously|I absolutely} love {your blog|your site|your website}.. {Very nice|Excellent|Pleasant|Great} colors \u0026amp; theme. Did you {create|develop|make|build} {this website|this site|this web site|this amazing site} yourself? Please reply back as I‚Äôm {looking to|trying to|planning to|wanting to|hoping to|attempting to} create {my own|my very own|my own personal} {blog|website|site} and {would like to|want to|would love to} {know|learn|find out} where you got this from or {what the|exactly what the|just what the} theme {is called|is named}. {Thanks|Many thanks|Thank you|Cheers|Appreciate it|Kudos}!| {Hi there|Hello there|Howdy}! This {post|article|blog post} {couldn‚Äôt|could not} be written {any better|much better}! {Reading through|Looking at|Going through|Looking through} this {post|article} reminds me of my previous roommate! He {always|constantly|continually} kept {talking about|preaching about} this. {I will|I‚Äôll|I am going to|I most certainly will} {forward|send} {this article|this information|this post} to him. {Pretty sure|Fairly certain} {he will|he‚Äôll|he‚Äôs going to} {have a good|have a very good|have a great} read. {Thank you for|Thanks for|Many thanks for|I appreciate you for} sharing!| {Wow|Whoa|Incredible|Amazing}! This blog looks {exactly|just} like my old one! It‚Äôs on a {completely|entirely|totally} different {topic|subject} but it has pretty much the same {layout|page layout} and design. {Excellent|Wonderful|Great|Outstanding|Superb} choice of colors!| {There is|There‚Äôs} {definately|certainly} {a lot to|a great deal to} {know about|learn about|find out about} this {subject|topic|issue}. {I like|I love|I really like} {all the|all of the} points {you made|you‚Äôve made|you have made}.| {You made|You‚Äôve made|You have made} some {decent|good|really good} points there. I {looked|checked} {on the internet|on the web|on the net} {for more info|for more information|to find out more|to learn more|for additional information} about the issue and found {most individuals|most people} will go along with your views on {this website|this site|this web site}.| {Hi|Hello|Hi there|What‚Äôs up}, I {log on to|check|read} your {new stuff|blogs|blog} {regularly|like every week|daily|on a regular basis}. Your {story-telling|writing|humoristic} style is {awesome|witty}, keep {doing what you‚Äôre doing|up the good work|it up}!| I {simply|just} {could not|couldn‚Äôt} {leave|depart|go away} your {site|web site|website} {prior to|before} suggesting that I {really|extremely|actually} {enjoyed|loved} {the standard|the usual} {information|info} {a person|an individual} {supply|provide} {for your|on your|in your|to your} {visitors|guests}? Is {going to|gonna} be {back|again} {frequently|regularly|incessantly|steadily|ceaselessly|often|continuously} {in order to|to} {check up on|check out|inspect|investigate cross-check} new posts| {I wanted|I needed|I want to|I need to} to thank you for this {great|excellent|fantastic|wonderful|good|very good} read!! I {definitely|certainly|absolutely} {enjoyed|loved} every {little bit of|bit of} it. {I have|I‚Äôve got|I have got} you {bookmarked|book marked|book-marked|saved as a favorite} {to check out|to look at} new {stuff you|things you} post‚Ä¶| {Hi|Hello|Hi there|What‚Äôs up}, just wanted to {mention|say|tell you}, I {enjoyed|liked|loved} this {article|post|blog post}. It was {inspiring|funny|practical|helpful}. Keep on posting!| I {{leave|drop|{write|create}} a {comment|leave a response}|drop a {comment|leave a response}|{comment|leave a response}} {each time|when|whenever} I {appreciate|like|especially enjoy} a {post|article} on a {site|{blog|website}|site|website} or {I have|if I have} something to {add|contribute|valuable to contribute} {to the discussion|to the conversation}. {It is|Usually it is|Usually it‚Äôs|It‚Äôs} {a result of|triggered by|caused by} the {passion|fire|sincerness} {communicated|displayed} in the {post|article} I {read|looked at|browsed}. And {on|after} this {post|article} Kopiowanie wolumen√≥w LVM z dd i netcat | timor‚Äôs site. I {{was|was actually} moved|{was|was actually} excited} enough to {drop|{leave|drop|{write|create}}|post} a {thought|{comment|{comment|leave a response}a response}} {:-P|:)|;)|;-)|: -)} I {do have|actually do have} {{some|a few} questions|a couple of questions|2 questions} for you {if you {don‚Äôt|do not|usually do not|tend not to} mind|if it‚Äôs {allright|okay}}. {Is it|Could it be} {just|only|simply} me or {do|does it {seem|appear|give the impression|look|look as if|look like} like} {some|a few} of {the|these} {comments|responses|remarks} {look|appear|come across} {like they are|as if they are|like} {coming from|written by|left by} brain dead {people|visitors|folks|individuals}? :-P And, if you are {posting|writing} {on|at} {other|additional} {sites|social sites|online sites|online social sites|places}, {I‚Äôd|I would} like to {follow|keep up with} {you|{anything|everything} {new|fresh} you have to post}. {Could|Would} you {list|make a list} {all|every one|the complete urls} of {your|all your} {social|communal|community|public|shared} {pages|sites} like your {twitter feed, Facebook page or linkedin profile|linkedin profile, Facebook page or twitter feed|Facebook page, twitter feed, or linkedin profile}?| {Hi there|Hello}, I enjoy reading {all of|through} your {article|post|article post}. I {like|wanted} to write a little comment to support you.| I {always|constantly|every time} spent my half an hour to read this {blog|weblog|webpage|website|web site}‚Äôs {articles|posts|articles or reviews|content} {everyday|daily|every day|all the time} along with a {cup|mug} of coffee.| I {always|for all time|all the time|constantly|every time} emailed this {blog|weblog|webpage|website|web site} post page to all my {friends|associates|contacts}, {because|since|as|for the reason that} if like to read it {then|after that|next|afterward} my {friends|links|contacts} will too.| My {coder|programmer|developer} is trying to {persuade|convince} me to move to .net from PHP. I have always disliked the idea because of the {expenses|costs}. But he‚Äôs tryiong none the less. I‚Äôve been using {Movable-type|WordPress} on {a number of|a variety of|numerous|several|various} websites for about a year and am {nervous|anxious|worried|concerned} about switching to another platform. I have heard {fantastic|very good|excellent|great|good} things about blogengine.net. Is there a way I can {transfer|import} all my wordpress {content|posts} into it? {Any kind of|Any} help would be {really|greatly} appreciated!| {Hello|Hi|Hello there|Hi there|Howdy|Good day}! I could have sworn I‚Äôve {been to|visited} {this blog|this web site|this website|this site|your blog} before but after {browsing through|going through|looking at} {some of the|a few of the|many of the} {posts|articles} I realized it‚Äôs new to me. {Anyways|Anyhow|Nonetheless|Regardless}, I‚Äôm {definitely|certainly} {happy|pleased|delighted} {I found|I discovered|I came across|I stumbled upon} it and I‚Äôll be {bookmarking|book-marking} it and checking back {frequently|regularly|often}!| {Terrific|Great|Wonderful} {article|work}! {This is|That is} {the type of|the kind of} {information|info} {that are meant to|that are supposed to|that should} be shared {around the|across the} {web|internet|net}. {Disgrace|Shame} on {the {seek|search} engines|Google} for {now not|not|no longer} positioning this {post|submit|publish|put up} {upper|higher}! Come on over and {talk over with|discuss with|seek advice from|visit|consult with} my {site|web site|website} . {Thank you|Thanks} =)| Heya {i‚Äôm|i am} for the first time here. I {came across|found} this board and I find It {truly|really} useful \u0026amp; it helped me out {a lot|much}. I hope to give something back and {help|aid} others like you {helped|aided} me.| {Hi|Hello|Hi there|Hello there|Howdy|Greetings}, {I think|I believe|I do believe|I do think|There‚Äôs no doubt that} {your site|your website|your web site|your blog} {might be|may be|could be|could possibly be} having {browser|internet browser|web browser} compatibility {issues|problems}. {When I|Whenever I} {look at your|take a look at your} {website|web site|site|blog} in Safari, it looks fine {but when|however when|however, if|however, when} opening in {Internet Explorer|IE|I.E.}, {it has|it‚Äôs got} some overlapping issues. {I just|I simply|I merely} wanted to {give you a|provide you with a} quick heads up! {Other than that|Apart from that|Besides that|Aside from that}, {fantastic|wonderful|great|excellent} {blog|website|site}!| {A person|Someone|Somebody} {necessarily|essentially} {lend a hand|help|assist} to make {seriously|critically|significantly|severely} {articles|posts} {I would|I might|I‚Äôd} state. {This is|That is} the {first|very first} time I frequented your {web page|website page} and {to this point|so far|thus far|up to now}? I {amazed|surprised} with the {research|analysis} you made to {create|make} {this actual|this particular} {post|submit|publish|put up} {incredible|amazing|extraordinary}. {Great|Wonderful|Fantastic|Magnificent|Excellent} {task|process|activity|job}!| Heya {i‚Äôm|i am} for {the primary|the first} time here. I {came across|found} this board and I {in finding|find|to find} It {truly|really} {useful|helpful} \u0026amp; it helped me out {a lot|much}. {I am hoping|I hope|I‚Äôm hoping} {to give|to offer|to provide|to present} {something|one thing} {back|again} and {help|aid} others {like you|such as you} {helped|aided} me.| {Hello|Hi|Hello there|Hi there|Howdy|Good day|Hey there}! {I just|I simply} {would like to|want to|wish to} {give you a|offer you a} {huge|big} thumbs up {for the|for your} {great|excellent} {info|information} {you have|you‚Äôve got|you have got} {here|right here} on this post. {I will be|I‚Äôll be|I am} {coming back to|returning to} {your blog|your site|your website|your web site} for more soon.| I {always|all the time|every time} used to {read|study} {article|post|piece of writing|paragraph} in news papers but now as I am a user of {internet|web|net} {so|thus|therefore} from now I am using net for {articles|posts|articles or reviews|content}, thanks to web.| Your {way|method|means|mode} of {describing|explaining|telling} {everything|all|the whole thing} in this {article|post|piece of writing|paragraph} is {really|actually|in fact|truly|genuinely} {nice|pleasant|good|fastidious}, {all|every one} {can|be able to|be capable of} {easily|without difficulty|effortlessly|simply} {understand|know|be aware of} it, Thanks a lot.| {Hi|Hello} there, {I found|I discovered} your {blog|website|web site|site} {by means of|via|by the use of|by way of} Google {at the same time as|whilst|even as|while} {searching for|looking for} a {similar|comparable|related} {topic|matter|subject}, your {site|web site|website} {got here|came} up, it {looks|appears|seems|seems to be|appears to be like} {good|great}. {I have|I‚Äôve} bookmarked it in my google bookmarks. {Hello|Hi} there, {simply|just} {turned into|became|was|become|changed into} {aware of|alert to} your {blog|weblog} {thru|through|via} Google, {and found|and located} that {it is|it‚Äôs} {really|truly} informative. {I‚Äôm|I am} {gonna|going to} {watch out|be careful} for brussels. {I will|I‚Äôll} {appreciate|be grateful} {if you|should you|when you|in the event you|in case you|for those who|if you happen to} {continue|proceed} this {in future}. {A lot of|Lots of|Many|Numerous} {other folks|folks|other people|people} {will be|shall be|might be|will probably be|can be|will likely be} benefited {from your|out of your} writing. Cheers!| {I am|I‚Äôm} curious to find out what blog {system|platform} {you have been|you happen to be|you are|you‚Äôre} {working with|utilizing|using}? I‚Äôm {experiencing|having} some {minor|small} security {problems|issues} with my latest {site|website|blog} and {I would|I‚Äôd} like to find something more {safe|risk-free|safeguarded|secure}. Do you have any {solutions|suggestions|recommendations}?| {I am|I‚Äôm} {extremely|really} impressed with your writing skills {and also|as well as} with the layout on your {blog|weblog}. Is this a paid theme or did you {customize|modify} it yourself? {Either way|Anyway} keep up the {nice|excellent} quality writing, {it‚Äôs|it is} rare to see a {nice|great} blog like this one {these days|nowadays|today}.| {I am|I‚Äôm} {extremely|really} {inspired|impressed} {with your|together with your|along with your} writing {talents|skills|abilities} {and also|as {smartly|well|neatly} as} with the {layout|format|structure} {for your|on your|in your|to your} {blog|weblog}. {Is this|Is that this} a paid {subject|topic|subject matter|theme} or did you {customize|modify} it {yourself|your self}? {Either way|Anyway} {stay|keep} up the {nice|excellent} {quality|high quality} writing, {it‚Äôs|it is} {rare|uncommon} {to peer|to see|to look} a {nice|great} {blog|weblog} like this one {these days|nowadays|today}..| {Hi|Hello}, Neat post. {There is|There‚Äôs} {a problem|an issue} {with your|together with your|along with your} {site|web site|website} in {internet|web} explorer, {may|might|could|would} {check|test} this? IE {still|nonetheless} is the {marketplace|market} {leader|chief} and {a large|a good|a big|a huge} {part of|section of|component to|portion of|component of|element of} {other folks|folks|other people|people} will {leave out|omit|miss|pass over} your {great|wonderful|fantastic|magnificent|excellent} writing {due to|because of} this problem.| {I‚Äôm|I am} not sure where {you are|you‚Äôre} getting your {info|information}, but {good|great} topic. I needs to spend some time learning {more|much more} or understanding more. Thanks for {great|wonderful|fantastic|magnificent|excellent} {information|info} I was looking for this {information|info} for my mission.| {Hi|Hello}, i think that i saw you visited my {blog|weblog|website|web site|site} {so|thus} i came to ‚Äúreturn the favor‚Äù.{I am|I‚Äôm} {trying to|attempting to} find things to {improve|enhance} my {website|site|web site}!I suppose its ok to use {some of|a few of} your ideas ","permalink":"https://timor.site/2013/09/spammer-screwed-up/","summary":"I\u0026rsquo;ve just received a comment on my blog with text attached below. It looks like spam message template. I think it could be easily used for creation of banning rules, etc. Use it in a way that will make this dumbass spammer look even more stupid ;-)\n{ {I have|I‚Äôve} been {surfing|browsing} online more than {three|3|2|4} hours today, yet I never found any interesting article like yours. {It‚Äôs|It is} pretty worth enough for me.","title":"Spammer screwed up"},{"content":"Wyprzeƒá siƒô nie mogƒô ≈ºe gad≈ºety dzia≈ÇajƒÖce na Linuksie po prostu mnie krƒôcƒÖ, wiƒôc tylko kwestiƒÖ czasu by≈Ço a≈º Pi zawita na moim biurku. Zakupi≈Çem wiƒôc model B w drugiej wersji, obudowƒô z mo≈ºliwo≈õciƒÖ mocowania VESA, kabelek HDMI, ≈Çadowarka z mojej myszy pasowa≈Ça idealnie (5.05V i 1A) i na poczƒÖtek karta SD klasa 10 4GB.\nSzukajƒÖc r√≥≈ºnych system√≥w (a mo≈ºe ROM\u0026rsquo;√≥w) natrafi≈Çem na oficjalnƒÖ stronƒô: http://www.raspberrypi.org/downloads\nNa poczƒÖtek wystarczy a sprawdzajƒÖc na stronach projekt√≥w okaza≈Ço siƒô ≈ºe wersje na tej stronie sƒÖ ca≈Çkiem aktualne.\nChocia≈º paczka NOOB wyglƒÖda≈Ça kuszƒÖco to by≈ÇƒÖ najwiƒôksza i ≈õciƒÖgnƒô≈Ça mi siƒô jak ju≈º dzia≈Ça≈Çem na OpenELEC\u0026rsquo;u. Na dobrƒÖ sprawƒô nie rozumiem czemu tƒô drogƒô opisano jako trudniejszƒÖ\u0026hellip;\nNagrywanie obrazu na kartƒô SD Wpinamy kartƒô SD w czytnik Odpalamy: dmesg i szukamy jakƒÖ literkƒô jej przyporzƒÖdkowano w systemie\nOdpalamy na tym dysku: fdisk -l /dev/sdh (w moim przypadku) i upewniamy siƒô ≈ºe rozmiar i tablica partycji odpowiadajƒÖ naszej karcie (pomy≈Çka literki to bankowa utrata danych i co najmniej jedna nieprzespana noc\u0026hellip;)\nRozpakowujemy obraz: unzip 2013-07-26-wheezy-raspbian.zip Mo≈ºemy wgrywaƒá: sudo dd bs=1M if=2013-07-26-wheezy-raspbian.img of=/dev/sdh status kopiowania mo≈ºemy podglƒÖdaƒá tak:\nkill -USR1 `pidof dd` (o ile mamy uruchomiony jeden proces dd w systemi)\nW przypadku niekt√≥rych obraz√≥w mo≈ºe byƒá potrzebna zmiana rozmiaru partycji (bo domy≈õlnie przygotowane sƒÖ dla mniejszych kart - polecam parted lub gparted do tego zadania Mo≈ºna startowaƒá!\n","permalink":"https://timor.site/2013/09/raspberry-pi-pierwsze-kroki/","summary":"Wyprzeƒá siƒô nie mogƒô ≈ºe gad≈ºety dzia≈ÇajƒÖce na Linuksie po prostu mnie krƒôcƒÖ, wiƒôc tylko kwestiƒÖ czasu by≈Ço a≈º Pi zawita na moim biurku. Zakupi≈Çem wiƒôc model B w drugiej wersji, obudowƒô z mo≈ºliwo≈õciƒÖ mocowania VESA, kabelek HDMI, ≈Çadowarka z mojej myszy pasowa≈Ça idealnie (5.05V i 1A) i na poczƒÖtek karta SD klasa 10 4GB.\nSzukajƒÖc r√≥≈ºnych system√≥w (a mo≈ºe ROM\u0026rsquo;√≥w) natrafi≈Çem na oficjalnƒÖ stronƒô: http://www.raspberrypi.org/downloads\nNa poczƒÖtek wystarczy a sprawdzajƒÖc na stronach projekt√≥w okaza≈Ço siƒô ≈ºe wersje na tej stronie sƒÖ ca≈Çkiem aktualne.","title":"Raspberry Pi: pierwsze kroki"},{"content":"Polubi≈Çem Nginx\u0026rsquo;a i wykorzystujƒô go na coraz wiƒôcej sposob√≥w. Kilka rzeczy uda≈Ço mi siƒô ca≈Çkiem fajnie w nim skonfigurowaƒá i postanowi≈Çem zebraƒá te przyk≈Çady by nastƒôpnym razem gdy postanowiƒô do nich siƒôgnƒÖƒá nie musieƒá wertowaƒá konfig√≥w po serwerach üòÉ\nS≈Çowo wstƒôpu Niekt√≥re rewrite\u0026rsquo;y ko≈ÑczƒÖ siƒô znakiem ? - czemu?\nOt√≥≈º Nginx pr√≥buje automatycznie dodawaƒá parametry na ko≈Ñcu przepisanego adresu. Je≈õli jednak wykorzystamy zmiennƒÖ $request_uri to ona sama w sobie zawiera ju≈º parametry zapytania (czyli to co w URI znajduje siƒô po znaku ?) i w≈Ça≈õnie dodanie pytajnika tu≈º za tƒÖ zmiennƒÖ powoduje ≈ºe argumenty nie sƒÖ dublowane.\nMa to te≈º zastosowanie gdy chcemy by rewrite kierowa≈Ç np. na g≈Ç√≥wnƒÖ stronƒô be≈º ≈ºadnych dodatkowych argument√≥w (zostanƒÖ one obciƒôte).\nWiƒôcej na ten temat mo≈ºna znale≈∫ƒá w dokumentacji Nginx.\nInna warta wspomnienia uwaga dotyczy drobnej optymalizacji, o kt√≥rej warto pamiƒôtaƒá na etapie tworzenia rewrite\u0026rsquo;√≥w (mo≈ºna znale≈∫ƒá masƒô kiepskich przyk≈Çad√≥w w sieci): na poczƒÖtku najlepiej jest stworzyƒá co≈õ co dzia≈Ça (i przy ma≈Çym ruchu mo≈ºe to byƒá wystarczajƒÖce) a p√≥≈∫niej optymalizowaƒá - moje przyk≈Çady stara≈Çem siƒô zoptymalizowaƒá wed≈Çug zalecanych praktyk.\nDlatego zamiast pisaƒá:\nrewrite ^(.*)$ $scheme://www.domain.com$1 permanent; lepiej napisaƒá:\nrewrite ^ $scheme://www.domain.com$request_uri? permanent; (nie wykorzystujemy przechowywania warto≈õci dopasowania - mniejsze zu≈ºycie pamiƒôci i l≈ºejsza interpretacja REGEXP\u0026rsquo;a).\nA jeszcze lepiej napisaƒá:\nreturn 301 $scheme://www.domain.com$request_uri; (w og√≥le nie wykorzystujemy REGEXP\u0026rsquo;√≥w praktycznie zerowy narzut na przetwarzanie) - dziƒôki za uwagƒô: lukasamd.\nPrzekierowanie starej domeny na nowƒÖ server { listen 80; server_name old-domain.com www.old-domain.com; return 301 $scheme://www.new-domain.com$request_uri; # rewrite ^ $scheme://www.new-domain.com$request_uri? permanent; # or # rewrite ^ $scheme://www.new-domain.com? permanent; } Wykorzystanie return w tej sytuacji jest nieco bardziej optymalne gdy≈º nie anga≈ºuje w og√≥le silnika REGEXP a w tej sytuacji jest wystarczajƒÖce.\nPierwsza linia z rewrite i $request_uri spowoduje przepisywanie te≈º parametr√≥w wywo≈Ça≈Ñ do nowej lokalizacji co jest jak najbardziej sensowne gdy pomimo domeny nie zmieni≈Ça siƒô zbytnio struktura strony.\nJe≈õli strona jednak siƒô zmieni≈Ça to mo≈ºemy zdecydowaƒá o przekierowaniu bez parametr√≥w - po prostu na g≈Ç√≥wnƒÖ stronƒô - i to robi druga linia.\nW obu przypadkach parametr permanent nakazuje u≈ºycie kodu przekierowania HTTP 301 (Moved Permanently), co u≈Çatwi zorientowanie siƒô crawlerom ≈ºe ta zmiana jest ju≈º na sta≈Çe.\nDodanie WWW na poczƒÖtku domeny server { listen 80; server_name domaim.com; return 301 $scheme://www.domain.com$request_uri; #rewrite ^ $scheme://www.domain.com$request_uri? permanent; # or #rewrite ^(.*)$ $scheme://www.domain.com$1 permanent; } Przyk≈Çad zakomentowany jest wed≈Çug dokumentacji mniej optymalny ale r√≥wnie≈º zadzia≈Ça. Reszta jest prosta i samoopisujƒÖca siƒô üòÉ\nA to jeszcze bardziej og√≥lna wersja dla wielu domen:\nserver { listen 195.117.254.80:80; server_name domain.pl domain.eu domain.com; return 301 $scheme://www.$http_host$request_uri; #rewrite ^ $scheme://www.$http_host$request_uri? permanent; } Ta wersja wykorzystuje zmiennƒÖ $http_host do przekierowania na domenƒô z zapytania (zmienna ta zawiera te≈º numer portu je≈õli jest niestandardowy np. 8080, w przeciwie≈Ñstwie do zmiennej $host, kt√≥ra zawiera tylko domenƒô).\nUsuniƒôcie WWW z poczƒÖtku domeny server { listen 80; server_name www.domain.com; return 301 $scheme://domain.com$request_uri; #rewrite ^ $scheme://domain.com$request_uri? permanent; } Czasami mo≈ºe siƒô przydaƒá jeszcze inny kawa≈Çek, gdy strona dzia≈Ça na wielu domenach i chcemy przekierowaƒá wszystkie:\nserver { server_name www.domain.com _ ; # server_name www.domain1.com www.domain2.com www.domain3.eu www.domain.etc.com; if ($host ~* www\\.(.*)) { set $pure_host $1; return 301 $scheme://$pure_host$request_uri; #rewrite ^ $scheme://$pure_host$request_uri? permanent; #rewrite ^(.*)$ $scheme://$pure_host$1 permanent; } } Choƒá to podej≈õcie nie jest zalecane (pomimo zwiƒôz≈Ço≈õci). Lepiej zdefiniowaƒá dwa bloki server z domenami www.* i domenami bez www na poczƒÖtku. Ale z drugiej strony to cholernie wygodne\u0026hellip; üòÉ\nPrzekierowanie \u0026ldquo;pozosta≈Çych\u0026rdquo; zapyta≈Ñ na domy≈õlnƒÖ domenƒô server { listen 80 default; server_name _; rewrite ^ $scheme://www.domena.com; #rewrite ^ $scheme://www.domena.com/search/$host; } To bardzo przydatny przyk≈Çad - czyli domy≈õlny vhost, kt√≥ry \u0026ldquo;przyjmie\u0026rdquo; wszystkie zapytania do domen nie zdefiniowanych w konfiguracji i przekieruje na naszƒÖ \u0026ldquo;g≈Ç√≥wnƒÖ stronƒô\u0026rdquo;.\nZakomentowany przyk≈Çad jest nieco bardziej przekombinowany bo pr√≥buje wykorzystaƒá wyszukiwarkƒô na naszej stronie do wyszukania \u0026ldquo;czego≈õ\u0026rdquo; pomocnego - z tym przyk≈Çadem nale≈ºy uwa≈ºaƒá bo je≈õli do serwera trafi du≈ºo b≈Çƒôdnych zapyta≈Ñ to mo≈ºe zostaƒá przeciƒÖ≈ºony \u0026ldquo;bzdetnymi\u0026rdquo; wyszukiwaniami.\nPrzekierowanie pewnych podstron po zmianie struktury strony server { listen 80; server_name www.domain.com; location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /tag/something $scheme://new.domain.com permanent; rewrite /category/hobby /category/painting permanent; # etc ... rewrite ^ /index.php last; } } Im starsza strona tym wiƒôcej zbiera siƒô link√≥w, kt√≥rych po prostu nie mo≈ºna usunƒÖƒá, a kt√≥re z racji wprowadzonych zmian nie majƒÖ prawa bytu w nowym uk≈Çadzie. Warto je przekierowaƒá w nowe miejsca, lub najbardziej odpowiadajƒÖce/bliskie tym starym. Problemem mo≈ºe siƒô wkr√≥tce staƒá du≈ºa lista przekierowa≈Ñ, kt√≥ra zaciemni konfiguracjƒô.\nPowy≈ºszy spos√≥b w do≈õƒá optymalny spos√≥b porzƒÖdkuje takie przekierowania - najpierw sprawdza czy przypadkiem nie pr√≥bujemy pobraƒá istniejƒÖcych plik√≥w, je≈õli nie to wrzuca nas nas na listƒô przekierowa≈Ñ, a je≈õli i tu nic nie znajdzie to zapytanie przekazywane jest do g≈Ç√≥wnego skryptu strony.\nPrzekierowanie w zale≈ºno≈õci od warto≈õci parametru w URI if ($args ~ producent=toyota){ rewrite ^ $scheme://toyota.domena.com$request_uri? permanent; } To rzadko stosowane przekierowanie a w dodatku ma≈Ço czytelnie i ponoƒá ma≈Ço wydajne\u0026hellip; Ale potrafi byƒá bardzo przydatne gdy chcemy przepisaƒá adres w zale≈ºno≈õci od warto≈õci parametru np. gdy pewna podstrona doczeka siƒô rozbudowy w zupe≈Çnie nowym serwisie lub gdy chcemy ≈Çadnie przekierowaƒá adresy ze starej strony na nowƒÖ.\nBlokowanie dostƒôpu do ukrytych plik√≥w location ~ /\\. { access_log off; log_not_found off; deny all; } Przyznajƒô - to nie rewrite\u0026hellip; Ale ta linijka jest r√≥wnie przydatna - pozwala zablokowaƒá mo≈ºliwo≈õƒá pobierania ukrytych plik√≥w (np. .htaccess\u0026rsquo;√≥w po konfiguracji z Apachego).\nWy≈ÇƒÖczenie logowania dla robots.txt i favicon.ico location = /favicon.ico { try_files /favicon.ico =204; access_log off; log_not_found off; } location = /robots.txt { try_files /robots.txt =204; access_log off; log_not_found off; } To te≈º nie rewrite - ale bardzo fajnie obs≈Çuguje sytuacjƒô gdy mamy i gdy nie mamy powy≈ºszych dw√≥ch pliczk√≥w. Po pierwsze wy≈ÇƒÖcza logowanie i serwuje je gdy sƒÖ dostƒôpne. Gdy nie istniejƒÖ to serwuje puste pliki (kod 204) dziƒôki czemu nie przeszkadzajƒÖ nam 404-ki üòÉ\nBlokowanie dostƒôpu do obrazk√≥w dla nieznanych refererer√≥w location ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf)$ { # definiujemy poprawnych refererow valid_referers none blocked *.domain.com domain.com; if ($invalid_referer) { return 444; } expires max; break; } Zabezpieczenie warte tyle co nic bo banalne do ominiƒôcia - ale je≈õli zdarzy siƒô ≈ºe kto≈õ postanowi wykorzystaƒá grafikƒô z naszej stronki np. w aukcji na allegro czy w≈Çasnym sklepie to tƒÖ prostƒÖ sztuczkƒÖ mo≈ºemy go przyciƒÖƒá i przewa≈ºnie jest to wystarczajƒÖce.\nMuszƒô te≈º zaznaczyƒá szczeg√≥lne znaczenie warto≈õci kodu b≈Çƒôdu 444 w Nginx\u0026rsquo;ie - powoduje on zerwanie po≈ÇƒÖczenia bez wysy≈Çania jakiejkolwiek odpowiedzi. Je≈õli nie chcemy byƒá tak okrutni to mo≈ºemy u≈ºyƒá innego kodu, np.: 403 albo 402 üòÉ\nPrzekierowanie ciekawskich w \u0026ldquo;ciemnƒÖ dupƒô\u0026rdquo; location ~* ^/(wp-)?admin(istrator)?/? { rewrite ^ http://whatismyipaddress.com/ip/$remote_addr redirect; } Ten prosty redirect odwodzi wielu amator√≥w zbyt g≈Çƒôbokiego penetrowania naszej strony\u0026hellip; A pozosta≈Çych na pewno rozbawi üòÉ\nInne przyk≈Çady konfiguracji na mojej stronie:\nNginx - hide server version and name in Server header and error pages Nginx - kompresowanie plik√≥w dla gzip_static Nginx - konfiguracja pod WordPress‚Äôa Nginx - ustawienie domy≈õlnego vhosta Nginx - m√≥j domy≈õlny config ≈πr√≥d≈Ça http://www.engineyard.com/blog/2011/useful-rewrites-for-nginx/\nhttp://wiki.nginx.org/HttpRewriteModule\n","permalink":"https://timor.site/2013/09/nginx-przydatne-rewritey-i-rozne-sztuczki/","summary":"Polubi≈Çem Nginx\u0026rsquo;a i wykorzystujƒô go na coraz wiƒôcej sposob√≥w. Kilka rzeczy uda≈Ço mi siƒô ca≈Çkiem fajnie w nim skonfigurowaƒá i postanowi≈Çem zebraƒá te przyk≈Çady by nastƒôpnym razem gdy postanowiƒô do nich siƒôgnƒÖƒá nie musieƒá wertowaƒá konfig√≥w po serwerach üòÉ\nS≈Çowo wstƒôpu Niekt√≥re rewrite\u0026rsquo;y ko≈ÑczƒÖ siƒô znakiem ? - czemu?\nOt√≥≈º Nginx pr√≥buje automatycznie dodawaƒá parametry na ko≈Ñcu przepisanego adresu. Je≈õli jednak wykorzystamy zmiennƒÖ $request_uri to ona sama w sobie zawiera ju≈º parametry zapytania (czyli to co w URI znajduje siƒô po znaku ?","title":"Nginx - przydatne rewrite‚Äôy i r√≥≈ºne sztuczki"},{"content":"ChcƒÖc pobawiƒá siƒô tor\u0026rsquo;em postanowi≈Çem udostƒôpniƒá jedna z moich stron z wykorzystaniem mechanizmu hidden service. Nie spodoba≈Ç mi siƒô jedynie spos√≥b generowania nazw, kt√≥re by≈Çy ma≈Ço opisowe - ale najwidoczniej nie mnie jednemu, bo szybko namierzy≈Çem Shallot, kt√≥ry generuje kolejne nazwy a≈º trafi na pasujƒÖcƒÖ do zadanego regexp\u0026rsquo;a.\n","permalink":"https://timor.site/2013/09/tor-generowanie-milszej-nazwy-dla-hidden-service/","summary":"ChcƒÖc pobawiƒá siƒô tor\u0026rsquo;em postanowi≈Çem udostƒôpniƒá jedna z moich stron z wykorzystaniem mechanizmu hidden service. Nie spodoba≈Ç mi siƒô jedynie spos√≥b generowania nazw, kt√≥re by≈Çy ma≈Ço opisowe - ale najwidoczniej nie mnie jednemu, bo szybko namierzy≈Çem Shallot, kt√≥ry generuje kolejne nazwy a≈º trafi na pasujƒÖcƒÖ do zadanego regexp\u0026rsquo;a.","title":"tor: generowanie milszej nazwy dla hidden service"},{"content":"Ostatnio zbyt du≈ºo grzebiƒô przy \u0026ldquo;windach\u0026rdquo; - ale c√≥≈º, czasem trzeba. Ostatnio ustawia≈Çem DFS\u0026rsquo;a z replikacjƒÖ dla dw√≥ch sporych zasob√≥w i jedna z rzeczy, o kt√≥rƒÖ siƒô rozbi≈Çem to brak jakiegokolwiek podglƒÖdu tej synchronizacji z GUI. Ale znalaz≈Çem jedno polecenie, kt√≥re dzia≈Ça w shellu (choƒá to siƒô chyba batch tutaj nazywa) od Windows Server 2008 R2:\ndfsrdiag ReplicationState /member:nazwaservera Polecenie co prawda nie podaje postƒôpu procentowego ale mo≈ºna zobaczyƒá \u0026ldquo;czy co≈õ jeszcze siƒô synchronizuje\u0026rdquo; i czy nie ma ≈ºadnych b≈Çƒôd√≥w. Je≈ºeli to polecenie to za ma≈Ço to mo≈ºna spr√≥bowaƒá bardziej gadatliwej wersji:\ndfsrdiag ReplicationState /member:nazwaservera /all ","permalink":"https://timor.site/2013/09/dfs-sprawdzanie-statusu-replikacji/","summary":"Ostatnio zbyt du≈ºo grzebiƒô przy \u0026ldquo;windach\u0026rdquo; - ale c√≥≈º, czasem trzeba. Ostatnio ustawia≈Çem DFS\u0026rsquo;a z replikacjƒÖ dla dw√≥ch sporych zasob√≥w i jedna z rzeczy, o kt√≥rƒÖ siƒô rozbi≈Çem to brak jakiegokolwiek podglƒÖdu tej synchronizacji z GUI. Ale znalaz≈Çem jedno polecenie, kt√≥re dzia≈Ça w shellu (choƒá to siƒô chyba batch tutaj nazywa) od Windows Server 2008 R2:\ndfsrdiag ReplicationState /member:nazwaservera Polecenie co prawda nie podaje postƒôpu procentowego ale mo≈ºna zobaczyƒá \u0026ldquo;czy co≈õ jeszcze siƒô synchronizuje\u0026rdquo; i czy nie ma ≈ºadnych b≈Çƒôd√≥w.","title":"DFS - sprawdzanie statusu replikacji"},{"content":"Ostatnio aktualizowa≈Çem swojego Nexusa do 4.3 i by≈Ço mi≈Ço tylko mi roota i recovery wystrzeli≈Ço\u0026hellip; No ale ≈ºaden problem - chwila googlania, kilka polece≈Ñ i mam recovery i roota. Dzisiaj wrzuci≈Çem niedu≈ºƒÖ aktualizacjƒô, kt√≥ra ≈Çata kilka bug√≥w i zn√≥w po root\u0026rsquo;cie ;/\nZapisze sobie wiƒôc instrukcjƒô by kolejnym razem ju≈º nie googlaƒá üòÉ\nP.S. Wiem co robiƒô ryzykujƒÖc uceglenie swojego urzƒÖdzenia - u mnie ta instrukcja dzia≈Ça ale nie mogƒô tego zagwarantowaƒá ka≈ºdemu - dlatego je≈õli ju≈º siƒô zdecydujesz to ROBISZ TO NA W≈ÅASNƒÑ ODPOWIEDZIALNO≈öƒÜ!\nBƒôdzie potrzebne Android SDK lub przynajmniej ma≈Ça paczka z fastboot i adb (wiƒôcej info o instalacji w linkach na ko≈Ñcu).\nNa poczƒÖtek sprawdzamy czy nie ma nowego recovery, preferujƒô CWM\u0026rsquo;a wiƒôc zerkamy tutaj Pobieram wersjƒô touch (bo na tak du≈ºym ekranie ca≈Çkiem komfortowo siƒô jƒÖ obs≈Çuguje): recovery-clockwork-touch-6.0.3.6-grouper.img Nastƒôpna wa≈ºna paczka to SuperSU - sprawdzamy jaka jest najnowsza wersja: www.chainfire.eu, a gdy ju≈º to wiemy to googlamy za niƒÖ na xda developers lub download.chainfire.eu Aktualna paczka to: UPDATE-SuperSU-v1.55.zip ZaktualizowanƒÖ paczkƒô SuperSU pobieramy/wrzucamy na urzƒÖdzenie. Wy≈ÇƒÖczamy tablet W≈ÇƒÖczamy tablet przytrzymujƒÖc r√≥wnocze≈õnie przyciski Volume down + Power do czasu a≈º urzƒÖdzenie zacznie startowaƒá - czekamy a≈º zobaczymy Bootmanagera (rozgrzebany robot üòÉ ) Przechodzimy do katalogu z fastboot i otwieramy tam Command Line (np. przytrzymujƒÖc Shift na folderze i wybierajƒÖc opcjƒô Otw√≥rz tutaj okno wiersza polece≈Ñ - u mnie: cd D:\\Nexus 7\\4.3\\adt-bundle-windows-x86_64-20130729\\sdk\\platform-tools Wpisujemy polecenie (uwzglƒôdniajƒÖ wersjƒô recovery, kt√≥rƒÖ wgrywamy): fastboot flash recovery recovery-clockwork-touch-6.0.3.6-grouper.img Je≈ºeli wszystko p√≥jdzie pomy≈õlnie to powinno wypisaƒá co≈õ w rodzaju:\nsending \u0026#39;recovery\u0026#39; (6740 KB)... OKAY [ 0.826s] writing \u0026#39;recovery\u0026#39;... OKAY [ 0.486s] finished. total time: 1.312s Je≈ºeli zobaczymy: \u0026lt; waiting for device \u0026gt; tzn. ≈ºe urzƒÖdzenie nie jest gotowe i mo≈ºna wcisnƒÖƒá Ctrl+C, mo≈ºe nawet trzeba bƒôdzie poszukaƒá na to rozwiƒÖzania\u0026hellip; KorzystajƒÖc z klawiszy Volume down/up wybieramy Recovery Mode i wyb√≥r potwierdzamy przyciskiem Power Po za≈Çadowaniu CWM\u0026rsquo;a wybieramy kolejno: install zip choose zip from sdcard (lƒÖdujemy po tym w /sdcard) i w moim przypadku idƒô dalej do 0/Download/ wybieramy - UPDATE-SuperSU-v1.55.zip potwierdzamy wyb√≥r Yes - Install UPDATE-SuperSU-v1.55.zip po instalacji wybieramy +++++Go Back+++++ nastƒôpnie reboot system now gdy pojawi siƒô ROM may flash stock recovery on boot. Fix? THIS CAN NOT BE UNDONE. - wybranie YES nadpisze stock\u0026rsquo;owe recovery (w≈Ça≈õciwie po to instalujƒô CWM\u0026rsquo;a), ale mo≈ºna te≈º wybraƒá NO a CWM przepadnie No i brawo - mamy 4.3 build JWR66Y i root\u0026rsquo;a z CWM recovery üòÉ\nPo restarcie sprawdzamy czy mamy roota odpalajƒÖc choƒáby SuperSU.\n≈πr√≥d≈Ça http://www.info-pc.info/2013/07/how-to-root-nexus-7-android-43-jwr66v.html\nhttp://forum.xda-developers.com/showthread.php?t=2377511\nhttp://forum.xda-developers.com/showthread.php?t=1538053\n","permalink":"https://timor.site/2013/09/rootowanie-androida-4-3-na-google-nexus-7-po-aktualizacji-do-jwr66y/","summary":"Ostatnio aktualizowa≈Çem swojego Nexusa do 4.3 i by≈Ço mi≈Ço tylko mi roota i recovery wystrzeli≈Ço\u0026hellip; No ale ≈ºaden problem - chwila googlania, kilka polece≈Ñ i mam recovery i roota. Dzisiaj wrzuci≈Çem niedu≈ºƒÖ aktualizacjƒô, kt√≥ra ≈Çata kilka bug√≥w i zn√≥w po root\u0026rsquo;cie ;/\nZapisze sobie wiƒôc instrukcjƒô by kolejnym razem ju≈º nie googlaƒá üòÉ\nP.S. Wiem co robiƒô ryzykujƒÖc uceglenie swojego urzƒÖdzenia - u mnie ta instrukcja dzia≈Ça ale nie mogƒô tego zagwarantowaƒá ka≈ºdemu - dlatego je≈õli ju≈º siƒô zdecydujesz to ROBISZ TO NA W≈ÅASNƒÑ ODPOWIEDZIALNO≈öƒÜ!","title":"Root‚Äôowanie Androida 4.3 na Google Nexus 7 po aktualizacji do JWR66Y"},{"content":"Niedawno chcia≈Çem skopiowaƒá maszynƒô wirtualnƒÖ z jednego hypervisora na innego. By≈Çy to 3 wolumeny LVM o rozmiarach od 50 do 100GB. Dawno temu zrobi≈Çem sobie skrypty do backupu - jeden kompresuje wolumeny LVM - a drugi pozwala odtworzyƒá z dekompresja na drugim serwerze. Tyle ≈ºe przy tak du≈ºej maszynce bƒôdzie to trwa≈Ço masakrycznie d≈Çugo - fajnie by≈Çoby m√≥c r√≥wnocze≈õnie kopiowaƒá i odtwarzaƒá (live)\u0026hellip;\nI wtedy przypomnia≈Ço mi siƒô narzƒôdzie netcat - zrobi≈Çem snapshoty wolumen√≥w i mog≈Çem zaczynaƒá. W najbardziej podstawowej wersji potrzebowa≈Çem tylko tyle:\nna ≈∫r√≥dle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | nc -l -p 8888 na docelowym: nc 192.168.1.10 8888 | dd of=/dev/vgsas/vm1-sys Lub wariacje z kompresjƒÖ:\nna ≈∫r√≥dle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | pigz -2 | nc -l -p 8888 na docelowym: nc 192.168.1.10 8888 | pigz -d | dd of=/dev/vgsas/vm1-sys No dobra - pv nie jest najbardziej podstawowe\u0026hellip; Ale umo≈ºliwia podglƒÖd postƒôpu wysy≈Çania/obierania (zale≈ºy, z kt√≥rej strony go wrzuciƒá) co przy tak d≈Çugim procesie jest niezmiernie przydatne.\nDo kompresji zaleca≈Çbym pigz (czyli Parallel GZIP) z ratio dostosowanym do przepustowo≈õci sieci - po gigabicie siƒô nie op≈Çaca≈Ço nawet na o≈õmiordzeniowcu.\n","permalink":"https://timor.site/2013/09/kopiowanie-wolumenow-lvm-z-dd-i-netcat/","summary":"Niedawno chcia≈Çem skopiowaƒá maszynƒô wirtualnƒÖ z jednego hypervisora na innego. By≈Çy to 3 wolumeny LVM o rozmiarach od 50 do 100GB. Dawno temu zrobi≈Çem sobie skrypty do backupu - jeden kompresuje wolumeny LVM - a drugi pozwala odtworzyƒá z dekompresja na drugim serwerze. Tyle ≈ºe przy tak du≈ºej maszynce bƒôdzie to trwa≈Ço masakrycznie d≈Çugo - fajnie by≈Çoby m√≥c r√≥wnocze≈õnie kopiowaƒá i odtwarzaƒá (live)\u0026hellip;\nI wtedy przypomnia≈Ço mi siƒô narzƒôdzie netcat - zrobi≈Çem snapshoty wolumen√≥w i mog≈Çem zaczynaƒá.","title":"Kopiowanie wolumen√≥w LVM z dd i netcat"},{"content":"Raz na jaki≈õ czas trzeba co≈õ niestandardowego wrzuciƒá do instalacji w Active Directory a ≈ºe nie wszystkie aplikacje majƒÖ dostƒôpne paczki MSI to trzeba siƒô nieco natrudziƒá.\nPoni≈ºej wrzucam skrypt, kt√≥ry instaluje GIMP\u0026rsquo;a 2.8 z domy≈õlnego instalatora (wersja InnoSetup) przy okazji odinstalowujƒÖc wcze≈õniejsze wersje zainstalowane rƒôcznie.\nZapisujemy poni≈ºszy kod jako np. gimp-install.cmd\n@echo off REM Installs GIMP cls echo ---------------------------------------------------- echo . echo . echo . Installing/Updating GIMP - Please Wait echo . echo . echo ---------------------------------------------------- REM Test if actual IF exist \u0026#34;%ProgramFiles%\\GIMP\\bin\\gimp-2.8.exe\u0026#34; GOTO SkipInstall REM Exit the application taskkill.exe /F /FI \u0026#34;IMAGENAME eq gimp-2.8.exe\u0026#34; \u0026gt;nul REM Uninstall existing GIMP version, delete folder if exist \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; /VERYSILENT :: Wait for 20 seconds ping -n 40 127.0.0.1 \u0026gt; NUL if exist \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; rd \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; /Q /S REM Install new version \u0026#34;\\\\serwerplikow.local\\Instalki\\GIMP\\gimp-2.8.4-setup.exe\u0026#34; /VERYSILENT /NORESTART /DIR=\u0026#34;%PROGRAMFILES%\\GIMP 2.8\u0026#34; REM Skip installation if acctuall :SkipInstall REM Return exit code to SCCM exit /B %EXIT_CODE% Tworzymy nowƒÖ regu≈Çkƒô GPO i zmierzamy do: Computer Configuration\\Policies\\Windows Settings\\Scripts\\Startup\nW nowym okienku wybieramy Show Files\u0026hellip;\nWklejamy plik skryptu do tego folderu i teraz mo≈ºemy dodaƒá go w tym samym oknie (Add\u0026hellip;) - dziƒôki wrzuceniu skryptu w tym miejscu bƒôdzie siƒô on automatycznie replikowaƒá na inne kontrolery. Skrypt bƒôdzie co prawda uruchamiany przy ka≈ºdym starcie komputera ale pierwszy warunek bƒôdzie sprawdzaƒá czy aplikacja jest zainstalowana wiƒôc nie spowolni to znacznie startu.\nSkrypt znalaz≈Çem gdzie≈õ na sieci ale nie mogƒô namierzyƒá ≈∫r√≥d≈Ça.\n","permalink":"https://timor.site/2013/08/gpo-instalacja-gimpa-2-8/","summary":"Raz na jaki≈õ czas trzeba co≈õ niestandardowego wrzuciƒá do instalacji w Active Directory a ≈ºe nie wszystkie aplikacje majƒÖ dostƒôpne paczki MSI to trzeba siƒô nieco natrudziƒá.\nPoni≈ºej wrzucam skrypt, kt√≥ry instaluje GIMP\u0026rsquo;a 2.8 z domy≈õlnego instalatora (wersja InnoSetup) przy okazji odinstalowujƒÖc wcze≈õniejsze wersje zainstalowane rƒôcznie.\nZapisujemy poni≈ºszy kod jako np. gimp-install.cmd\n@echo off REM Installs GIMP cls echo ---------------------------------------------------- echo . echo . echo . Installing/Updating GIMP - Please Wait echo .","title":"GPO: Instalacja GIMP‚Äôa 2.8"},{"content":"Narzƒôdzia xz-utils dostƒôpne w nowszych systemach korzystajƒÖ z mocniejszych algorytm√≥w kompresji (jaka≈õ odmiana LZMA, co≈õ w stylu 7zip\u0026rsquo;a) przy zachowaniu kompatybilno≈õci sk≈Çadni polece≈Ñ z gzip\u0026rsquo;em/bzip\u0026rsquo;em - da siƒô je zatem ≈Çatwo zintegrowaƒá w obecnych systemach. Ja chcia≈Çem wykorzystaƒá xz do kompresji log√≥w, kt√≥re bywajƒÖ przydatne ale przez wiƒôkszo≈õƒá czasy tylko zajmujƒÖ miejsce :simple_smile:\nW /etc/logrotate.conf dopisujemy:\ncompresscmd /usr/bin/xz uncompresscmd /usr/bin/unxz compressext .xz compressoptions -9T2 compressoptions mo≈ºna nie ustawiaƒá bo domy≈õlnie ma warto≈õƒá -9 (czyli kompresuj na maxa), m√≥j dodatek (czyli -T2) u≈ºyje dw√≥ch wƒÖtk√≥w procesora gdy ju≈º ten mechanizm zostanie zimplementowany (bo na razie nie jest) :simple_smile:\n","permalink":"https://timor.site/2013/07/logrotate-kompresja-logow-xz/","summary":"Narzƒôdzia xz-utils dostƒôpne w nowszych systemach korzystajƒÖ z mocniejszych algorytm√≥w kompresji (jaka≈õ odmiana LZMA, co≈õ w stylu 7zip\u0026rsquo;a) przy zachowaniu kompatybilno≈õci sk≈Çadni polece≈Ñ z gzip\u0026rsquo;em/bzip\u0026rsquo;em - da siƒô je zatem ≈Çatwo zintegrowaƒá w obecnych systemach. Ja chcia≈Çem wykorzystaƒá xz do kompresji log√≥w, kt√≥re bywajƒÖ przydatne ale przez wiƒôkszo≈õƒá czasy tylko zajmujƒÖ miejsce :simple_smile:\nW /etc/logrotate.conf dopisujemy:\ncompresscmd /usr/bin/xz uncompresscmd /usr/bin/unxz compressext .xz compressoptions -9T2 compressoptions mo≈ºna nie ustawiaƒá bo domy≈õlnie ma warto≈õƒá -9 (czyli kompresuj na maxa), m√≥j dodatek (czyli -T2) u≈ºyje dw√≥ch wƒÖtk√≥w procesora gdy ju≈º ten mechanizm zostanie zimplementowany (bo na razie nie jest) :simple_smile:","title":"logrotate: kompresja log√≥w xz"},{"content":"Na jednym z urzƒÖdze≈Ñ mia≈Çem problem z odtworzeniem plik√≥w (g≈Ç√≥wnie MKV) z d≈∫wiƒôkiem zakodowanym w DTS. PomijajƒÖc ≈ºe np. na tablecie 6-cio kana≈Çy DTS jest mi \u0026ldquo;niezbƒôdny inaczej\u0026rdquo; to konwertujƒÖc go do AAC stereo plik jest po prostu sporo mniejszy. Oczywi≈õcie nie zamierzam transkodowaƒá ≈õcie≈ºki video i na moje potrzeby mog≈Çem sobie odpu≈õciƒá zmianƒô czƒôstotliwo≈õci pr√≥bkowania.\nNajpro≈õciej wykorzystaƒá pakiet ffmpeg (po nowemu avconv) lub mencoder (choƒá ten miewa≈Ç niegdy≈õ problem z poprawnym zapisywaniem wynikowych plik√≥w mkv, wiƒôc potrzebny jest dodatkowo mkvmerge z pakietu mkvtoolnix). mencoder transkoduje szybciej wykorzystujƒÖc wiƒôcej rdzeniu CPU, ale p√≥≈∫niej potrzebny by≈Ç drugi przebieg z mkvmerge. ffmpeg jecha≈Ç na jednym procku nawet z opcjƒÖ threads ale za to wszystko mogƒô zrobiƒá jednym poleceniem.\nPaczki instalujemy tak:\napt-get install libav-tools libavcodec-extra MKV DTS do MKV AAC stereo avconv -i input.mkv -c copy -c:a libvo_aacenc -b:a 128k -ac 2 -threads auto output-stereo-aac.mkv Gdyby≈õmy chcieli zakodowaƒá wszystkie kana≈Çy z DTS do AAC to wystarczy pominƒÖƒá parametr -ac 2.\nMKV DTS do MKV AC3 (wszystkie kana≈Çy) avconv -i input.mkv -c copy -c:a¬†libvo_aacenc -b:a 128k -threads auto output-ac3.mkv MKV DTS do MKV AC3 z mencoder\u0026rsquo;em AVI=`mktemp video.XXXXXX.avi` mencoder \u0026#34;input.mkv\u0026#34; -o $AVI -oac lavc -lavcopts acodec=ac3:abitrate=448 -ovc copy mkvmerge $AVI -o output.AC3.mkv rm $AVI ","permalink":"https://timor.site/2013/07/bezstratna-konwersja-mkv-z-dts-do-ac3-lub-aac/","summary":"Na jednym z urzƒÖdze≈Ñ mia≈Çem problem z odtworzeniem plik√≥w (g≈Ç√≥wnie MKV) z d≈∫wiƒôkiem zakodowanym w DTS. PomijajƒÖc ≈ºe np. na tablecie 6-cio kana≈Çy DTS jest mi \u0026ldquo;niezbƒôdny inaczej\u0026rdquo; to konwertujƒÖc go do AAC stereo plik jest po prostu sporo mniejszy. Oczywi≈õcie nie zamierzam transkodowaƒá ≈õcie≈ºki video i na moje potrzeby mog≈Çem sobie odpu≈õciƒá zmianƒô czƒôstotliwo≈õci pr√≥bkowania.\nNajpro≈õciej wykorzystaƒá pakiet ffmpeg (po nowemu avconv) lub mencoder (choƒá ten miewa≈Ç niegdy≈õ problem z poprawnym zapisywaniem wynikowych plik√≥w mkv, wiƒôc potrzebny jest dodatkowo mkvmerge z pakietu mkvtoolnix).","title":"Bezstratna konwersja MKV z DTS do AC3 lub AAC"},{"content":"Raz na jaki≈õ czas gdy grzebiƒô przy maciorach muszƒô \u0026ldquo;odkryƒá\u0026rdquo; nowy volumen FC (lub rzadziej SCSI), kt√≥ry w≈Ça≈õnie utworzy≈Çem a restart serwera nie wchodzi w rachubƒô (zresztƒÖ na czƒô≈õci system√≥w nic on nie da).\nBy to zrobiƒá sƒÖ dwie mo≈ºliwo≈õci:\nRƒôczne wydanie polece≈Ñ odkrywajƒÖcych volumeny (na jajkach od 2.6.x) Sprawdzamy jakie mamy karty:\nls /sys/class/fc_host/ (wypisze siƒô co≈õ w stylu: host1, host2)\nWydajemy do wybranej przez nas karty ≈ºƒÖdanie wykonania LIP (to siƒô chyba t≈Çumaczy jako loopback initialization) co skutkuje przeskanowaniem szyny FC:\necho 1 \u0026gt;/sys/class/fc_host/host1/issue_lip Czekamy 15~30 sekund aby zadzia≈Ça≈Ço polecenie.\nRzƒÖdamy przeskanowania dostƒôpnych volumen√≥w SCSI/FC:\necho - - - \u0026gt;/sys/class/scsi_host/host1/scan (my≈õlniki w echo oznaczajƒÖ sprawdzenie wszystkich kana≈Ç√≥w, target√≥w i lun\u0026rsquo;√≥w - je≈ºeli mamy bardzo du≈ºo volumen√≥w to mo≈ºna tutaj nieco optymalizowaƒá, ale to nie by≈Ç m√≥j problem)\nOdpalamy np. dmesg aby zobaczyƒá jakie nowe volumeny siƒô pojawi≈Çy.\n≈πr√≥d≈Ço: http://misterd77.blogspot.com/2007/12/how-to-scan-scsi-bus-with-26-kernel.html\nKorzystamy ze skryptu rescan-scsi-bus.sh Skrypt ten automatycznie robi to co potrzebujemy, skanujƒÖc wszystkie karty FC pod kƒÖtem nowych volumen√≥w.\nwget http://rescan-scsi-bus.sh/ -O rescan-scsi-bus.sh chmod +x rescan-scsi-bus.sh ./rescan-scsi-bus.sh I tyle!\n≈πr√≥d≈Ço: http://rescan-scsi-bus.sh/\n","permalink":"https://timor.site/2013/07/dodawanie-urzadzen-scsifc-bez-restartu-serwera/","summary":"Raz na jaki≈õ czas gdy grzebiƒô przy maciorach muszƒô \u0026ldquo;odkryƒá\u0026rdquo; nowy volumen FC (lub rzadziej SCSI), kt√≥ry w≈Ça≈õnie utworzy≈Çem a restart serwera nie wchodzi w rachubƒô (zresztƒÖ na czƒô≈õci system√≥w nic on nie da).\nBy to zrobiƒá sƒÖ dwie mo≈ºliwo≈õci:\nRƒôczne wydanie polece≈Ñ odkrywajƒÖcych volumeny (na jajkach od 2.6.x) Sprawdzamy jakie mamy karty:\nls /sys/class/fc_host/ (wypisze siƒô co≈õ w stylu: host1, host2)\nWydajemy do wybranej przez nas karty ≈ºƒÖdanie wykonania LIP (to siƒô chyba t≈Çumaczy jako loopback initialization) co skutkuje przeskanowaniem szyny FC:","title":"Dodawanie urzƒÖdze≈Ñ SCSI/FC bez restartu serwera"},{"content":"Bardzo spodoba≈Ça mi siƒô nowa aplikacja do zdalnej synchronizacji folder√≥w z wykorzystaniem P2P. Ja wykorzysta≈Çem jƒÖ do automatycznych backup√≥w archiwum zdjƒôƒá - zebra≈Ço mi siƒô tego ju≈º prawie 130GB! Ka≈ºde narzƒôdzie, kt√≥re chce np. je kompresowaƒá i raz na czas robiƒá FULL backup jest skazane na pora≈ºkƒô - a fotek przybywa.\nNa poczƒÖtek pobieramy interesujƒÖcƒÖ nas wersjƒô:\nwget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_i386.tar.gz wget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_x64.tar.gz Oraz paczkƒô ze skryptami dla Debiana:\nwget http://www.yeasoft.com/downloads/various/btsync-linux-deploy.tar.gz Ja wyciƒÖgam z niej skrypt /etc/init.d/btsync i konfiguracjƒô /etc/*\nTeraz user z ograniczonymi uprawnieniami, na kt√≥rym bƒôdzie dzia≈Çaƒá nasza us≈Çuga (mo≈ºna wykorzystaƒá swojego usera aby m√≥c synchronizowaƒá swoje foldery):\nuseradd -d /var/lib/btsync -m -s /bin/sh btsync Odpalamy z domy≈õlnƒÖ konfiguracjƒÖ ≈ºeby tylko dzia≈Ça≈Ço - dokonfiguruje p√≥≈∫niej:\ncd /etc/btsync mv default.conf default.btsync.btsync.conf chown root:btsync default.btsync.btsync.conf chmod 640 default.btsync.btsync.conf service btsync start Powinno dzia≈Çaƒá wiƒôc sprawd≈∫my:\nps axu | grep bts netstat -nlp Tadam!\n","permalink":"https://timor.site/2013/07/debian-instalacja-bittorrent-sync-btsync/","summary":"Bardzo spodoba≈Ça mi siƒô nowa aplikacja do zdalnej synchronizacji folder√≥w z wykorzystaniem P2P. Ja wykorzysta≈Çem jƒÖ do automatycznych backup√≥w archiwum zdjƒôƒá - zebra≈Ço mi siƒô tego ju≈º prawie 130GB! Ka≈ºde narzƒôdzie, kt√≥re chce np. je kompresowaƒá i raz na czas robiƒá FULL backup jest skazane na pora≈ºkƒô - a fotek przybywa.\nNa poczƒÖtek pobieramy interesujƒÖcƒÖ nas wersjƒô:\nwget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_i386.tar.gz wget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_x64.tar.gz Oraz paczkƒô ze skryptami dla Debiana:\nwget http://www.yeasoft.com/downloads/various/btsync-linux-deploy.tar.gz Ja wyciƒÖgam z niej skrypt /etc/init.","title":"Debian - Instalacja Bittorrent Sync (btsync)"},{"content":"Jest kilka powod√≥w dla kt√≥rych tworzenie patchy jest przydatne - je≈õli tu jeste≈õ to pewnie masz jaki≈õ w≈Çasny\u0026hellip;\nTworzenie patch\u0026rsquo;a diff -crB old new \u0026gt; from-old-to-new.patch W powy≈ºszym poleceniu za≈Ço≈ºy≈Çem ≈ºe old i new to katalogi z wieloma podkatalogami i plikami - stƒÖd opcja -r. -c dodaje kilka linijek \u0026ldquo;kontekstu\u0026rdquo; przez co ≈Çatwiej rozeznaƒá siƒô w patch\u0026rsquo;u. Opcja -B ignoruje puste linie, kt√≥rych patchowanie mnie nie interesuje.\nPatchowanie Na poczƒÖtek zawsze warto wywo≈Çaƒá polecenie z opcjƒÖ -dry-run by zobaczyƒá czy patch wykona siƒô poprawnie:\npatch --dry-run -p1 -i from-old-to-new.patch Opcja -p1 zak≈Çada ≈ºe uruchamiamy patcha z folderu \u0026ldquo;projektu\u0026rdquo;, kt√≥ry chcemy patchowaƒá. Opcjƒô -i mo≈ºna z powodzeniem zastƒÖpiƒá przekierowaniem \u0026lt; .\nJe≈ºeli nie widzimy ≈ºadnych komunikat√≥w \u0026ldquo;FAILED\u0026rdquo; to mo≈ºemy uruchomiƒá patcha:\npatch -p1 -i from-old-to-new.patch ","permalink":"https://timor.site/2013/04/tworzenie-patchy-z-poleceniami-diff-i-patch/","summary":"Jest kilka powod√≥w dla kt√≥rych tworzenie patchy jest przydatne - je≈õli tu jeste≈õ to pewnie masz jaki≈õ w≈Çasny\u0026hellip;\nTworzenie patch\u0026rsquo;a diff -crB old new \u0026gt; from-old-to-new.patch W powy≈ºszym poleceniu za≈Ço≈ºy≈Çem ≈ºe old i new to katalogi z wieloma podkatalogami i plikami - stƒÖd opcja -r. -c dodaje kilka linijek \u0026ldquo;kontekstu\u0026rdquo; przez co ≈Çatwiej rozeznaƒá siƒô w patch\u0026rsquo;u. Opcja -B ignoruje puste linie, kt√≥rych patchowanie mnie nie interesuje.\nPatchowanie Na poczƒÖtek zawsze warto wywo≈Çaƒá polecenie z opcjƒÖ -dry-run by zobaczyƒá czy patch wykona siƒô poprawnie:","title":"Tworzenie patch‚Äôy z poleceniami diff i patch"},{"content":"Mia≈Çem ostatnio zabawnƒÖ sytuacjƒô gdy kilka serwer√≥w z zainstalowanym NTPD mia≈Ço rozjazdy rzƒôdu kilkunastu sekund. Wysz≈Ço na to ≈ºe moje serwery synchronizowa≈Çy siƒô z r√≥≈ºnymi zewnƒôtrznymi serwerami NTP pomiƒôdzy, kt√≥rymi by≈Çy rozjazdy i te rozjazdy synchronizowa≈Çy siƒô na moich serwerach. Jeden \u0026ldquo;z moich\u0026rdquo; ustanowi≈Çem g≈Ç√≥wnym a wszystkie inne przekierowa≈Çem na niego (komentujƒÖc wszystkie inne serwery NTP w konfiguracji). Wymusi≈Çem synchronizacjƒô:\nntp -q Sprawdzi≈Çem jak du≈ºy jest offset i jitter (powinny byƒá bardzo ma≈Çe):\nntpq -p i tak rzeczywi≈õcie by≈Ço - problem z g≈Çowy.\nP.S. Je≈ºeli nie potrzebujemy/chcemy/etc instalowaƒá serwera NTP to mo≈ºemy zamiast tego u≈ºyƒá polecenia ntpdate w cronie (np. co godzinƒô):\nntpdate moj.serwer.ntp ","permalink":"https://timor.site/2013/03/rozsynchronizowane-serwery-ntp/","summary":"Mia≈Çem ostatnio zabawnƒÖ sytuacjƒô gdy kilka serwer√≥w z zainstalowanym NTPD mia≈Ço rozjazdy rzƒôdu kilkunastu sekund. Wysz≈Ço na to ≈ºe moje serwery synchronizowa≈Çy siƒô z r√≥≈ºnymi zewnƒôtrznymi serwerami NTP pomiƒôdzy, kt√≥rymi by≈Çy rozjazdy i te rozjazdy synchronizowa≈Çy siƒô na moich serwerach. Jeden \u0026ldquo;z moich\u0026rdquo; ustanowi≈Çem g≈Ç√≥wnym a wszystkie inne przekierowa≈Çem na niego (komentujƒÖc wszystkie inne serwery NTP w konfiguracji). Wymusi≈Çem synchronizacjƒô:\nntp -q Sprawdzi≈Çem jak du≈ºy jest offset i jitter (powinny byƒá bardzo ma≈Çe):","title":"Rozsynchronizowane serwery NTP"},{"content":"Do niedawna na moim telefonie VPN\u0026rsquo;ami by≈Çy: PPTP lub L2TP - oba niespecjalnie mi siƒô podoba≈Çy. Ale od wersji 4-tej pojawi≈Çy siƒô dwa nowe tryby: IPSec Xauth PSK i IPSec Xauth RSA. W pierwszym autoryzacja wykorzystuje login i has≈Ço, w drugim certyfikaty.\nTryb IPSec Xauth PSK jest bardzo wygodny bo ≈Çatwo mo≈ºna po≈ÇƒÖczyƒá go z zewnƒôtrznymi mechanizmami¬†uwierzytelniajƒÖcymi¬†np. LDAP, Active Directory, itp.\nPoka≈ºƒô jak skonfigurowaƒá swojego Fortigate\u0026rsquo;a by umo≈ºliwiƒá po≈ÇƒÖczenie z telefon√≥w i tablet√≥w na Androidzie 4.x do \u0026ldquo;Intranetu\u0026rdquo;. Wiƒôkszo≈õƒá konfiguracji mo≈ºna przeprowadziƒá tylko w trybie CLI - zak≈Çadam ≈ºe wiesz jak to zrobiƒá. To co wygodniej mo≈ºna zrobiƒá w trybie WWW to g≈Ç√≥wnie tworzenie regu≈Ç dostƒôpu na zaporze.\nP.S. Teoretycznie powinno to te≈º zadzia≈Çaƒá na IPhone\u0026rsquo;ach/IPad\u0026rsquo;ach itp., przy czym uda≈Ço mi siƒô to zestawiƒá na 4GS ale na 5-ce ju≈º nie - nie mia≈Çem tych aparat√≥w na tyle d≈Çugo by dok≈Çadniej to zbadaƒá.\nNa poczƒÖtek konfigurujemy fazƒô pierwszƒÖ - ja lubiƒô interface mode ale oczywi≈õcie bƒôdzie to dzia≈Çaƒá r√≥wnie≈º w trybie policy (trzeba bƒôdzie nieco polecenia zmieniƒá).\nconfig vpn ipsec phase1-interface edit \u0026#34;vpn-android\u0026#34; set type dynamic set interface \u0026#34;port1\u0026#34; set dhgrp 2 set peertype one set xauthtype auto set mode aggressive set mode-cfg enable set proposal aes128-sha1 aes128-md5 3des-sha1 set negotiate-timeout 15 set peerid \u0026#34;jakis-mobilny-identyfikator\u0026#34; set authusrgrp \u0026#34;grupa-ludzikow\u0026#34; set psksecret haslo set unity-support enable set ipv4-start-ip 172.16.0.5 set ipv4-end-ip 172.16.0.100 set ipv4-netmastk 255.255.255.0 set ipv4-dns-server1 172.16.0.1 set ipv4-dns-server2 8.8.8.8 next end Jedna ciekawostka - opcja unity-support domy≈õlnie jest ustawiana na enable i proponujƒô¬†jƒÖ¬†tak zostawiƒá - dziƒôki temu ta sama konfiguracja bƒôdzie dzia≈Çaƒá¬†na urzƒÖdzeniach z iOS\u0026rsquo;e i OS X, wystarczy skonfigurowaƒá VPN jako Cisco üòÉ\nNo to faza druga:\nconfig vpn ipsec phase2-interface edit \u0026#34;mobile-vpn-p2\u0026#34; set keepalive enable set pfs disable set phase1name \u0026#34;vpn-android\u0026#34; set proposal aes128-sha1 aes128-md5 3des-sha1 set keylifeseconds 3600 next end Kilka ustawie≈Ñ na interfejsie:\nconfig system interface edit \u0026#34;vpn-android\u0026#34; set ip 172.16.0.1 255.255.255.255 set allowaccess ping set type tunnel set remote-ip 139.x.x.10 set interface \u0026#34;port1\u0026#34; next end W≈ÇƒÖczymy sobie serwer DNS na interfejsie VPN:\nconfig system dns-server edit \u0026#34;vpn-android\u0026#34; next end No to teraz trzeba utworzyƒá co najmniej jednƒÖ regu≈Çƒô na zaporze zezwalajƒÖcƒÖ na dostƒôp z interfejsu vpn-android (z adres√≥w tej sieci) do jakiej≈õ sieci intranetowej (np. wewnƒôtrznego serwera WWW), na odpowiednich portach.\nPonadto je≈õli chcemy m√≥c korzystaƒá z cache DNS\u0026rsquo;a na interfejsie vpn-android to musimy umo≈ºliwiƒá do niego dostƒôp tworzƒÖc regu≈Çƒô z interfejsu vpn-android (z adres√≥w przydzielanych przez mode-cfg) do vpn-android (przynajmniej do DNS\u0026rsquo;a - 172.16.0.1).\nJe≈ºeli chcemy by urzƒÖdzenia mobilne mog≈Çy ≈ÇƒÖczyƒá siƒô do zewnƒôtrznych us≈Çug przez tunel VPN (co w niekt√≥rych przypadkach mo≈ºe byƒá bardzo cenne) to musimy te≈º utworzyƒá regu≈Çƒô z interfejsu vpn-android do kt√≥rego≈õ z interfejs√≥w WAN\u0026rsquo;owskich.\nA tak nale≈ºy to wyklikaƒá na telefonie\nhttp://kb.fortinet.com/kb/viewContent.do?externalId=FD31619\u0026amp;sliceId=1\n","permalink":"https://timor.site/2013/03/fortigate-vpn-ipsec-psk-xauth-z-androida-4-x/","summary":"Do niedawna na moim telefonie VPN\u0026rsquo;ami by≈Çy: PPTP lub L2TP - oba niespecjalnie mi siƒô podoba≈Çy. Ale od wersji 4-tej pojawi≈Çy siƒô dwa nowe tryby: IPSec Xauth PSK i IPSec Xauth RSA. W pierwszym autoryzacja wykorzystuje login i has≈Ço, w drugim certyfikaty.\nTryb IPSec Xauth PSK jest bardzo wygodny bo ≈Çatwo mo≈ºna po≈ÇƒÖczyƒá go z zewnƒôtrznymi mechanizmami¬†uwierzytelniajƒÖcymi¬†np. LDAP, Active Directory, itp.\nPoka≈ºƒô jak skonfigurowaƒá swojego Fortigate\u0026rsquo;a by umo≈ºliwiƒá po≈ÇƒÖczenie z telefon√≥w i tablet√≥w na Androidzie 4.","title":"Fortigate - VPN IPSec PSK XAuth z Android‚Äôa 4.x"},{"content":"On Debian you have to install nginx-extras package (because it have built in headers_more module). Then you need two options (best in global configuration /etc/nginx/nginx.conf file, http part):\nserver_tokens off; more_set_headers \u0026#39;Server: BadAss\u0026#39;; And it\u0026rsquo;s good to setup non standard error pages on every site (500 and 404 at minimum):\nerror_page 403 404 http://mysite.com/areyoulost; error_page 502 503 504 /500.html; ","permalink":"https://timor.site/2013/01/nginx-hide-server-version-and-name-in-server-header-and-error-pages/","summary":"On Debian you have to install nginx-extras package (because it have built in headers_more module). Then you need two options (best in global configuration /etc/nginx/nginx.conf file, http part):\nserver_tokens off; more_set_headers \u0026#39;Server: BadAss\u0026#39;; And it\u0026rsquo;s good to setup non standard error pages on every site (500 and 404 at minimum):\nerror_page 403 404 http://mysite.com/areyoulost; error_page 502 503 504 /500.html; ","title":"Nginx - hide server version and name in Server header and error pages"},{"content":"W PHP 5.3 pojawi≈Ça siƒô nowa zmienna: max_input_vars, kt√≥ra limituje ilo≈õƒá p√≥l mo≈ºliwych do przes≈Çania przez formularz, obcinajƒÖc nadmiarowe. Pozwala to zapobiec atakom DoS na tablice hashujƒÖce (przynajmniej w tym jednym miejscu). Domy≈õlna warto≈õƒá tej zmiennej to 1000 i kreatywnym programistom udaje siƒô tƒÖ warto≈õƒá bez problemu osiƒÖgnƒÖƒá üòÉ\nWarte odnotowania jest to ≈ºe majƒÖc suhosin\u0026rsquo;a trzeba pamiƒôtaƒá o jeszcze dw√≥ch innych zmiennych:\nmax_input_vars = 3000 suhosin.post.max_vars = 3000 suhosin.request.max_vars = 3000 Zmienne mo≈ºna zmieniƒá od razu w /etc/php5/apache2/php.ini (choƒá te dla suhosin\u0026rsquo;a lepiej wrzuciƒá do /etc/php5/conf.d/suhosin.ini). A jeszcze lepszym pomys≈Çem jest ustawienie tych zmiennych bezpo≈õrednio dla danego vhost\u0026rsquo;a, w Apache\u0026rsquo;m np. tak:\nphp_value max_input_vars 3000 php_value suhosin.post.max_vars 3000 php_value suhosin.request.max_vars 3000 ","permalink":"https://timor.site/2013/01/php-max_input_vars/","summary":"W PHP 5.3 pojawi≈Ça siƒô nowa zmienna: max_input_vars, kt√≥ra limituje ilo≈õƒá p√≥l mo≈ºliwych do przes≈Çania przez formularz, obcinajƒÖc nadmiarowe. Pozwala to zapobiec atakom DoS na tablice hashujƒÖce (przynajmniej w tym jednym miejscu). Domy≈õlna warto≈õƒá tej zmiennej to 1000 i kreatywnym programistom udaje siƒô tƒÖ warto≈õƒá bez problemu osiƒÖgnƒÖƒá üòÉ\nWarte odnotowania jest to ≈ºe majƒÖc suhosin\u0026rsquo;a trzeba pamiƒôtaƒá o jeszcze dw√≥ch innych zmiennych:\nmax_input_vars = 3000 suhosin.post.max_vars = 3000 suhosin.request.max_vars = 3000 Zmienne mo≈ºna zmieniƒá od razu w /etc/php5/apache2/php.","title":"PHP - max_input_vars"},{"content":"Dyski siƒô zu≈ºywajƒÖ i w ko≈Ñcu wcze≈õniej czy p√≥≈∫niej pojawiajƒÖ siƒô na nich bad sectory. Jeden z moich dysk√≥w ciut siƒô posypa≈Ç a ≈ºe s≈Çu≈ºy wy≈ÇƒÖcznie do backup√≥w to mogƒô z tym ≈ºyƒá. Ale z drugiej strony je≈ºeli ju≈º bƒôdƒô musia≈Ç siƒôgnƒÖƒá do backup√≥w to chcƒô mieƒá pewno≈õƒá ≈ºe co≈õ odzyskam, dlatego postanowi≈Çem zrobiƒá kilka test√≥w. Nawet je≈õli nie naprawi to sektor√≥w to przynajmniej zostanƒÖ zaznaczone jako uszkodzone i realokowane.\nNa poczƒÖtek zaczƒÖ≈Çem od pr√≥by puszczenia badblocks w trybie nie destruktywnym na ca≈Çym dysku:\nbadblocks -b 4096 -nsv /dev/sdf Poszukiwanie wadliwych blok√≥w w trybie z niedestruktywnym zapisem Od bloku 0 do 488378645 Poszukiwanie wadliwych blok√≥w (odczyt i niedestruktywny zapis) Testowanie wzorcem losowym: ^Ctowe w 1.28%, minƒô≈Ço 10:47 (b≈Çƒôd√≥w: 0/0/0) Interrupted at block 6262912 Otrzymano przerwanie, sprzƒÖtam Zaczeka≈Çem 10 minut i na podstawie bie≈ºƒÖcych statystyk oszacowa≈Çem ≈ºe test zajmie ponad 13 godzin\u0026hellip; Jak widaƒá przerwa≈Çem - sprawd≈∫my wiƒôc co powiedzƒÖ testy smart\u0026rsquo;a. Co prawda niczego one nie naprawiƒÖ ale zidentyfikujƒÖ, na kt√≥rym sektorze zaczynajƒÖ siƒô problemy. Bƒôdƒô m√≥g≈Ç wtedy pu≈õciƒá badblocks ju≈º od tego miejsca oszczƒôdzajƒÖc nieco czasu\u0026hellip; Wiƒôc:\nsmartctl -t long /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34;. Drive command \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34; successful. Testing has begun. Please wait 255 minutes for test to complete. Test will complete after Sat Nov 24 03:59:21 2012 Use smartctl -X to abort test. 255 minut to trochƒô ponad 4 godziny - przynajmniej sko≈Ñczy siƒô do rana. No to jeszcze:\nshutdown -h 4:15 I zerkniemy jutro co znajdzie test üòÉ\nDzie≈Ñ drugi Zerkamy\u0026hellip; i:\nsmartctl -l selftest /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Extended offline Completed: read failure 90% 15196 164529056 # 2 Short offline Aborted by host 10% 15192 - Widaƒá tutaj wyniki dw√≥ch test√≥w - przy czym interesuje nas ten d≈Çu≈ºszy z numerkiem 1. Niestety mam jaki≈õ bad sektor i mam pierwszy adres jego wystƒÖpienia - szkoda ≈ºe to gdzie≈õ w pierwszych 10% dysku - mia≈Çem nadziejƒô ≈ºe gdzie≈õ dalej (by≈Çoby mniej do sprawdzenia). Ale je≈õli badsectory zaczynajƒÖ siƒô tak wcze≈õnie to puszczƒô badblocks od poczƒÖtku - niech przejedzie ca≈Çy dysk - albo siƒô co≈õ naprawi, albo padnie. Ale przynajmniej sprawa siƒô wyja≈õni üòâ\n≈πr√≥d≈Ça http://smartmontools.sourceforge.net/badblockhowto.html\nhttp://sourceforge.net/apps/trac/smartmontools/wiki/SamsungF4EGBadBlocks\nman badblocks\n","permalink":"https://timor.site/2012/12/linux-naprawianie-bad-sectorow/","summary":"Dyski siƒô zu≈ºywajƒÖ i w ko≈Ñcu wcze≈õniej czy p√≥≈∫niej pojawiajƒÖ siƒô na nich bad sectory. Jeden z moich dysk√≥w ciut siƒô posypa≈Ç a ≈ºe s≈Çu≈ºy wy≈ÇƒÖcznie do backup√≥w to mogƒô z tym ≈ºyƒá. Ale z drugiej strony je≈ºeli ju≈º bƒôdƒô musia≈Ç siƒôgnƒÖƒá do backup√≥w to chcƒô mieƒá pewno≈õƒá ≈ºe co≈õ odzyskam, dlatego postanowi≈Çem zrobiƒá kilka test√≥w. Nawet je≈õli nie naprawi to sektor√≥w to przynajmniej zostanƒÖ zaznaczone jako uszkodzone i realokowane.","title":"Linux - naprawianie bad sector√≥w"},{"content":" Getting Things DoneCzyli sztuka bezstresowej efektywno≈õci\nAuthor: David Allen\namazon.pl ","permalink":"https://timor.site/books/2012/getting-things-done/","summary":" Getting Things DoneCzyli sztuka bezstresowej efektywno≈õci\nAuthor: David Allen\namazon.pl ","title":"Getting Things Done"},{"content":"Korzystam z instancji Piwik\u0026rsquo;a do monitorowania odwiedzin na stronie i postanowi≈Çem pokombinowaƒá czy da siƒô w ten spos√≥b monitorowaƒá wej≈õcia konkretnych os√≥b na bazie wpisanego w polu komentarza loginu/ksywki. Jak zaczƒÖ≈Çem grzebaƒá to przy okazji zmieni≈Çem te≈º spos√≥b ≈Çadowania skrypt√≥w Piwika na asynchroniczny.\nA leci to mniej wiƒôcej tak:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _paq = _paq || []; (function(){ var u=((\u0026#34;https:\u0026#34; == document.location.protocol) ? \u0026#34;https://url.instancji.piwika.pl/\u0026#34; : \u0026#34;http://url.instancji.piwika.pl/\u0026#34;); _paq.push([\u0026#39;setSiteId\u0026#39;, 1]); _paq.push([\u0026#39;setTrackerUrl\u0026#39;, u+\u0026#39;piwik.php\u0026#39;]); _paq.push([\u0026#39;setCustomVariable\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;Author\u0026#39;, comment_author]); _paq.push([\u0026#39;trackPageView\u0026#39;]); _paq.push([\u0026#39;enableLinkTracking\u0026#39;]); var d=document, g=d.createElement(\u0026#39;script\u0026#39;), s=d.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; g.type=\u0026#39;text/javascript\u0026#39;; g.defer=true; g.async=true; g.src=u+\u0026#39;piwik.js\u0026#39;; s.parentNode.insertBefore(g,s); })(); \u0026lt;/script\u0026gt; ≈πr√≥d≈Ço http://piwik.org/docs/javascript-tracking/#toc-asynchronous-tracking http://codex.wordpress.org/WordPress_Cookies#Commenters\n","permalink":"https://timor.site/2012/12/piwik-sledzenie-asynchroniczne-logowanie-ksywy-komentujacego-w-wordpressie/","summary":"Korzystam z instancji Piwik\u0026rsquo;a do monitorowania odwiedzin na stronie i postanowi≈Çem pokombinowaƒá czy da siƒô w ten spos√≥b monitorowaƒá wej≈õcia konkretnych os√≥b na bazie wpisanego w polu komentarza loginu/ksywki. Jak zaczƒÖ≈Çem grzebaƒá to przy okazji zmieni≈Çem te≈º spos√≥b ≈Çadowania skrypt√≥w Piwika na asynchroniczny.\nA leci to mniej wiƒôcej tak:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.","title":"Piwik: ≈õledzenie asynchroniczne + logowanie ksywy komentujƒÖcego w WordPress‚Äôie"},{"content":"Dawno, dawno temu\u0026hellip; Za g√≥rami, za lasami\u0026hellip; czyta≈Çem sobie tekst Lemat\u0026rsquo;a o dokuczaniu spamerom i pomy≈õla≈Çem ≈ºe sam te≈º tak mogƒô i nawet chcƒô wiƒôc pope≈Çni≈Çem skrypcik, kt√≥ry dla losowych s≈Ç√≥w generowa≈Ç maile. Skrypcik dzia≈Ça≈Ç z dwa lata na mojej poprzedniej stronie i nie raz zdarzy≈Ço siƒô tam jakiej≈õ mendzie zapƒôtliƒá. Jako≈õ nie mia≈Çem czasu od razu, a p√≥≈∫niej zapomnia≈Çem wrzuciƒá go na nowƒÖ stronie i tak zosta≈Ço - na pewien czas.\nNiedawno przeglƒÖdajƒÖc logi zauwa≈ºy≈Çem ≈ºe jakie≈õ spam-boty jednak tƒôskniƒÖ za tƒÖ podstronƒÖ i postanowi≈Çem jƒÖ wskrzesiƒá.\nPomys≈Ç jest prosty i polega na generowaniu na pewnej podstronie du≈ºej ilo≈õci \u0026ldquo;z dupy\u0026rdquo; maili. Oczywi≈õcie w robots.txt dajemy Disallow dla tej ≈õcie≈ºki i je≈õli bot jest kulturalny to nie bƒôdzie tam zaglƒÖda≈Ç. Crawlery spamer√≥w przewa≈ºnie¬†kulturalne¬†nie sƒÖ wiƒôc tam wpadnƒÖ i siƒô zapƒôtlƒÖ zapychajƒÖc sobie bazƒô ≈õmieciami - taka baza traci warto≈õƒá dla klient√≥w spamera, wiƒôc przewa≈ºnie skutkuje to dodaniem naszej strony na spamerskƒÖ \u0026ldquo;black listƒô - nie crawlowaƒá\u0026rdquo;. Dla mnie cool üòÉ (w rzadszych przypadkach spamer mo≈ºe chcieƒá siƒô zem≈õciƒá\u0026hellip;).\nJe≈ºeli kto≈õ bƒôdzie zainteresowany to mogƒô udostƒôpniƒá skrypcik (ma postaƒá szablonu strony dla WordPress\u0026rsquo;a) - choƒá zachƒôcam do samodzielnego napisania w≈Çasnego - zawsze trudniej bƒôdzie spamerom z automatu go przeskoczyƒá (jak np. antyspam.pl). Nale≈ºy przy tym zwr√≥ciƒá uwagƒô na kilka rzeczy:\nnie zr√≥b sobie kuku - nie generuj fake emaili dla w≈Çasnych, istniejƒÖcych domen (bo mo≈ºe siƒô okazaƒá ≈ºe kt√≥ry≈õ spamer jednak spr√≥buje wys≈Çaƒá te setki tysiƒôcy b≈Çƒôdnych maili i zrobiƒá Ci DDOS\u0026rsquo;a zabijajƒÖc serwer b≈Çƒôdnie nadanymi mailami), nie r√≥b innym kuku - nie generuj fake maili w cudzych domenach (no chyba ≈ºe w domenach Canadian Pharmacy), je≈ºeli jednak nadal upierasz siƒô przy istniejƒÖcej domenie (np. by ≈õledziƒá statystyki zapyta≈Ñ\u0026hellip;) to ustaw rekord MX na hosta z IP 127.0.0.1 - jak dobrze p√≥jdzie spamer sam sobie zrobi DOS\u0026rsquo;a (spodziewa≈Çbym siƒô jednak w takiej sytuacji odwetu) :simple_smile: ≈πr√≥d≈Ço szablonu Paczkƒô z szablonem mo≈ºna pobraƒá¬†tutaj.\nPaczka zawiera plik havefun_template.php, kt√≥ry nale≈ºy umie≈õciƒá w katalogu szablonu WordPress\u0026rsquo;a. Ja wykorzysta≈Çem w generatorze wp_post z WordPress\u0026rsquo;a - zmodyfikowania zapytania dla innej bazy nie powinno byƒá zbyt trudne. Samo zapytanie nieco zoptymalizowa≈Çem przez co zwraca mniej losowe wyniki (zwraca losowy blok n wierszy z tabeli a nie ca≈Çkiem losowe elementy) co dla potrzeb tego skryptu jest ca≈Çkiem OK. Je≈ºeli pomys≈Ç z bazƒÖ siƒô nie podoba (bo np. mo≈ºe generowaƒá zbyt du≈ºe obciƒÖ≈ºenie) to proponujƒô wyszukaƒá sobie jaki≈õ generator Lorem ipsum\u0026hellip;\nMi≈Çej zabawy.\nBawiƒÖc siƒô ostatnio nowym szablonem Twenty Thirteen przygotowa≈Çem nowƒÖ wersjƒô pliku havefun_template.php dla tego szablonu.\n","permalink":"https://timor.site/2012/12/jak-dokuczac-spamerom/","summary":"Dawno, dawno temu\u0026hellip; Za g√≥rami, za lasami\u0026hellip; czyta≈Çem sobie tekst Lemat\u0026rsquo;a o dokuczaniu spamerom i pomy≈õla≈Çem ≈ºe sam te≈º tak mogƒô i nawet chcƒô wiƒôc pope≈Çni≈Çem skrypcik, kt√≥ry dla losowych s≈Ç√≥w generowa≈Ç maile. Skrypcik dzia≈Ça≈Ç z dwa lata na mojej poprzedniej stronie i nie raz zdarzy≈Ço siƒô tam jakiej≈õ mendzie zapƒôtliƒá. Jako≈õ nie mia≈Çem czasu od razu, a p√≥≈∫niej zapomnia≈Çem wrzuciƒá go na nowƒÖ stronie i tak zosta≈Ço - na pewien czas.","title":"Jak dokuczaƒá spamerom"},{"content":"Ruski serwer WWW ma przydatnƒÖ funkcjƒô serwowania wersji plik√≥w skompresowanych gzip\u0026rsquo;em - przez co mo≈ºemy plik skompresowaƒá raz i bƒôdzie on serwowany klientom obs≈ÇugujƒÖcym kompresjƒô HTTP ale ju≈º bez ka≈ºdorazowego kompresowania go. Jest to bardzo przydatne na stronach z du≈ºym ruchem gdzie mo≈ºna w ten spos√≥b zaoszczƒôdziƒá takty CPU na w≈Ça≈õciwƒÖ obs≈Çugƒô po≈ÇƒÖcze≈Ñ a nie kompresjƒô. Drugie miejsce gdzie mo≈ºe to byƒá przydatne to VPS\u0026rsquo;y i \u0026ldquo;cienkie\u0026rdquo; serwery, kt√≥re na kompresji przy wiƒôkszym¬†obciƒÖ≈ºeniu¬†spƒôdzajƒÖ zbyt du≈ºo czasu i daje siƒô to odczuƒá w dzia≈Çaniu strony. A ≈ºe obecnie standardem jest ≈Çadowanie przez stronki np. jQuery, paru plugin√≥w do niego, jQueryUI, masy CSS\u0026rsquo;√≥w, itd - to na prawdƒô jest co kompresowaƒá üòÉ\nDodatkowa zaleta tego rozwiƒÖzania jest taka, ≈ºe o ile w przypadku kompresji online raczej kompresujemy na niskich poziomach by nieco zmniejszyƒá dane a r√≥wnocze≈õnie nie zabijaƒá CPU - to w przypadku kompresji pod ten modu≈Ç mo≈ºemy raz na jaki≈õ czas \u0026ldquo;przejechaƒá\u0026rdquo; pliki maksymalnƒÖ kompresjƒÖ by by≈Çy jeszcze mniejsze (zaoszczƒôdzi to nieco pasma i r√≥wnocze≈õnie przyspieszy ≈Çadowanie strony bo jest mniej do pobrania).\nPrekompresjƒô wybranych typ√≥w plik√≥w mo≈ºna przeprowadziƒá np. tak:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; gzip skompresuje a touch zadba by timestampy obu plik√≥w by≈Çy takie same (co jest zalecanƒÖ praktykƒÖ przy wykorzystaniu tego modu≈Çu). Dwa powy≈ºsze polecenia mo≈ºna odpaliƒá praktycznie w ciemno na obecnie serwowanej stronie i nie powinny spowodowaƒá zbyt du≈ºego chaosu\u0026hellip; Trzeba pamiƒôtaƒá jedynie by zosta≈Çy uruchomione z u≈ºytkownika, kt√≥ry ma dostƒôp do zapisu w danym vho≈õcie i by serwer WWW mia≈Ç dostƒôp do tak utworzonego pliku.\nZ moich obserwacji wynika ≈ºe gdy timestamp pliku bez gz jest nowszy ni≈º wersji skompresowanej to serwowana jest ta ≈õwie≈ºsza wersja ale wtedy ju≈º z ewentualnƒÖ kompresjƒÖ online.\nMocniejsze kompresowanie plik√≥w PNG Mo≈ºe nie do ko≈Ñca w temacie tego posta ale bardzo czƒôsto gdy stosujƒô gzip_static_module to optymalizujƒô te≈º PNG\u0026rsquo;i. Pliki PNG sƒÖ kompresowane w spos√≥b bezstratny ale przewa≈ºnie programy graficzne nie zapisujƒÖ ich w najmniejszej mo≈ºliwej postaci bo trwa≈Çoby to zbyt d≈Çugo. Mo≈ºna u≈ºyƒá do tego celu narzƒôdzi takich jak: optipng lub pngcrush. A poni≈ºej odpowiedni one-liner by skompresowaƒá wszystkie PNG\u0026rsquo;i w serwisach WWW:\nfind /var/www -iname *.png -print0 |xargs -0 optipng -quiet -preserve -o7 Uwaga! Wywo≈Çanie tego skryptu mo≈ºe potrwaƒá bardzo d≈Çugo i z tego powodu raczej nie nadaje siƒô do umieszczenia w cronie. Jednak rƒôczne puszczenie po du≈ºych zmianach w serwisie lub z filtrem w find by kompresowane by≈Çy tylko nowsze pliki bƒôdzie OK.\nOptymalizowanie plik√≥w JPG/JPEG Co prawda pliki JPG/JPEG kompresujƒÖ¬†siƒô¬†stratnie ale mo≈ºne je nieco zoptymalizowaƒá np. usuwajƒÖc niepotrzebne dane z nag≈Ç√≥wk√≥w i zamieniajƒÖc na progresywne. Mo≈ºemy do tego wykorzystaƒá¬†takie polecenie:\nfind /var/www -iname \u0026#39;*.jpg\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;jpegtran -optimize -progressive -copy none -outfile \u0026#34;{}\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; ","permalink":"https://timor.site/2012/12/nginx-kompresowanie-plikow-dla-gzip_static/","summary":"Ruski serwer WWW ma przydatnƒÖ funkcjƒô serwowania wersji plik√≥w skompresowanych gzip\u0026rsquo;em - przez co mo≈ºemy plik skompresowaƒá raz i bƒôdzie on serwowany klientom obs≈ÇugujƒÖcym kompresjƒô HTTP ale ju≈º bez ka≈ºdorazowego kompresowania go. Jest to bardzo przydatne na stronach z du≈ºym ruchem gdzie mo≈ºna w ten spos√≥b zaoszczƒôdziƒá takty CPU na w≈Ça≈õciwƒÖ obs≈Çugƒô po≈ÇƒÖcze≈Ñ a nie kompresjƒô. Drugie miejsce gdzie mo≈ºe to byƒá przydatne to VPS\u0026rsquo;y i \u0026ldquo;cienkie\u0026rdquo; serwery, kt√≥re na kompresji przy wiƒôkszym¬†obciƒÖ≈ºeniu¬†spƒôdzajƒÖ zbyt du≈ºo czasu i daje siƒô to odczuƒá w dzia≈Çaniu strony.","title":"Nginx - kompresowanie plik√≥w dla gzip_static"},{"content":"Gdy ju≈º siƒô dorobi systemu Active Directory wygodnie jest wykorzystaƒá jego bazƒô u≈ºytkownik√≥w do autoryzacji w r√≥≈ºnych miejscach, np. do pewnych \u0026ldquo;tajnych i tajniejszych\u0026rdquo; stron w Apache. Najpro≈õciej mo≈ºna to zrobiƒá z wykorzystaniem LDAP.\nWarto sprawdziƒá czy i jak mo≈ºemy dostaƒá siƒô do kontroler√≥w. Gdy ju≈º mamy wszystkie potrzebne parametry konfigurujemy Apachego - na poczƒÖtek aktywujemy modu≈Çy:\na2enmod ldap a2enmod authnz_ldap Teraz mo≈ºemy edytujemy globalny plik konfiguracyjny mod_ldap\u0026rsquo;a by ustawiƒá nieco cache\u0026rsquo;y (bardzo przydatne). Warto≈õci mo≈ºna dostosowaƒá do potrzeb ale przyk≈Çadowe powinny wystarczyƒá na poczƒÖtku:\nLDAPSharedCacheSize 500000 LDAPCacheEntries 1024 LDAPCacheTTL 600 LDAPOpCacheEntries 1024 LDAPOpCacheTTL 600 \u0026lt;Location /ldap-status\u0026gt; SetHandler ldap-status Order deny,allow Deny from all # Allow from 127.0.0.1 ::1 Allow from 192.168.1.15/32 Satisfy all \u0026lt;/Location\u0026gt; Warto zwr√≥ciƒá uwagƒô na drugƒÖ czƒô≈õƒá - mo≈ºna aktywowaƒá podglƒÖd statusu mod_ldap\u0026rsquo;a co bywa przydatne na etapie test√≥w (np. by zobaczyƒá kto aktualnie jest zalogowany) - ja umo≈ºliwi≈Çem dostƒôp ze swojego zdalnego komputera. Teraz restart Apachego aby wszystko siƒô za≈Çadowa≈Ço:\ninvoke-rc.d apache2 restart ≈örodowisko mamy gotowe, mo≈ºemy skonfigurowaƒá vhosta. Poni≈ºsze opcje wrzucamy np. do klauzuli \u0026lt;Directory\u0026gt; lub \u0026lt;Location\u0026gt;:\nAuthtype basic Authname \u0026#34;Zaloguj sie kontem domenowym\u0026#34; AuthBasicProvider ldap AuthzLDAPAuthoritative Off AuthUserFile /dev/null AuthLDAPUrl \u0026#34;ldap://server1.nazwadomeny.local:389 10.0.0.32:389/DC=nazwadomeny,DC=local?sAMAccountName?sub?(objectClass=person)\u0026#34; NONE AuthLDAPBindDN \u0026#34;CN=kontodomenowe,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; AuthLDAPBindPassword \u0026#34;haslo konta domenowego\u0026#34; Require valid-user # Require ldap-dn ou=Jakies przykladowe OU Najwa≈ºniejsze parametry to:\nAuthLDAPUrl - tutaj podajemy parametry do po≈ÇƒÖczenia LDAP, w pierwszej czƒô≈õci mo≈ºemy podaƒá kilka serwer√≥w by mieƒá backup, p√≥≈∫niej korze≈Ñ od kt√≥rego bƒôdzie rozpoczynane przeszukiwanie drzewa (tutaj ca≈Ça domena), na ko≈Ñcu filtry, AuthLDAPBindDN - konto kt√≥rym autoryzujemy siƒô do kontrolera by m√≥c przeszukiwaƒá na nim obiekty (podane jako ≈õcie≈ºka LDAP), AuthLDAPBindPassword - has≈Ço do powy≈ºszego konta, Require ldap-dn - mo≈ºna nie tylko sprawdzaƒá czy u≈ºytkownik jest prawid≈Çowy ale r√≥wnie≈º czy jest w pewnym OU, grupie itd\u0026hellip; Teoretycznie wszystko jest poprawnie ale za cholerƒô nie chcia≈Ço dzia≈Çaƒá dopiero dodanie do /etc/ldap/ldap.conf opcji: REFERRALS off sprawi≈Ço ≈ºe autoryzacja dzia≈Ça poprawnie - oneliner by to uzyskaƒá:\necho \u0026#34;REFERRALS off\u0026#34; \u0026gt;\u0026gt; /etc/ldap/ldap.conf Ja mam Apachego 2.2 i jest to jedyna opcja by uzyskaƒá ten efekt. W wersji od 2.4 jest dodatkowa opcja¬†LDAPReferrals, kt√≥ra pozwala na zmianƒô tego zachowania wprost z konfiguracji Apachego.\n","permalink":"https://timor.site/2012/12/apache-mod_authnz_ldap-z-active-directory/","summary":"Gdy ju≈º siƒô dorobi systemu Active Directory wygodnie jest wykorzystaƒá jego bazƒô u≈ºytkownik√≥w do autoryzacji w r√≥≈ºnych miejscach, np. do pewnych \u0026ldquo;tajnych i tajniejszych\u0026rdquo; stron w Apache. Najpro≈õciej mo≈ºna to zrobiƒá z wykorzystaniem LDAP.\nWarto sprawdziƒá czy i jak mo≈ºemy dostaƒá siƒô do kontroler√≥w. Gdy ju≈º mamy wszystkie potrzebne parametry konfigurujemy Apachego - na poczƒÖtek aktywujemy modu≈Çy:\na2enmod ldap a2enmod authnz_ldap Teraz mo≈ºemy edytujemy globalny plik konfiguracyjny mod_ldap\u0026rsquo;a by ustawiƒá nieco cache\u0026rsquo;y (bardzo przydatne).","title":"Apache: mod_authnz_ldap z Active Directory"},{"content":"Chcia≈Çem wys≈Çaƒá z Python\u0026rsquo;a maila z krzakami tab by ≈Çadnie siƒô wy≈õwietla≈Çy i okaza≈Ço siƒô to ca≈Çkiem nietrywialne.\nNa szczƒô≈õcie googiel podpowiedzia≈Ç mi doskona≈Çego gotowca, kt√≥rego zamierzam zapisaƒá by mi nie zginƒÖ≈Ç:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import smtplib from email.mime.text import MIMEText from email.Header import Header from email.Utils import parseaddr, formataddr def send_email(sender, recipient, subject, body): \u0026#34;\u0026#34;\u0026#34;Send an email. All arguments should be Unicode strings (plain ASCII works as well). Only the real name part of sender and recipient addresses may contain non-ASCII characters. The email will be properly MIME encoded and delivered though SMTP to localhost port 25. This is easy to change if you want something different. The charset of the email will be the first one out of US-ASCII, ISO-8859-1 and UTF-8 that can represent all the characters occurring in the email. \u0026#34;\u0026#34;\u0026#34; # Header class is smart enough to try US-ASCII, then the charset we # provide, then fall back to UTF-8. header_charset = \u0026#39;ISO-8859-2\u0026#39; # We must choose the body charset manually for body_charset in \u0026#39;US-ASCII\u0026#39;, \u0026#39;UTF-8\u0026#39;, \u0026#39;ISO-8859-2\u0026#39;: try: body.encode(body_charset) except UnicodeError: pass else: break # Split real name (which is optional) and email address parts sender_name, sender_addr = parseaddr(sender) recipient_name, recipient_addr = parseaddr(recipient) # We must always pass Unicode strings to Header, otherwise it will # use RFC 2047 encoding even on plain ASCII strings. sender_name = str(Header(unicode(sender_name), header_charset)) recipient_name = str(Header(unicode(recipient_name), header_charset)) # Make sure email addresses do not contain non-ASCII characters sender_addr = sender_addr.encode(\u0026#39;ascii\u0026#39;) recipient_addr = recipient_addr.encode(\u0026#39;ascii\u0026#39;) # Create the message (\u0026#39;plain\u0026#39; stands for Content-Type: text/plain) msg = MIMEText(body.encode(body_charset), \u0026#39;plain\u0026#39;, body_charset) msg[\u0026#39;From\u0026#39;] = formataddr((sender_name, sender_addr)) msg[\u0026#39;To\u0026#39;] = formataddr((recipient_name, recipient_addr)) msg[\u0026#39;Subject\u0026#39;] = Header(unicode(subject), header_charset) # Send the message via SMTP to localhost:25 smtp = smtplib.SMTP(\u0026#34;localhost\u0026#34;) smtp.sendmail(sender, recipient, msg.as_string()) smtp.quit() Wykorzystanie:\nsend_email( u\u0026#34;GƒÖska \u0026lt;gaska@test.pl\u0026gt;\u0026#34;, u\u0026#34;Tch√≥rz \u0026lt;tchorz@test2.pl\u0026gt;\u0026#34;, u\u0026#34;Grzegrz√≥≈Çkƒô testujƒÖc..\u0026#34;, u\u0026#34;ƒÖ≈õ≈Ç√≥≈ß itd...\u0026#34; ) ≈πr√≥d≈Ço http://mg.pov.lt/blog/unicode-emails-in-python.html\n","permalink":"https://timor.site/2012/12/python-wysylanie-maili-w-unicode/","summary":"Chcia≈Çem wys≈Çaƒá z Python\u0026rsquo;a maila z krzakami tab by ≈Çadnie siƒô wy≈õwietla≈Çy i okaza≈Ço siƒô to ca≈Çkiem nietrywialne.\nNa szczƒô≈õcie googiel podpowiedzia≈Ç mi doskona≈Çego gotowca, kt√≥rego zamierzam zapisaƒá by mi nie zginƒÖ≈Ç:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import smtplib from email.mime.text import MIMEText from email.Header import Header from email.Utils import parseaddr, formataddr def send_email(sender, recipient, subject, body): \u0026#34;\u0026#34;\u0026#34;Send an email. All arguments should be Unicode strings (plain ASCII works as well).","title":"Python - wysy≈Çanie maili w unicode"},{"content":"Mo≈ºna lubieƒá AD, mo≈ºna go nie lubieƒá\u0026hellip; Ale jak ju≈º siƒô ma to warto czasem zintegrowaƒá go z tym\u0026hellip; i tamtym\u0026hellip; Od strony Linuksa najwygodniej mo≈ºna to osiƒÖgnƒÖƒá przez LDAP. A ≈ºeby to dobrze zrobiƒá trzeba najpierw przetestowaƒá czy aby wszystko dzia≈Ça jak by≈õmy sobie tego ≈ºyczyli. I tutaj bardzo przydatne jest narzƒôdzie ldapsearch.\nDo odpytywania LDAP\u0026rsquo;a potrzebujemy jeden pakiecik, kt√≥ry zawiera kilka narzƒôdzi do jego obs≈Çugi:\napt-get install ldap-utils Teraz mo≈ºemy pr√≥bowaƒá przeszukiwaƒá katalog np. tak:\nldapsearch -L -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 389 -W Polecenie to odpyta kontroler o adresie¬†kontroler.nazwadomeny.local¬†(oczywi≈õcie mo≈ºemy u≈ºyƒá te≈º adresu IP) o wszystkie elementy w domenie. Port 389 na kontrolerze domeny musi byƒá otwarty na zaporze - mo≈ºna te≈º wykorzystaƒá 3268 (o ile jest otwarty).\nParametr -b okre≈õla poczƒÖtkowƒÖ ga≈ÇƒÖ≈∫ wyszukiwania - mo≈ºemy tu dodaƒá konkretne OU itd.. by zmniejszyƒá liczbƒô element√≥w.\n-D to u≈ºytkownik na kt√≥rego siƒô logujemy by uzyskaƒá dostƒôp do katalogu.\n-W zapyta nas o has≈Ço dla tego u≈ºytkownika.\nOczywi≈õcie mo≈ºemy nie potrzebowaƒá wszystkich obiekt√≥w z katalogu a tylko np. loginy kont u≈ºytkownik√≥w (ale ju≈º nie kont maszyn) - do tego celu mo≈ºemy u≈ºyƒá filtr√≥w, np. tak:\nldapsearch -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 3268 -w \u0026#39;haslo czystym tekstem\u0026#39; \u0026#39;(\u0026amp;(objectClass=person)(!(objectClass=computer)))\u0026#39; sAMAccountName No i dostajemy listƒô login√≥w ≈ºywych¬†u≈ºytkownik√≥w. U≈ºycie opcji¬†-w¬†mo≈ºe wydawaƒá siƒô nieco kontrowersyjne ale z drugiej strony jest bardzo wygodne gdy chcemy dane wyj≈õciowe wykorzystaƒá w skrypcie. Przyk≈Çadowo mo≈ºemy wynik tego polecenie pu≈õciƒá przez awk by otrzymaƒá same loginy i dodatkowo wszystkie ma≈Çymi/du≈ºymi literami:\npoprzednie_polecenie | awk \u0026#39;/sAMAccountName/ {print tolower($2);}\u0026#39; W podobny spos√≥b mo≈ºemy wyciƒÖgnƒÖƒá wszystkie emaile os√≥b z pewnej grupy itd, itp\u0026hellip;.\nTutaj mo≈ºna znale≈∫ƒá podstawowe info o regu≈Çach tworzenia filtr√≥w: http://www.ldapexplorer.com/en/manual/109010000-ldap-filter-syntax.htm\n","permalink":"https://timor.site/2012/12/ldapsearch-w-active-directory/","summary":"Mo≈ºna lubieƒá AD, mo≈ºna go nie lubieƒá\u0026hellip; Ale jak ju≈º siƒô ma to warto czasem zintegrowaƒá go z tym\u0026hellip; i tamtym\u0026hellip; Od strony Linuksa najwygodniej mo≈ºna to osiƒÖgnƒÖƒá przez LDAP. A ≈ºeby to dobrze zrobiƒá trzeba najpierw przetestowaƒá czy aby wszystko dzia≈Ça jak by≈õmy sobie tego ≈ºyczyli. I tutaj bardzo przydatne jest narzƒôdzie ldapsearch.\nDo odpytywania LDAP\u0026rsquo;a potrzebujemy jeden pakiecik, kt√≥ry zawiera kilka narzƒôdzi do jego obs≈Çugi:\napt-get install ldap-utils Teraz mo≈ºemy pr√≥bowaƒá przeszukiwaƒá katalog np.","title":"ldapsearch w Active Directory"},{"content":"Sk≈Çadniki 1 paczka bezsmakowego kleiku ry≈ºowego (190~170 g), 2 jajka, 1 kostka miƒôkkiego mas≈Ça (250 g), 1 p≈Çaska ≈Çy≈ºeczka proszku do pieczenia, p√≥≈Ç szklanki cukru, cukier waniliowy (16 g) (niekoniecznie), 8 czubatych ≈Çy≈ºek wi√≥rk√≥w kokosowych, d≈ºem do wype≈Çnienia ciastek (opcjonalnie - ja lubiƒô je r√≥wnie≈º bez d≈ºemu). Spos√≥b przygotowania Wszystkie sk≈Çadniki po≈ÇƒÖczyƒá razem i wyrobiƒá (najlepiej siƒô ugniata rƒôkƒÖ). Formowaƒá kulki wielko≈õci ma≈Çego orzecha w≈Çoskiego, uk≈Çadaƒá na blaszce wy≈Ço≈ºonej papierem do pieczenia w niewielkich odleg≈Ço≈õciach (trochƒô urosnƒÖ). Je≈ºeli planujemy opcjƒô z d≈ºemem to w ka≈ºdej kulce nale≈ºy zrobiƒá wg≈Çƒôbienie - ja lekko przyciska≈Çem palcem.\nPiec w temperaturze 180¬∫C przez 15~20 minut, a≈º zrobiƒÖ siƒô z≈Çociste. Po ostudzeniu wg≈Çƒôbienia nape≈Çniƒá d≈ºemem.\n≈πr√≥d≈Ço (moja wersja jest nieco przerobiona): http://www.mojewypieki.com/przepis/ciasteczka-z-kleiku-ryzowego\n","permalink":"https://timor.site/2012/11/ciastka-z-kleiku-ryzowego/","summary":"Sk≈Çadniki 1 paczka bezsmakowego kleiku ry≈ºowego (190~170 g), 2 jajka, 1 kostka miƒôkkiego mas≈Ça (250 g), 1 p≈Çaska ≈Çy≈ºeczka proszku do pieczenia, p√≥≈Ç szklanki cukru, cukier waniliowy (16 g) (niekoniecznie), 8 czubatych ≈Çy≈ºek wi√≥rk√≥w kokosowych, d≈ºem do wype≈Çnienia ciastek (opcjonalnie - ja lubiƒô je r√≥wnie≈º bez d≈ºemu). Spos√≥b przygotowania Wszystkie sk≈Çadniki po≈ÇƒÖczyƒá razem i wyrobiƒá (najlepiej siƒô ugniata rƒôkƒÖ). Formowaƒá kulki wielko≈õci ma≈Çego orzecha w≈Çoskiego, uk≈Çadaƒá na blaszce wy≈Ço≈ºonej papierem do pieczenia w niewielkich odleg≈Ço≈õciach (trochƒô urosnƒÖ).","title":"Ciastka z kleiku ry≈ºowego"},{"content":"Sk≈Çadniki 2 szklanki miƒÖ≈ºszu dyni (startego lub zmiksowanego), 2 szklanki mƒÖki, 0,75 szklanki cukru, 3 jajka, 5 ≈Çy≈ºeczek cynamonu, 16 g cukru wanilinowego, 0,75 szklanki oleju, ≈Çy≈ºeczka sody oczyszczonej, gar≈õƒá orzech√≥w w≈Çoskich. Spos√≥b przygotowania Ubijamy bia≈Çka z cukrem. Do ubitej piany dodajemy ≈º√≥≈Çtka i kolejne sk≈Çadniki: mƒÖkƒô, olej, cukier wanilinowy, sodƒô i cynamon. Ca≈Ço≈õƒá miksujemy jeszcze przez chwilƒô, a nastƒôpnie dodajemy posiekany lub starty i odsƒÖczony (mo≈ºna np. mocno ≈õcisnƒÖƒá w d≈Çoniach)¬†miƒÖ≈ºsz dyni¬†oraz drobno pokrojone¬†orzechy w≈Çoskie i jeszcze chwilƒô mieszamy.\nMasƒô przek≈Çadamy do wysmarowanej mas≈Çem i opr√≥szonej mƒÖkƒÖ formy - na du≈ºej blaszce ciasto wyjdzie do≈õƒá cienkie, wiƒôc mo≈ºna piec np. w foremce keksowej. Blaszkƒô z ciastem wk≈Çadamy do nagrzanego do 180¬∞C piekarnika i w tej temperaturze¬†pieczemy przez ok. 40-50 minut. Mo≈ºna podawaƒá posypane cukrem pudrem lub cynamonem.\n","permalink":"https://timor.site/2012/11/ciasto-z-dynia/","summary":"Sk≈Çadniki 2 szklanki miƒÖ≈ºszu dyni (startego lub zmiksowanego), 2 szklanki mƒÖki, 0,75 szklanki cukru, 3 jajka, 5 ≈Çy≈ºeczek cynamonu, 16 g cukru wanilinowego, 0,75 szklanki oleju, ≈Çy≈ºeczka sody oczyszczonej, gar≈õƒá orzech√≥w w≈Çoskich. Spos√≥b przygotowania Ubijamy bia≈Çka z cukrem. Do ubitej piany dodajemy ≈º√≥≈Çtka i kolejne sk≈Çadniki: mƒÖkƒô, olej, cukier wanilinowy, sodƒô i cynamon. Ca≈Ço≈õƒá miksujemy jeszcze przez chwilƒô, a nastƒôpnie dodajemy posiekany lub starty i odsƒÖczony (mo≈ºna np. mocno ≈õcisnƒÖƒá w d≈Çoniach)¬†miƒÖ≈ºsz dyni¬†oraz drobno pokrojone¬†orzechy w≈Çoskie i jeszcze chwilƒô mieszamy.","title":"Ciasto z dyniƒÖ"},{"content":"CouchDB databases on version 0.11.x swell very fast. They should be compacted daily for best performance and space usage. Here is my script that could be run in cron and will compact all databases:\n#!/bin/bash IP=\u0026#34;10.0.0.121\u0026#34; DBS=`curl -sS -X GET http://$IP:5984/_all_dbs | sed -r \u0026#34;s/([,\\\u0026#34;[])|(\\])+/ /g\u0026#34;` for d in $DBS; do curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://$IP:5984/$d/_compact done More informations about compacting could be found here (also for version 1.2.x).\n","permalink":"https://timor.site/2012/11/automatically-compact-couchdb-databases-in-0-11-x/","summary":"CouchDB databases on version 0.11.x swell very fast. They should be compacted daily for best performance and space usage. Here is my script that could be run in cron and will compact all databases:\n#!/bin/bash IP=\u0026#34;10.0.0.121\u0026#34; DBS=`curl -sS -X GET http://$IP:5984/_all_dbs | sed -r \u0026#34;s/([,\\\u0026#34;[])|(\\])+/ /g\u0026#34;` for d in $DBS; do curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://$IP:5984/$d/_compact done More informations about compacting could be found here (also for version 1.","title":"Automatically compact CouchDB databases in version 0.11.x"},{"content":"Po zakupie nowych dysk√≥w zamierzam utworzyƒá zdegradowanƒÖ macierz RAID5 z dw√≥ch dysk√≥w (na trzecim na razie znajdujƒÖ siƒô dane), potem utworzyƒá wolumen LVM, sformatowaƒá go, przekopiowaƒá dane z pojedynczego dysku na macierz i do≈ÇƒÖczyƒá trzeci dysk do macierzy odbudowujƒÖc¬†parzysto≈õƒá. Zadanie bƒôdzie o tyle ciekawe ≈ºe dysk ma 4KB sektory i trzeb bƒôdzie dbaƒá o wyr√≥wnanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyr√≥wnanie do chunk\u0026rsquo;a z macierzy.\nPrawid≈Çowe wyr√≥wnanie partycji KupujƒÖc nowy dysk (o pojemno≈õci od 500GB w g√≥rƒô), mamy spore szanse ≈ºe trafimy na sztukƒô, kt√≥ra wykorzystuje 4KB sektory do alokacji danych. Poniewa≈º statystyczny rozmiar przechowywanych plik√≥w ro≈õnie i nawet proste zdjƒôcie ma powy≈ºej 1MB to wykorzystanie blok√≥w o tym rozmiarze wiƒôkszym ni≈º 512 bajt√≥w jest jak najbardziej uzasadnione - zresztƒÖ wiƒôkszo≈õƒá system√≥w plik√≥w i tak wykorzystuje bloki 4~8KB. Jest tylko jedno ALE: je≈ºeli nie uwzglƒôdnimy tego podczas partycjonowania dysku to sektory 4KB systemu plik√≥w zamiast znajdowaƒá siƒô w r√≥wno w odpowiadajƒÖcych im fizycznych 4KB sektorach dysku - mogƒÖ zachodziƒá na 2 sektory fizyczne - w takiej sytuacji ka≈ºde odwo≈Çanie to takiego sektora w systemie plik√≥w wymaga odczytania/zapisanie dw√≥ch sektor√≥w fizycznych. Co prawda w dyskach stosuje siƒô mechanizmy, kt√≥re powinny zoptymalizowaƒá takie sytuacje ale jak potwierdzajƒÖ benchmarki ≈∫le wyr√≥wnane partycja mogƒÖ znacznie obni≈ºyƒá wydajno≈õƒá dysku. A jeszcze zabawniej jest je≈õli kupimy dysk SSD bo w nich bardzo czƒôsto fizyczne bloki sƒÖ 128~512KB i ≈ºeby by≈Ço zabawniej to bardzo czƒôsto dyski SSD deklarujƒÖ (choƒáby przez SMART\u0026rsquo;a) ≈ºe majƒÖ bloki 512B - SIC!\nJe≈õli mamy fart i nasz dysk raportuje rozmiar fizycznego sektora to mo≈ºemy to sprawdziƒá tak:\ncat /sys/block/sda/queue/physical_block_size U mnie to polecenie zwr√≥ci≈Ço 4096 czyli 4KB co jest zgodne z deklaracjƒÖ producenta.\nDobre podej≈õcie do tematu wyr√≥wnania partycji zaproponowa≈Ç Teo Tso czyli wybranie takich parametr√≥w g≈Çowic/sektor√≥w na ≈õcie≈ºce by fdisk automatycznie wyr√≥wnywa≈Ç partycje do oczekiwanej przez nas wielko≈õci bloku. Teo proponowa≈Ç u≈ºycie 224 g≈Çowic i 56 sektor√≥w - co da wyr√≥wnanie do 128KB dla wszystkich partycji z wyjƒÖtkiem pierwszej (pierwsza bƒôdzie wyr√≥wnana do 4KB o ile nie wymusimy startu z 256 sektora). Je≈ºeli mamy dysk z blokami 4KB lub pierwszƒÖ partycjƒô zamierzamy wykorzystaƒá jako np. /boot (gdzie wydajno≈õƒá nie ma a≈º takiego du≈ºego znaczenia) to jest to ok. Ale je≈õli kompatybilno≈õƒá z DOS\u0026rsquo;em mamy w powa≈ºaniu to mo≈ºemy w fdisku utworzyƒá pierwszƒÖ partycjƒô wyr√≥wnanƒÖ do 128KB lub 1MB.\nPrzewa≈ºnie wola≈Çem¬†cfdiska od fdiska¬†(bo po co siƒô mƒôczyƒá z topornym interfejsem) ale nie uda≈Ço mi siƒô skuba≈Ñca zmusiƒá by tworzy≈Ç pierwszƒÖ partycjƒô w spos√≥b nie kompatybilny z DOS\u0026rsquo;em. fdisk pomimo toporno≈õci po podaniu liczby g≈Çowic i sektor√≥w w ≈õcie≈ºce podpowiedzia≈Ç mi poprawne wyr√≥wnanie partycji (a gdyby≈õmy posiadali starszƒÖ wersjƒô, kt√≥ra nie jest tak sprytna to przynajmniej mo≈ºemy podaƒá rƒôcznie od kt√≥rego sektora ma zaczynaƒá siƒô partycja).\nWyr√≥wnanie do 128KB fdisk -u -H 224 -S 56 /dev/sdX Opcja -u zmienia domy≈õlnƒÖ jednostkƒô na sektory (mamy wtedy mniejsze liczby, kt√≥re ≈Çatwiej siƒô przelicza). Dla wyr√≥wnania do 128KB pierwsza partycja powinna siƒô zaczynaƒá na 256 sektorze. Do wyr√≥wnania do 4KB wystarczy zaczƒÖƒá na 56 sektorze (wystarczajƒÖce przy czƒô≈õci nowszych dysk√≥w twardych).\nWyr√≥wnanie do 1MB Je≈õli jednak dysponujemy dyskiem SSD z ciƒô≈ºko powiedzieƒá jak du≈ºym blokiem to lepiej wykorzystaƒá wyr√≥wnanie do 1MB - zmarnujemy trochƒô wiƒôcej miejsca (tych parƒô MB jako≈õ prze≈ºyjemy) ale w tym rozmiarze na pewno zmie≈õci siƒô ka≈ºdy sektor (a mo≈ºe nawet Erase Block, kt√≥ry obecnie przewa≈ºnie ma 512KB choƒá zdarzajƒÖ siƒô sztuki z 4MB). Mo≈ºna to osiƒÖgnƒÖƒá z ustawieniem 64 g≈Çowic i 32 sektor√≥w na ≈õcie≈ºkƒô, robimy to tak:\nfdisk -u -H 64 -S 32 /dev/sdX Pierwsza partycja powinna siƒô zaczynaƒá na 2048 sektorze dla wyr√≥wnania do 1MB lub 8192 sektorze je≈õli chcemy zaczƒÖƒá od 4MB.\nPo odpaleniu fdiska z tymi parametrami wybieramy opcjƒô n i lecimy dalej zgodnie z podpowiedziami, np.:\nfdisk -u -H 224 -S 56 /dev/sdc Polecenie (m wy≈õwietla pomoc): \u0026lt;strong\u0026gt;m\u0026lt;/strong\u0026gt; Polecenie a zmiana flagi rozruchu b modyfikacja etykiety dysku BSD c zmiana flagi kompatybilno≈õci z DOS-em d usuniƒôcie partycji l wypisanie znanych typ√≥w partycji m wy≈õwietlenie tego menu \u0026lt;strong\u0026gt;n dodanie nowej partycji\u0026lt;/strong\u0026gt; o utworzenie nowej, pustej DOS-owej tablicy partycji p wypisanie tablicy partycji q zako≈Ñczenie bez zapisywania zmian s utworzenie nowej, pustej etykiety dysku Suna t zmiana ID systemu partycji u zmiana jednostek wy≈õwietlania/wprowadzania v weryfikacja tablicy partycji w zapis tablicy partycji na dysk i zako≈Ñczenie x dodatkowe funkcje (tylko dla ekspert√≥w) Polecenie (m wy≈õwietla pomoc): \u0026lt;strong\u0026gt;n\u0026lt;/strong\u0026gt; Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt;): Using default response p Numer partycji (1-4, domy≈õlnie \u0026lt;strong\u0026gt;1\u0026lt;/strong\u0026gt;): Przyjƒôto warto≈õƒá domy≈õlnƒÖ 1 Pierwszy sektor (2048-15240575, domy≈õlnie \u0026lt;strong\u0026gt;2048\u0026lt;/strong\u0026gt;): Przyjƒôto warto≈õƒá domy≈õlnƒÖ 2048 Ostatni sektor, +sektor√≥w lub +rozmiar{K,M,G} (2048-15240575, domy≈õlnie 15240575): \u0026lt;strong\u0026gt;+100M\u0026lt;/strong\u0026gt; Polecenie (m wy≈õwietla pomoc): \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt; Dysk /dev/sdc: 7803 MB, bajt√≥w: 7803174912 g≈Çowic: 224, sektor√≥w/≈õcie≈ºkƒô: 56, cylindr√≥w: 1214, w sumie sektor√≥w: 15240576 Jednostka = sektor√≥w, czyli 1 * 512 = 512 bajt√≥w Rozmiar sektora (logiczny/fizyczny) w bajtach: 512 / 512 Rozmiar we/wy (minimalny/optymalny) w bajtach: 512 / 512 Identyfikator dysku: 0xc3072e18 UrzƒÖdzenie Rozruch PoczƒÖtek Koniec Blok√≥w ID System /dev/sdc1 2048 206847 102400 83 Linux Polecenie (m wy≈õwietla pomoc): \u0026lt;strong\u0026gt;w\u0026lt;/strong\u0026gt; Tablica partycji zosta≈Ça zmodyfikowana! Wywo≈Çywanie ioctl() w celu ponownego odczytu tablicy partycji. UWAGA: ponowny odczyt tablicy partycji zako≈Ñczy≈Ç siƒô b≈Çƒôdem 16: UrzƒÖdzenie lub zasoby zajƒôte. JƒÖdro nadal u≈ºywa starej tablicy. Nowa tablica bƒôdzie u≈ºywana po nastƒôpnym restarcie systemu albo po uruchomieniu partprobe(8) lub kpartx(8) Synchronizacja dysk√≥w. P.S. Po co w og√≥le partycjonowaƒá - mo≈ºna u≈ºyƒá ca≈Çych urzƒÖdze≈Ñ Co prawda w przypadku gdy zamierzam ca≈Çe dyski po≈õwiƒôciƒá na RAID\u0026rsquo;a m√≥g≈Çbym ich nie partycjonowaƒá tylko u≈ºyƒá ca≈Çych urzƒÖdze≈Ñ i zadbaƒá tylko by mdadm poprawnie siƒô wyr√≥wna≈Ç (co zresztƒÖ robi automatycznie do 4KB), ale obawia≈Çem siƒô jak wzglƒôdem takich dysk√≥w zachowajƒÖ siƒô inne systemy kt√≥re mam zainstalowane na komputerze - nie chcia≈Çbym aby przypadkiem uzna≈Çy ≈ºe to jaki≈õ uszkodzony dysk po czym zainicjowa≈Çy tablicƒô partycji\u0026hellip; Mo≈ºe to nieuzasadniona fobia ale wiem ≈ºe wzglƒôdem partycji RAID Autodetect nic takiego mi siƒô nie przytrafi üòÉ\nTworzenie macierzy RAID Jak ju≈º utworzymy po≈ºƒÖdane partycje na pierwszym dysku to musimy je powieliƒá na wszystkich pozosta≈Çych dyskach, kt√≥re zamierzamy w≈ÇƒÖczyƒá do macierzy - najpro≈õciej zrobiƒá to sfdisk\u0026rsquo;iem:\nsfdisk -d /dev/sdX | sfdisk /dev/sdY Przy czym dysk sdX to ≈∫r√≥d≈Çowy dysk z gotowymi partycjami, a dysk sdY to ka≈ºdy na kt√≥rym chcemy odtworzyƒá partycje. Mo≈ºna by zrobiƒá na to ≈ÇadnƒÖ pƒôtelkƒô je≈õli mamy wiƒôcej tych dysk√≥w ale celowo tego nie zrobiƒô bo jak znam ≈ºycie to kiedy≈õ kto≈õ to skopiuje ze znakiem nowego wiersza i wrzuci do konsoli\u0026hellip; üòÄ\nTeraz tworzƒô zdegradowanƒÖ (z dw√≥ch dysk√≥w) macierz RAID5\u0026hellip; Ale na dobrƒÖ sprawƒô bezpieczniej by≈Çoby utworzyƒá macierz RAID1 z dw√≥ch dysk√≥w (o ile miejsca by≈Çoby wystarczajƒÖco na przeniesienie danych) a po przeniesieniu danych na macierz dodanie trzeciego dysku i powiƒôkszenie macierzy z reshapingiem do RAID5 - postanowi≈Çem pominƒÖƒá takie rozwiƒÖzanie bo nie ba≈Çem siƒô utraty danych, mia≈Çem dok≈ÇadnƒÖ kopiƒô na innej macierzy üòÉ\nWiƒôc tworzymy macierz:\nmdadm --create /dev/md0 --level=5 --raid-devices=2 --chunk=512k /dev/sdX1 /dev/sdY1 Na dobrƒÖ sprawƒô z powy≈ºszych opcji tylko chunk nadaje siƒô do \u0026ldquo;dopasowania\u0026rdquo; - ja mam zamiar utworzyƒá macierz z do≈õƒá du≈ºych zasob√≥w (3x2TB) i przechowywaƒá na niej raczej du≈ºe pliki wiƒôc 512KB wydaje mi siƒô dobrƒÖ warto≈õciƒÖ. Gdyby≈õmy jednak potrzebowali wiƒôkszej liczby I/O dla ma≈Çych pliczk√≥w to mniejsza warto≈õƒá mo≈ºe byƒá lepsza. Warto zrobiƒá kilka benchmark√≥w dla r√≥≈ºnych warto≈õci chunk\u0026rsquo;a i dopasowaƒá do przewidywanego przez nas obciƒÖ≈ºenia.\nAby macierz by≈Ça widoczne ju≈º w czasie startu systemu nale≈ºy dodatkowo wykonaƒá polecenie:\nmdadm --detail --scan \u0026gt;\u0026gt; /etc/mdadm/mdadm.conf Warto te≈º w pliku /etc/mdadm/mdadm.conf odkomentowaƒá i wpisaƒá jakie≈õ sensowne warto≈õci dla HOMEHOST, MAILADDR.\nI teraz mo≈ºemy odbudowaƒá obraz initrd:\nupdate-initramfs -u Optymalizacja macierzy RAID5 Sam proces tworzenia macierzy mo≈ºe zajƒÖƒá kilka/kilkana≈õcie godzin - dlatego warto mu pom√≥c kilkoma zmianami.\nspeed_limit_xxx Na pierwszy rzut - domy≈õlne warto≈õci dla minimalnej i maksymalnej prƒôdko≈õci budowania/regenerowania/odtwarzania macierzy RAID5, mo≈ºna je sprawdziƒá:\ncat /proc/sys/dev/raid/speed_limit_max 200000 cat /proc/sys/dev/raid/speed_limit_min 1000 Domy≈õlnie jest to minimum 1MB/s i maksimum 200MB/s. PodbijajƒÖc minimalnƒÖ prƒôdko≈õƒá do 10~50MB/s mo≈ºna kosztem wiƒôkszego obciƒÖ≈ºenia systemu zmusiƒá macierz by odbudowywa≈Ça siƒô szybciej. Osobi≈õcie uwa≈ºam tƒÖ optymalizacjƒô za ma≈Ço znaczƒÖcƒÖ bo ca≈Çy mechanizm do≈õƒá elastycznie reaguje na obciƒÖ≈ºenie systemu i je≈õli nic nie robimy to prƒôdko≈õci odbudowy sƒÖ do≈õƒá wysokie. Ale mo≈ºna to zrobiƒá tak:\necho 50000 \u0026gt; /proc/sys/dev/raid/speed_limit_min lub tak:\nsysctl -w dev.raid.speed_limit_min=50000 stripe_cache_size Ta optymalizacja pomog≈Ça mi du≈ºo - ok. 30% wzrost wydajno≈õci macierzy (r√≥wnie≈º w czasie odbudowy parzysto≈õci). Mo≈ºemy sprawdziƒá warto≈õƒá tego parametru, np. tak:\ncat /sys/block/md0/md/stripe_cache_size Domy≈õlnie jest to warto≈õƒá 128 - bida z nƒôdzƒÖ, u mnie ustawienie na 32768 (maksymalna warto≈õƒá tego parametru) da≈Ço najwiƒôkszy wzrost wydajno≈õci - ale ju≈º 8192 znacznie poprawi≈Ço wydajno≈õƒá.\nfor i in 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;Testowanie $i\u0026#34; echo $i \u0026gt; /sys/block/md0/md/stripe_cache_size sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=/dev/zero of=file bs=1M count=10000 done Wy≈ÇƒÖczenie NCQ dla wszystkich dysk√≥w w macierzy Wyda≈Ço mi siƒô to nieco kontrowersyjne bo NCQ powinno pomagaƒá przy losowych odczytach/zapisach - ale zapu≈õci≈Çem test bonnie++ i okaza≈Ço siƒô ≈ºe z w≈ÇƒÖczonym NCQ czasy dostƒôpu dla niekt√≥rych obciƒÖ≈ºe≈Ñ rosnƒÖ nawet dziesiƒôciokrotnie! Warto wiƒôc sprawdziƒá tƒÖ opcjƒô pod przewidywanym przez nas scenariuszem obciƒÖ≈ºenia.\nNCQ dla poszczeg√≥lnych dysk√≥w mo≈ºna wy≈ÇƒÖczyƒá np. tak:\nfor d in sdb sdc sdd do echo 1 \u0026gt; /sys/block/$d/device/queue_depth done Wy≈ÇƒÖczenie cache dyskowych Wy≈ÇƒÖczenie wbudowanej w dyski pamiƒôci cache akurat nie zwiƒôksza wydajno≈õci ale w przypadku awarii zasilania (lub innej gwa≈Çtownej awarii systemu) zwiƒôksza szanse macierzy na prze≈ºycie takiego incydentu. Obecnie wiƒôkszo≈õƒá system√≥w plik√≥w korzysta z op√≥≈∫nienia zapisu by bardziej optymalnie zapisaƒá dane na dysku - dlatego gdy zapisujemy dany to najpierw trafiajƒÖ one do cache systemowego. Dopiero po sync\u0026rsquo;u sƒÖ przesy≈Çane do cache dyskowego skƒÖd dopiero po pewnym czasie trafiajƒÖ na dysk. Wy≈ÇƒÖczenie pamiƒôci cache na dyskach \u0026ldquo;usuwa\u0026rdquo; nam to drugie op√≥≈∫nienie.\nhdparm -W0 /dev/sd* Zmiana parametr√≥w odczytu z wyprzedzeniem Bardzo wa≈ºnym parametrem majƒÖcym wp≈Çyw na wydajno≈õƒá macierzy jest odpowiednie ustawienie odczytu z wyprzedzeniem. Obecnie ustawionƒÖ warto≈õƒá mo≈ºemy sprawdziƒá tak (warto≈õƒá wyra≈ºona jest w 512 bajtowych sektorach):\nblockdev --getra /dev/md0 U mnie domy≈õlnie by≈Ço 4096 (a na innym starszym systemie tylko 1536) to mo≈ºe byƒá zbyt ma≈Ço dla konfiguracji RAID. Wiƒôksze warto≈õci mo≈ºna ustawiƒá np. tak:\nblockdev --setra 65536 /dev/md0 A tak mo≈ºna wykonaƒá sprawdzanie, kt√≥ra warto≈õƒá bƒôdzie dla nas najbardziej optymalna:\ndd if=/dev/zero of=file bs=1M count=10000 for i in 1536 4096 8192 16384 32768 65536 131072 262144 524288; do echo \u0026#34;Testowanie $i\u0026#34; blockdev --setra $i /dev/md0 sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=file of=/dev/null bs=1M done Tworzenie zasobu LVM MajƒÖc ju≈º macierze tworzƒô na niej volumen LVM - najpierw przygotowanie zasobu:\npvcreate /dev/md0 Je≈ºeli w /etc/lvm/lvm.conf mamy ustawione opcje:\nmd_component_detection = 1 md_chunk_alignment = 1 data_alignment_detection = 1 To LVM powinien automatycznie wykryƒá rozmiar chunk\u0026rsquo;a z RAID\u0026rsquo;a i dostosowaƒá swoje metadane, oraz poczƒÖtek danych tak by wszystko by≈Ço prawid≈Çowo wyr√≥wnane wzglƒôdem macierzy.\nJe≈õli nie mamy szczƒô≈õcia (bardzo stare jajko/LVM) to bƒôdziemy musieli u≈ºyƒá opcji -metadatasize i/lub -dataalignment:\npvcreate --metadatasize 500k /dev/md0 Teraz ciekawostka - LVM potrzebuje na 192KB na dane nag≈Ç√≥wkowe ka≈ºdego wolumenu i ka≈ºdy utworzony p√≥≈∫niej zas√≥b by≈Çby o te 192KB przesuniƒôty, wiƒôc \u0026hellip; trafia nasze wyr√≥wnanie do 128KB. Dlatego zmuszamy LVM\u0026rsquo;a by zaalokowa≈Ç nieco wiƒôcej - tutaj 256KB. OK - ale w poleceniu jest 250 - WTF? I to aby by≈Ço zabawnie jest to jak najbardziej prawid≈Çowa warto≈õƒá - nie wiem jaka w tym logika, ale by metadane zajmowa≈Çy 256KB podajemy 250k, by zajmowa≈Çy 512KB podajemy 500k itd\u0026hellip;\nInaczej jest z opcjƒÖ -dataalignment, tutaj najbardziej optymalnie nale≈ºy podaƒá rozmiar chunk\u0026rsquo;a*ilo≈õƒá aktywnych dysk√≥w (dla RAID5 odejmujemy jeden) - albo minimalnie rozmiar chunka, np.:\npvcreate --dataalignment 512K /dev/md0 Poprawno≈õƒá mo≈ºemy sprawdziƒá poleceniem:\npvs /dev/md0 -o+pe_start PV VG Fmt Attr PSize PFree 1st PE /dev/md0 vgraid lvm2 a- 1,82t 488,89g 512,00k U mnie 1st PE zaczyna siƒô na 512KB, wiƒôc jest OK.\nTeraz tworzymy grupƒô:\nvgcreate vgraid /dev/md0 Polecenie vgcreate posiada parametr -s kt√≥ry pozwala okre≈õliƒá do wielokrotno≈õci jakiej warto≈õci bƒôdzie zaokrƒÖglana wielko≈õƒá wolumenu - warto≈õƒá ta powinna byƒá wielokrotno≈õciƒÖ chunk\u0026rsquo;a z macierzy. Domy≈õlnie ma ona warto≈õƒá 4MB wiƒôc wszystko bƒôdzie ≈Çadnie wyr√≥wnane.\nI mo≈ºemy zaczƒÖƒá tworzyƒá volumeny logiczne:\nlvcreate -L1T -nsrv vgraid Formatowanie To teraz formatowanie - tutaj te≈º czasem trzeba siƒô wysiliƒá by utworzony przez nas filesystem dzia≈Ça≈Ç mo≈ºliwie optymalnie na macierzy i bloku o odpowiednim rozmiarze. W przypadku filesystemu na macierzy sƒÖ dwa wa≈ºne parametry: stride i stripe-width. Stride powinien odpowiadaƒá rozmiarowi podanemu jako chunk podczas tworzenia macierzy ale wyra≈ºonego w blokach systemu plik√≥w (domy≈õlnie 4KB). Stripe-width powinno byƒá ustawione na: stride * N, gdzie N to ilo≈õƒá aktywnych dysk√≥w w macierzy (dla RAID5 jest to ilo≈õƒá dysk√≥w minus 1) - przyk≈Çadowo dla bloku 4KB i chunk\u0026rsquo;a 512KB, stride powinien wynosiƒá 128 (512/4). Z kolei stripe-width dla 3 dysk√≥w to 128*(3-1)=256. Prawid≈Çowe dobranie tych parametr√≥w mo≈ºe daƒá wzrost wydajno≈õci rzƒôdu 40% (wed≈Çug moich test√≥w). Teoretycznie na nowszych systemach tworzone systemy plik√≥w automatycznie powinny wykryƒá najbardziej optymalne wyr√≥wnanie - mo≈ºemy wiƒôc spr√≥bowaƒá pu≈õciƒá format bez tych parametr√≥w i p√≥≈∫niej skontrolowaƒá ich warto≈õci poleceniem:\ntune2fs -l /dev/vgraid/srv Na poczƒÖtek przyk≈Çad dla systemu plik√≥w dla ma≈Çych i ≈õrednich plik√≥w:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -m0 /dev/vgraid/srv Jeden dodatkowy parametr to resize - pozwala on zmieniƒá domy≈õlne ustawienie maksymalnego rozmiaru do kt√≥rego mo≈ºemy powiƒôkszyƒá dany filesystem - domy≈õlnie jest to warto≈õƒá 1000 razy wiƒôksza od poczƒÖtkowej wielko≈õci - ciut przekozaczone. Mo≈ºe nie oszczƒôdzi to du≈ºo miejsca na dysku (kilkadziesiƒÖt/kilkaset MB) ale na pewno skr√≥ci czas fsck\u0026rsquo;a.\nKolejny dodatek to -m0 kt√≥re wy≈ÇƒÖcza alokacjƒô 5% przestrzeni dyskowej dla root\u0026rsquo;a i us≈Çug systemowych - po prostu tutaj tego nie potrzebujƒô a 5% z 1TB to 50GB marnujƒÖcego siƒô miejsca!\nJe≈õli wiemy ≈ºe bƒôdziemy przechowywaƒá na danym zasobie tylko stosunkowo du≈ºe pliki to mo≈ºna u≈ºyƒá takich opcji:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -T largefile -m0 /dev/vgraid/srv Opcja -T to wykorzystanie szablon√≥w opcji dla tworzenia system√≥w plik√≥w, kt√≥re mo≈ºna edytowaƒá i dodawaƒá w pliku: /etc/mke2fs.conf. Szablon largefile wykorzystuje mniejszƒÖ liczbƒô inod√≥w dla nowego filesystemu, je≈õli zamierzamy przechowywaƒá g≈Ç√≥wnie du≈ºe pliki to zmniejszy to czas formatowania i p√≥≈∫niejszych fsck\u0026rsquo;√≥w na tym systemie plik√≥w.\nTestowanie wydajno≈õci Zaleca≈Çbym testowanie wydajno≈õci na kolejnych etapach przygotowania dysk√≥w i powtarzaƒá te same benchmarki po ka≈ºdej zmianie, tak wiƒôc testujemy:\nNa poczƒÖtek wszystkie dyski, z kt√≥rych zamierzamy zbudowaƒá RAID\u0026rsquo;a by wykluczyƒá ewentualne \u0026ldquo;padaki\u0026rdquo;/uszkodzone kable, itp. Np. hdparm/dd na czystym dysku i dodatkowo iozone/bonnie++ na filesystemie by sprawdziƒá czy nie skopali≈õmy wyr√≥wnania partycji. Po zbudowaniu RAID\u0026rsquo;a powtarzamy testy - na ca≈Çym urzƒÖdzeniu (dd) i po sformatowaniu (iozone/bonnie++) by upewniƒá siƒô ≈ºe RAID jest prawid≈Çowo wyr√≥wnany (przy czym przy RAID5 mo≈ºemy siƒô spodziewaƒá wynik√≥w przy zapisie ni≈ºszych ni≈º przy odczycie - jest to OK). Jest to te≈º dobry moment na sprawdzenie kilku optymalizacji dla macierzy: stripe_cache_size, wy≈ÇƒÖczenie NCQ, ustawienia odczytu z wyprzedzeniem, wy≈ÇƒÖczenie cache dysk√≥w - po ka≈ºdej z tych zmian ponawiamy benchmarki by upewniƒá siƒô ≈ºe uzyskali≈õmy poprawƒô/lub nie. Po przygotowaniu LVM\u0026rsquo;a - ponawiamy benchmarki na volumenie by upewniƒá siƒô ≈ºe nie skopali≈õmy wyr√≥wnania partycji/chunk\u0026rsquo;a/itd\u0026hellip; Dopieszczamy opcje formatowania filesystemu i jego montowania (np. noatime, commit, data, itd) - i zn√≥w posi≈Çkujemy siƒô benchmarkami by potwierdziƒá ≈ºe pniemy siƒô z wydajno≈õciƒÖ w g√≥rƒô. Je≈õli my≈õlicie ≈ºe to du≈ºo to zaleca≈Çbym powt√≥rzenie czƒô≈õci benchmark√≥w 2~3 krotnie by upewniƒá siƒô ≈ºe wyniki nie odbiegajƒÖ znacznie od siebie. Dodatkowo powinni≈õmy czy≈õciƒá przed ka≈ºdym banchmarkiem cache dyskowy¬†aby mieƒá pewno≈õƒá ≈ºe lepsze wyniki nie sƒÖ skutkiem wczytania danych do pamiƒôci. Z tego samego powodu je≈õli uruchamiamy benchmarki to ilo≈õƒá zapisywanych/odczytywanych danych powinna byƒá minimum 1,5~2 razy wiƒôksza ni≈º pamieƒá RAM zainstalowana w systemie by na pewnie wszystkie dane nie zmie≈õci≈Çy siƒô w cache\u0026rsquo;u. Oczywi≈õcie mo≈ºna to zlaƒá ale p√≥≈∫niej nie ma siƒô co dziwiƒá ≈ºe system dzia≈Ça wolno - a na produkcyjnej maszynie du≈ºo trudniej zaoraƒá ca≈ÇƒÖ konfiguracjƒÖ i utworzyƒá od poczƒÖtku z prawid≈Çowym wyr√≥wnaniem.\nZalecane jest wykorzystanie IOZone lub Bonnie++ poniewa≈º testujƒÖ one nie tylko prosty sekwencyjny odczyt, ale r√≥wnie≈º tworzenie/kasowanie plik√≥w o r√≥≈ºnych rozmiarach i w r√≥≈ºnej ilo≈õci - to pozwala lepiej sprawdziƒá op√≥≈∫nienia wystƒôpujƒÖce przy przewidywanych przez nas obciƒÖ≈ºeniach oraz upewniƒá siƒô ≈ºe ca≈Ça zabawa z wyr√≥wnywaniem zasob√≥w mia≈Ça sens. Oczywi≈õcie to tylko zalecenia üòâ\nhdparm W przypadku macierzy wykorzystanie prostego hdparm\u0026rsquo;a do test√≥w:\nhdparm -tT /dev/md0 nie zwr√≥ci realnych i sensownych wynik√≥w. Pomiar jest zbyt kr√≥tki by uzyskaƒá sensowne wyniki - lepiej wykorzystaƒá dd z du≈ºƒÖ ilo≈õciƒÖ danych do odczytu.\ndd Narzƒôdzie jak≈ºe prymitywne a tak przydatne. Mo≈ºemy nim zmierzyƒá sekwencyjny odczyt/zapis z/do macierzy i uzyskaƒá bardziej realne wyniki ni≈º hdparm\u0026rsquo;em. Wystarczy wymusiƒá operacjƒô na ok. dwukrotnie wiƒôkszej ilo≈õci danych ni≈º ilo≈õƒá pamiƒôci RAM. Dodatkowo czy≈õcimy cache\u0026rsquo;e przed i po mierzƒÖc ca≈Ço≈õciowy czas, np. tak:\nsync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) Jest to szczeg√≥lnie przydatne np. przy dopasowywaniu optymalnej dla nas warto≈õci parametru stripe_cache_size, wystarczy przygotowaƒá odpowiedniƒÖ pƒôtlƒô:\nfor i in 128 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;stripe_cache_size $i\u0026#34; echo $i \u0026gt;¬†/sys/block/md0/md/stripe_cache_size sync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) done Parametr bs okre≈õla bufor wykorzystywany przy operacjach odczytu/zapisu - mo≈ºna go dostosowaƒá do rozmiaru chunk\u0026rsquo;a/stripe-width/itp.. count okre≈õla jak du≈ºo takich bufor√≥w odczytaƒá/zapisaƒá - w powy≈ºszym przypadku jest to 10 tys. jednomegabajtowych bufor√≥w wiƒôc ≈ÇƒÖcznie 10GB.\nbonnie++ Bonnie++ wymaga nieco przygotowania ale wyniki w moim przypadku bardzo odpowiada≈Çy rzeczywistym. Co pokr√≥tce trzeba zrobiƒá:\nmkdir /srv/test chown -R guest:guest /srv/test bonnie++ -d /srv/test -s 16g -m nazwa_maszyny -f -u guest Mo≈ºemy dodaƒá opcjƒô -b by po ka≈ºdej operacji wykonywany by≈Ç sync - to by≈Çoby co≈õ podobnego do system√≥w bazodanowych lub pocztowych. Je≈ºeli chcemy zasymulowaƒá standardowe operacje na plikach to nie potrzebujemy tej opcji.\niozone W najprostszym wykonaniu:\niozone -a Wykona to seriƒô pomiar√≥w na r√≥≈ºnych rozmiarach plik√≥w, ilo≈õci powt√≥rze≈Ñ itd. Najbardziej interesujƒÖca opcjƒÖ w konfiguracji RAID jest \u0026ldquo;Stride read\u0026rdquo;.\niozone -S 8192 -t 1 Parametr -S przekazuje rozmiar pamiƒôci cache procesora - wykorzystywany do alokacji pamiƒôci blokami itp (sprawdza≈Çem czy to cokolwiek pomo≈ºe.. ale nie widzia≈Çem du≈ºej r√≥≈ºnicy).\nParametr -t 1 to benchmark przepustowo≈õci dysku a parametr cyfrowy okre≈õla liczbƒô r√≥wnoczesnych wƒÖtk√≥w, kt√≥re bƒôdƒÖ odczytywaƒá/zapisywaƒá - mo≈ºna w ten spos√≥b zasymulowaƒá np. r√≥wnoczesny streaming dla wielu ≈∫r√≥de≈Ç, itp.\niozone -S 8192 -a -s 40960 Tutaj parametr -s wskazuje na jakim rozmiarze pliku w KB ma byƒá prowadzony benchmark - tutaj 40MB.\nPodsumowanie Og√≥lnie zadowolony jestem ≈ºe uda≈Ço mi siƒô to wszystko zebraƒá w jednym po≈õcie bo dotychczas mia≈Çem to zapisane w wielu r√≥≈ºnych zak≈Çadkach i spory problem gdy potrzebowa≈Çem \u0026ldquo;w≈Ça≈õnie tego jednego polecenia\u0026rdquo;. Ale niezadowolony jestem z tego ≈ºe nie uda≈Ço mi siƒô ustaliƒá ca≈Ço≈õci tego postƒôpowania dla dysk√≥w o sektorach/erase block\u0026rsquo;ach wiƒôkszych ni≈º 4 KB. Chodzi mi szczeg√≥lnie o brak jasnego wyja≈õnienia czy RAID5 jest prawid≈Çowo wyr√≥wnywany bo wed≈Çug jednych tak w≈Ça≈õnie jest, a wed≈Çug innych tak nie jest. StƒÖd niekt√≥rzy zalecajƒÖ by stosowaƒá format metadanych dla macierzy w wersji 0.9 lub 1.0 zamiast 1.2 ale nie ma jasnych ≈∫r√≥de≈Ç tego rozumowania. Mam nadziejƒô ≈ºe kiedy≈õ uda mi siƒô to jednoznacznie rozsƒÖdziƒá - na pewno zaktualizujƒô wtedy tego posta.\n≈πr√≥de≈Çka na kt√≥rych opar≈Çem to HOWTO http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing\nhttp://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously\nhttp://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding\nhttp://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html\nhttp://askubuntu.com/questions/20852/making-stripe-cache-size-permanent\nhttp://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56\nhttps://raid.wiki.kernel.org/index.php/Performance\nhttp://www.mjmwired.net/kernel/Documentation/md.txt\nhttp://wiki.hetzner.de/index.php/Partition_Alignment/en\nhttp://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment\nO tym ≈ºe LVM na RAID5 sam¬†siƒô¬†wyr√≥wnuje:\nhttp://marc.info/?l=linux-raid\u0026amp;m=126267824425009\u0026amp;w=2\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\nMDADM metadata format 1.2 wyrownuje sie do 4KiB:\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\nKalkulator stride\u0026rsquo;a\nhttp://busybox.net/~aldot/mkfs_stride.html\nFormat raid\u0026rsquo;a:\nhttps://raid.wiki.kernel.org/index.php/RAID_superblock_formats\nWyr√≥wnanie do 4kb - choƒá wydaje mi siƒô ≈ºe go≈õciu robi to na czuja i ledwie mu siƒô uda≈Ço:\nhttp://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors\n","permalink":"https://timor.site/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/","summary":"Po zakupie nowych dysk√≥w zamierzam utworzyƒá zdegradowanƒÖ macierz RAID5 z dw√≥ch dysk√≥w (na trzecim na razie znajdujƒÖ siƒô dane), potem utworzyƒá wolumen LVM, sformatowaƒá go, przekopiowaƒá dane z pojedynczego dysku na macierz i do≈ÇƒÖczyƒá trzeci dysk do macierzy odbudowujƒÖc¬†parzysto≈õƒá. Zadanie bƒôdzie o tyle ciekawe ≈ºe dysk ma 4KB sektory i trzeb bƒôdzie dbaƒá o wyr√≥wnanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyr√≥wnanie do chunk\u0026rsquo;a z macierzy.\nPrawid≈Çowe wyr√≥wnanie partycji KupujƒÖc nowy dysk (o pojemno≈õci od 500GB w g√≥rƒô), mamy spore szanse ≈ºe trafimy na sztukƒô, kt√≥ra wykorzystuje 4KB sektory do alokacji danych.","title":"LVM na RAID5 i dysku z sektorami 4KB"},{"content":"Po ka≈ºdej aktualizacji Ubuntu mam trochƒô zabawy by pozbieraƒá do kupy skaner i drukarkƒô z mojego urzƒÖdzenia wielofunkcyjnego Brother DCP-130C. Wybra≈Çem je bo by≈Ç to jedyny producent, kt√≥ry deklarowa≈Ç wsparcie dla Linux\u0026rsquo;a\u0026hellip; choƒá z perspektywy czasu nie jestem pewien czy otrzyma≈Çem to czego siƒô spodziewa≈Çem\u0026hellip; Co prawda zamieszczajƒÖ instrukcje i aktualizujƒÖ drivery ale jeszcze ani raz nie zdarzy≈Ço mi siƒô by po aktualizacji systemu postƒôpowanie wed≈Çug tych instrukcji zadzia≈Ça≈Ço bez¬†dodatkowej¬†pomocy.. Olaƒá!\nInstrukcja jest dla mojej drukarki ale powinna zadzia≈Çaƒá r√≥wnie≈º dla innych modeli Brother\u0026rsquo;a - w razie potrzeby mo≈ºna siƒô posi≈Çkowaƒá instrukcjami producenta.\nUruchomienie drukarki Pobieramy drivery dla drukarki z poni≈ºszej strony:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_prn.html\nInstalujemy zale≈ºno≈õci:\napt-get install¬†ia32-libs lib32stdc++ Instalujemy drivery (w moim przypadku dla DCP-130C):\ndpkg -i --force-all dcp130clpr-1.0.1-1.i386.deb dpkg -i --force-all dcp135ccupswrapper-1.0.1-1.i386.deb Teraz w przeglƒÖdarce wchodzimy na stronƒô konfiguracji CUPS\u0026rsquo;a:¬†http://localhost:631/printers¬†- powinna tam widnieƒá nasza drukarka z URI usb://Brother/¬†(np.¬†usb://Brother/DCP-130C?serial=BRO00000000). Je≈õli URI jest inne to modyfikujemy drukarkƒô by wybraƒá odpowiednie urzƒÖdzenie i sterownik. Gdyby jednak drukarka nie zosta≈Ça automatycznie dodana to wchodzimy na¬†http://localhost:631/admin¬†i¬†klikamy Dodawanie drukarki - podajemy dane autoryzacyjne dla root\u0026rsquo;a, wybieramy urzƒÖdzenie i sterownik.\nPo dodaniu drukarki mo≈ºna wej≈õƒá w jej ustawienia (u mnie¬†http://localhost:631/printers/DCP-130C) i w menu Administracja wybraƒá opcjƒô: Ustaw domy≈õlne opcje by okre≈õliƒá domy≈õlne parametry wydruk√≥w.\nZ dodawaniem drukarki nie mia≈Çem du≈ºych problem√≥w, ciekawiej jest ze skanerem\u0026hellip;\nUruchomienie skanera Z poni≈ºszej strony pobieramy drivery dla naszego modelu drukarki:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_scn.html\nJa pobra≈Çem deb\u0026rsquo;a dla wersji 64-bitowej:¬†brscan2-0.2.5-1.amd64.deb.\nInstalujemy (instrukcja producenta tutaj):\napt-get install sane-utils dpkg -i --force-all brscan2-0.2.5-1.amd64.deb Teoretycznie powinno to wystarczyƒá by root m√≥g≈Ç korzystaƒá ze skanera, wiƒôc popracujmy by u≈ºytkownicy te≈º mogli. Edytujemy jako root plik¬†/lib/udev/rules.d/40-libsane.rules i odszukujemy liniƒô \u0026ldquo;# The following rule will disable\u0026rdquo; (u mnie gdzie≈õ w okolicy 1180 linii), przed niƒÖ wklejamy tekst:\n# Brother scanners ATTRS{idVendor}==\u0026#34;04f9\u0026#34;, ENV{libsane_matched}=\u0026#34;yes\u0026#34; Zapisujemy plik. Teraz musimy dodaƒá zainteresowanych u≈ºytkownik√≥w do grupy scanner, robimy to np. tak:\ngpasswd -a roman scanner I powtarzamy dla innych user√≥w.\nI na koniec wisienka na torcie - na 64 bitowym systemie postƒôpowanie wed≈Çug instrukcji zwyczajnie nie dzia≈Ça bo potrzebne biblioteki instalujƒÖ siƒô w z≈Çym miejscu\u0026hellip; sic!\nPliki trafiajƒÖ do /usr/lib64 zamiast do /usr/lib - aby wszystko dzia≈Ça≈Ço jak trzeba musimy przekopiowaƒá (albo chocia≈º podlinkowaƒá) z /usr/lib64 do /usr/lib - co dok≈Çadnie nale≈ºy skopiowaƒá mo≈ºna znale≈∫ƒá tutaj.\n","permalink":"https://timor.site/2012/11/instalacja-drukarki-i-skanera-brother-dcp-130c-na-ubuntu-12-04/","summary":"Po ka≈ºdej aktualizacji Ubuntu mam trochƒô zabawy by pozbieraƒá do kupy skaner i drukarkƒô z mojego urzƒÖdzenia wielofunkcyjnego Brother DCP-130C. Wybra≈Çem je bo by≈Ç to jedyny producent, kt√≥ry deklarowa≈Ç wsparcie dla Linux\u0026rsquo;a\u0026hellip; choƒá z perspektywy czasu nie jestem pewien czy otrzyma≈Çem to czego siƒô spodziewa≈Çem\u0026hellip; Co prawda zamieszczajƒÖ instrukcje i aktualizujƒÖ drivery ale jeszcze ani raz nie zdarzy≈Ço mi siƒô by po aktualizacji systemu postƒôpowanie wed≈Çug tych instrukcji zadzia≈Ça≈Ço bez¬†dodatkowej¬†pomocy.","title":"Instalacja drukarki i skanera Brother DCP-130C na Ubuntu 12.04"},{"content":"Czasami potrzebny jest nam serwer pocztowy, kt√≥ry przy≈õle informacje dla root\u0026rsquo;a (np. monity smartd, mdadm, sypniƒôte crony itp) ale r√≥wnocze≈õnie nie chcemy stawiaƒá pe≈Çnego serwera typu postfix/exim. Warto w tym celu wykorzystaƒá zestaw heirloom-mailx + ssmtp. hairloom-mailx jest prostym shellowym klientem SMTP - przy okazji linkuje polecenie mail (przydatne w skryptach). ssmtp pe≈Çni funkcjƒô serwera SMTP ale nie dzia≈Ça jako demon - proces uruchamia siƒô gdy jest potrzebny i znika po wys≈Çaniu maili. Dodatkowo ssmtp mo≈ºe zostaƒá skonfigurowany by wysy≈Çaƒá maile nie tylko przez relay\u0026rsquo;a ale r√≥wnie≈º autoryzujƒÖc siƒô na zewnƒôtrznym serwerze pocztowym, np. gmail\u0026rsquo;u czy gdziekolwiek indziej.\nInstalacja apt-get install ssmtp heirloom-mailx Konfiguracja z relay\u0026rsquo;em Edytujemy plik /etc/ssmtp/ssmtp.conf - w konfiguracji z relay\u0026rsquo;em wystarczƒÖ te linijki:\nroot=postmaster@domena.pl mailhub=domena.pl hostname=serwerek.domena.pl Warto zwr√≥ciƒá uwagƒô ≈ºe dobrze by by≈Ço aby domena serwerek.domena.pl istnia≈Ça i wskazywa≈Ça na nasz serwer - dziƒôki temu poczta nie odpadnie na prostych filtrach antyspamowych.\nKonfiguracja pod gmail\u0026rsquo;a W ssmtp.conf potrzebujemy:\nroot=postmaster@domena.pl mailhub=smtp.gmail.com:587 hostname=serwerek.domena.pl AuthUser=twojekonto@gmail.com AuthPass=twoje_haslo_do_gmaila UseTLS=YES UseSTARTTLS=YES AuthMethod=LOGIN Test Warto teraz sprawdziƒá czy poczta wychodzi jak powinna:\necho test | mail -s \u0026#34;testowa wiadomosc\u0026#34; postmaster@domena.pl ","permalink":"https://timor.site/2012/11/prosty-mta-z-heirloom-mailx-i-ssmtp/","summary":"Czasami potrzebny jest nam serwer pocztowy, kt√≥ry przy≈õle informacje dla root\u0026rsquo;a (np. monity smartd, mdadm, sypniƒôte crony itp) ale r√≥wnocze≈õnie nie chcemy stawiaƒá pe≈Çnego serwera typu postfix/exim. Warto w tym celu wykorzystaƒá zestaw heirloom-mailx + ssmtp. hairloom-mailx jest prostym shellowym klientem SMTP - przy okazji linkuje polecenie mail (przydatne w skryptach). ssmtp pe≈Çni funkcjƒô serwera SMTP ale nie dzia≈Ça jako demon - proces uruchamia siƒô gdy jest potrzebny i znika po wys≈Çaniu maili.","title":"Prosty MTA z heirloom-mailx i ssmtp"},{"content":"Lubiƒô mieƒá porzƒÖdek w folderach i jedna rzecz, kt√≥ra nie daje mi spokoju w systemach plik√≥w ext to widoczno≈õƒá folderu lost+found - niby mo≈ºna go skasowaƒá i powinien siƒô odtworzyƒá (choƒá podobno odtworzenie w czasie fsck\u0026rsquo;a mo≈ºe spowodowaƒá utratƒô danych - trochƒô to dziwne i nie znalaz≈Çem ≈∫r√≥d≈Ça no ale powiedzmy ≈ºe nie chcƒô go usuwaƒá). Chcia≈Çem go ukryƒá (choƒáby w Nautilusie) by mnie nie dra≈ºni≈Ç. Oczywi≈õcie opcja z \u0026ldquo;.\u0026rdquo; na poczƒÖtku odpada, ale na szczƒô≈õcie Nautilus wykorzystuje pewien hack, kt√≥ry umo≈ºliwia ukrycie dowolnego pliku/folderu.\nTworzymy w g≈Ç√≥wnym katalogu danego punktu montowania, plik .hidden i wpisujemy do niego nazwy plik√≥w, kt√≥re chcemy ukryƒá - w moim przypadku lost+found:\ncd /root/katalogu echo \u0026#34;lost+found\u0026#34; \u0026gt;\u0026gt; .hidden Po od≈õwie≈ºeniu Nautilus nie pokazuje ju≈º katalogu lost+found - dla mnie bomba.\n","permalink":"https://timor.site/2012/10/nautilus-ukrywanie-lostfound/","summary":"Lubiƒô mieƒá porzƒÖdek w folderach i jedna rzecz, kt√≥ra nie daje mi spokoju w systemach plik√≥w ext to widoczno≈õƒá folderu lost+found - niby mo≈ºna go skasowaƒá i powinien siƒô odtworzyƒá (choƒá podobno odtworzenie w czasie fsck\u0026rsquo;a mo≈ºe spowodowaƒá utratƒô danych - trochƒô to dziwne i nie znalaz≈Çem ≈∫r√≥d≈Ça no ale powiedzmy ≈ºe nie chcƒô go usuwaƒá). Chcia≈Çem go ukryƒá (choƒáby w Nautilusie) by mnie nie dra≈ºni≈Ç. Oczywi≈õcie opcja z \u0026ldquo;.\u0026rdquo; na poczƒÖtku odpada, ale na szczƒô≈õcie Nautilus wykorzystuje pewien hack, kt√≥ry umo≈ºliwia ukrycie dowolnego pliku/folderu.","title":"Nautilus - ukrywanie lost+found"},{"content":"Sk≈Çadniki 1 kostka mas≈Ça, 1 ≈º√≥≈Çtko, 6 ≈Çy≈ºek cukru pudru, 1 szklanka mleka, 10 dkg wi√≥rek kokosowych, 1 kieliszek w√≥dki (ale z dwoma jest ostrzejsze), 3 ≈Çy≈ºki kakao, 37 dkg herbatnik√≥w kakaowych (doskonale spisujƒÖ siƒô dwie paczki ma≈õlanych herbatnik√≥w kakaowych). Masa kakaowa Herbatniki dok≈Çadnie rozwa≈Çkowaƒá, dodaƒá kakao i wymieszaƒá. Zagotowaƒá p√≥≈Ç szklanki mleka i gorƒÖcym zalaƒá herbatniki. Ca≈Ço≈õƒá wymieszaƒá na jednolitƒÖ masƒô. W drugim pojemniku rozmiksowaƒá p√≥≈Ç kostki mas≈Ça i 3 ≈Çy≈ºki cukru pudru - po chwili dodaƒá ≈º√≥≈Çtko i w√≥dkƒô. Do tej masy stopniowo dodajemy herbatniki.\nMasa kokosowa Zagotowaƒá p√≥≈Ç szklanki mleka i gorƒÖcym zalaƒá wi√≥rki kokosowe, wymieszaƒá by r√≥wnomiernie namok≈Çy. Rozmiksowaƒá p√≥≈Ç kostki mas≈Ça z 3 ≈Çy≈ºkami cukru pudru. Stopniowo dodawaƒá wi√≥rki i nadal miksowaƒá.\nSk≈Çadamy ca≈Ço≈õƒá Wa≈Çkujemy masƒô kakaowƒÖ na papierze do pieczenie lub folii (papier jest lepszy) - staramy siƒô uzyskaƒá kszta≈Çt zbli≈ºony do prostokƒÖta (u≈Çatwi to zwijanie rolady). Tak przygotowanƒÖ ciemnƒÖ masƒô pokrywamy r√≥wnomiernƒÖ warstwƒÖ masy kokosowej. P√≥≈∫niej delikatnie zaczynamy zwijaƒá z jednej strony jak makowiec - na koniec staramy siƒô do≈õƒá dok≈Çadnie docisnƒÖƒá papier by rolada nie \u0026ldquo;rozdƒô≈Ça\u0026rdquo; siƒô przed stwardnieniem. Taki pakunek wrzucamy do lod√≥wki na minimum 2~3 godziny.\nP.S. Chcia≈Çem wrzuciƒá fotkƒô ale zbyt szybko zniknƒô≈Ça - mo≈ºe innym razem üòÑ\n","permalink":"https://timor.site/2012/10/rolada-kokosowo-czekoladowa-na-zimno/","summary":"Sk≈Çadniki 1 kostka mas≈Ça, 1 ≈º√≥≈Çtko, 6 ≈Çy≈ºek cukru pudru, 1 szklanka mleka, 10 dkg wi√≥rek kokosowych, 1 kieliszek w√≥dki (ale z dwoma jest ostrzejsze), 3 ≈Çy≈ºki kakao, 37 dkg herbatnik√≥w kakaowych (doskonale spisujƒÖ siƒô dwie paczki ma≈õlanych herbatnik√≥w kakaowych). Masa kakaowa Herbatniki dok≈Çadnie rozwa≈Çkowaƒá, dodaƒá kakao i wymieszaƒá. Zagotowaƒá p√≥≈Ç szklanki mleka i gorƒÖcym zalaƒá herbatniki. Ca≈Ço≈õƒá wymieszaƒá na jednolitƒÖ masƒô. W drugim pojemniku rozmiksowaƒá p√≥≈Ç kostki mas≈Ça i 3 ≈Çy≈ºki cukru pudru - po chwili dodaƒá ≈º√≥≈Çtko i w√≥dkƒô.","title":"Rolada kokosowo-czekoladowa na zimno"},{"content":"Zdarzy≈Ço mi siƒô kilka us≈Çug dzia≈ÇajƒÖcych na serwerach Windows, kt√≥re zwyczajnie sypa≈Çy siƒô i to tak brzydko ≈ºe systemowe ustawienia by restartowa≈Çy siƒô po padzie nie wystarcza≈Çy. Us≈Çuga dzia≈Ça≈Ça np. 3 godziny by potem siƒô po≈Ço≈ºyƒá, albo nigdy nie wstawa≈Ça po starcie systemu - po prostu jaka≈õ masakra. Zg≈Çoszenia do producenta czƒôsto wyglƒÖda≈Çy tak ≈ºe w jego testowym ≈õrodowisku b≈Çƒôdu nie udaje siƒô powt√≥rzyƒá (i nic dziwnego bo nie sƒÖdzƒô by z ich konfiguracji korzystali rzeczywi≈õci u≈ºytkownicy). Oczywi≈õcie mo≈ºna zaczekaƒá a≈º uka≈ºe siƒô magiczna ≈Çata usuwajƒÖca b≈ÇƒÖd ale do tego czasu to na nas wszyscy bƒôdƒÖ \u0026ldquo;wieszaƒá psy\u0026rdquo; za niedostƒôpno≈õƒá us≈Çugi.\nW najprostszym podej≈õciu mo≈ºna napisaƒá bat\u0026rsquo;cha, kt√≥ry bƒôdzie restartowa≈Ç us≈Çugƒô (k≈Çad≈Ç i startowa≈Ç, sprawdza≈Ç czy dzia≈Ça i tylko przy braku dzia≈Çania uruchamia≈Ç, itp) - taki skrypcik odpalamy co 5 minut i powinno to za≈Çatwiƒá sprawƒô. Za≈Çatwi to najprostsze przypadki - ale nie bƒôdzie ca≈Çkiem elastyczne.\nSzuka≈Çem programu, kt√≥ry dzia≈Ça≈Çby jako us≈Çuga i \u0026ldquo;pilnowa≈Ç\u0026rdquo; innych us≈Çug systemowych i\u0026hellip; mo≈ºna co≈õ takiego kupiƒá za 200$ (sic!). Postanowi≈Çem poszukaƒá na forum AutoIt\u0026rsquo;a - basciopodobnego jƒôzyka skryptowego, kt√≥ry wielokrotnie pomaga≈Ç mi zautomatyzowaƒá pewne zadania administracyjne - trafi≈Çem na tego posta:¬†http://www.autoitscript.com/forum/topic/80201-service-udf-v2-run-your-exe-as-service/\nZ tego adresu pobieramy ≈∫r√≥d≈Ça - zmieniamy nazwƒô pliczku ServiceExample.au3 na np. ServiceKeeper.au3 i edytujemy (szukamy linijki region -\u0026gt; insert your running code here:\n#region --\u0026gt; insert your running code here Local $status = _Service_QueryStatus(\u0026#34;shittyservice\u0026#34;) If $status[1] \u0026lt;\u0026gt; $SERVICE_RUNNING Then If $bDebug Then logprint(\u0026#34;shittyservice service not running!\u0026#34;) If $status[1] == $SERVICE_PAUSED Then If _Service_Resume(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service resumed successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service resume failded!\u0026#34;) If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) EndIf EndIf EndIf If $status[1] == $SERVICE_STOPPED Then If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service start failded!\u0026#34;) EndIf EndIf EndIf Sleep(60000) # jak czesto sprawdzac san uslug - czas w milisekundach #endregion --\u0026gt; insert your running code here Powy≈ºszy przyk≈Çad sprawdza czy pewna \u0026ldquo;shittyus≈Çuga\u0026rdquo; dzia≈Ça, a je≈õli nie to podejmuje r√≥≈ºne pr√≥by jej uruchomienia. Przyk≈Çad najprostszy z mo≈ºliwych ale bardzo ≈Çatwo mo≈ºna zamiast sprawdzenia jedne us≈Çugi powieliƒá ten kod dla kilku us≈Çug i sprawdzaƒá je w pƒôtli (w wolnej chwili mo≈ºe wrzucƒô taki kod).\nSkrypcik dostosowujemy do swoich potrzeb, kompilujemy i wrzucamy na serwer potrzebujƒÖcy \u0026ldquo;rozrusznika\u0026rdquo;. Us≈Çugƒô trzeba zainstalowaƒá przez odpalenie skryptu z parametrem -i, w naszym przypadku wyglƒÖda≈Ço by to tak:\nServiceKeeper.exe -i Us≈Çugƒô mo≈ºna odinstalowaƒá przez uruchomienie z parametrem -u.\nDobrze ustawiƒá tƒÖ us≈Çugƒô by startowa≈Ça z op√≥≈∫nieniem. Nie wydaje mi siƒô sensowe sprawdzanie stanu us≈Çug czƒô≈õciej ni≈º raz na minutƒô.\nZdarza≈Ço mi siƒô ≈ºe ta us≈Çuga sypa≈Ça siƒô przy jej zatrzymywaniu (jest o tym wzmianka na forum), ale to mnie akurat ma≈Ço boli bo innych problem√≥w nie mia≈Çem.\n","permalink":"https://timor.site/2012/10/utrzymanie-przy-zyciu-sypiacych-sie-uslug-na-serwerach-windows/","summary":"Zdarzy≈Ço mi siƒô kilka us≈Çug dzia≈ÇajƒÖcych na serwerach Windows, kt√≥re zwyczajnie sypa≈Çy siƒô i to tak brzydko ≈ºe systemowe ustawienia by restartowa≈Çy siƒô po padzie nie wystarcza≈Çy. Us≈Çuga dzia≈Ça≈Ça np. 3 godziny by potem siƒô po≈Ço≈ºyƒá, albo nigdy nie wstawa≈Ça po starcie systemu - po prostu jaka≈õ masakra. Zg≈Çoszenia do producenta czƒôsto wyglƒÖda≈Çy tak ≈ºe w jego testowym ≈õrodowisku b≈Çƒôdu nie udaje siƒô powt√≥rzyƒá (i nic dziwnego bo nie sƒÖdzƒô by z ich konfiguracji korzystali rzeczywi≈õci u≈ºytkownicy).","title":"Utrzymanie przy ≈ºyciu sypiƒÖcych siƒô us≈Çug na serwerach Windows"},{"content":"Je≈õli szukamy statystyk dla strony internetowej i ze wzglƒôdu na jej zawarto≈õƒá (np. sklep, co≈õ ze zwiƒôkszonym naciskiem na poufno≈õƒá etc..) nie potrafimy zaufaj wujkowi Googlowi to warto przyglƒÖdnƒÖƒá siƒô Piwikowi.\nJest to system statystyk aspirujƒÖcy do bycia Open Source\u0026rsquo;owƒÖ alternatywƒÖ dla Google Analytics. AspirujƒÖcy (a nie bƒôdƒÖcy) z tego wzglƒôdu ≈ºe Google przechodzƒÖc na domy≈õlny HTTPS (SPDY) dla zalogowanych u≈ºytkownik√≥w uniemo≈ºliwi≈Ç ≈õledzenie stron z kt√≥rych pochodzƒÖ odwiedziny (tzw. refferals) - tym prostym sposobem tylko GA jest w stanie dostarczyƒá pe≈Çnych informacji o wszystkich u≈ºytkownikach.\nNiemniej Piwik nadal mo≈ºe byƒá bardzo atrakcyjnym narzƒôdziem choƒáby dlatego ≈ºe:\nstatystyki zbieramy \u0026ldquo;lokalnie\u0026rdquo;, czyli gdzie≈õ na naszym serwerze - mamy nad nimi w≈Çadzƒô (z wszystkimi konsekwencjami), do Piwik\u0026rsquo;a mo≈ºemy ≈Çatwo stworzyƒá w≈Çasne rozszerzenia, kt√≥re przedstawiƒÖ to co chcemy w spos√≥b jakiego oczekujemy, statystyki mo≈ºemy zbieraƒá przez dodanie odwo≈Çania do skryptu w kodzie strony lub okresowo importujƒÖc logi z serwera (jak wspomniany przeze mnie kiedy≈õ awstats) lub na oba wspomniane sposoby - stƒÖd Piwik ≈õwietnie sprawdza siƒô w hostingach (mo≈ºemy pokazaƒá klientowi ile mia≈Ç wej≈õƒá, zrobiƒá statystyki wykorzystania pasma itp), oprawa graficzna jest ≈õwie≈ºsza ni≈º w AWStats üòÉ ma niedu≈ºe wymagania, g≈Ç√≥wnie: PHP + MySQL. Instrukcjƒô instalacji i wiƒôcej informacji mo≈ºna znale≈∫ƒá tutaj:¬†http://pl.piwik.org/dokumentacja/instalacja-piwika/\n","permalink":"https://timor.site/2012/09/piwik-alternatywa-dla-google-analytics/","summary":"Je≈õli szukamy statystyk dla strony internetowej i ze wzglƒôdu na jej zawarto≈õƒá (np. sklep, co≈õ ze zwiƒôkszonym naciskiem na poufno≈õƒá etc..) nie potrafimy zaufaj wujkowi Googlowi to warto przyglƒÖdnƒÖƒá siƒô Piwikowi.\nJest to system statystyk aspirujƒÖcy do bycia Open Source\u0026rsquo;owƒÖ alternatywƒÖ dla Google Analytics. AspirujƒÖcy (a nie bƒôdƒÖcy) z tego wzglƒôdu ≈ºe Google przechodzƒÖc na domy≈õlny HTTPS (SPDY) dla zalogowanych u≈ºytkownik√≥w uniemo≈ºliwi≈Ç ≈õledzenie stron z kt√≥rych pochodzƒÖ odwiedziny (tzw. refferals) - tym prostym sposobem tylko GA jest w stanie dostarczyƒá pe≈Çnych informacji o wszystkich u≈ºytkownikach.","title":"Piwik - alternatywa dla Google Analytics"},{"content":"Co prawda adresy URL pozwalajƒÖ na stosowanie zar√≥wno du≈ºych jak i ma≈Çych liter ale r√≥≈ºne systemy mogƒÖ je r√≥≈ºnie obs≈Çugiwaƒá i mo≈ºe siƒô trafiƒá sytuacja, w kt√≥rej nie zechcemy by np. du≈ºe litery w og√≥le pojawia≈Çy siƒô w adresach URL. Doskona≈Çy przyk≈Çad to m√≥j niedawny wpis: Apache: ograniczenie dostƒôpu dla zalogowanych u≈ºytkownik√≥w z mod_rewrite i mod_auth_basic.\nZachodzi tam sytuacja, w kt√≥rej katalog u≈ºytkownika jest jego loginem ma≈Çymi literami (bƒÖd≈∫ du≈ºymi - jak kto woli), a u≈ºytkownik wpisujƒÖc login mo≈ºe u≈ºyƒá zar√≥wno ma≈Çych jak i du≈ºych liter i tutaj zaczyna siƒô jazda. Mo≈ºna u≈ºyƒá modyfikatora NC (no case), ale to wp≈Çynie tylko na por√≥wnania - przepisanie ≈õcie≈ºki na nazwƒô podanƒÖ przez usera (z du≈ºymi i ma≈Çymi literami) przekieruje do katalogu, kt√≥rego nie ma (bo jest katalog tylko ma≈Çymi/du≈ºymi).\nI wtedy przyda siƒô taka sztuczka:\nRewriteEngine On RewriteMap lc int:tolower RewriteCond %{REQUEST_URI} [A-Z] RewriteRule (.*) ${lc:$1} [R=301,L] Definiujemy mapƒô korzystajƒÖc z wbudowanego w modu≈Ç s≈Çownika tolower, a nastƒôpnie je≈õli w URL\u0026rsquo;u wystƒôpujƒÖ (w tym przypadku) du≈ºe litery to przekierowujemy na URL\u0026rsquo;a z ma≈Çymi.\n","permalink":"https://timor.site/2012/09/mod_rewrite-wymuszenie-malych-liter-w-adresie-url/","summary":"Co prawda adresy URL pozwalajƒÖ na stosowanie zar√≥wno du≈ºych jak i ma≈Çych liter ale r√≥≈ºne systemy mogƒÖ je r√≥≈ºnie obs≈Çugiwaƒá i mo≈ºe siƒô trafiƒá sytuacja, w kt√≥rej nie zechcemy by np. du≈ºe litery w og√≥le pojawia≈Çy siƒô w adresach URL. Doskona≈Çy przyk≈Çad to m√≥j niedawny wpis: Apache: ograniczenie dostƒôpu dla zalogowanych u≈ºytkownik√≥w z mod_rewrite i mod_auth_basic.\nZachodzi tam sytuacja, w kt√≥rej katalog u≈ºytkownika jest jego loginem ma≈Çymi literami (bƒÖd≈∫ du≈ºymi - jak kto woli), a u≈ºytkownik wpisujƒÖc login mo≈ºe u≈ºyƒá zar√≥wno ma≈Çych jak i du≈ºych liter i tutaj zaczyna siƒô jazda.","title":"mod_rewrite - wymuszenie ma≈Çych liter w adresie URL"},{"content":"Obecnie dostƒôpna jest ju≈º beta 2 Debiana Wheezy i utrzymywane sƒÖ aktualizacje bezpiecze≈Ñstwa wiƒôc powolutku mo≈ºna na testowych maszynach sprawdzaƒá co i jak siƒô zmieni≈Ço.\nPoniewa≈º do finalnej wersji pewnie sporo siƒô jeszcze zmieni to postaram siƒô z czasem aktualizowaƒá ten post by zawiera≈Ç bie≈ºƒÖce informacje.\nW razie wƒÖtpliwo≈õci patrz tutaj:¬†http://wiki.debian.org/DebianTesting\nRobimy backup Aktualizujemy ≈∫r√≥d≈Ça wskazywa≈Çy na paczki ga≈Çƒôzi testing (poni≈ºsze polecenie nadpisze Twoje obecne repozytoria): cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb http://security.debian.org/ wheezy/updates main contrib non-free deb-src http://security.debian.org/ wheezy/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib SRC Teraz od≈õwie≈ºamy repozytoria:\nsudo apt-get update Proponujƒô pobraƒá te≈º pliki by podczas aktualizacji wszystkie le≈ºa≈Çy w cache\u0026rsquo;u - na wypadek gdyby nagle pad≈Ço ≈ÇƒÖcze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz zaktualizujemy kluczowe paczki:\nsudo apt-get install apt dpkg I resztƒô systemu:\nsudo apt-get dist-upgrade To teraz pozosta≈Ço sprawdziƒá co posz≈Ço nie tak\u0026hellip; Powodzenia üòâ\n","permalink":"https://timor.site/2012/09/aktualizacja-debian-squeeze-do-wheezy/","summary":"Obecnie dostƒôpna jest ju≈º beta 2 Debiana Wheezy i utrzymywane sƒÖ aktualizacje bezpiecze≈Ñstwa wiƒôc powolutku mo≈ºna na testowych maszynach sprawdzaƒá co i jak siƒô zmieni≈Ço.\nPoniewa≈º do finalnej wersji pewnie sporo siƒô jeszcze zmieni to postaram siƒô z czasem aktualizowaƒá ten post by zawiera≈Ç bie≈ºƒÖce informacje.\nW razie wƒÖtpliwo≈õci patrz tutaj:¬†http://wiki.debian.org/DebianTesting\nRobimy backup Aktualizujemy ≈∫r√≥d≈Ça wskazywa≈Çy na paczki ga≈Çƒôzi testing (poni≈ºsze polecenie nadpisze Twoje obecne repozytoria): cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.","title":"Aktualizacja Debian Squeeze do Wheezy"},{"content":"Dzi≈õ TIP z przeciwnego obozu - opr√≥cz linuksowych system√≥w administrujƒô r√≥wnie≈º paroma serwerami windowsowymi i tutaj r√≥wnie≈º (a czasem nawet bardziej) uda mi siƒô znale≈∫ƒá co≈õ wartego zapamiƒôtania.\nJedna z najbardziej charakterystycznych rzeczy na komputerach przy≈ÇƒÖczonych do domeny Windows to wy≈õwietlanie \u0026ldquo;r√≥≈ºnych dziwnych rzeczy\u0026rdquo; przy starcie systemu. Zar√≥wno na Windowsie 2000 jak i na XP\u0026rsquo;ku na ma≈Çym okienku przewijajƒÖ informacje o aktualizacji polityk, instalacji oprogramowania itp\u0026hellip;\nZachowanie to zmieni≈Ço siƒô na Vistach i 7-kach, kt√≥re sƒÖ nieco mniej rozmowne i wy≈õwietlajƒÖ jedynie komunikat typu \u0026ldquo;Trwa uruchamianie systemu\u0026hellip;\u0026rdquo; i tyla\u0026hellip; Za≈Ç√≥≈ºmy ≈ºe wrzucimy do instalacji kilka paczek i jeszcze zmienimy kilka polityk i przez to komputer na tym napisie zatrzyma siƒô na 5~10 minut - co zrobi u≈ºyszkodnik po 3 minutach? Dojdzie do wniosku ≈ºe \u0026ldquo;co≈õ siƒô zwiesi≈Ço\u0026rdquo; i zresetuje komputer, a ≈ºe akurat by≈Ço nieco operacji dyskowych to mamy praktycznie pewnƒÖ rozwa≈Çkƒô tego systemu.\nJednym z rozwiƒÖza≈Ñ jest zwiƒôkszenie \u0026ldquo;gadatliwo≈õci\u0026rdquo; tego etapu uruchamiania systemu - jest to mo≈ºliwe o ile posiadamy kontroler domeny na minimum Windows Server 2008. Tworzymy GPO w kt√≥rym ustawiamy:¬†Computer Config -\u0026gt; Admin Templates -\u0026gt; System -\u0026gt; Verbose vs normal status messages na Enabled.\nW≈ÇƒÖczenie tej opcji spowoduje zwiƒôkszenie liczby wypisywanych komunikat√≥w, nazw instalowanych program√≥w i operacji wykonywanych przez system w fazie uruchamiania - wiƒôkszo≈õƒá u≈ºyszkodnik√≥w widzƒÖc ≈ºe zmieniajƒÖ siƒô statusy (co≈õ instaluje itp) zwiƒôksza \u0026ldquo;okienko czasowe\u0026rdquo; do naci≈õniƒôcia resetu do ok 15 minut üòâ\n","permalink":"https://timor.site/2012/09/gpo-windows-7-postep-przetwarzania-polityk-przy-starcie-systemu/","summary":"Dzi≈õ TIP z przeciwnego obozu - opr√≥cz linuksowych system√≥w administrujƒô r√≥wnie≈º paroma serwerami windowsowymi i tutaj r√≥wnie≈º (a czasem nawet bardziej) uda mi siƒô znale≈∫ƒá co≈õ wartego zapamiƒôtania.\nJedna z najbardziej charakterystycznych rzeczy na komputerach przy≈ÇƒÖczonych do domeny Windows to wy≈õwietlanie \u0026ldquo;r√≥≈ºnych dziwnych rzeczy\u0026rdquo; przy starcie systemu. Zar√≥wno na Windowsie 2000 jak i na XP\u0026rsquo;ku na ma≈Çym okienku przewijajƒÖ informacje o aktualizacji polityk, instalacji oprogramowania itp\u0026hellip;\nZachowanie to zmieni≈Ço siƒô na Vistach i 7-kach, kt√≥re sƒÖ nieco mniej rozmowne i wy≈õwietlajƒÖ jedynie komunikat typu \u0026ldquo;Trwa uruchamianie systemu\u0026hellip;\u0026rdquo; i tyla\u0026hellip; Za≈Ç√≥≈ºmy ≈ºe wrzucimy do instalacji kilka paczek i jeszcze zmienimy kilka polityk i przez to komputer na tym napisie zatrzyma siƒô na 5~10 minut - co zrobi u≈ºyszkodnik po 3 minutach?","title":"GPO: Windows 7 - postƒôp przetwarzania polityk przy starcie systemu"},{"content":"Gdy administruje siƒô du≈ºymi stronami internetowymi raz na czas np. po wiƒôkszych zmianach w konfiguracji zachodzi potrzeba sprawdzenia czy na stronie nie ma stron prowadzƒÖcych donikƒÖd. O ile w ma≈Çych serwisach mo≈ºna samemu szybko przeklikaƒá siƒô przez stronkƒô to dla starych rozro≈õniƒôtych serwis√≥w nie jest to takie proste.\nJest kilka narzƒôdzi kt√≥rych mo≈ºna u≈ºyƒá do testowania link√≥w na stronach - ka≈ºde z nich ma swoje zalety i wady, postaram siƒô je przybli≈ºyƒá.\nwget Wget\u0026rsquo;a najprawdopodobniej ju≈º masz i mo≈ºesz zaczynaƒá:\nwget -o /tmp/wget.log -nv -r -p http://example.com W pliku /tmp/wget.log mo≈ºemy znale≈∫ƒá komunikaty b≈Çƒôd√≥w i na dobrƒÖ sprawƒô tyle. Ciƒô≈ºko to przetworzyƒá ale je≈õli nasz serwis ma mechanizm do np. mailowego powiadomienia w momencie wystƒÖpienia krytycznego b≈Çƒôdu to wget\u0026rsquo;em najszybciej mo≈ºna takie strony wy≈Çapaƒá.\nlinkchecker Uruchamia siƒô go tak:\nlinkchecker -t3 --no-warnings http://example.com Program jako≈õ nie przypad≈Ç mi do gustu - dzia≈Ça≈Ç cholernie wolno i to na ca≈Çkiem ma≈Çej stronie.\nlinklint Z trzech program√≥w ten ma najdziwniejszƒÖ sk≈Çadniƒô - ale da siƒô tego nauczyƒá, a mo≈ºliwo≈õci ma chyba najwiƒôcej.\nlinklint -error -warn -xref -forward -out report.txt -net -http -host example.com /@ WystarczajƒÖco szybki i generuje przejrzyste raporty.przejrzyste raporty.\n","permalink":"https://timor.site/2012/09/sprawdzanie-nieaktywnych-linkow-na-stronie/","summary":"Gdy administruje siƒô du≈ºymi stronami internetowymi raz na czas np. po wiƒôkszych zmianach w konfiguracji zachodzi potrzeba sprawdzenia czy na stronie nie ma stron prowadzƒÖcych donikƒÖd. O ile w ma≈Çych serwisach mo≈ºna samemu szybko przeklikaƒá siƒô przez stronkƒô to dla starych rozro≈õniƒôtych serwis√≥w nie jest to takie proste.\nJest kilka narzƒôdzi kt√≥rych mo≈ºna u≈ºyƒá do testowania link√≥w na stronach - ka≈ºde z nich ma swoje zalety i wady, postaram siƒô je przybli≈ºyƒá.","title":"Sprawdzanie nieaktywnych link√≥w na stronie"},{"content":"Je≈õli posiadasz napƒôd ta≈õmowy LTO do archiwizacji/backupu danych to wiesz ≈ºe bardzo wa≈ºne jest opisywanie ta≈õm szczeg√≥lnie gdy trzeba co≈õ odzyskaƒá. Je≈ºeli masz szczƒô≈õcie to posiadasz nie pojedynczy napƒôd a autoloader obs≈ÇugujƒÖcy wiele ta≈õm i wiesz ≈ºe najlepiej gdy ta≈õmy sƒÖ opisane kodami paskowymi kt√≥re autoloader potrafi rozpoznaƒá. Dziƒôki temu nie ma potrzeby odczytywania nr. seryjnego z ta≈õmy tylko szybciutko skanerem kod√≥w. Tasiemki mo≈ºna kupowaƒá ju≈º z kodami ale ta≈Ñsze sƒÖ te bez kod√≥w\u0026hellip; i tu pytanie - czy da siƒô tanio zdobyƒá etykiety?\nKto≈õ po≈õwiƒôci≈Ç trochƒô czasu i przygotowa≈Ç webowe narzƒôdzie do generowania kod√≥w paskowych dla napƒôd√≥w LTO - plik generowany jest do postaci pdf\u0026rsquo;a kt√≥ry mo≈ºna p√≥≈∫niej wydrukowaƒá na papierze samoprzylepnym, pomachaƒá chwilƒô no≈ºyczkami i TA DAM! Mamy etykiety üòâ\nOczywi≈õcie zabawa ma sens je≈õli potrzebujemy tylko kilku szt. ta≈õm tygodniowo - przy wiƒôkszej ilo≈õci lepiej wydaƒá kasƒô i niech kto≈õ inny powycina etykiety za nas.\n","permalink":"https://timor.site/2012/09/generator-kodow-paskowych-dla-napedow-tasmowych-lto/","summary":"Je≈õli posiadasz napƒôd ta≈õmowy LTO do archiwizacji/backupu danych to wiesz ≈ºe bardzo wa≈ºne jest opisywanie ta≈õm szczeg√≥lnie gdy trzeba co≈õ odzyskaƒá. Je≈ºeli masz szczƒô≈õcie to posiadasz nie pojedynczy napƒôd a autoloader obs≈ÇugujƒÖcy wiele ta≈õm i wiesz ≈ºe najlepiej gdy ta≈õmy sƒÖ opisane kodami paskowymi kt√≥re autoloader potrafi rozpoznaƒá. Dziƒôki temu nie ma potrzeby odczytywania nr. seryjnego z ta≈õmy tylko szybciutko skanerem kod√≥w. Tasiemki mo≈ºna kupowaƒá ju≈º z kodami ale ta≈Ñsze sƒÖ te bez kod√≥w\u0026hellip; i tu pytanie - czy da siƒô tanio zdobyƒá etykiety?","title":"Generator kod√≥w paskowych dla napƒôd√≥w ta≈õmowych LTO"},{"content":"Szkoda ≈ºe polecenia do obs≈Çugi NFS\u0026rsquo;a nie zaczynajƒÖ siƒô od nfs* - ≈Çatwiej by≈Çoby mi je zapamiƒôtaƒá. A jednym z takich, zapominanych najczƒô≈õciej jest listowanie zasob√≥w, szczeg√≥lnie przydatne gdy korzysta siƒô z NFS\u0026rsquo;a na jakim≈õ NAS\u0026rsquo;ie (kt√≥rego magiczny soft nie pokazuje gdzie i co eksportuje):\nshowmount -e 192.168.1.10 Export list for 192.168.1.10: /mnt/pools/A/A0/Music * /mnt/pools/A/A0/Movies * /mnt/pools/A/A0/Backups * /mnt/pools/A/A0/Pictures * /mnt/pools/A/A0/Documents * Teraz mo≈ºemy zamontowaƒá zas√≥b:\nmount 192.168.1.10:/mnt/pools/A/A0/Music /mnt/music ","permalink":"https://timor.site/2012/09/listowanie-zasobow-nfs/","summary":"Szkoda ≈ºe polecenia do obs≈Çugi NFS\u0026rsquo;a nie zaczynajƒÖ siƒô od nfs* - ≈Çatwiej by≈Çoby mi je zapamiƒôtaƒá. A jednym z takich, zapominanych najczƒô≈õciej jest listowanie zasob√≥w, szczeg√≥lnie przydatne gdy korzysta siƒô z NFS\u0026rsquo;a na jakim≈õ NAS\u0026rsquo;ie (kt√≥rego magiczny soft nie pokazuje gdzie i co eksportuje):\nshowmount -e 192.168.1.10 Export list for 192.168.1.10: /mnt/pools/A/A0/Music * /mnt/pools/A/A0/Movies * /mnt/pools/A/A0/Backups * /mnt/pools/A/A0/Pictures * /mnt/pools/A/A0/Documents * Teraz mo≈ºemy zamontowaƒá zas√≥b:\nmount 192.168.1.10:/mnt/pools/A/A0/Music /mnt/music ","title":"Listowanie zasob√≥w NFS"},{"content":"Niedawno trafi≈Çem na ciekawy problem w mod_rewrite - by przekierowywaƒá u≈ºytkownik√≥w logujƒÖcych siƒô jednym z modu≈Ç√≥w mod_auth_basic do dedykowanych im katalog√≥w, r√≥wnocze≈õnie blokujƒÖc dostƒôp do katalog√≥w innych u≈ºytkownik√≥w. Nie brzmi to jako≈õ strasznie ale problem okaza≈Ç siƒô byƒá ca≈Çkiem nietrywialnym. Teoretyczne rozwiƒÖzanie sprowadza≈Ço siƒô do wyszukania loginu u≈ºytkownika ze ≈õcie≈ºki URI i por√≥wnania z nazwƒÖ u≈ºytkownika ze zmiennej %{REMOTE_USER} - je≈õli warto≈õci siƒô r√≥≈ºniƒÖ to Forbidden. Ale szybko okaza≈Ço siƒô ≈ºe w RewriteCond zmienne z dopasowa≈Ñ mo≈ºna podstawiaƒá tylko w pierwszym parametrze i ≈ºe o ile mo≈ºna RewriteCond\u0026rsquo;y po≈ÇƒÖczyƒá wyra≈ºeniami logicznymi typu AND/OR to nie ma mo≈ºliwo≈õci por√≥wnania czy dopasowania z kolejnych RewriteCond\u0026rsquo;√≥w sƒÖ identyczne. Po kilku dniach szperania w dokumentacji i r√≥≈ºnych tutorialach uda≈Ço mi siƒô trafiƒá na jednƒÖ warto≈õciowƒÖ wskaz√≥wkƒô ale tej stronki ju≈º nie ma, wiƒôc opiszƒô problem dla potomnych.\nZa≈Ço≈ºenia sƒÖ takie:\nmamy vhost\u0026rsquo;a kt√≥ry udostƒôpnia wszystkie foldery u≈ºytkownik√≥w, ka≈ºdy u≈ºytkownik posiada folder o nazwie dok≈Çadnie takiej samej jak jego login, u≈ºytkownik po zalogowaniu ma byƒá przekierowany do swojego folderu i przy pr√≥bie przej≈õcia do folder√≥w innych u≈ºytkownik√≥w albo nawracamy go do jego folderu/albo dajemy forbidden. Konfiguracja vhost\u0026rsquo;a Poni≈ºej podstawowa konfiguracja vhost\u0026rsquo;a:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Rewrite\u0026rsquo;y I najciekawsza czƒô≈õƒá czyli rewrite\u0026rsquo;y. Zaczynamy od przekierowania u≈ºytkownika do jego folderu:\nRewriteRule ^$ /%{REMOTE_USER} [R,L] Powy≈ºszy rewrite sprawdza czy pr√≥bujemy wej≈õƒá do g≈Ç√≥wnego katalogu, je≈õli tak to przekierowujemy do katalogu u≈ºytkownika.\nTo teraz magia, kt√≥rej d≈Çugo szuka≈Çem üòÉ\nRewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] Ju≈º wyja≈õniam co to robi - zacznƒô od przypomnienia ≈ºe pomimo takiego zapisu w konfiguracji regu≈Çy sƒÖ przetwarzane nieco inaczej: Apache najpierw sprawdza czy dany URI pasuje do wyra≈ºenia w RewriteRule, a dopiero gdy tak jest sprawdzane sƒÖ warunki w RewriteCond. Czyli RewriteRule dopasowuje pierwszƒÖ czƒô≈õƒá URI a≈º do znaku uko≈õnika / i zapamiƒôtuje w zmiennej $1. Dopiero teraz RewriteCond dopasowuje i zapamiƒôtuje login u≈ºytkownika w zmiennej %1 (tak rule zapamiƒôtuje w zmiennych z $, cond w zmiennych z %). Teraz gdy mamy ju≈º zapamiƒôtane loginy z URI i zmiennej to mo≈ºemy je zapisaƒá obok siebie w kolejnym RewriteCond oddzielajƒÖc znakiem kt√≥ry w loginie wystƒÖpiƒá nie powinien (np. dwukropkiem) - $1:%1. Teraz dopasowujemy pierwszƒÖ czƒô≈õƒá \u0026ldquo;sklejki\u0026rdquo;, czyli ^([^:]+): i zaraz potem wymagamy by pojawi≈Ça siƒô ta sama warto≈õƒá przez wstecznƒÖ referencjƒô¬†\\1$¬†- to por√≥wnywane jest z pierwszym parametrem cond\u0026rsquo;a. To dopasowanie jest prawdziwe gdy u≈ºytkownik loguje siƒô prawid≈Çowo, wiƒôc negujemy je stawiajƒÖc ! na poczƒÖtku regexp\u0026rsquo;a, by ka≈ºde b≈Çƒôdne logowanie powodowa≈Ço wywo≈Çanie RewriteRule, czyli Forbidden.\nPogmatwane? Wiƒôc teraz na przyk≈Çadzie:\nb≈Çƒôdne logowanie (bo prostsze):\nlogin: roman\nuri: zbyszek/\npo dopasowaniu w $1 mamy zbyszek, a w %1 mamy roman, wiƒôc $1:%1 to zbyszek:roman, ostatni cond sprawdza czy zbyszek:roman r√≥≈ºni siƒô od zbyszek:zbyszek - a skoro tak to blokujemy dostƒôp, dobre logowanie:\nlogin: roman\nuri: roman/\npo dopasowaniu w $1 i %1 mamy roman i sprawdzamy czy $1:%1 jest zgodne z roman:roman, a jest wiƒôc po negacji nie blokujemy dostƒôpu. Skoro dostƒôpu nie blokujemy to roman mo≈ºe dostaƒá siƒô do swoich plik√≥w. Finalna konfiguracja Zosta≈Ço przedstawienie ca≈Ço≈õciowo konfiguracji:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user RewriteEngine on RewriteRule ^$ /%{REMOTE_USER} [R,L] RewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Du≈ºe i ma≈Çe litery wpisywane w loginie przez user√≥w Zerknij tutaj.\n","permalink":"https://timor.site/2012/09/apache-ograniczenie-dostepu-dla-zalogowanych-uzytkownikow-z-mod_rewrite-i-mod_auth_basic/","summary":"Niedawno trafi≈Çem na ciekawy problem w mod_rewrite - by przekierowywaƒá u≈ºytkownik√≥w logujƒÖcych siƒô jednym z modu≈Ç√≥w mod_auth_basic do dedykowanych im katalog√≥w, r√≥wnocze≈õnie blokujƒÖc dostƒôp do katalog√≥w innych u≈ºytkownik√≥w. Nie brzmi to jako≈õ strasznie ale problem okaza≈Ç siƒô byƒá ca≈Çkiem nietrywialnym. Teoretyczne rozwiƒÖzanie sprowadza≈Ço siƒô do wyszukania loginu u≈ºytkownika ze ≈õcie≈ºki URI i por√≥wnania z nazwƒÖ u≈ºytkownika ze zmiennej %{REMOTE_USER} - je≈õli warto≈õci siƒô r√≥≈ºniƒÖ to Forbidden. Ale szybko okaza≈Ço siƒô ≈ºe w RewriteCond zmienne z dopasowa≈Ñ mo≈ºna podstawiaƒá tylko w pierwszym parametrze i ≈ºe o ile mo≈ºna RewriteCond\u0026rsquo;y po≈ÇƒÖczyƒá wyra≈ºeniami logicznymi typu AND/OR to nie ma mo≈ºliwo≈õci por√≥wnania czy dopasowania z kolejnych RewriteCond\u0026rsquo;√≥w sƒÖ identyczne.","title":"Apache: ograniczenie dostƒôpu dla zalogowanych u≈ºytkownik√≥w z mod_rewrite i mod_auth_basic"},{"content":"Wyb√≥r dobrego X-terminala to w ≈ºyciu admina prawie jak wyb√≥r ≈ºony\u0026hellip; spƒôdza siƒô wsp√≥lnie du≈ºo czasu i mi≈Ço gdy estetycznie wyglƒÖda, robi to co chcemy, itd\u0026hellip; üòâ\nNie lubiƒô gnome-terminal'a bo domy≈õlnie binduje F10 co wnerwia mnie w midnight commanderze, stƒÖd szuka≈Çem i szuka≈Çem i jak dotychczas najbardziej podpasowa≈Ç mi unicode-rxvt. Mo≈ºna uruchamiaƒá go po prostu jako urxvt lub uruchomiƒá demona urxvtd po zalogowaniu i potem odpalaƒá tylko klienta urxvtc. Druga metoda skutkuje natychmiastowym startem terminala, wiec gdy podbindujƒô go sobie pod F12 mam terminal zawsze pod rƒôkƒÖ w mniej ni≈º sekundƒô. Dodatkowo fajnie wyglƒÖda z p√≥≈Çprzezroczysto≈õciƒÖ i nie ma ≈ºadnych dodatkowych menu/gad≈ºet√≥w.\nSkr√≥ty klawiaturowe Wygodne skr√≥ty klawiaturowe to kolejny atut tego terminala, najczƒô≈õciej korzystam z:\nshift+d√≥≈Ç - otwarcie nowej karty, shift+lewo/shift+prawo - przej≈õcie na kartƒô w lewo/prawo, ctrl+lewo/ctrl+prawo - przeniesienie karty w lewo/prawo, ctrl+d - zamknij kartƒô. Wiƒôcej ciekawych informacji o tym terminalu mo≈ºna znale≈∫ƒá tutaj.\nKonfig dla urxvt cat \u0026gt; ~/.Xdefaults \u0026lt;\u0026lt;SRC visualBell: False Xft.antialias: true Xft.hinting: true Xft.hintstyle: 0 Xft.dpi: 75 urxvt*termName: rxvt-unicode urxvt*geometry: 110x35 urxvt*background: rgba:2000/2000/2000/dddd urxvt*foreground: white urxvt*depth: 32 urxvt*fading: 40 urxvt*shading: 40 # fajny font ale w nowszych Ubuntu dziwnie zachowuje sie kursor ;-( #urxvt*font: xft:Terminus:pixelsize=14 #urxvt*font: xft:Monospace:pixelsize=12 urxvt*font: xft:Ubuntu Mono:pixelsize=14 urxvt*scrollBar: false urxvt*saveLines: 30000 urxvt*tintColor: gray urxvt*perl-ext-common: default,matcher,selection-autotransform,tabbed,selection-pastebin # urlLauncher otwiera wpisanƒÖ przeglƒÖdarkƒô # po klikniƒôciu ≈õrodkowym klawiszem myszy #urxvt*urlLauncher: firefox urxvt*urlLauncher: chromium-browser # to ju≈º nie jest config dla urxvt # ale przewa≈ºnie te≈º go dorzucam xterm*geometry: 110x35 xterm*background: black xterm*foreground: grey xterm*fading: 90 xterm*shading: 50 xterm*inheritPixmap: true xterm*font: -misc-fixed-medium-r-*-*-12-*-*-*-*--iso10646-1 xterm*scrollBar: false xterm*saveLines: 30000 xterm*tintColor: gray SRC P.S. Je≈õli urxvt wydaje Ci siƒô zbyt sparta≈Ñski to zerknij na terminator\u0026rsquo;a.\n","permalink":"https://timor.site/2012/09/unicode-rxvt-moje-ustawienia/","summary":"Wyb√≥r dobrego X-terminala to w ≈ºyciu admina prawie jak wyb√≥r ≈ºony\u0026hellip; spƒôdza siƒô wsp√≥lnie du≈ºo czasu i mi≈Ço gdy estetycznie wyglƒÖda, robi to co chcemy, itd\u0026hellip; üòâ\nNie lubiƒô gnome-terminal'a bo domy≈õlnie binduje F10 co wnerwia mnie w midnight commanderze, stƒÖd szuka≈Çem i szuka≈Çem i jak dotychczas najbardziej podpasowa≈Ç mi unicode-rxvt. Mo≈ºna uruchamiaƒá go po prostu jako urxvt lub uruchomiƒá demona urxvtd po zalogowaniu i potem odpalaƒá tylko klienta urxvtc. Druga metoda skutkuje natychmiastowym startem terminala, wiec gdy podbindujƒô go sobie pod F12 mam terminal zawsze pod rƒôkƒÖ w mniej ni≈º sekundƒô.","title":"unicode-rxvt - moje ustawienia"},{"content":"Je≈ºeli chcemy by po ponownym uruchomieniu w czasie startu zosta≈Çy sprawdzone wszystkie dyski narzƒôdziem to mo≈ºna to osiƒÖgnƒÖƒá na dwa sposoby. Zawsze gdy tego potrzebujƒô zastanawiam siƒô tylko jaki plik trzeba by≈Ço utworzyƒá\u0026hellip; forcefsck, fsck, fsckforce\u0026hellip; wiƒôc notujƒô üòâ\nUtworzenie pliku /forcefsck Pierwsza metoda polega na utworzeniu pliku forcefsck w g≈Ç√≥wnym katalog, robimy to poni≈ºszym poleceniem:\nsudo touch /forcefsck Polecenie shutdown Druga metoda wykorzystuje parametr -F polecenia shutdown ale nie dzia≈Ça na wszystkich dystrybucjach (w takiej sytuacji patrz pierwsza metoda):\nsudo shutdown -rF now ","permalink":"https://timor.site/2012/09/wymuszenie-fsck-po-restarcie/","summary":"Je≈ºeli chcemy by po ponownym uruchomieniu w czasie startu zosta≈Çy sprawdzone wszystkie dyski narzƒôdziem to mo≈ºna to osiƒÖgnƒÖƒá na dwa sposoby. Zawsze gdy tego potrzebujƒô zastanawiam siƒô tylko jaki plik trzeba by≈Ço utworzyƒá\u0026hellip; forcefsck, fsck, fsckforce\u0026hellip; wiƒôc notujƒô üòâ\nUtworzenie pliku /forcefsck Pierwsza metoda polega na utworzeniu pliku forcefsck w g≈Ç√≥wnym katalog, robimy to poni≈ºszym poleceniem:\nsudo touch /forcefsck Polecenie shutdown Druga metoda wykorzystuje parametr -F polecenia shutdown ale nie dzia≈Ça na wszystkich dystrybucjach (w takiej sytuacji patrz pierwsza metoda):","title":"Wymuszenie fsck po restarcie"},{"content":"Znajomi co jaki≈õ czas pytajƒÖ mnie: jak nazywa siƒô ta aplikacja, kt√≥rƒÖ masz na telefonie do\u0026hellip;? Z jakiego programu do poczty korzystasz na tel\u0026hellip;? itd\u0026hellip;\nPytacie - wiƒôc macie üòÉ\nDGT GTD Bardzo przydatna lista TODO. Stworzona z my≈õlƒÖ o metodzie Getting Things Done i bardzo u≈Çatwia pamiƒôtanie u r√≥≈ºnych zadaniach. Posiada te≈º bardzo wygodny wid≈ºet, na kt√≥rym mo≈ºemy podglƒÖdnƒÖƒá nasze najbli≈ºsze zadania.\nOpera Mini Wbudowana przeglƒÖdarka jest niez≈Ça, ale Opera Mini kompresuje mocno strony wykorzystujƒÖc po≈õredniczƒÖce serwery proxy co znacznie obni≈ºa koszty transmisji danych. Od niedawna posiada tez prymitywny ale niezgorszy czytnik RSS\u0026rsquo;√≥w.\nWiFinder Prosty i funkcjonalny skaner sieci WiFi - wolƒô go zamiast domy≈õlnego narzƒôdzie do wyszukiwania i pod≈ÇƒÖczania sieci WiFi.\nWiFi Analyzer Bardziej zaawansowany skaner, pokazujƒÖcy moc sygna≈Çy do poszczeg√≥lnych AP, jako≈õƒá danego kana≈Çu. Szpanerska aplikacja przy kolegach adminach üòâ\nK-9 Mail Klient pocztowy - do≈õƒá zaawansowany funkcjonalnie ale z bardzo prostym interfejsem (choƒá poczƒÖtkowa konfiguracja bywa nieco \u0026ldquo;tricky\u0026rdquo;).\nAdFree Android Na root\u0026rsquo;owanym telefonie trzeba to mieƒá - aplikacja blokuje reklamy w wiƒôkszo≈õci popularnych aplikacji.\nAdvanced Task Killer Bardzo przydatny programik - szczeg√≥lnie na s≈Çabszych telefonach (jak m√≥j). Zabija aplikacje dzia≈ÇajƒÖce w tle, zwalniajƒÖc pamiƒôƒá i odciƒÖ≈ºajƒÖc procesor. Dziƒôki temu nawet cienki telefon dzia≈Ça ca≈Çkiem ≈ºwawo. Dostajemy r√≥wnie≈º wid≈ºet, kt√≥ry za jednym dotkniƒôciem zabija wszystko üòâ\nTransportoid Rozk≈Çad jazdy komunikacji miejskiej dla wielu polskich miast. Mo≈ºna sprawdziƒá po≈ÇƒÖczenia danej linii, odjazdy z danego przystanku, wyszukaƒá po≈ÇƒÖczenia. Jeste≈õmy automatycznie informowani o zmianach rozk≈Çadu, kt√≥ry mo≈ºna z poziomu aplikacji pobraƒá (bez ≈ºadnych rejestracji).\n","permalink":"https://timor.site/2012/09/moje-ulubione-aplikacje-na-androida/","summary":"Znajomi co jaki≈õ czas pytajƒÖ mnie: jak nazywa siƒô ta aplikacja, kt√≥rƒÖ masz na telefonie do\u0026hellip;? Z jakiego programu do poczty korzystasz na tel\u0026hellip;? itd\u0026hellip;\nPytacie - wiƒôc macie üòÉ\nDGT GTD Bardzo przydatna lista TODO. Stworzona z my≈õlƒÖ o metodzie Getting Things Done i bardzo u≈Çatwia pamiƒôtanie u r√≥≈ºnych zadaniach. Posiada te≈º bardzo wygodny wid≈ºet, na kt√≥rym mo≈ºemy podglƒÖdnƒÖƒá nasze najbli≈ºsze zadania.\nOpera Mini Wbudowana przeglƒÖdarka jest niez≈Ça, ale Opera Mini kompresuje mocno strony wykorzystujƒÖc po≈õredniczƒÖce serwery proxy co znacznie obni≈ºa koszty transmisji danych.","title":"Moje ulubione aplikacje na Android‚Äôa"},{"content":"JednƒÖ z rzeczy, kt√≥re podobajƒÖ mi siƒô w maszynach wirtualnych Xen jest mo≈ºliwo≈õƒá zrobienia backupu ca≈Çego obrazu i szybkie odzyskanie ju≈º w trakcie ciƒô≈ºkiej awarii. Gdy dodatkowo korzysta siƒô z LVM\u0026rsquo;a to mo≈ºna na chwilƒô wy≈ÇƒÖczyƒá DomU, utworzyƒá snapshot jego dysk√≥w, uruchomiƒá DomU i w trakcie dzia≈Çania robiƒá sp√≥jny backup ze snapshot\u0026rsquo;a. Dziƒôki takiemu mechanizmowi serwer jest niedostƒôpny przez kilkana≈õcie sekund, a backup sp√≥jny jakby zosta≈Ç wykonany przy ca≈Çkowicie wy≈ÇƒÖczonej maszynie. Taki backup sprowadza siƒô do kilku polece≈Ñ kt√≥re mo≈ºna oskryptowaƒá np.:\nlvcreate -L1000M -s -n volumendomu-snap /dev/vg/volumendomu dd if=/dev/vg/volumendomu | gzip -9 \u0026gt; backup.img.gz Problem pojawia siƒô przy pr√≥bie montowania takiego snapshot\u0026rsquo;a by uzyskaƒá dostƒôp do plik√≥w gdy na volumenie LVM zostanie utworzona partycja i dopiero ona formatowana (domy≈õlnie przy ext3/4). Czyli potrzebujemy zamontowaƒá partycjƒô z volumenu LVM ale ta nie jest wprost widoczna (nie ma urzƒÖdzenia np. /dev/vg/volumendomu1).\nTen sam problem pojawia siƒô przy dostƒôpie do partycji \u0026ldquo;zaszytych\u0026rdquo; w obrazie zrzuconym narzƒôdziem dd z ca≈Çego dysku, np.:\ndd if=/dev/sda of=/mnt/backups/somewhere.img W obu przypadkach w obrazie/wolumenie jest zaszyta partycja i przy pr√≥bie montowania dostaniemy tylko monit o nieznanym typie systemu plik√≥w.\nBy wylistowaƒá partycje wewnƒÖtrz obrazu lub wolumenu najwygodniej¬†pos≈Çu≈ºymƒá siƒô narzƒôdziem parted :\nsudo¬†parted -s /mnt/backups/somewhere.img \u0026#34;unit B print\u0026#34; Model: (file) Dysk /mnt/backups/somewhere.img: 500105740288B Rozmiar sektora (logiczny/fizyczny): 512B/512B Tablica partycji: msdos Numer PoczƒÖtek Koniec Rozmiar Typ System plik√≥w Flaga 1 1048576B 500105740287B 500104691712B primary ntfs Parted mo≈ºe dzia≈Çaƒá albo w trybie interaktywnym albo razem z parametrem -s¬†podajemy na ko≈Ñcu¬†skrypt z poleceniami kt√≥re majƒÖ zostaƒá podane. Powy≈ºsze wywo≈Çanie zmienia jednostki z kilo/megabajt√≥w na bajty (dok≈Çadnie tego potrzebujemy jako offset - nic nie bƒôdziemy musieli przeliczaƒá).\nTeraz mo≈ºemy pr√≥bowaƒá zamontowaƒá danƒÖ partycjƒô korzystajƒÖc z parametru offset w mount, np. tak:\nsudo mount -o loop,ro,offset=1048576 -t ntfs /mnt/backups/somewhere.img /mnt/test Je≈õli posiadamy stosunkowo aktualne wersje jajka i pakietu util-linux to powy≈ºsza sztuczka powinna siƒô udaƒá. Je≈õli takowych nie posiadamy to mo≈ºemy mieƒá problemy przy pr√≥bie montowania kolejnych partycji. Wtedy mo≈ºe byƒá potrzebne rozpakowania pojedynczej partycji z obrazu poleceniem dd.\n","permalink":"https://timor.site/2012/09/montowanie-partycji-z-obrazu-dysku/","summary":"JednƒÖ z rzeczy, kt√≥re podobajƒÖ mi siƒô w maszynach wirtualnych Xen jest mo≈ºliwo≈õƒá zrobienia backupu ca≈Çego obrazu i szybkie odzyskanie ju≈º w trakcie ciƒô≈ºkiej awarii. Gdy dodatkowo korzysta siƒô z LVM\u0026rsquo;a to mo≈ºna na chwilƒô wy≈ÇƒÖczyƒá DomU, utworzyƒá snapshot jego dysk√≥w, uruchomiƒá DomU i w trakcie dzia≈Çania robiƒá sp√≥jny backup ze snapshot\u0026rsquo;a. Dziƒôki takiemu mechanizmowi serwer jest niedostƒôpny przez kilkana≈õcie sekund, a backup sp√≥jny jakby zosta≈Ç wykonany przy ca≈Çkowicie wy≈ÇƒÖczonej maszynie.","title":"Montowanie partycji z obrazu dysku"},{"content":"Kiedy≈õ poproszono mnie o przeszukanie wszystkich plik√≥w php na serwerze webowym po kƒÖtem wywo≈Çania pewnej funkcji. Oczywiste wyda≈Ço mi siƒô u≈ºycie rekurencyjnie grep\u0026rsquo;a, wiƒôc:\ngrep -R \u0026#34;JAKAS_FUNKCJA\u0026#34; /var/www/*.php Ale szybko okaza≈Ço siƒô ≈ºe grep dopasowuje maskƒô *.php r√≥wnie≈º do katalog√≥w, wiƒôc nie przeszukiwa≈Ç katalog√≥w kt√≥re nie ko≈Ñczy≈Çy siƒô na .php ehhh\u0026hellip;..\nDrugie podej≈õcie okaza≈Ço siƒô trafniejsze - najpierw poleceniem find wyszukujƒô wszystkie pliki php, a dopiero p√≥≈∫niej grepujƒô (wypisujƒÖc nazwƒô pliku i numer linii):\nfind /var/www/ -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;JAKAS_FUNKCJA\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; Przyk≈Çadowo wynik:\nfind /var/www -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;eval(\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; /var/www/*****/wp-admin/press-this.php:208: var my_src = eval( /var/www/*****/wp-admin/press-this.php:219: var my_src = eval( /var/www/*****/wp-admin/press-this.php:402: eval(data); /var/www/*****/wp-admin/includes/class-pclzip.php:4063:// eval(\u0026#39;$v_result = \u0026#39;.$p_options[PCLZIP_CB_PRE_EXTRACT].\u0026#39;(PCLZIP_CB_PRE_EXTRACT, $v_local_header);\u0026#39;); /var/www/*****/wp-includes/class-phpmailer.php:1916: //TODO using /e (equivalent to eval()) is probably not a good idea /var/www/*****/wp-includes/class-json.php:22: * Javascript, and can be directly eval()\u0026#39;ed with no further parsing /var/www/*****/wp-includes/functions.php:190: if ( doubleval($bytes) \u0026gt;= $mag ) ","permalink":"https://timor.site/2012/08/przeszukiwanie-plikow-danego-typu-pod-katem-tekstu/","summary":"Kiedy≈õ poproszono mnie o przeszukanie wszystkich plik√≥w php na serwerze webowym po kƒÖtem wywo≈Çania pewnej funkcji. Oczywiste wyda≈Ço mi siƒô u≈ºycie rekurencyjnie grep\u0026rsquo;a, wiƒôc:\ngrep -R \u0026#34;JAKAS_FUNKCJA\u0026#34; /var/www/*.php Ale szybko okaza≈Ço siƒô ≈ºe grep dopasowuje maskƒô *.php r√≥wnie≈º do katalog√≥w, wiƒôc nie przeszukiwa≈Ç katalog√≥w kt√≥re nie ko≈Ñczy≈Çy siƒô na .php ehhh\u0026hellip;..\nDrugie podej≈õcie okaza≈Ço siƒô trafniejsze - najpierw poleceniem find wyszukujƒô wszystkie pliki php, a dopiero p√≥≈∫niej grepujƒô (wypisujƒÖc nazwƒô pliku i numer linii):","title":"Przeszukiwanie plik√≥w danego typu pod kƒÖtem tekstu"},{"content":"To raczej nie jest podstawowy konfig i pr√≥≈ºno szukaƒá go na stronie WordPress\u0026rsquo;a, wiƒôc odradzam tƒô zabawƒô je≈õli nie zna siƒô zbyt dobrze nginx\u0026rsquo;a.\nPoniewa≈º serwerek, na kt√≥rym dzia≈Ça stronka to sprzƒôcik z Atomem 330 i mocy na CPU zbyt wiele nie ma to popularne pluginy (np. W3 Total Cache) potencjalnie zwiƒôkszajƒÖce wydajno≈õƒá tak na prawdƒô zmula≈Çy stronkƒô jeszcze bardziej. Plugin√≥w sprawdzi≈Çem kilka i ka≈ºdorazowo efekt by≈Ç podobny - stronka dzia≈Ça≈Ça wolniej ni≈º bez ich pomocy.\nDruga sprawa to zwiƒôkszony ruch - w takiej konfiguracji ju≈º przy kilku osobach r√≥wnocze≈õnie przeglƒÖdajƒÖcych blog, serwerek zwyczajnie nie radzi≈Ç sobie z dynamicznym generowaniem strony.\nPomys≈Ç na rozwiƒÖzanie problemu z wydajno≈õciƒÖ polega≈Ç na odpaleniu cache\u0026rsquo;ujƒÖcego reverse proxy przed w≈Ça≈õciwƒÖ stronƒÖ, z kr√≥tkim okresem wa≈ºno≈õci cache\u0026rsquo;u (max kilka sekund) tak by przy du≈ºym obciƒÖ≈ºeniu strony serwowaƒá g≈Ç√≥wnie z cache\u0026rsquo;u (tylko co pewien czas kto≈õ bƒôdzie mia≈Ç niefart i bƒôdzie musia≈Ç zaczekaƒá na wygenerowanie strony), przy czym komentarze i panel administracyjny dzia≈ÇajƒÖ z pominiƒôciam cache\u0026rsquo;owania (czyli ka≈ºdorazowo trafiajƒÖ przez proxy do aplikacji).\nWa≈ºne jednak by osoba wysy≈ÇajƒÖca komentarz mog≈Ça wynik swojego dzia≈Çania zobaczyƒá od razu na stronie. Poniewa≈º komentarze wysy≈Çane sƒÖ metodƒÖ HTTP POST to w momencie odebrania takiego po≈ÇƒÖczenia bƒôdzie ustawiane ciasteczko dezaktywujƒÖce cache dla danego po≈ÇƒÖczenia na kilka sekund (do momentu jego wyga≈õniƒôcia).\nPoni≈ºej plik konfiguracyjny, kt√≥ry nale≈ºy zapisaƒá np. w: /etc/nginx/sites-available/wordpress\n# na poczƒÖtek ustawiamy lokalizacjƒô dla cache\u0026#39;u proxy_cache_path /var/cache/nginx/wordpress levels=1:2 keys_zone=WORDPRESS:10m inactive=24h max_size=100m; # nie chcƒô stronki z www na poczƒÖtku wiƒôc ca≈Çy ruch przekierowujƒÖ # na stronkƒô bez www server { listen 10.0.1.2:80; server_name www.example.com; rewrite ^ http://example.com$request_uri? permanent; } # tutaj ma miejsce magia - g≈Ç√≥wny host obs≈ÇugujƒÖcy stronkƒô # to tak na prawdƒô cache\u0026#39;ujƒÖce proxy serwujƒÖce okresowo # generowane pliki server { listen 10.0.1.2:80 default; access_log /var/log/nginx/wordpress.access.log; server_name example.com; # ten rewrite przerzuca do panelu admina nawet je≈õli # na ko≈Ñcu nie wpiszemy uko≈õnika # bez niego te≈º to dzia≈Ça ale przekierowanie jest przetwarzane # przez skrypt stronki i dzia≈Ça wolniej rewrite ^/wp-admin$ /wp-admin/ last; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s # (moja stronka nie obs≈Çuguje zbyt du≈ºego ruchu i rzadko # jest modyfikowana - np. przez komentarze - wiƒôc 60s jest OK, # na bardziej aktywnych stronkach mo≈ºna siƒô pokusiƒá o ustawienie # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to # cache zapewni obs≈Çugƒô nawet kilku tys. zapyta≈Ñ na sekundƒô proxy_cache_valid 200 60s; # informacje dla backendu na jakiego host siƒô wbijamy # i z jakiego \u0026#34;prawdziwego\u0026#34; IP proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # mo≈ºemy ukryƒá niekt√≥re nag≈Ç√≥wki np. by nie podpowiadaƒá # z jakich plugin√≥w korzystamy w WordPressie proxy_hide_header X-Powered-By; # kilka ustawie≈Ñ timeout\u0026#39;√≥w proxy_connect_timeout 60; proxy_read_timeout 120; proxy_send_timeout 120; # Wa≈ºne - poni≈ºsza opcja ustawia w jaki spos√≥b generowane sƒÖ # nazwy plik√≥w w cache\u0026#39;u, # dodajƒÖc np. kolejne zmienne mo≈ºemy zr√≥≈ºnicowaƒá cache dla # pewnych grup u≈ºytkownik√≥w proxy_cache_key \u0026#34;$scheme$request_method$host$request_uri\u0026#34;; # domy≈õlna lokalizacja location / { # ustawiamy domy≈õlnƒÖ warto≈õƒá zmiennej set $no_cache \u0026#34;\u0026#34;; # If non GET/HEAD, don\u0026#39;t cache \u0026amp; mark user as uncacheable for 1 second via cookie # je≈õli metoda inna ni≈º GET/HEAD to oznacz usera przez cookie jako niecachowanego # na czas 60s (ustawiany poni≈ºej) if ($request_method !~ ^(GET|HEAD)$) { set $no_cache \u0026#34;1\u0026#34;; } # je≈õli zalogowany to nie cache\u0026#39;uj if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache \u0026#34;1\u0026#34;; } # je≈ºeli kt√≥ry≈õ z wcze≈õniejszych warunk√≥w jest spe≈Çniony # to ustawiamy cookie, kt√≥re poinformuje nas by nie cachowaƒá # kolejnych zapyta≈Ñ # (z powodu \u0026#34;dziwnego\u0026#34; zachowania if w nginx\u0026#39;ie ustawienie # tego bezpo≈õrednio we wcze≈õniejszych warunkach nie dzia≈Ça) if ($no_cache = \u0026#34;1\u0026#34;) { add_header Set-Cookie \u0026#34;_mcnc=1; Max-Age=61; Path=/\u0026#34;; add_header X-Microcachable \u0026#34;0\u0026#34;; } # je≈õli cookie jest ustawione to pomijamy cache i serwujemy # ≈õwie≈ºƒÖ tre≈õƒá if ($http_cookie ~* \u0026#34;_mcnc\u0026#34;) { set $no_cache \u0026#34;1\u0026#34;; } # dwie poni≈ºsze opcje zapewniajƒÖ pominiƒôcia cache\u0026#39;owania # w przypadku gdy wystƒÖpi kt√≥ry≈õ z wcze≈õniejszych warunk√≥w proxy_no_cache $no_cache; proxy_cache_bypass $no_cache; # Serwujemy cache je≈õli strona jest obecnie od≈õwie≈ºana # lub wystƒÖpi b≈ÇƒÖd proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; # pliki wiƒôksze ni≈º 1M nie bƒôdƒÖ cache\u0026#39;owane proxy_max_temp_file_size 1M; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s # (moja stronka nie obs≈Çuguje zbyt du≈ºego ruchu i rzadko # jest modyfikowana - np. przez komentarze - wiƒôc 60s jest OK, # na bardziej aktywnych stronkach mo≈ºna siƒô pokusiƒá o ustawienie # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to # cache zapewni obs≈Çugƒô nawet kilku tys. zapyta≈Ñ na sekundƒô proxy_cache_valid 200 60s; # zmieniamy domy≈õlny klucz cache\u0026#39;owania tak by uwzglƒôdnia≈Ç # naszƒÖ zmiennƒÖ proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; # wskazujemy konkretnƒÖ lokalizacjƒô cache\u0026#39;u proxy_cache WORDPRESS; # podajemy lokalizacjƒô backendu (nie widzia≈Çem sensu # by udostƒôpniaƒá go na zewnƒôtrznym adresie) proxy_pass http://127.0.0.1:81; # mo≈ºna ustawiƒá dodatkowo cache\u0026#39;owanie strony w przeglƒÖdarce # (ca≈Çkiem niezale≈ºnie od tego co bƒôdzie w cache\u0026#39;u na serwerze) expires 60s; } # dla panelu administracyjnego ustawiamy proxy bez cache\u0026#39;u location ~* wp\\-(admin|login) { # dostƒôp do panelu administracyjnego dodatkowo chronimy # has≈Çem - po co? # bo w tym katalogu sƒÖ r√≥≈ºne fajne skrypty, w kt√≥rych ju≈º # nie raz znaleziono dziury auth_basic \u0026#34;Go Away\u0026#34;; auth_basic_user_file htpasswd; # proxy bez cache\u0026#39;u proxy_pass http://127.0.0.1:81; } # statykƒô cache\u0026#39;ujemy mocniej ni≈º tre≈õci dynamiczne # a czemu nie puszczam jej bezpo≈õrednio? bo wyplute przez # backend zostanƒÖ skompresowane i w takiej postaci zachowajƒÖ # siƒô w cache\u0026#39;u - gdybym serwowa≈Ç je bezpo≈õrednio to nginx # kompresowa≈Çby np. css\u0026#39;y/js\u0026#39;y przy ka≈ºdym dostƒôpie do nich location ~* \\.(jpg|png|gif|jpeg|css|js|mp3|wav|swf|mov|doc|pdf|xls|ppt|docx|pptx|xlsx)$ { # cache\u0026#39;ujemy statykƒô przez 2 godziny proxy_cache_valid 200 120m; # dodatkowo ustawiamy d≈Çugie cache\u0026#39;owanie w przeglƒÖdarkach expires 864000; # puszczamy wszystko w proxy + cache proxy_pass http://127.0.0.1:81; proxy_cache WORDPRESS; # wy≈ÇƒÖczam logowanie dostƒôpu do statyki nawet w przypadku b≈Çƒôd√≥w # to ma≈Ço istotne log_not_found off; access_log off; } # jeszcze inaczej ustawiam cache dla RSS\u0026#39;√≥w location ~* \\/[^\\/]+\\/(feed|\\.xml)\\/? { # cache\u0026#39;ujemy RSS\u0026#39;y przez 45 minut if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache 1; } proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; proxy_cache_valid 200 45m; proxy_cache MYSITE; proxy_pass http://127.0.0.1:81; } } # to teraz konfiguracja serwera serwujƒÖcego tre≈õci dynamiczne server { # nas≈Çuchujemy lokalnie bo z zewnƒÖtrz strona dostƒôpna # jest przez proxy listen 127.0.0.1:81; error_log /var/log/nginx/mysite.error.log; root /var/www/wordpress; index index.php; # logujemy prawdziwe IP dziƒôki odpowiednim nag≈Ç√≥wkom # przesy≈Çanym przez proxy set_real_ip_from 127.0.0.0/24; real_ip_header X-Real-IP; # tutaj praktycznie klasyka - z tym ≈ºe zamiast na ko≈Ñcu # wskazywaƒá index.php robiƒô najpierw kilka rewrite\u0026#39;√≥w # np. dla przeniesionych stron, itp location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /main http://example.com/about/? permanent; rewrite /projekty http://example.com/category/projects/? permanent; rewrite /tag/gd http://example.com? permanent; rewrite /category/hobby http://roman.com? permanent; rewrite ^ /index.php last; } # na bardziej obleganych stronach limitowanie wyszukiwania mo≈ºe # pom√≥c, ale to nie m√≥j przypadek # location /search { limit_req zone=mysitesearch burst=3 nodelay; rewrite ^ /index.php; } location ~* \\.(?:ico|css|js|gif|jpe?g|png)$ { # cache\u0026#39;owanie atrybut√≥w statycznych plik√≥w open_file_cache max=1000 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; # maksymalne cache\u0026#39;owanie statyki w przeglƒÖdarkach expires max; } # no i na ko≈Ñcu obs≈Çuga skrypt√≥w php location ~* \\.php$ { # albo plik istnieje i go serwujemy albo dajemy Forbidden try_files $uri =403; # kilka standardowych ustawie≈Ñ include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # blokujemy mo≈ºliwo≈õƒá wykonywania skrypt√≥w z katalogu upload # (nawet je≈õli komu≈õ uda siƒô je wepchnƒÖƒá) if ($uri !~ \u0026#34;^/wp-content/uploads/\u0026#34;) { fastcgi_pass php-fastcgi; } # informacje dla proxy jak d≈Çugo mo≈ºe cache\u0026#39;owaƒá add_header Cache-Control \u0026#34;max-age:60, public\u0026#34;; expires 60s; } # blokujƒô dostƒôp do plik√≥w zaczynajƒÖcych siƒô od kropki location ~ /\\. { access_log off; log_not_found off; deny all; } # wy≈ÇƒÖczam logowanie do nieistotnych plik√≥w location = /robots.txt { allow all; access_log off; log_not_found off; } location = /favicon.ico { access_log off; log_not_found off; } } Konfiguracja potrzebuje jednego folderu na cache, do kt√≥rego dostƒôp do zapisu ma nginx (u≈ºytkownik na kt√≥rym dzia≈Ça proces):\nmkdir -p /var/cache/nginx/wordpress chown -R www-data:www-data /var/cache/nginx/wordpress A ≈ºeby zaczƒÖ≈Ç dzia≈Çaƒá trzeba go \u0026ldquo;w≈ÇƒÖczyƒá\u0026rdquo; i prze≈Çadowaƒá nginx\u0026rsquo;a:\ncd /etc/nginx/sites-available/ ln -s wordpress /etc/nginx/sites-enabled/wordpress service nginx reload Jedyna rzecz, kt√≥rej brakuje w tym configu to konfiguracja backend\u0026rsquo;u do PHP\u0026rsquo;a (u mnie nazwana¬†php-fastcgi) - mo≈ºe kiedy≈õ zrobiƒô HOWTO o konfiguracji NGINX+PHP, ale na tƒÖ chwilƒô zak≈Çadam ≈ºe sobie poradzisz üòÉ\nBenchmark Sprawd≈∫my jak to wyglƒÖda teraz:\nab -n 1000 -c 10 https://gagor.pl/ This is ApacheBench, Version 2.3 \u0026lt; $Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking gagor.pl (be patient) Completed 100 requests Completed 200 requests Completed 300 requests Completed 400 requests Completed 500 requests Completed 600 requests Completed 700 requests Completed 800 requests Completed 900 requests Completed 1000 requests Finished 1000 requests Server Software: nginx Server Hostname: gagor.pl Server Port: 80 Document Path: / Document Length: 36263 bytes Concurrency Level: 10 Time taken for tests: 8.716 seconds Complete requests: 1000 Failed requests: 22 (Connect: 0, Receive: 0, Length: 22, Exceptions: 0) Write errors: 0 Total transferred: 36601094 bytes HTML transferred: 36263088 bytes Requests per second: 114.73 [#/sec] (mean) Time per request: 87.159 [ms] (mean) Time per request: 8.716 [ms] (mean, across all concurrent requests) Transfer rate: 4100.91 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 1 16 7.1 15 55 Processing: 17 67 97.0 53 789 Waiting: 5 28 55.6 19 456 Total: 37 83 98.0 68 803 Percentage of the requests served within a certain time (ms) 50% 68 66% 74 75% 78 80% 80 90% 87 95% 95 98% 665 99% 755 100% 803 (longest request) Dla mnie bomba üòÉ\nPomys≈Ç na taki rodzaj cache\u0026rsquo;owania zaczerpnƒÖ≈Çem stƒÖd.\n","permalink":"https://timor.site/2012/06/nginx-konfiguracja-pod-wordpressa/","summary":"To raczej nie jest podstawowy konfig i pr√≥≈ºno szukaƒá go na stronie WordPress\u0026rsquo;a, wiƒôc odradzam tƒô zabawƒô je≈õli nie zna siƒô zbyt dobrze nginx\u0026rsquo;a.\nPoniewa≈º serwerek, na kt√≥rym dzia≈Ça stronka to sprzƒôcik z Atomem 330 i mocy na CPU zbyt wiele nie ma to popularne pluginy (np. W3 Total Cache) potencjalnie zwiƒôkszajƒÖce wydajno≈õƒá tak na prawdƒô zmula≈Çy stronkƒô jeszcze bardziej. Plugin√≥w sprawdzi≈Çem kilka i ka≈ºdorazowo efekt by≈Ç podobny - stronka dzia≈Ça≈Ça wolniej ni≈º bez ich pomocy.","title":"Nginx - konfiguracja pod WordPress‚Äôa"},{"content":"Pisa≈Çem ju≈º HOWTO o konfiguracji Xen\u0026rsquo;a¬†ale nie opisa≈Çem jak siƒô bawiƒá wirtualkami gdy Xen\u0026rsquo;a ju≈º mamy. To nadrabiam.\nTworzenie i usuwanie maszyn wirtualnych Do tworzenia/niszczenia DomU wykorzystujƒô pakiet xen-tools dostarczajƒÖcy m.in. dwa narzƒôdzia:\nxen-create¬†- dla kt√≥rego przygotowa≈Çem do≈õƒá¬†skomplikowanƒÖ¬†konfiguracjƒô przy okazji wcze≈õniejszego posta: Instalacja i konfiguracja DomU. Przyk≈Çad u≈ºycia: xen-create --hostname example-domu --ip 10.0.0.77 \\ --gateway 10.0.0.1 --broadcast 10.0.0.255 --netmask 255.255.255.0 \\ --bridge br10 --vcpus 2 --memory=2G xen-delete-image¬†- narzƒôdzie do kasowania wirtualnych maszyn. Maszyna musi byƒá wy≈ÇƒÖczona aby mo≈ºna by≈Ço jƒÖ usunƒÖƒá. Narzƒôdzie to kasuje plik konfiguracyjny maszyny wirtualnej oraz przydzielone jej volumeny lvm. Przyk≈Çad u≈ºycia: xen-delete-image nazwamaszyny ZarzƒÖdzanie maszynami wirtualnymi Do uruchamiania, wy≈ÇƒÖczanie, resetowania (i og√≥lnie zarzƒÖdzania) maszynami wirtualnymi s≈Çu≈ºy tylko jedno polecenie:¬†xm¬†z r√≥≈ºnymi parametrami:\nxm list- listuje uruchomione w danej chwili wirtualne maszyny, wy≈õwietlajƒÖc przydzielonƒÖ im ilo≈õƒá pamiƒôci, procesor√≥w, stan (uruchomiona, zawieszona), czas dzia≈Çania (uptime). Dla obja≈õnienia Domain-0 (zwane te≈º Dom-O) to hyperwisor czyli fizyczna maszyna na kt√≥rej uruchomione sƒÖ wirtualki. xm top¬†- polecenie wy≈õwietla dok≈Çadne dane chwilowego zu≈ºycia zasob√≥w dla r√≥≈ºnych wirtualnych maszyn i Dom-0. xm create nazwapliku.cfg¬†- uruchamia maszynƒô wirtualnƒÖ zgodnie z instrukcjami zawartymi w pliku konfiguracyjnym (przydzielone dyski, pamiƒôƒá, etc). xm shutdown nazwamaszyny¬†- wysy≈Ça sygna≈Ç wy≈ÇƒÖczenia maszyny wirtualnej i wraca do wiersza polece≈Ñ. Dok≈Çadnie to polecenie wysy≈Ça sygna≈Ç ACPI r√≥wnowa≈ºny przyci≈õniƒôciu przycisku Power na obudowie komputera - system operacyjny wykrywa to zdarzenie i zaczyna siƒô wy≈ÇƒÖczaƒá. Jest to zalecana instrukcja do wy≈ÇƒÖczania wirtualek. Nale≈ºy pamiƒôtaƒá, ≈ºe po wykonaniu tego polecenia jeszcze przez kilka/kilkana≈õcie sekund maszyna dzia≈Ça - do puki nie sko≈Ñczy siƒô wy≈ÇƒÖczaƒá. xm shutdown -w nazwamaszyny¬†- dzia≈Ça jak powy≈ºsze polecenie, ale dodatkowo czeka a≈º maszyna wirtualna zostanie wy≈ÇƒÖczona a przydzielone jej zasoby zwolnione. Gdy to polecenie sko≈Ñczy siƒô wykonywaƒá mamy pewno≈õƒá, ≈ºe maszyna jest ju≈º wy≈ÇƒÖczona. xm destroy nazwamaszyny¬†- polecenie do twardego resetu maszyny wirtualnej. Najpierw odbierany jest czas procesora dla maszyny, potem zwalniana pamiƒôƒá i zarezerwowane uchwyty. WykorzystujƒÖc to polecenie mo≈ºe doj≈õƒá do utraty danych lub uszkodzenia¬†OS‚Äòu na wirtualnej maszynie. xm reboot nazwamaszyny¬†- restartuje maszynƒô wirtualnƒÖ w bezpieczny spos√≥b (czyli wysy≈Ça sygna≈Ç ACPI do¬†wy≈ÇƒÖczenia¬†i startuje DomU). Gdy zmodyfikujemy plik konfiguracyjny danego DomU nie wystarczy wywo≈Çaƒá xm reboot - przewa≈ºnie potrzeba po≈Ço≈ºyƒá maszynƒô i ponownie jƒÖ uruchomiƒá, np. tak: xm shutdown -w maszyna \u0026amp;\u0026amp; xm create maszyna.cfg xm pause nazwamaszyny - pauzuje wirtualnƒÖ maszynƒô, zamra≈ºajƒÖc jƒÖ w obecnym stanie razem z pamiƒôciƒÖ itd. xm unpause nazwamaszyny¬†- uruchamia zapauzowanƒÖ wcze≈õniej maszynƒô wirtualnƒÖ. Dzia≈Ça odwrotnie do polecenia powy≈ºej. xm console nazwamaszyny - polecenie dzia≈Ça jak ‚Äûpodpiƒôcie monitora‚Äù do fizycznej maszyny, na pierwszy terminal. Bardzo przydatne zaraz po utworzeniu wirtualki jak r√≥wnie≈º w r√≥≈ºnych sytuacjach kryzysowych üòâ Jest jeszcze kilka innych polece≈Ñ np. dodajƒÖcych na gorƒÖco urzƒÖdzenia blokowe ale ich dzia≈Çanie mocno zale≈ºy od wersji Xen\u0026rsquo;a i jajka.\n","permalink":"https://timor.site/2012/06/xen-podstawowe-polecenia/","summary":"Pisa≈Çem ju≈º HOWTO o konfiguracji Xen\u0026rsquo;a¬†ale nie opisa≈Çem jak siƒô bawiƒá wirtualkami gdy Xen\u0026rsquo;a ju≈º mamy. To nadrabiam.\nTworzenie i usuwanie maszyn wirtualnych Do tworzenia/niszczenia DomU wykorzystujƒô pakiet xen-tools dostarczajƒÖcy m.in. dwa narzƒôdzia:\nxen-create¬†- dla kt√≥rego przygotowa≈Çem do≈õƒá¬†skomplikowanƒÖ¬†konfiguracjƒô przy okazji wcze≈õniejszego posta: Instalacja i konfiguracja DomU. Przyk≈Çad u≈ºycia: xen-create --hostname example-domu --ip 10.0.0.77 \\ --gateway 10.0.0.1 --broadcast 10.0.0.255 --netmask 255.255.255.0 \\ --bridge br10 --vcpus 2 --memory=2G xen-delete-image¬†- narzƒôdzie do kasowania wirtualnych maszyn.","title":"Xen - Podstawowe polecenia"},{"content":"Ustawienie domy≈õlnego vhosta w nginx\u0026rsquo;ie jest ≈Çadnie opisane w dokumentacji i poczƒÖtkowo wydawa≈Ço siƒô dobrze dzia≈Çaƒá ale gdy wykorzysta≈Çem tƒÖ konfiguracjƒô na serwerze z wieloma adresami IP i nas≈Çuchiwaniem na porcie 80 (bez podania IP) to zachowywa≈Ço siƒô to do≈õƒá dziwnie (przewa≈ºnie nie ≈Çadowa≈Ço tej strony kt√≥rƒÖ chcia≈Çem). Od teraz tworzƒô konfiguracjƒô domy≈õlnego vhosta dla ka≈ºdego z dostƒôpnych adres√≥w IP. Powiem szczerze ≈ºe nie mia≈Çem czasu na g≈Çƒôbsze zbadanie tego zachowania i wykorzysta≈Çem¬†rozwiƒÖzanie, kt√≥re dzia≈Ça≈Ço w ka≈ºdym przypadku czyli po jednym konfigu na IP + przekierowanie na og√≥lnƒÖ stronƒô.\nserver { listen 10.0.0.100:80 default_server; server_name _; server_name_in_redirect off; rewrite ^ http://www.gagor.pl permanent; } ","permalink":"https://timor.site/2012/06/nginx-ustawienie-domyslnego-vhosta/","summary":"Ustawienie domy≈õlnego vhosta w nginx\u0026rsquo;ie jest ≈Çadnie opisane w dokumentacji i poczƒÖtkowo wydawa≈Ço siƒô dobrze dzia≈Çaƒá ale gdy wykorzysta≈Çem tƒÖ konfiguracjƒô na serwerze z wieloma adresami IP i nas≈Çuchiwaniem na porcie 80 (bez podania IP) to zachowywa≈Ço siƒô to do≈õƒá dziwnie (przewa≈ºnie nie ≈Çadowa≈Ço tej strony kt√≥rƒÖ chcia≈Çem). Od teraz tworzƒô konfiguracjƒô domy≈õlnego vhosta dla ka≈ºdego z dostƒôpnych adres√≥w IP. Powiem szczerze ≈ºe nie mia≈Çem czasu na g≈Çƒôbsze zbadanie tego zachowania i wykorzysta≈Çem¬†rozwiƒÖzanie, kt√≥re dzia≈Ça≈Ço w ka≈ºdym przypadku czyli po jednym konfigu na IP + przekierowanie na og√≥lnƒÖ stronƒô.","title":"Nginx - ustawienie domy≈õlnego vhosta"},{"content":"VLAN\u0026rsquo;y sƒÖ prostƒÖ metodƒÖ na separacjƒô sieci. Gdy mamy wiele sieci mo≈ºe zaj≈õƒá potrzeba by poszczeg√≥lne DomU mia≈Çy dostƒôp do r√≥≈ºnych VLAN\u0026rsquo;√≥w (czasem nawet wielu r√≥wnocze≈õnie). Je≈ºeli serwer z Dom0 posiada minimum¬†giga-bitowƒÖ¬†kartƒô sieciowƒÖ (a najlepiej kilka) to powinni≈õmy byƒá w stanie z godziwƒÖ jako≈õciƒÖ udostƒôpniƒá systemom DomU r√≥≈ºne VLAN\u0026rsquo;y z interfejs√≥w gospodarza.\nZaprezentowane poni≈ºej skrypty zapo≈ºyczy≈Çem z tej strony:¬†http://renial.net/weblog/2007/02/27/xen-vlan\nNa poczƒÖtek musimy zainstalowaƒá pakiet vlan:\napt-get install vlan P√≥≈∫niej w pliku /etc/xen/xend-config.sxp ustawiamy taka skrypt dla konfiguracji sieci:\n(network-script network-multi-vlan) Tworzymy plik /etc/xen/scripts/network-multi-vlan i wpisujemy w nim (no dobra - komentarze mo≈ºna pominƒÖƒá):\n#!/bin/sh #=================================================================== # Xen vlan bridge start/stop script. # Xend calls a network script when it starts. # The script name to use is defined in /etc/xen/xend-config.sxp # in the network-script field. # # This script creates multiple bridges to segregate individual # domUs to separate VLANs. Customize to fit your needs. # # Usage: # # network-multi-vlan (start|stop|status) # #=================================================================== dir=$(dirname \u0026#34;$0\u0026#34;) # Poni≈ºsza linijka pozwala udostƒôpniƒá dany interfejs sieciowy # w ca≈Ço≈õci jako domy≈õlny bridge - je≈õli chcemy siƒô ograniczyƒá # wy≈ÇƒÖcznie do bridge\u0026#39;y na VLAN\u0026#39;ach to mo≈ºemy tƒÖ linijkƒô # zakomentowaƒá (jak ja) \u0026#34;$dir/network-bridge\u0026#34; \u0026#34;$@\u0026#34; vifnum=0 netdev=eth0 # No i teraz odpalamy kolejne VLAN\u0026#39;y na poszczeg√≥lnych interfejsach. # Brak parametru netdev jest r√≥wnoznaczny wybraniu netdev=eth0 # Parametr VLAN jest... hm... samoopisujƒÖcy \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=10 netdev=eth1 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=11 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=23 netdev=eth2 Pobieramy skrypt¬†network-bridge-vlan¬†do lokalizacji:¬†/etc/xen/scripts/network-bridge-vlan.\ncd /tmp wget https://gagor.pl/wp-content/uploads/2012/06/network-bridge-vlan.gz gunzip network-bridge-vlan.gz mv network-bridge-vlan¬†/etc/xen/scripts/network-bridge-vlan ≈πr√≥d≈Ça http://wiki.xensource.com/xenwiki/XenDom0VLANstoDomUVirtualNICs¬†- og√≥lne schematy konfiguracji VLAN\u0026rsquo;√≥w na Xen\u0026rsquo;ie, http://renial.net/weblog/2007/02/27/xen-vlan¬†- skrypty do konfiguracji VLAN\u0026rsquo;√≥w pochodzƒÖ z tej strony. ","permalink":"https://timor.site/2012/06/xen-konfiguracja-interfejsu-sieciowego-dom0-jako-brdigea-dla-roznych-vlanow/","summary":"VLAN\u0026rsquo;y sƒÖ prostƒÖ metodƒÖ na separacjƒô sieci. Gdy mamy wiele sieci mo≈ºe zaj≈õƒá potrzeba by poszczeg√≥lne DomU mia≈Çy dostƒôp do r√≥≈ºnych VLAN\u0026rsquo;√≥w (czasem nawet wielu r√≥wnocze≈õnie). Je≈ºeli serwer z Dom0 posiada minimum¬†giga-bitowƒÖ¬†kartƒô sieciowƒÖ (a najlepiej kilka) to powinni≈õmy byƒá w stanie z godziwƒÖ jako≈õciƒÖ udostƒôpniƒá systemom DomU r√≥≈ºne VLAN\u0026rsquo;y z interfejs√≥w gospodarza.\nZaprezentowane poni≈ºej skrypty zapo≈ºyczy≈Çem z tej strony:¬†http://renial.net/weblog/2007/02/27/xen-vlan\nNa poczƒÖtek musimy zainstalowaƒá pakiet vlan:\napt-get install vlan P√≥≈∫niej w pliku /etc/xen/xend-config.","title":"Xen - Konfiguracja interfejsu sieciowego Dom0 jako brdige‚Äôa dla VLAN‚Äô√≥w"},{"content":"Ostatnio trafi≈Çem na ciekawy problem, kt√≥ry wielokrotnie rozwiƒÖzywa≈Çem w nginx\u0026rsquo;ie ale tym razem musia≈Çem zrobiƒá to w Apache. Pewna stronka dzia≈Ça sobie na HTTPS\u0026rsquo;ie i chcia≈Çem by wszystkie powiƒÖzane z niƒÖ pliki by≈Çy serwowane z jej adresu szyfrowanym po≈ÇƒÖczeniem by nie pojawia≈Çy siƒô w przeglƒÖdarce monity ≈ºe \u0026ldquo;czƒô≈õƒá ruchu nie jest szyfrowana\u0026rdquo;. Tyle ≈ºe czƒô≈õƒá potrzebnych plik√≥w by≈Ça ju≈º obecnie serwowana na innym serwerze (w innej domenie) poprzez HTTP.\nMog≈Çem albo skopiowaƒá te pliki i wykombinowaƒá jaki≈õ mechanizm synchronizujƒÖcy albo wykorzystaƒá proxy + cache. Drugie¬†rozwiƒÖzanie¬†wyda≈Ço mi siƒô prostsze i ≈Çadniejsze üòÉ\nNa poczƒÖtek w≈ÇƒÖczamy w Apache\u0026rsquo;m odpowiednie modu≈Çy:\na2enmod proxy a2enmod proxy_http Teraz przyk≈Çadowa konfiguracja vhosta:\n\u0026lt;VirtualHost *:80\u0026gt; #podstawowa konfiguracja vhosta ServerAdmin webmaster@example.com ServerName www.example.com ServerAlias example.com # wy≈ÇƒÖcza dzia≈Çanie Apache\u0026#39;go jako przekazujƒÖcego proxy (forwarding proxy) ProxyRequests Off # nie chcia≈Çem by b≈Çƒôdy HTTP z backendowego serwera by≈Çy przekazywane # zamiast nich bƒôdƒÖ b≈Çƒôdy Apache\u0026#39;go ProxyErrorOverride On # przyk≈Çad prostego reverse proxy - wystarczƒÖ dwa poni≈ºsze polecenia # ProxyPass proxuje ruch do danego serwera pod wskazanym URL\u0026#39;em ProxyPass /stats/ http://google-anal.com/ # ProxyPassReverse modyfikuje nag≈Ç√≥wki odpowiedzi ze zdalnego serwera # tak by odpowied≈∫ wyglƒÖda≈Ça na wys≈ÇanƒÖ z lokalnego serwera ProxyPassReverse /stats/ http://google-anal.com/ # ciekawszy przyk≈Çad proxowania z dodatkowymi ustawieniami cachowania # najpierw konfiguracja cache\u0026#39;u dyskowego \u0026lt;IfModule mod_disk_cache.c\u0026gt; # CacheRoot to wymagany parametr - ≈õcie≈ºka w kt√≥rej znajduje siƒô cache CacheRoot /var/cache/apache2/mod_disk_cache/example # poniewa≈º przewidywa≈Çem ≈ºe cache\u0026#39;owanych bƒôdzie kilka GB ma≈Çych plik√≥w # to by listowanie ich w cache\u0026#39;u by≈Ço efektywne warto wykorzystaƒá wielo # poziomowe zag≈Çƒôbienie katalog√≥w - wtedy na ka≈ºdym poziomie, w danym # katalogu bƒôdzie stosunkowo ma≈Ço plik√≥w, indeksy mniejsze, listowanie # szybsze CacheDirLevels 5 CacheDirLength 2 # teraz czas na konfiguracje cache\u0026#39;u \u0026lt;IfModule mod_cache.c\u0026gt; # pozwolƒô sobie na zignorowanie nag≈Ç√≥wk√≥w Expires/Cache-Control # z aplikacji sam lepiej wiem ≈ºe te pliki nie zmieniajƒÖ siƒô # zbyt czƒôsto CacheIgnoreCacheControl On # pliki stracƒÖ wa≈ºno≈õƒá w cache\u0026#39;u po tygodniu CacheDefaultExpire 604800 # poniewa≈º zasoby nie r√≥≈ºniƒÖ siƒô dla r√≥≈ºnych zalogowanych u≈ºytkownik√≥w # zignorujƒô cookies\u0026#39;y CacheIgnoreHeaders Set-Cookie # nie chcƒô by proxy weryfikowa≈Ço czy pojawi≈Ça siƒô nowa wersja obrazka # bo raczej rzadko pojawiajƒÖ siƒô zmiany CacheIgnoreNoLastMod On # cache uruchamiany dla dw√≥ch \u0026#34;subkatalog√≥w\u0026#34; # http://example.com/images oraz http://example.com/files CacheEnable disk /images CacheEnable disk /files \u0026lt;/IfModule\u0026gt; \u0026lt;/IfModule\u0026gt; # w≈ÇƒÖczamy mod_rewrite - bƒôdzie za chwilkƒô potrzebny RewriteEngine on # poprzednio do uruchomienia proxy wykorzysta≈Çem opcjƒô ProxyPass, # ale czƒôsto potrzebujemy bardziej zaawansowanego przekierowania # i wtedy warto wykorzystaƒá mod_rewrite do modyfikacji URL\u0026#39;i w locie # koniecznie z flagƒÖ [P] RewriteRule ^/images/(.+)/(.+) http://10.0.0.100:8080/example-images/get.php?id=$1\u0026amp;width=$2 [P] \u0026lt;Location /images\u0026gt; # jak powy≈ºej - modyfikacja zwrotnych nag≈Ç√≥wk√≥w ProxyPassReverse /example-images/ # kilka nag≈Ç√≥wk√≥w z backendu ukrywam by nie by≈Çy przekazywane dalej Header unset Server Header unset Expires Header unset ETag # akurat nag≈Ç√≥wek Via mo≈ºna wy≈ÇƒÖczyƒá w konfiguracji modu≈Çu proxy # ale poniewa≈º bywa przydatny przy debugowaniu to w niekt√≥rych miejscach # wolƒô gdy jest ustawiony - a w innych nie Header unset Via # rƒôczne ustawienie nag≈Ç√≥wka Cache-Control i zezwolenie # na cachowanie przez inne proxy lub przeglƒÖdarki Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # i drugie przekierowanie RewriteRule ^/files/(.+)/(.+) http://10.0.0.100:8080/example-files/$1/$2 [P] \u0026lt;Location /files\u0026gt; ProxyPassReverse /example-files/ Header unset Server Header unset Via Header unset ETag Header unset Expires Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # standardowa konfiguracja DocumentRoot /var/www/example \u0026lt;Directory /var/www/example\u0026gt; Options -Indexes FollowSymLinks Includes AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; LogLevel warn CustomLog /var/log/apache2/example_access.log combined \u0026lt;/VirtualHost\u0026gt; ","permalink":"https://timor.site/2012/06/apache-reverse-proxy-z-cacheowaniem/","summary":"Ostatnio trafi≈Çem na ciekawy problem, kt√≥ry wielokrotnie rozwiƒÖzywa≈Çem w nginx\u0026rsquo;ie ale tym razem musia≈Çem zrobiƒá to w Apache. Pewna stronka dzia≈Ça sobie na HTTPS\u0026rsquo;ie i chcia≈Çem by wszystkie powiƒÖzane z niƒÖ pliki by≈Çy serwowane z jej adresu szyfrowanym po≈ÇƒÖczeniem by nie pojawia≈Çy siƒô w przeglƒÖdarce monity ≈ºe \u0026ldquo;czƒô≈õƒá ruchu nie jest szyfrowana\u0026rdquo;. Tyle ≈ºe czƒô≈õƒá potrzebnych plik√≥w by≈Ça ju≈º obecnie serwowana na innym serwerze (w innej domenie) poprzez HTTP.\nMog≈Çem albo skopiowaƒá te pliki i wykombinowaƒá jaki≈õ mechanizm synchronizujƒÖcy albo wykorzystaƒá proxy + cache.","title":"Apache - reverse proxy z cache‚Äôowaniem"},{"content":"Gdy tworzymy pierwszƒÖ aplikacjƒô webowƒÖ, kt√≥ra umo≈ºliwia upload plik√≥w przewa≈ºnie lƒÖdujƒÖ one lokalnie w pewniej lokalizacji. Gdy druga aplikacja potrzebuje dostƒôpu do tych plik√≥w wystarczy podaƒá ≈õcie≈ºkƒô. Problemy zaczynajƒÖ siƒô gdy aplikacji jest kilka i rozmieszczonych na kilku serwerach. Mo≈ºna korzystaƒá z sieciowych system√≥w plik√≥w ale to czƒôsto nie jest zbyt wygodne - ciƒô≈ºko odpowiednio ustawiƒá uprawnienia by pewne aplikacje mia≈Çy dostƒôp do zapisu plik√≥w a inne nie, trzeba skonfigurowaƒá dany katalog w kilku miejscach w konfiguracji serwera WWW aby serwowaƒá pliki itp\u0026hellip;\nJe≈ºeli w utrzymaniu swoich aplikacji dociera siƒô do takiego momentu to kolejne pomys≈Çy zak≈ÇadajƒÖ wykorzystanie bazy danych do przechowywania plik√≥w. Ale chwila googlania i ju≈º wiemy ≈ºe bazy typu SQL ≈õrednio radzƒÖ sobie z przetwarzaniem tak du≈ºych rekord√≥w jak pliki. Kolejny etap to sprawdzenie co mogƒÖ nam zaoferowaƒá bazy NoSQL.\nW tym miejscu przeczyta≈Çem wiele r√≥≈ºnych artyku≈Ç√≥w i ostatecznie zastanawia≈Çem siƒô czy wybraƒá MongoDB czy CouchDB. Oba projekty sƒÖ bardzo podobne a g≈Ç√≥wnƒÖ ich zaletƒÖ jest ≈Çatwo≈õƒá wykorzystania w istniejƒÖcych aplikacjach webowych. Tutaj szczeg√≥lnie CouchDB wypada bardzo dobrze bo zarzƒÖdzanie i dostƒôp do bazy odbywa siƒô standardowym protoko≈Çem HTTP - polecenia wydaje siƒô POST\u0026rsquo;ami, a dane pobiera GET\u0026rsquo;ami. Dziƒôki takiej budowie ≈Çatwo mo≈ºna schowaƒá CouchDB za reverse proxy i serwowaƒá np. pliki uploadowane przez u≈ºytkownik√≥w wprost z bazy - bez dorabiania dodatkowych interfejs√≥w. Bardzo ≈Çatwo te≈º obs≈Çuguje siƒô bazƒô z poziomu aplikacji AJAX. Wiƒôc w moim przypadku wypad≈Ço na CouchDB.\nInstalacja Instalacja na Debianie jest banalna i sprowadza siƒô do:\napt-get install -y couchdb TADAM! Mamy dzia≈ÇajƒÖce CouchDB. Domy≈õlnie baza nas≈Çuchuje na adresie 127.0.0.1 i porcie 5984.\nDodatkowo Couch posiada webowy interfejs (zwany Futon\u0026rsquo;em) zarzƒÖdzajƒÖcy dostƒôpny na tym samym porcie pod adresem, np. http://127.0.0.1:5984/_utils/\nOptymalizacja NODELAY W moim przypadku CouchDB s≈Çu≈ºy do przechowywania zar√≥wno bardzo wielu ma≈Çych plik√≥w, jak i kilku ca≈Çkiem sporych. W przypadku bardzo ma≈Çych plik√≥w CouchDB w wersji 0.11 czeka z wys≈Çaniem odpowiedzi od razu bo byƒá mo≈ºe uda siƒô \u0026ldquo;dorzuciƒá co≈õ jeszcze\u0026rdquo;. Takie zachowanie mo≈ºe powodowaƒá op√≥≈∫nienia przy pobieraniu ma≈Çych plik√≥w, warto wiƒôc zmieniƒá w pliku /etc/couchdb/local.ini¬†takƒÖ opcjƒô:\n[httpd] socket_options = [{nodelay, true}] Ustawienie opcji TCP NODELAY wy≈ÇƒÖczy op√≥≈∫nienie przy wysy≈Çaniu ma≈Çych plik√≥w.\nIdentyfikatory Warto zastanowiƒá siƒô nad d≈Çugo≈õciƒÖ i schematem identyfikator√≥w dla rekord√≥w. Domy≈õlnie majƒÖ one 32 bajty co przy ma≈Çej liczbie element√≥w w bazie jest mocnƒÖ przesadƒÖ. Rozmiar identyfikator√≥w znacznie wp≈Çywa na rozmiar bazy i jej wydajno≈õƒá - dlatego czasem warto opracowaƒá w≈Çasny schemat generowanych identyfikator√≥w. Przyk≈Çadowo je≈õli identyfikator bƒôdzie generowany z cyfr oraz du≈ºych i ma≈Çych liter to dla 3 znak√≥w mo≈ºemy wygenerowaƒá identyfikatory dla ponad 260 tys. element√≥w, dla 4 znak√≥w ju≈º ponad 14 milion√≥w co powinno wystarczyƒá dla ≈õredniej wielko≈õci bazy.\nUstawienia bezpiecze≈Ñstwa Trochƒô mnie zaskoczy≈Ço podej≈õcie do bezpiecze≈Ñstwa w bazie CouchDB - domy≈õlnie po instalacji baza dzia≈Ça w trybie Admin Party, czyli ka≈ºda osoba kt√≥ra wejdzie do zarzƒÖdzania bez logowania ma uprawnienia administratora üòÑ\nMnie ten stan rzeczy nie bardzo odpowiada≈Ç wiƒôc:\nodpalamy interfejs zarzƒÖdzajƒÖcy - Futon i w prawym dolnym rogu szukamy tekstu: \u0026ldquo;Welcome to Admin Party! Everyone is admin. Fix this\u0026rdquo; - klikamy na \u0026ldquo;Fix this\u0026rdquo;, w okienku kt√≥re siƒô pojawi podajemy login i has≈Ço administratora.\nPo ustawieniu konta administratora dalsze musimy zadbaƒá o ustawienie odpowiednich uprawnie≈Ñ dla ka≈ºdej nowo tworzonej bazy, czyli bazy tworzƒÖ siƒô dostƒôpem dla wszystkich ale mo≈ºemy ograniczyƒá np. zapis/odczyt dla pewnych grup u≈ºytkownik√≥w.\nZabezpieczenie bazy u≈ºytkownik√≥w Po ustawieniu pierwszego u≈ºytkownika kolejna rzecz, o kt√≥rƒÖ powinni≈õmy zadbaƒá do dostƒôpna dla ka≈ºdego do odczytu baza u≈ºytkownik√≥w. Najlepiej gdy tylko administratorzy bƒôdƒÖ mieli do niej dostƒôp. By to osiƒÖgnƒÖƒá:\nw Futonie wchodzimy do bazy _users i klikamy przycisk \u0026ldquo;Security\u0026rdquo; na g√≥rze strony, w okienku kt√≥re siƒô pojawi w polu Readers -\u0026gt; Roles (pole na samym dole) wpisujemy [\u0026quot;admin\u0026quot;] Od teraz tylko administratorzy majƒÖ dostƒôp do bazy _users.\nUstawienie bazy jako tylko do odczytu To co chcia≈Çem osiƒÖgnƒÖƒá korzystajƒÖc z CouchDB to jakie≈õ repozytorium, do kt√≥rego wrzucaƒá mogƒÖ wybra≈Ñcy a czytaƒá wszyscy (np. taki CDN dla stronki itp) ale otrzymanie takiego rezultatu jest nieco\u0026hellip; nieintuicyjne. By to osiƒÖgnƒÖƒá:\nw bazie kt√≥rƒÖ chcemy ustawiƒá jako tylko do odczytu wchodzimy w Security i w polu Admin Roles (drugie z g√≥ry) wpisujemy [\u0026quot;admin\u0026quot;]¬†- to zablokuje dostƒôp do panelu Security i mo≈ºliwo≈õci modyfikowania design dokument√≥w, nadal mo≈ºliwe jest jednak dodawanie, modyfikowani i kasowanie dokument√≥w,\nby zablokowaƒá dostƒôp do zapisu w CouchDB trzeba wykorzystaƒá funkcjƒô¬†validate_doc_update kt√≥ra bƒôdzie wywo≈Çywana przy ka≈ºdej pr√≥bie dostƒôpu do pojedynczego dokumentu, by z niej skorzystaƒá tworzymy nowy pusty dokument, zmieniamy pole _id dokumentu na¬†_design/auth dodajemy pole nazwane language z warto≈õciƒÖ javascript dodajemy kolejne pole nazwane¬†validate_doc_update z warto≈õciƒÖ: function(newDoc, oldDoc, userCtx) { if (userCtx.roles.indexOf(\u0026#39;_admin\u0026#39;) !== -1) { return; } else { throw({forbidden: \u0026#39;Nie jeste≈õ administratorem!\u0026#39;}); } } zapisujemy dokument klikajƒÖc na \u0026ldquo;Save Document\u0026rdquo;\n","permalink":"https://timor.site/2012/06/couchdb-instalacja-i-wstepna-konfiguracja/","summary":"Gdy tworzymy pierwszƒÖ aplikacjƒô webowƒÖ, kt√≥ra umo≈ºliwia upload plik√≥w przewa≈ºnie lƒÖdujƒÖ one lokalnie w pewniej lokalizacji. Gdy druga aplikacja potrzebuje dostƒôpu do tych plik√≥w wystarczy podaƒá ≈õcie≈ºkƒô. Problemy zaczynajƒÖ siƒô gdy aplikacji jest kilka i rozmieszczonych na kilku serwerach. Mo≈ºna korzystaƒá z sieciowych system√≥w plik√≥w ale to czƒôsto nie jest zbyt wygodne - ciƒô≈ºko odpowiednio ustawiƒá uprawnienia by pewne aplikacje mia≈Çy dostƒôp do zapisu plik√≥w a inne nie, trzeba skonfigurowaƒá dany katalog w kilku miejscach w konfiguracji serwera WWW aby serwowaƒá pliki itp\u0026hellip;","title":"CouchDB - Instalacja i wstƒôpna konfiguracja"},{"content":"U≈ºywam LVM\u0026rsquo;a zar√≥wno na desktopie jak i wielu serwerach bo bardzo podoba mi siƒô mo≈ºliwo≈õƒá powiƒôkszenia akurat tej partycji, na kt√≥rej brakuje miejsca. O ile pamiƒôtam jak powiƒôkszyƒá partycjƒô XFS (xfs_growfs /punkt/montowania) to zawsze mam problem jak to zrobiƒá na EXT3/4, wiƒôc notujƒô.\nPowiƒôkszenie wolumenu LVM (np. o 10 gigabajt√≥w):\nlvextend -L+10G /dev/vggroup/vol Zwiƒôkszenie rozmiaru systemu plik√≥w do nowego rozmiaru wolumenu:\nresize2fs /dev/vggroup/vol Powy≈ºsze polecenie mo≈ºna wykonaƒá na zamontowanym zasobie - online.\n","permalink":"https://timor.site/2012/06/dynamiczna-zmiana-rozmiaru-partycji-ext4-na-lvmie/","summary":"U≈ºywam LVM\u0026rsquo;a zar√≥wno na desktopie jak i wielu serwerach bo bardzo podoba mi siƒô mo≈ºliwo≈õƒá powiƒôkszenia akurat tej partycji, na kt√≥rej brakuje miejsca. O ile pamiƒôtam jak powiƒôkszyƒá partycjƒô XFS (xfs_growfs /punkt/montowania) to zawsze mam problem jak to zrobiƒá na EXT3/4, wiƒôc notujƒô.\nPowiƒôkszenie wolumenu LVM (np. o 10 gigabajt√≥w):\nlvextend -L+10G /dev/vggroup/vol Zwiƒôkszenie rozmiaru systemu plik√≥w do nowego rozmiaru wolumenu:\nresize2fs /dev/vggroup/vol Powy≈ºsze polecenie mo≈ºna wykonaƒá na zamontowanym zasobie - online.","title":"Dynamiczna zmiana rozmiaru partycji EXT4 na LVM‚Äôie"},{"content":"W tym po≈õcie nie rozpiszƒô siƒô zbytnio - wrzucam tylko config od kt√≥rego zaczynam konfiguracjƒô nginx\u0026rsquo;a.\nuser www-data; worker_processes 4; pid /var/run/nginx.pid; events { worker_connections 1024; ## zaakceptuj tak du≈ºo po≈ÇƒÖcze≈Ñ jak to mo≈ºliwe multi_accept on; ## epoll jest preferowany na jajkach od 2.6 ## http://www.kegel.com/c10k.html#nb.epoll use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## opcje TCP sendfile on; tcp_nopush on; tcp_nodelay on; ## maksymalny rozmiar zapytnia client_max_body_size 10m; ## timeout\u0026#39;y client_body_timeout 60; client_header_timeout 60; keepalive_timeout 10; send_timeout 60; ## kompresja gzip on; gzip_static on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml; ## bezpiecze≈Ñstwo ## security by obscurity - ukrywamy wersjƒô nginx\u0026#39;a server_tokens off; ignore_invalid_headers on; ## resetuj zbyt d≈Çugie po≈ÇƒÖczenia - powinno pom√≥c na slowlorisa reset_timedout_connection on; ## w≈ÇƒÖczenie ochrony przed clickjackingiem - uruchamiam to per vhost ## https://developer.mozilla.org/en/The_X-FRAME-OPTIONS_response_header #add_header X-Frame-Options SAMEORIGIN; ## potrzebne zeby zbudowac mape ponizej map_hash_bucket_size 256; ## blacklist\u0026#39;a bot√≥w i referer\u0026#39;√≥w include blacklist.conf; ## # je≈õli po≈ÇƒÖczenie na HTTPS\u0026#39;a to ustawiamy zmiennƒÖ by mog≈Ço byƒá obs≈Çu≈ºone ## map $scheme $server_https { default off; https on; } include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } No i jeszcze zawarto≈õƒá blacklist.conf - na poczƒÖtek wystarczy a mo≈ºna rozszerzyƒá pod siebie:\n## blokowanie agent√≥w map $http_user_agent $bad_bot { default 0; libwww-perl 1; ~(?i)(httrack|htmlparser|libwww) 1; } ## blokowanie referer√≥w map $http_referer $bad_referer { default 0; ~(?i)(babes|click|diamond|forsale|girl|jewelry|love|nudit|organic|poker|porn|poweroversoftware|sex|teen|webcam|zippo|casino|replica) 1; } ","permalink":"https://timor.site/2012/06/nginx-moj-domyslny-config/","summary":"W tym po≈õcie nie rozpiszƒô siƒô zbytnio - wrzucam tylko config od kt√≥rego zaczynam konfiguracjƒô nginx\u0026rsquo;a.\nuser www-data; worker_processes 4; pid /var/run/nginx.pid; events { worker_connections 1024; ## zaakceptuj tak du≈ºo po≈ÇƒÖcze≈Ñ jak to mo≈ºliwe multi_accept on; ## epoll jest preferowany na jajkach od 2.6 ## http://www.kegel.com/c10k.html#nb.epoll use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## opcje TCP sendfile on; tcp_nopush on; tcp_nodelay on; ## maksymalny rozmiar zapytnia client_max_body_size 10m; ## timeout\u0026#39;y client_body_timeout 60; client_header_timeout 60; keepalive_timeout 10; send_timeout 60; ## kompresja gzip on; gzip_static on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml; ## bezpiecze≈Ñstwo ## security by obscurity - ukrywamy wersjƒô nginx\u0026#39;a server_tokens off; ignore_invalid_headers on; ## resetuj zbyt d≈Çugie po≈ÇƒÖczenia - powinno pom√≥c na slowlorisa reset_timedout_connection on; ## w≈ÇƒÖczenie ochrony przed clickjackingiem - uruchamiam to per vhost ## https://developer.","title":"Nginx - m√≥j domy≈õlny config"},{"content":"Je≈õli po aktualizacji firmware na swoim firewall\u0026rsquo;u do wersji MR3 natrafisz na komunikat Warning: SQL Logging is not enabled przy dostƒôpie do log√≥w to prawdopodobnie musisz zmieniƒá ≈∫r√≥d≈Ço log√≥w dla interfejsu gui. Poni≈ºej polecenie CLI i mo≈ºliwe opcje:\nconfig log gui set logdevice {memory | disk | fortianalyzer} end Ja potrzebowa≈Çem ustawiƒá tƒô opcjƒô na fortianalyzer by uzyskaƒá dostƒôp do moich log√≥w.\n","permalink":"https://timor.site/2012/04/fortigate-warning-sql-logging-is-not-enabled/","summary":"Je≈õli po aktualizacji firmware na swoim firewall\u0026rsquo;u do wersji MR3 natrafisz na komunikat Warning: SQL Logging is not enabled przy dostƒôpie do log√≥w to prawdopodobnie musisz zmieniƒá ≈∫r√≥d≈Ço log√≥w dla interfejsu gui. Poni≈ºej polecenie CLI i mo≈ºliwe opcje:\nconfig log gui set logdevice {memory | disk | fortianalyzer} end Ja potrzebowa≈Çem ustawiƒá tƒô opcjƒô na fortianalyzer by uzyskaƒá dostƒôp do moich log√≥w.","title":"Fortigate: Warning: SQL Logging is not enabled"},{"content":"Chcia≈Çem zaimportowaƒá m√≥j certyfikat self-signed do Nokii E72 by nie krzycza≈Ça przy sprawdzaniu poczty. Potrzebowa≈Çem certyfikatu w formacie DER, a mia≈Çem w PEM - chwilƒô szuka≈Çem jak dokonaƒá konwersji, wiƒôc ku pamiƒôci zapisujƒô kilka gotowych polece≈Ñ:\nKonwersja certyfikatu z PEM na DER openssl x509 -in in.crt -inform PEM -out out.crt -outform DER Konwersja certyfikatu z DER na PEM openssl x509 -in in.crt -inform DER -out out.crt -outform DER Konwersja klucza z formatu PEM na DER openssl rsa -in in.crt -inform PEM -out out.crt -outform DER Konwersja klucza z formatu DER na PEM openssl rsa -in in.crt -inform DER -out out.crt -outform PEM Po konwersji certyfikat w formacie DER wystarczy wrzuciƒá na kartƒô i otworzyƒá z menad≈ºera plik√≥w, zainstalowaƒá.\n","permalink":"https://timor.site/2012/04/konwersja-formatu-certyfikatu-dla-telefonow-nokia/","summary":"Chcia≈Çem zaimportowaƒá m√≥j certyfikat self-signed do Nokii E72 by nie krzycza≈Ça przy sprawdzaniu poczty. Potrzebowa≈Çem certyfikatu w formacie DER, a mia≈Çem w PEM - chwilƒô szuka≈Çem jak dokonaƒá konwersji, wiƒôc ku pamiƒôci zapisujƒô kilka gotowych polece≈Ñ:\nKonwersja certyfikatu z PEM na DER openssl x509 -in in.crt -inform PEM -out out.crt -outform DER Konwersja certyfikatu z DER na PEM openssl x509 -in in.crt -inform DER -out out.crt -outform DER Konwersja klucza z formatu PEM na DER openssl rsa -in in.","title":"Konwersja formatu certyfikatu dla telefon√≥w Nokia"},{"content":"Czasami odpalam klona jakiego≈õ systemu by p√≥≈∫niej po drobnych zmianach uczyniƒá go osobnym bytem. Jednym z krok√≥w po odtworzeniu systemu jest wygenerowanie nowego zestawu kluczy dla serwera OpenSSH (by m√≥j klient ssh nie sia≈Ç warning\u0026rsquo;ami). Mo≈ºna to wykonaƒá tak:\nNajpierw kasujemy obecne klucze:\nrm /etc/ssh/ssh_host_* Teraz generujemy nowe:\ndpkg-reconfigure openssh-server I na koniec restartujemy us≈Çugƒô by za≈Çadowaƒá nowy zestaw kluczy (nie powinno to zerwaƒá obecnej sesji, ale dla pewno≈õci lepiej zadanie odpaliƒá w screen\u0026rsquo;ie):\nservice ssh restart Je≈ºeli zmieniali≈õmy klucze dla obecnego hosta to konieczne mo≈ºe byƒá usuniƒôcie nieaktualnego wpisu z ~/.ssh/known_hosts.\n","permalink":"https://timor.site/2012/02/ponowne-wygenerowanie-kluczy-serwera-openssh/","summary":"Czasami odpalam klona jakiego≈õ systemu by p√≥≈∫niej po drobnych zmianach uczyniƒá go osobnym bytem. Jednym z krok√≥w po odtworzeniu systemu jest wygenerowanie nowego zestawu kluczy dla serwera OpenSSH (by m√≥j klient ssh nie sia≈Ç warning\u0026rsquo;ami). Mo≈ºna to wykonaƒá tak:\nNajpierw kasujemy obecne klucze:\nrm /etc/ssh/ssh_host_* Teraz generujemy nowe:\ndpkg-reconfigure openssh-server I na koniec restartujemy us≈Çugƒô by za≈Çadowaƒá nowy zestaw kluczy (nie powinno to zerwaƒá obecnej sesji, ale dla pewno≈õci lepiej zadanie odpaliƒá w screen\u0026rsquo;ie):","title":"Ponowne wygenerowanie kluczy serwera OpenSSH"},{"content":"Aby wybrane systemy DomU startowa≈Çy automatycznie po restarcie hypervisora nale≈ºy podlinkowaƒá ich pliki konfiguracyjne w katalogu /etc/xen/auto po uprzednim jego utworzeniu. Przyk≈Çadowo:\nmkdir /etc/xen/auto ln -s /etc/xen/example.cfg /etc/xen/auto/example.cfg Od teraz DomU example bƒôdzie startowaƒá automatycznie.\n","permalink":"https://timor.site/2012/02/xen-ustawienie-autostartu-domu/","summary":"Aby wybrane systemy DomU startowa≈Çy automatycznie po restarcie hypervisora nale≈ºy podlinkowaƒá ich pliki konfiguracyjne w katalogu /etc/xen/auto po uprzednim jego utworzeniu. Przyk≈Çadowo:\nmkdir /etc/xen/auto ln -s /etc/xen/example.cfg /etc/xen/auto/example.cfg Od teraz DomU example bƒôdzie startowaƒá automatycznie.","title":"Xen - ustawienie autostartu DomU"},{"content":"Je≈ºeli zdecydowali≈õmy siƒô na systemu DomU w obrazach to mo≈ºemy korzystaƒá z live migration. By uruchomiƒá jej obs≈Çugƒô, trzeba w pliku /etc/xen/xend-config.sxp¬†odkomentowaƒá¬†odpowiednie linie i¬†ustawiƒá¬†adres IP:\n(xend-relocation-server yes) (xend-relocation-port 8002) (xend-relocation-address \u0026#39;10.0.10.91\u0026#39;) Wykonywanie migracji xm migrate --live nazwa-domu nazwa.lub.ip.zdalnego.hosta ","permalink":"https://timor.site/2012/02/xen-wlaczenie-live-migration/","summary":"Je≈ºeli zdecydowali≈õmy siƒô na systemu DomU w obrazach to mo≈ºemy korzystaƒá z live migration. By uruchomiƒá jej obs≈Çugƒô, trzeba w pliku /etc/xen/xend-config.sxp¬†odkomentowaƒá¬†odpowiednie linie i¬†ustawiƒá¬†adres IP:\n(xend-relocation-server yes) (xend-relocation-port 8002) (xend-relocation-address \u0026#39;10.0.10.91\u0026#39;) Wykonywanie migracji xm migrate --live nazwa-domu nazwa.lub.ip.zdalnego.hosta ","title":"Xen - W≈ÇƒÖczenie Live Migration"},{"content":"Ostatnio pisa≈Çem o konfiguracji Dom0 - dzisiaj napiszƒô o uruchamianiu DomU.\nDo instalacji DomU wykorzystujƒô skrypty z pakietu xen-tools, mo≈ºna go zainstalowaƒá poleceniem:\napt-get install xen-tools Oczywi≈õcie aby wszystko dzia≈Ça≈Ço fajnie musimy ustawiƒá kilka domy≈õlnych opcji, robimy to edytujƒÖc plik¬†/etc/xen-tools/xen-tools.conf. Lecimy po kolei:\n# Virtual machine disks are created as logical volumes in # volume group \u0026#39;universe\u0026#39; (hint: LVM storage is much faster # than file) lvm = universe Osobi≈õcie korzystam z LVM\u0026rsquo;a kt√≥ry zgodnie z hint\u0026rsquo;em jest znacznie szybszy od plik√≥w obraz√≥w. Daje do tego bardzo wygodne mo≈ºliwo≈õƒá w zarzƒÖdzaniu rozmiarami wolumen√≥w przydzielonych DomU, mo≈ºliwo≈õƒá tworzenia snapshot√≥w (np. na potrzeby backupu). Postawienie LVM\u0026rsquo;a w skr√≥cie wyglƒÖda tak:\napt-get install lvm2 pvcreate /dev/sdb1 # to tylko przyklad - samodzielnie ustal partycje :) vgcreate universe /dev/sdb1 vgchange -a y universe Pozosta≈Çe opcje i bardziej skomplikowane przypadki tworzenia LVM\u0026rsquo;a mo≈ºna znale≈∫ƒá tutaj:¬†http://tldp.org/HOWTO/LVM-HOWTO/commontask.html\nUstalamy domy≈õlne parametry dla tworzonych DomU - oczywi≈õcie zawsze mo≈ºemy je nadpisaƒá poprzez podanie innej warto≈õci w linii polece≈Ñ, np. -memory 4Gb:\nsize = 8Gb # Disk image size. memory = 512Mb # Memory size swap = 1Gb # Swap size xfs_options = noatime,nodiratime Domy≈õlnie nie ma konfiguracji dla systemu plik√≥w ext4, wystarczy dodaƒá w odpowiednich miejscach:\nfs = ext4 ext4_options = noatime,nodiratime,errors=remount-ro Ustawienia sieci:\n# Default gateway and netmask for new VMs gateway = 10.0.10.1 netmask = 255.255.255.0 broadcast = 192.168.3.255 Warto ustawiƒá domy≈õlne ustawienia sieci - choƒá ja osobi≈õcie wolƒô zawsze podawaƒá te parametry rƒôcznie.\n# When creating an image, interactively setup root password passwd = 1 Dziƒôki tej opcji na koniec tworzenia DomU zostaniemy poproszeni o podanie has≈Ça dla root\u0026rsquo;a.\n# This is most useful on 64 bit host machines, for other systems it # doesn\u0026#39;t need to be used. # arch = amd64 Domy≈õlna architektura dla tworzonych DomU - mo≈ºna przedefiniowaƒá z linii polece≈Ñ.\n# Uncomment if you wish newly created images to boot once they\u0026#39;ve been # created. # boot = 0 Ja nie lubiƒô gdy DomU uruchamiajƒÖ siƒô automatycznie po przygotowaniu bo czƒôsto rƒôcznie modyfikujƒô konfiguracjƒô i dopiero uruchamiam wirtualkƒô - stƒÖd 0.\n# Let xen-create-image use pygrub, so that the grub from the VM is used, # which means you no longer need to store kernels outside the VMs. # Keeps things very flexible. pygrub=1 role = pygrub, myconfig Ta opcja powoduje ≈ºe DomU bƒôdzie posiadaƒá w≈Çasny kernel, kt√≥ry mo≈ºe byƒá aktualizowany niezale≈ºnie od kernela hypervisora. W≈ÇƒÖczenie tej opcji wymaga dodatkowej roli (pygrub) kt√≥ra zainstaluje wymagane pakiety, etc. Niestety roli tej nie ma w standardowej instalacji wiƒôc zamieszczam poni≈ºej plik znaleziony w sieci. Edytujemy plik¬†/etc/xen-tools/role.d/pygrub:\n#!/bin/sh # # Configure the new image to be suitable for booting via pygrub # # Wejn # -- # http://wejn.org/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Update APT lists. # chroot ${prefix} /usr/bin/apt-get update # # Install the packages # set -e installDebianPackage ${prefix} perl installDebianPackage ${prefix} libklibc installDebianPackage ${prefix} klibc-utils installDebianPackage ${prefix} initramfs-tools installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #chroot ${prefix} /usr/bin/dpkg -l | grep linux-image-xen-amd64 #if [ $? -ne 0 ]; then # installDebianPackage ${prefix} linux-image-2.6-xen-686 #else # installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #fi # Force initrd if none exists echo ${prefix}/boot/initrd* | grep -q 2\\\\.6 if [ $? -ne 0 ]; then chroot ${prefix} update-initramfs -c -k `ls -1 ${prefix}/lib/modules/ | head -n 1` fi # Generate grub menu.lst LNZ=`basename \\`ls -1 ${prefix}/boot/vmlinuz*|tail -n 1\\`` RD=`basename \\`ls -1 ${prefix}/boot/initrd*|tail -n 1\\`` mkdir -p ${prefix}/boot/grub cat - \u0026lt; ${prefix}/boot/grub/menu.lst default 0 timeout 1 title Debian root (hd0,0) kernel /boot/$LNZ root=/dev/xvda2 ro initrd /boot/$RD EOF Je≈ºeli na DomU mamy zamiar g≈Ç√≥wny system plik√≥w sformatowaƒá jako XFS to pygrub nie bƒôdzie potrafi≈Ç siƒô z niego zbootowaƒá - dobrze za to dzia≈Ça z ext3 i ext4.\nRola myconfig do wstƒôpnej konfiguracji systemu DomU Rola myconfig to m√≥j w≈Çasny zestaw skrypt√≥w, kt√≥ry konfiguruje DomU w taki spos√≥b jak lubiƒô - bym nie musia≈Ç ka≈ºdorazowo traciƒá czasu na ustawienie nowego systemu pod siebie. Zawarto≈õƒá poni≈ºej:\n#!/bin/sh # # Configure DomU to my needs # # Tomasz Gagor # -- # https://gagor.pl/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Moje ulubione zrodla w sources.list # echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list # Update APT lists. chroot ${prefix} /usr/bin/apt-get update # # Zainstaluj ulubione paczki # set -e installDebianPackage ${prefix} ssh installDebianPackage ${prefix} vim installDebianPackage ${prefix} mc installDebianPackage ${prefix} bash-completion installDebianPackage ${prefix} ethtool installDebianPackage ${prefix} less installDebianPackage ${prefix} screen installDebianPackage ${prefix} postfix installDebianPackage ${prefix} apticron # # Usun paczki ktorych nie potrzebuje lub nie lubie # # PPP removeDebianPackage ${prefix} pppconfig removeDebianPackage ${prefix} pppoeconf removeDebianPackage ${prefix} pppoe removeDebianPackage ${prefix} ppp removeDebianPackage ${prefix} libpcap0.7 # edytory removeDebianPackage ${prefix} nano removeDebianPackage ${prefix} ed removeDebianPackage ${prefix} nvi # inne #removeDebianPackage ${prefix} tasksel tasksel-data #removeDebianPackage ${prefix} pciutils #removeDebianPackage ${prefix} fdutils #removeDebianPackage ${prefix} cpio # skypt automatyczne czyszczacy cache apta # zmniejsza rozmiar backupow echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh echo \u0026#39;apt-get clean\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh chmod +x ${prefix}/etc/cron.daily/apt-get-clean.sh # ethtool - zwieksza wydajnosc wirtualizowanych urzadzen sieciowych echo \u0026#34;post-up ethtool -K eth0 tx off\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/network/interfaces # konfiguracja vima tak jak lubie chroot ${prefix} /usr/sbin/update-alternatives --set editor /usr/bin/vim.basic cat \u0026lt; ${prefix}/etc/vim/vimrc.local syntax on set background=dark if has(\u0026#34;autocmd\u0026#34;) filetype plugin indent on endif set showmatch VIMRC # konfiguracja bash-autocompletion cat \u0026lt;\u0026gt; ${prefix}/etc/bash.bashrc # enable bash completion in interactive shells if [ -f /etc/bash_completion ]; then . /etc/bash_completion fi BASHCOMP echo \u0026#39;source /etc/bash.bashrc\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/profile # instalacja i konfiguracja postfixa # poni≈ºej wpisz w≈Çasny alias dla hostmastera echo \u0026#34;root: hostmaster@mojadomena.pl\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/aliases chroot ${prefix} /usr/bin/newaliases chroot ${prefix} /usr/sbin/postconf -e \u0026#34;myhostname = `cat ${prefix}/etc/hostname`\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#34;mydestination = `cat ${prefix}/etc/hostname`, `cat ${prefix}/etc/hostname`.in.veracomp.pl, localhost.localdomain, localhost\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;relayhost = mail.example.pl\u0026#39; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;myorigin = /etc/mailname\u0026#39; # dostosuj domenƒô pod siebie echo \u0026#34;`cat ${prefix}/etc/hostname`.internal.example.pl\u0026#34; \u0026gt; ${prefix}/etc/mailname # konfiguracja locales chroot ${prefix} /usr/sbin/locale-gen # konfiguracja timezone echo \u0026#39;Europe/Warsaw\u0026#39; \u0026gt; ${prefix}/etc/timezone chroot ${prefix} dpkg-reconfigure -f noninteractive tzdata # konfiguracja mc i authorized_keys ssh takie jak na dom0 cp -r /root/.mc/ ${prefix}/root/ cp -r /root/.ssh/ ${prefix}/root/ ","permalink":"https://timor.site/2012/02/xen-na-squeeze-instalowanie-i-konfiguracja-hostow-gosci-domu/","summary":"Ostatnio pisa≈Çem o konfiguracji Dom0 - dzisiaj napiszƒô o uruchamianiu DomU.\nDo instalacji DomU wykorzystujƒô skrypty z pakietu xen-tools, mo≈ºna go zainstalowaƒá poleceniem:\napt-get install xen-tools Oczywi≈õcie aby wszystko dzia≈Ça≈Ço fajnie musimy ustawiƒá kilka domy≈õlnych opcji, robimy to edytujƒÖc plik¬†/etc/xen-tools/xen-tools.conf. Lecimy po kolei:\n# Virtual machine disks are created as logical volumes in # volume group \u0026#39;universe\u0026#39; (hint: LVM storage is much faster # than file) lvm = universe Osobi≈õcie korzystam z LVM\u0026rsquo;a kt√≥ry zgodnie z hint\u0026rsquo;em jest znacznie szybszy od plik√≥w obraz√≥w.","title":"Xen na Squeeze - Instalowanie i konfiguracja host√≥w go≈õci (DomU)"},{"content":"Bardzo udany przepis - gdzie≈õ musia≈Çem go udokumentowaƒá by nie zginƒÖ≈Ç :)\nSk≈Çadniki 1 du≈ºa kaczka (2-2,5 kg) 4 zƒÖbki czosnku 1 kg jab≈Çek (w zale≈ºno≈õci od kaczki mogƒÖ wystarczyƒá 3 wiƒôksze sztuki) sok z cytryny do skropienia jab≈Çek 200 g suszonej ≈ºurawiny p√≥≈Ç butelki czerwonego wytrawnego wina rozmaryn 4 ≈Çy≈ºki miodu cynamon kardamon s√≥l pieprz Spos√≥b wykonania Kaczkƒô nacieramy solƒÖ, pieprzem, rozmarynem oraz roztartym czosnkiem i pozostawiamy na min. 3-4 godziny w lod√≥wce (mo≈ºna te≈º przygotowaƒá jƒÖ wieczorem by by≈Ça gotowa na nastƒôpny dzie≈Ñ).\nJab≈Çka obieramy, usuwamy z nich nasiona i kroimy w ≈õredniej wielko≈õci kostkƒô. Do jab≈Çek dodajemy ≈ºurawinƒô, skrapiamy ca≈Ço≈õƒá sokiem z cytryny, po czym dodajemy cynamon, kardamon i mi√≥d.\nPo wyjƒôciu z lod√≥wki wype≈Çniamy wnƒôtrze kaczki wcze≈õniej przygotowanym farszem i zaszywamy. Do rozgrzanego piekarnika (170-190 stopni) wstawiamy kaczkƒô umieszczonƒÖ w naczyniu pod przykryciem. Po oko≈Ço 1,5 godziny wyjmujemy kaczkƒô z piekarnika i z obu stron nacinamy sk√≥rƒô pomiƒôdzy udami a korpusem - umo≈ºliwi to wyp≈Çyniƒôcie zebranego pod sk√≥rƒÖ t≈Çuszczu. Co 30 minut nale≈ºy kaczkƒô podlewaƒá powsta≈Çym sosem.\nCzas pieczenia kaczki zale≈ºy od jej wagi. Wa≈ºƒÖca 1,5-2 kg sztukƒô nale≈ºy piec od 90 minut do 2 godzin. Na godzinƒô przed ko≈Ñcem pieczenia wlewamy czerwone wino (ok. p√≥≈Ç butelki). Na ostatnie 20 minut zdejmujemy pokrywƒô i prze≈ÇƒÖczamy piekarnik na zapiekanie od g√≥ry by sk√≥rka mog≈Ça siƒô zrumieniƒá.\nWynik Ca≈Çkiem pu≈õci≈Ço moje kiepskie szycie ale smaku to nie popsu≈Ço :)\n≈πr√≥d≈Ço: Ugotowani\n","permalink":"https://timor.site/2012/02/kaczka-pieczona-z-zurawina-i-jablkami/","summary":"Bardzo udany przepis - gdzie≈õ musia≈Çem go udokumentowaƒá by nie zginƒÖ≈Ç :)\nSk≈Çadniki 1 du≈ºa kaczka (2-2,5 kg) 4 zƒÖbki czosnku 1 kg jab≈Çek (w zale≈ºno≈õci od kaczki mogƒÖ wystarczyƒá 3 wiƒôksze sztuki) sok z cytryny do skropienia jab≈Çek 200 g suszonej ≈ºurawiny p√≥≈Ç butelki czerwonego wytrawnego wina rozmaryn 4 ≈Çy≈ºki miodu cynamon kardamon s√≥l pieprz Spos√≥b wykonania Kaczkƒô nacieramy solƒÖ, pieprzem, rozmarynem oraz roztartym czosnkiem i pozostawiamy na min. 3-4 godziny w lod√≥wce (mo≈ºna te≈º przygotowaƒá jƒÖ wieczorem by by≈Ça gotowa na nastƒôpny dzie≈Ñ).","title":"Kaczka pieczona z ≈ºurawinƒÖ i jab≈Çkami"},{"content":"Po co mi to? Wiele razy mia≈Çem do czynienia z serwerami na kt√≥rych dzia≈Ça≈Ço kilka/kilkana≈õcie us≈Çug r√≥wnocze≈õnie, np. Apache (kilka stronek, webmail, phpmyadmin, itp), Postfix/Exim (poczta i ≈ºeby by≈Ço fajnie to na kontach systemowych), Samba (jakie≈õ zasoby dla pracownik√≥w), MySQL (baza dla stronek), PostgreSQL (bo jedna stronka potrzebowa≈Ça), itd\u0026hellip;. Przy takiej konfiguracji pomijane sƒÖ kwestie separacji us≈Çug zewnƒôtrznych/wewnƒôtrznych - no ale firma/instytucja ma≈Ça nie ma sensu kasy na 3 kolejne serwery wydawaƒá skoro dzia≈Ça\u0026hellip;.\nTaka konfiguracja nie jest zbyt bezpieczna i ma wiele wad:\nciƒô≈ºko backupowaƒá tak du≈ºy system w postaci obraz√≥w, a odzyskiwanie z backup√≥w ca≈Ço≈õci bƒôdzie trwaƒá wieki, s≈Çaba separacja us≈Çug u≈Çatwia ataki na zasoby wewnƒôtrzne (np. dziura w stronie mo≈ºe pozwoliƒá na dostƒôp do plik√≥w z Samby), trudno rozgraniczyƒá zasoby sprzƒôtowe (procesor, pamiƒôƒá) konkretnym us≈Çugom, trudno jest migrowaƒá us≈Çugi na inne serwery. Wirtualizacja pozwala na zminimalizowanie tych problem√≥w:\nbackupowaƒá mo≈ºna pojedyncze us≈Çugi (dzia≈ÇajƒÖce na r√≥≈ºnych maszynach wirtualnych) np. z pomocƒÖ snapshot√≥w z minimalnym czasem przerwy w dzia≈Çaniu, oddzielenie us≈Çug w osobnych maszynach wirtualnych pozwala \u0026ldquo;zamknƒÖƒá\u0026rdquo; napastnika w przypadku kompromitacji kt√≥rej≈õ z us≈Çug (choƒá zdarza≈Çy siƒô b≈Çƒôdy w implementacji maszyn wirtualnych pozwalajƒÖce \u0026ldquo;wyskoczyƒá\u0026rdquo; z wirtualki), mo≈ºemy przydzielaƒá pamiƒôƒá i konkretne rdzenie procesora danym maszynom wirtualnym, live migration pozwala w locie przenie≈õƒá maszynƒô wirtualnƒÖ na inny serwer. Xen jest szczeg√≥lnie dobrym wyborem je≈ºeli zamierzamy wirtualizowaƒá systemy Linux\u0026rsquo;owe. Mo≈ºna je uruchamiaƒá w trybie para-wirtualizacji, kt√≥ra w mniejszym stopniu ni≈º pe≈Çna wirtualizacja obciƒÖ≈ºa CPU.\nW trybie pe≈Çnej wirtualizacji mo≈ºna zainstalowaƒá praktycznie dowolny system (r√≥wnie≈º Windows) choƒá jest to tryb mniej wydajny.\nUwaga Post ten jest pr√≥bƒÖ zebrania w jednym miejscu r√≥≈ºnych moich do≈õwiadcze≈Ñ z Xen\u0026rsquo;em ale nie mia≈Çem czasu na testowƒÖ instalacjƒô wed≈Çug poni≈ºszego tekstu, wiƒôc mog≈Çem gdzie≈õ co≈õ zamieszaƒá. Dlatego proszƒô o komentowanie je≈õli co≈õ nie zadzia≈Ça.\nKonfiguracja hypervisor\u0026rsquo;a (Dom0) Je≈ºeli zastanawiasz siƒô czy stawiaƒá Xen\u0026rsquo;a na 32 czy 64-bitowym systemie to wybierz 64-bit. Na systemie 64-bitowym mo≈ºna uruchamiaƒá systemy go≈õci 32 i 64-bitowe (w przeciwie≈Ñstwie do hypervisora 32-bitowego) i nie bƒôdzie problem√≥w z obs≈ÇugƒÖ du≈ºej ilo≈õci pamiƒôci.\nZaczynamy od instalacji hypervisor\u0026rsquo;a, zmodyfikowanej do dzia≈Çania z Xen wersji jƒÖdra i pakietu podstawowych narzƒôdzi. Wszystko co potrzebne jest w jednym metapakiecie (xen-linux-system ale gdyby nie chcia≈Ç i≈õƒá to lepiej wskazaƒá konkretnƒÖ wersjƒô):\napt-get install xen-linux-system-2.6-xen-amd64 By mieƒá dostƒôp do pe≈Çnej wirtualizacji (np. by uruchamiaƒá systemy Windows) nale≈ºy doinstalowaƒá jeszcze jednƒÖ paczkƒô (je≈õli nie potrzebujemy to zawsze mo≈ºna doinstalowaƒá p√≥≈∫niej):\napt-get install xen-qemu-dm-4.0 Squeeze domy≈õlnie korzysta z GRUB\u0026rsquo;a 2 i niezbyt przeze mnie lubianej metody wykrywania system√≥w, kt√≥ra po zainstalowaniu jajka dla Xen\u0026rsquo;a nadal bƒôdzie domy≈õlnie startowaƒá podstawowy kernel. By to zmieniƒá proponujƒô przenie≈õƒá wykrywanie domy≈õlnego systemu na pozycjƒô p√≥≈∫niejszƒÖ ni≈º Xen\u0026rsquo;a - zmieniamy nazwƒô pliku:\nmv /etc/grub.d/10_linux /etc/grub.d/21_linux Ponadto je≈ºeli zamierzamy instalowaƒá DomU na LVM\u0026rsquo;ie to na pewno zainteresuje nas wy≈ÇƒÖczenie opcji OS prober by DomU nie by≈Çy wykrywane jako zainstalowane lokalnie systemy (wy≈ÇƒÖczy to te≈º wykrywanie Windowsa je≈õli jest zainstalowany na tej samej maszynie).\nBy wy≈ÇƒÖczyƒá OS prober\u0026rsquo;a otw√≥rz plik /etc/default/grub i dodaj na ko≈Ñcu:\n# Wylaczenie OS prober\u0026#39;a by nie wykrywac maszyn wirtualnych na # logicznych wolumenach LVM i nie pokazywac ich w menu grub\u0026#39;a GRUB_DISABLE_OS_PROBER=true Je≈ºeli chcesz dodaƒá jakie≈õ dodatkowe parametry przy bootowaniu Xen\u0026rsquo;a dodaj poni≈ºsze zmienne w /etc/default/grub¬†(przypisane warto≈õci nie sƒÖ prawid≈Çowe i je≈õli nic nie potrzebujesz dodawaƒá to pomi≈Ñ ten krok):\n# parametry dla wszystkich opcji startowych Xen\u0026#39;a (rowniez recovery) GRUB_CMDLINE_XEN=\u0026#34;something\u0026#34; # dodatkowe parametry dla opcji non-recovery GRUB_CMDLINE_XEN_DEFAULT=\u0026#34;something else\u0026#34; Po modyfikacji opcji gruba musimy wykonaƒá aktualizacjƒô poleceniem:\nupdate-grub Domy≈õlnym zachowaniem dom0 Xen\u0026rsquo;a przy wy≈ÇƒÖczaniu bƒÖd≈∫ restartowaniu jest pr√≥ba zahibernowania wszystkich dzia≈ÇajƒÖcych domU. Gdy nie przewidzimy takiej sytuacji i nie mamy wystarczajƒÖcej ilo≈õci wolnego miejsca w /var czƒôsto proces ten ko≈Ñczy siƒô b≈Çƒôdami. ZdarzajƒÖ siƒô te≈º problemy z prawid≈ÇowƒÖ obs≈ÇugƒÖ hibernacji po stronie system√≥w dzia≈ÇajƒÖcych w domU, wtedy po uruchomieniu dom0 i pr√≥bie za≈Çadowania zapisanych stan√≥w maszyn ko≈Ñczymy z wiszƒÖcymi wirtualkami.\nNa dobrƒÖ sprawƒô r√≥wnie dobrym zachowaniem by≈Çoby wy≈ÇƒÖczenie wszystkich domU i zaczekanie a≈º to nastƒÖpi a po starcie dom0 uruchomienie ich od zera. Mo≈ºe ca≈Çy proces bƒôdzie trwaƒá chwilƒô d≈Çu≈ºej ale mamy za to wiƒôkszƒÖ pewno≈õƒá ≈ºe przejdzie bez niespodzianek. By ustawiƒá takie zachowanie Xen\u0026rsquo;a edytujemy /etc/default/xendomains:\nXENDOMAINS_RESTORE=false XENDOMAINS_SAVE= Aby systemy domU mia≈Çy dostƒôp do sieci nale≈ºy w pliku /etc/xen/xend-config.sxp¬†upewniƒá siƒô ≈ºe network-script ustawione jest na network-bridge.\n(network-script \u0026#39;network-bridge antispoof=yes\u0026#39;) Przy takim ustawieniu systemu domU bƒôdƒÖ korzystaƒá z g≈Ç√≥wnego interfejsu dom0 aby uzyskaƒá dostƒôp do sieci. Dla wielu bƒôdzie to niezbyt optymalne rozwiƒÖzanie ale na poczƒÖtek wystarczy (niecierpliwi mogƒÖ zerknƒÖƒá do dokumentacji Xen\u0026rsquo;a aby sprawdziƒá inne opcje XenNetworking).\nDodatkowa opcja antispoof=yes aktywuje firewall, kt√≥ry uniemo≈ºliwi systemom domU przypisywanie sobie adres√≥w innych system√≥w domU. By to dzia≈Ça≈Ço w konfiguracji maszyny wirtualnej w definicji interfejsu (vif) nale≈ºy przypisaƒá adres IP danemu systemowi domU.\nNa moim systemie nie wszystkie skrypty dla konfiguracji sieci mia≈Çy atrybut do wykonywania co skutkowa≈Ço b≈Çƒôdami przy uruchamianiu domU (np. \u0026ldquo;missing vif-script, or network-script, in the Xen configuration file\u0026rdquo;). By temu zapobiec ustawmy wszystkim skryptom odpowiednie uprawnienia:\nchmod +x /etc/xen/scripts/* W pliku /etc/xen/xend-config.sxp mo≈ºna ponadto ustawiƒá jak du≈ºo pamiƒôci i procesor√≥w rezerwujemy dla systemu dom0. Ilo≈õƒá pamiƒôci dla dom0 mo≈ºna ograniczyƒá dodajƒÖc opcjƒô dom0_mem dla kernela w zmiennej¬†GRUB_CMDLINE_XEN w konfiguracji gruba.\nNiekt√≥rzy zalecajƒÖ wy≈ÇƒÖczenie dom0-balloning (czyli zajmowania przez dom0 ca≈Çej pozosta≈Çej pamiƒôci po uruchomieniu system√≥w domU), a zamiast tego przydzielenie minimalnej ilo≈õci pamiƒôci, np. tak:\n(dom0-min-mem 1024) (enable-dom0-ballooning no) Ja osobi≈õcie wolƒô pozostawiƒá balloning w≈ÇƒÖczony - po co blokowaƒá ram skoro nikt inny z niego nie skorzysta, a tak to przynajmniej jest wiƒôcej na bufory dyskowe üòâ\nUstawienie konsoli By mieƒá dostƒôp do wyj≈õcia z GRUB\u0026rsquo;a, Xen hypervisor\u0026rsquo;a, kernela i getty poprzez VGA i konsolƒô nale≈ºy dokonaƒá zmian w pliku¬†/etc/default/grub¬†jak poni≈ºej:\nGRUB_SERIAL_COMMAND=\u0026#34;serial --unit=0 --speed=9600 --word=8 --parity=no --stop=1\u0026#34; GRUB_TERMINAL=\u0026#34;console serial\u0026#34; GRUB_TIMEOUT=5 GRUB_CMDLINE_XEN=\u0026#34;com1=9600,8n1 console=com1,vga\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;console=tty0 console=hvc0\u0026#34; A nastƒôpnie w /etc/inittab aktualizujemy linijki:\n1:2345:respawn:/sbin/getty 38400 hvc0 2:23:respawn:/sbin/getty 38400 tty1 # NO getty on ttyS0! W ten spos√≥b tty1 pokazuje wyj≈õcie VGA, a hvc0 kosolƒô. Standardowo po modyfikacji opcji gruba musimy wykonaƒá aktualizacjƒô poleceniem:\nupdate-grub Je≈õli nic nie skopali≈õmy to po restarcie powinni≈õmy otrzymaƒá hypervisora gotowego do uruchamiania DomU.\n≈πr√≥d≈Ça http://wiki.debian.org/Xen ","permalink":"https://timor.site/2012/02/xen-na-squeeze-instalacja-i-konfiguracja-hypervisora/","summary":"Po co mi to? Wiele razy mia≈Çem do czynienia z serwerami na kt√≥rych dzia≈Ça≈Ço kilka/kilkana≈õcie us≈Çug r√≥wnocze≈õnie, np. Apache (kilka stronek, webmail, phpmyadmin, itp), Postfix/Exim (poczta i ≈ºeby by≈Ço fajnie to na kontach systemowych), Samba (jakie≈õ zasoby dla pracownik√≥w), MySQL (baza dla stronek), PostgreSQL (bo jedna stronka potrzebowa≈Ça), itd\u0026hellip;. Przy takiej konfiguracji pomijane sƒÖ kwestie separacji us≈Çug zewnƒôtrznych/wewnƒôtrznych - no ale firma/instytucja ma≈Ça nie ma sensu kasy na 3 kolejne serwery wydawaƒá skoro dzia≈Ça\u0026hellip;.","title":"Xen na Squeeze - instalacja i konfiguracja hypervisor‚Äôa"},{"content":"Jaki≈õ czas temu korzysta≈Çem z preload\u0026rsquo;a kt√≥ry sam uczy≈Ç siƒô jakie aplikacje odpalam i te programy ≈Çadowa≈Ç ju≈º podczas startu - przewa≈ºnie nieco spowalnia to start systemu ale gdy ju≈º siƒô za≈Çaduje to programy, kt√≥re uruchamiam jako pierwsze startujƒÖ \u0026ldquo;z kopa\u0026rdquo;. Od jakiego≈õ czasu popularniejszy jest instalowany domy≈õlnie w Ubuntu ureadahead - pe≈Çni on podobnƒÖ funkcjƒô jak preload.\nMo≈ºna zmusiƒá ureadahead do ponownego wygenerowania nowej listy program√≥w wczytywanych przy starcie do cache a oto jak zrobiƒá:\nNale≈ºy skasowaƒá pliki z rozszerzeniem pack w /var/lib/ureadahead/: sudo rm /var/lib/ureadahead/*.pack Mo≈ºna ustawiƒá autmatyczne logowanie. Restartujemy system. Szybko logujemy siƒô do systemu i uruchomiamy aplikacje, kt√≥re chcemy aby szybciej startowa≈Çy. Gdy wszystko siƒô ju≈º za≈Çaduje sprawdzamy czy za≈Çadowa≈Çy siƒô wszystkie programy, na kt√≥rych nam zale≈ºa≈Ço, np.: sudo ureadahead --dump | grep firefox 6.Je≈ºeli nie za≈Çadowa≈Çy siƒô wszystkie programy to w pliku¬†/etc/init/ureadahead.conf zwiƒôkszamy warto≈õƒá w linii:\npre-stop exec sleep 45 na np. 90 i wracamy do punktu 1.\n","permalink":"https://timor.site/2012/01/wstepne-ladowanie-programow-przy-starcie-z-ureadahead/","summary":"Jaki≈õ czas temu korzysta≈Çem z preload\u0026rsquo;a kt√≥ry sam uczy≈Ç siƒô jakie aplikacje odpalam i te programy ≈Çadowa≈Ç ju≈º podczas startu - przewa≈ºnie nieco spowalnia to start systemu ale gdy ju≈º siƒô za≈Çaduje to programy, kt√≥re uruchamiam jako pierwsze startujƒÖ \u0026ldquo;z kopa\u0026rdquo;. Od jakiego≈õ czasu popularniejszy jest instalowany domy≈õlnie w Ubuntu ureadahead - pe≈Çni on podobnƒÖ funkcjƒô jak preload.\nMo≈ºna zmusiƒá ureadahead do ponownego wygenerowania nowej listy program√≥w wczytywanych przy starcie do cache a oto jak zrobiƒá:","title":"Wstƒôpne ≈Çadowanie program√≥w przy starcie z ureadahead"},{"content":"Aby umo≈ºliwiƒá odwiedzajƒÖcym nasze strony cachowanie obrazk√≥w (tak by nie musieli pobieraƒá ich ka≈ºdorazowo bo przecie≈º nie zmieniajƒÖ siƒô a≈º tak czƒôsto) konieczne jest ustawienie nag≈Ç√≥wk√≥w: Cache-Control, Expires dla odpowiednich typ√≥w plik√≥w. W Apachem jest do tego dedykowany modu≈Ç - mod_expires. W Debianie dostarczany jest on bez domy≈õlnej globalnej konfiguracji - a ja lubiƒô gdy cacheuje mi siƒô wiƒôkszo≈õƒá statyki. Zawsze mo≈ºna dostosowaƒá czas cachowania pod siebie wzglƒôdem okre≈õlonego typu pliku, np. dla Java Script√≥w ustawiƒá na 1 dzie≈Ñ gdy czƒôsto siƒô zmieniajƒÖ. Mo≈ºna te≈º w samej aplikacji zmieniaƒá ≈õcie≈ºkƒô do pliku by wymusiƒá od≈õwie≈ºenie (lub podawaƒá ≈õcie≈ºkƒô z jakim≈õ losowym identyfikatorem wycinanym przez mod_rewrite).\nCache-Control, Expires Najpierw tworzymy globalny config (oczywi≈õcie mo≈ºna sobie robiƒá te≈º taki per host - jak kto woli):\ncat \u0026gt; /etc/apache2/mods-available/expires.conf \u0026lt; \u0026lt;CONF \u0026lt;IfModule mod_expires.c\u0026gt; # w≈ÇƒÖczamy modu≈Ç ExpiresActive on # cacheowanie typowych obraz√≥w przez 3 miesiƒÖce ExpiresByType image/jpg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/gif \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/jpeg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/png \u0026#34;access plus 3 months\u0026#34; # cacheowanie CSS i JavaScript przez 2-1 miesiƒÖca ExpiresByType text/css \u0026#34;access plus 2 month\u0026#34; ExpiresByType text/javascript \u0026#34;access plus 1 day\u0026#34; ExpiresByType application/javascript \u0026#34;access plus 1 day\u0026#34; # domy≈õlna warto≈õƒá cachowania ExpiresDefault \u0026#34;access plus 3 months\u0026#34; CONF Dok≈Çadniejsze wyja≈õnienie sk≈Çadni mo≈ºna znale≈∫ƒá tutaj. Teraz aktywujemy modu≈Ç i restartujemy Apache\u0026rsquo;go (nie jestem pewien czy sam reload by wystarczy≈Ç):\na2enmod expires service apache2 restart I tyle - je≈õli sprawdzimy nag≈Ç√≥wki (Firebug w Firefoxie albo Narzƒôdzia programistyczne w Chromie) dla odpowiednich typ√≥w plik√≥w to powinny one zawieraƒá przyk≈Çadowo takie warto≈õci:\nCache-Control:¬†max-age=7776000 Expires:¬†Thu, 19 Apr 2012 15:48:13 GMT FileETag UstawiajƒÖc powy≈ºszƒÖ konfiguracjƒô warto wy≈ÇƒÖczyƒá generowanie nag≈Ç√≥wka ETag. Co prawda w dawnym zamy≈õle mia≈Ç to byƒá mechanizm pozwalajƒÖcy zbli≈ºony do cachowania a dajƒÖcy 100% pewno≈õci ≈ºe nowy plik zostanie pobrany a aktualne w kopie w cache ju≈º nie. Polega na obliczeniu hasha na podstawie \u0026ldquo;pewnych\u0026rdquo; danych o pliku (np.¬†INode, data modyfikacji, rozmiar) - je≈ºeli posiadasz aktualny hash to nie musisz pobieraƒá danych.\nPomys≈Ç na poczƒÖtku nie by≈Ç z≈Çy\u0026hellip; ale obecne strony czƒôsto udostƒôpniajƒÖ nawet kilkadziesiƒÖt (i wiƒôcej) niedu≈ºych plik√≥w. By pobraƒá potrzebne do wygenerowania ETag\u0026rsquo;a dane trzeba wywo≈Çaƒá operacjƒô stat na pliku co przy wielu r√≥wnoczesnych odwo≈Çaniach potrafi mocno obciƒÖ≈ºyƒá dyski.\nNa serwerach ze ≈õrednim i du≈ºym obciƒÖ≈ºeniem warto wy≈ÇƒÖczyƒá generowanie nag≈Ç√≥wka w ten spos√≥b:\necho \u0026#34;FileETag None\u0026#34; \u0026gt;\u0026gt; /etc/apache2/httpd.conf service apache2 reload ","permalink":"https://timor.site/2012/01/apache-mod_expires-konfiguracja/","summary":"Aby umo≈ºliwiƒá odwiedzajƒÖcym nasze strony cachowanie obrazk√≥w (tak by nie musieli pobieraƒá ich ka≈ºdorazowo bo przecie≈º nie zmieniajƒÖ siƒô a≈º tak czƒôsto) konieczne jest ustawienie nag≈Ç√≥wk√≥w: Cache-Control, Expires dla odpowiednich typ√≥w plik√≥w. W Apachem jest do tego dedykowany modu≈Ç - mod_expires. W Debianie dostarczany jest on bez domy≈õlnej globalnej konfiguracji - a ja lubiƒô gdy cacheuje mi siƒô wiƒôkszo≈õƒá statyki. Zawsze mo≈ºna dostosowaƒá czas cachowania pod siebie wzglƒôdem okre≈õlonego typu pliku, np.","title":"Apache mod_expires konfiguracja"},{"content":"Objaw przewa≈ºnie jest taki: ≈ÇƒÖczysz siƒô po ssh podajƒÖc klucz/has≈Ço i czekasz nawet i 10 sekund a≈º pojawi siƒô prompt. Po po≈ÇƒÖczeniu wszystkie polecenia dzia≈ÇajƒÖ z normalnƒÖ szybko≈õciƒÖ. Brzmi znajomo? üòâ\nTaki objaw przewa≈ºnie jest skutkiem problem√≥w z dzia≈Çaniem DNS\u0026rsquo;a po stronie klienta lub serwera. Warto sprawdziƒá poleceniami host/dig/nslookup po obu stronach jak du≈ºo czasu potrzeba na rozwiƒÖzanie nazw. Najlepiej rozwiƒÖzaƒá problem z DNS\u0026rsquo;em ustawiajƒÖc szybkie serwery ale gdy nie mamy takiej mo≈ºliwo≈õci to po stronie serwera mo≈ºna ustawiƒá w /etc/sshd_config opcjƒô:\nUseDNS no I restart ssh:\nservice ssh restart Spowoduje to wy≈ÇƒÖczenie znacznej czƒô≈õci zapyta≈Ñ DNS po stronie serwera (w tym sprawdzanie reverse DNS\u0026rsquo;a dla hosta klienta w momencie ≈ÇƒÖczenia). Na kilku serwerach z kiepskim DNS\u0026rsquo;em opcja ta \u0026ldquo;daje niez≈Çego kopa\u0026rdquo;.\n","permalink":"https://timor.site/2012/01/dlugie-oczekiwanie-na-nawiazanie-polaczenia-ssh/","summary":"Objaw przewa≈ºnie jest taki: ≈ÇƒÖczysz siƒô po ssh podajƒÖc klucz/has≈Ço i czekasz nawet i 10 sekund a≈º pojawi siƒô prompt. Po po≈ÇƒÖczeniu wszystkie polecenia dzia≈ÇajƒÖ z normalnƒÖ szybko≈õciƒÖ. Brzmi znajomo? üòâ\nTaki objaw przewa≈ºnie jest skutkiem problem√≥w z dzia≈Çaniem DNS\u0026rsquo;a po stronie klienta lub serwera. Warto sprawdziƒá poleceniami host/dig/nslookup po obu stronach jak du≈ºo czasu potrzeba na rozwiƒÖzanie nazw. Najlepiej rozwiƒÖzaƒá problem z DNS\u0026rsquo;em ustawiajƒÖc szybkie serwery ale gdy nie mamy takiej mo≈ºliwo≈õci to po stronie serwera mo≈ºna ustawiƒá w /etc/sshd_config opcjƒô:","title":"D≈Çugie oczekiwanie na nawiƒÖzanie po≈ÇƒÖczenia ssh"},{"content":"Zawsze gdy potrzebujƒô zesniffowaƒá co≈õ na ≈ºywo na Fortigate\u0026rsquo;ach muszƒô przeszukaƒá Knowledge Base by przypomnieƒá sobie wszystkie polecenia do tego potrzebne. Tym razem robiƒô notatki üòÉ\nSniffowanie diagnose debug enable diagnose debug flow filter addr¬†ip address clear¬†clear filter daddr¬†dest ip address dport¬†destination port negate¬†inverse filter port¬†port proto¬†protocol number saddr¬†source ip address sport¬†source port vd¬†index of virtual domain # np. diagnose debug flow filter saddr 10.10.80.3 diagnose debug flow filter daddr 8.8.8.8 diagnose debug flow filter dport 53 # wy≈õwietl wyniki na konsoli diagnose debug flow show console enable # opcjonalne: wy≈õwietla nazwy funkcji np. odwo≈Çania do routingu, itp diagnose debug flow show function-name enable # uruchomienie sniffowania - warto podaƒá na ko≈Ñcu jaka≈õ warto≈õƒá # by sniffowanie zako≈Ñczy≈Ço siƒô po takiej liczbie pakiet√≥w # w przeciwnym wypadku wyniki bƒôdƒÖ siƒô wypisywaƒá na konsoli # a≈º uda nam siƒô na o≈õlep wy≈ÇƒÖczyƒá sniffowanie diagnose debug flow trace start 100 # zresetowanie filtrowania flow diagnose debug reset # wy≈ÇƒÖczenie sniffowania diagnose debug disable Diagnostyka tuneli IP-Sec # tutaj jest du≈ºo pro≈õciej, najpierw w≈ÇƒÖczamy debuga diagnose debug enable # a potem diagnose debug application ike 2 # lub dla bardzo, bardzo szczeg√≥≈Çowych log√≥w diagnose debug application ike -1 Niestety nie ma tutaj mo≈ºliwo≈õci filtrowania (albo jeszcze o tym nie wiem), wiƒôc je≈õli mamy du≈ºo aktywnych tuneli to najlepiej zbieraƒá wypisywane komunikaty do pliku i dopiero przeglƒÖdaƒá.\n","permalink":"https://timor.site/2012/01/sniffowanie-w-fortios/","summary":"Zawsze gdy potrzebujƒô zesniffowaƒá co≈õ na ≈ºywo na Fortigate\u0026rsquo;ach muszƒô przeszukaƒá Knowledge Base by przypomnieƒá sobie wszystkie polecenia do tego potrzebne. Tym razem robiƒô notatki üòÉ\nSniffowanie diagnose debug enable diagnose debug flow filter addr¬†ip address clear¬†clear filter daddr¬†dest ip address dport¬†destination port negate¬†inverse filter port¬†port proto¬†protocol number saddr¬†source ip address sport¬†source port vd¬†index of virtual domain # np. diagnose debug flow filter saddr 10.","title":"Sniffowanie w FortiOS"},{"content":"Co jaki≈õ czas powtarza siƒô sytuacja, gdy muszƒô zaktualizowaƒá jaki≈õ serwerek z Lennym do Squeeze\u0026rsquo;a i za ka≈ºdym razem muszƒô googlaƒá za odpowiednimi ≈∫r√≥d≈Çami, kt√≥re paczki najpierw, etc\u0026hellip; Wiƒôc sobie zebra≈Çem wszystko w poni≈ºszym po≈õcie.\nW razie wƒÖtpliwo≈õci patrz tutaj:¬†http://www.debian.org/releases/squeeze/releasenotes\nZr√≥b backup konfiguracji.\nTrzeba zaktualizowaƒá ≈∫r√≥d≈Ça by wskazywa≈Çy na squeeze\u0026rsquo;a (poni≈ºsze polecenie nadpisze Twoje obecne repozytoria):\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb http://security.debian.org/ squeeze/updates main contrib non-free deb-src http://security.debian.org/ squeeze/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free SRC Teraz trzeba od≈õwie≈ºyƒá repozytoria:\nsudo apt-get update Proponujƒô pobraƒá te≈º pliki by podczas aktualizacji wszystkie le≈ºa≈Çy w cache\u0026rsquo;u - na wypadek gdyby nagle pad≈Ço ≈ÇƒÖcze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz mo≈ºna zaktualizowaƒá kluczowe paczki:\nsudo apt-get install apt dpkg I aktualizacja ca≈Çego systemu:\nsudo apt-get dist-upgrade I mo≈ºna braƒá siƒô do ≈Çatania bug√≥w w starej konfiguracji\u0026hellip; üòâ\n","permalink":"https://timor.site/2012/01/upgrade-debian-lenny-do-squeeze/","summary":"Co jaki≈õ czas powtarza siƒô sytuacja, gdy muszƒô zaktualizowaƒá jaki≈õ serwerek z Lennym do Squeeze\u0026rsquo;a i za ka≈ºdym razem muszƒô googlaƒá za odpowiednimi ≈∫r√≥d≈Çami, kt√≥re paczki najpierw, etc\u0026hellip; Wiƒôc sobie zebra≈Çem wszystko w poni≈ºszym po≈õcie.\nW razie wƒÖtpliwo≈õci patrz tutaj:¬†http://www.debian.org/releases/squeeze/releasenotes\nZr√≥b backup konfiguracji.\nTrzeba zaktualizowaƒá ≈∫r√≥d≈Ça by wskazywa≈Çy na squeeze\u0026rsquo;a (poni≈ºsze polecenie nadpisze Twoje obecne repozytoria):\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb http://security.","title":"Upgrade Debian Lenny do Squeeze"},{"content":"Je≈ºeli jeste≈õ w≈Ça≈õcicielem Skody Fabii I to wcze≈õniej czy p√≥≈∫niej Tw√≥j BAT-Mobil zg≈Çosi kt√≥re≈õ z ostrze≈ºe≈Ñ serwisowych:\nOIL - pojawia siƒô przewa≈ºnie co 10 tys. km i wtedy gdy jeszcze nie trzeba wymieniaƒá oleju w silniku üòÑ service INSP - nie wiem jak czƒôsto siƒô pojawia a oznacza mniej wiƒôcej \u0026ldquo;czas wesprzeƒá finansowo lokalny autoryzowany serwis\u0026rdquo;. Kasowanie ostrze≈ºenia \u0026ldquo;OIL\u0026rdquo; Przy wy≈ÇƒÖczonym silniku wciskamy i przytrzymujemy przycisk kasowania¬†przebiegu dziennego/pokrƒôt≈Ço ustawiania godziny, od teraz nazywany po¬†prostu \u0026lsquo;przyciskiem\u0026rsquo;. Ca≈Çy czas przytrzymujƒÖc \u0026lsquo;przycisk\u0026rsquo;, w≈ÇƒÖczamy zap≈Çon. Powinni≈õmy zobaczyƒá napis OIL, puszczamy \u0026lsquo;przycisk\u0026rsquo;. Przekrƒôcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, a≈º zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Mo≈ºemy wy≈ÇƒÖczyƒá zap≈Çon. Kasowanie ostrze≈ºenia \u0026ldquo;service INSP\u0026rdquo; Wykonujemy czynno≈õci z punkt√≥w 1-3 podanych powy≈ºej. Wciskamy \u0026lsquo;przycisk\u0026rsquo;, powinni≈õmy zobaczyƒá napis INSP, puszczamy przycisk. Przekrƒôcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, a≈º zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Mo≈ºemy wy≈ÇƒÖczyƒá zap≈Çon. U mnie zadzia≈Ça≈Ço, mam nadziejƒô, ≈ºe siƒô komu≈õ przyda.\n","permalink":"https://timor.site/2012/01/skoda-fabia-kasownie-ostrzezen-oil-i-service-insp/","summary":"Je≈ºeli jeste≈õ w≈Ça≈õcicielem Skody Fabii I to wcze≈õniej czy p√≥≈∫niej Tw√≥j BAT-Mobil zg≈Çosi kt√≥re≈õ z ostrze≈ºe≈Ñ serwisowych:\nOIL - pojawia siƒô przewa≈ºnie co 10 tys. km i wtedy gdy jeszcze nie trzeba wymieniaƒá oleju w silniku üòÑ service INSP - nie wiem jak czƒôsto siƒô pojawia a oznacza mniej wiƒôcej \u0026ldquo;czas wesprzeƒá finansowo lokalny autoryzowany serwis\u0026rdquo;. Kasowanie ostrze≈ºenia \u0026ldquo;OIL\u0026rdquo; Przy wy≈ÇƒÖczonym silniku wciskamy i przytrzymujemy przycisk kasowania¬†przebiegu dziennego/pokrƒôt≈Ço ustawiania godziny, od teraz nazywany po¬†prostu \u0026lsquo;przyciskiem\u0026rsquo;.","title":"Skoda Fabia - Kasownie ostrze≈ºe≈Ñ OIL i service INSP"},{"content":"Uruchamiamy SciTE i klikamy menu Options -\u0026gt; Open User Options File, wpisujemy dane:\n# domy≈õlne korzystanie z font√≥w o sta≈Çej szeroko≈õci font.base=$(font.monospace) font.small=$(font.monospace) font.comment=$(font.monospace) font.text=$(font.monospace) font.text.comment=$(font.monospace) font.embedded.base=$(font.monospace) font.embedded.comment=$(font.monospace) font.vbs=$(font.monospace) # numerowanie wierszy line.margin.visible=1 line.margin.width=3+ # ikonki toolbara z tematu systemowego toolbar.usestockicons=1 # zaznaczanie blokowe rectangular.selection.modifier=8 # ustawienia wg≈Çƒôbie≈Ñ kodu indent.size=4 use.tabs=1 indent.automatic=1 Wiƒôcej opcji tutaj:¬†http://www.scintilla.org/SciTEDoc.html\nSam pewnie jeszcze nie raz zaktualizujƒô ten wpis üòÑ\n","permalink":"https://timor.site/2012/01/moj-domyslny-config-dla-scite/","summary":"Uruchamiamy SciTE i klikamy menu Options -\u0026gt; Open User Options File, wpisujemy dane:\n# domy≈õlne korzystanie z font√≥w o sta≈Çej szeroko≈õci font.base=$(font.monospace) font.small=$(font.monospace) font.comment=$(font.monospace) font.text=$(font.monospace) font.text.comment=$(font.monospace) font.embedded.base=$(font.monospace) font.embedded.comment=$(font.monospace) font.vbs=$(font.monospace) # numerowanie wierszy line.margin.visible=1 line.margin.width=3+ # ikonki toolbara z tematu systemowego toolbar.usestockicons=1 # zaznaczanie blokowe rectangular.selection.modifier=8 # ustawienia wg≈Çƒôbie≈Ñ kodu indent.size=4 use.tabs=1 indent.automatic=1 Wiƒôcej opcji tutaj:¬†http://www.scintilla.org/SciTEDoc.html\nSam pewnie jeszcze nie raz zaktualizujƒô ten wpis üòÑ","title":"M√≥j domy≈õlny config dla SciTE"},{"content":"W Gajim\u0026rsquo;ie od dawna brakowa≈Ço mi wygodnego przeszukiwania po li≈õcie kontakt√≥w (takiego jakie ma siƒô pojawiƒá ju≈º niebawem w wersji 0.15 - z p√≥≈Çtora roku ju≈º na to czekam\u0026hellip;). W miƒôdzy czasie znalaz≈Çem chwilƒô by pobawiƒá siƒô Empathy - brzydki, nie ma przeglƒÖdarki us≈Çug XMPP, ale wyszukiwanie na rosterze jest dok≈Çadnie takie jakiego szuka≈Çem (w miarƒô wpisywania znak√≥w zawƒô≈ºa listƒô kontakt√≥w by pasowa≈Çy do wpisywanego wzorca).\nTyle ≈ºe skr√≥ty klawiaturowe w tym programie zwyczajnie mnie rozwalajƒÖ - przez lata przyzwyczai≈Çem siƒô ≈ºe okienka czatu mo≈ºna zamknƒÖƒá Escape\u0026rsquo;m - a tutaj nawet nie ma opcji, kt√≥ra pozwala≈Ça by na takƒÖ zmianƒô zachowania.\nRozwiƒÖzaniem jest edycja z root\u0026rsquo;a pliku¬†/usr/share/empathy/empathy-chat-window.ui i zamiana linii:\n\u0026lt;accelerator key=\u0026#34;W\u0026#34; modifiers=\u0026lt;wbr\u0026gt;\u0026#34;GDK_CONTROL_\u0026lt;/wbr\u0026gt;\u0026lt;wbr\u0026gt;MASK\u0026#34;/\u0026gt;\u0026lt;/wbr\u0026gt; na:\n\u0026lt;accelerator key=\u0026#34;Escape\u0026#34; /\u0026gt; Ta da! Restart komunikatora i da siƒô z nim ≈ºyƒá.\n≈πr√≥d≈Ço:¬†https://bugs.launchpad.net/ubuntu/+source/empathy/+bug/486508\n","permalink":"https://timor.site/2011/12/empathy-zamykanie-okienka-chatu-przyciskiem-escape/","summary":"W Gajim\u0026rsquo;ie od dawna brakowa≈Ço mi wygodnego przeszukiwania po li≈õcie kontakt√≥w (takiego jakie ma siƒô pojawiƒá ju≈º niebawem w wersji 0.15 - z p√≥≈Çtora roku ju≈º na to czekam\u0026hellip;). W miƒôdzy czasie znalaz≈Çem chwilƒô by pobawiƒá siƒô Empathy - brzydki, nie ma przeglƒÖdarki us≈Çug XMPP, ale wyszukiwanie na rosterze jest dok≈Çadnie takie jakiego szuka≈Çem (w miarƒô wpisywania znak√≥w zawƒô≈ºa listƒô kontakt√≥w by pasowa≈Çy do wpisywanego wzorca).\nTyle ≈ºe skr√≥ty klawiaturowe w tym programie zwyczajnie mnie rozwalajƒÖ - przez lata przyzwyczai≈Çem siƒô ≈ºe okienka czatu mo≈ºna zamknƒÖƒá Escape\u0026rsquo;m - a tutaj nawet nie ma opcji, kt√≥ra pozwala≈Ça by na takƒÖ zmianƒô zachowania.","title":"Empathy - zamykanie okienka chatu przyciskiem Escape"},{"content":"Mo≈ºna znale≈∫ƒá wiele tutoriali jakimi narzƒôdziami wykonywaƒá backupy. W wiƒôkszo≈õci przypadk√≥w absolutnie wystarczajƒÖcy oka≈ºe siƒô flexbackup. Bardziej wymagajƒÖcy wykorzystajƒÖ BackupPC, Baculƒô lub Amandƒô. Narzƒôdzia te pozwalajƒÖ wykonaƒá kopie pe≈Çne, r√≥≈ºnicowe, przyrostowe - kompresujƒÖc je dla zaoszczƒôdzenia miejsca.\nWszystko fajnie - ale problemy pojawiajƒÖ siƒô przy dostƒôpie do tych danych. ≈ªeby odzyskaƒá plik zmodyfikowany dzisiaj trzeba rozpakowaƒá najpierw kopiƒô pe≈ÇnƒÖ, potem r√≥≈ºnicowƒÖ, przyrostowƒÖ by wreszcie wyciƒÖgnƒÖƒá plik z wczoraj\u0026hellip; hmm ten te≈º jest skopany. No to lecimy jeszcze raz\u0026hellip;\nDodatkowo je≈ºeli danych jest kilkadziesiƒÖt GB to ca≈Çy proces wielokrotnej dekompresji mo≈ºe trwaƒá nawet kilka godzin. ZaczƒÖ≈Çem szukaƒá alternatywnej metody backupowania i trafi≈Çem na ciekawy artyku≈Ç na podstawie, kt√≥rego opracowa≈Çem w≈Çasny skrypt do backupu: http://www.mikerubel.org/computers/rsync_snapshots/\n#!/bin/bash # zdalny serwer z kt√≥rego chcemy wykonaƒá backup RHOST=\u0026#34;server.test.pl\u0026#34; # zasoby rsync ze zdalnego serwera kt√≥re zostanƒÖ zsynchronizowane BACKUPDIRS=(\u0026#34;backup\u0026#34; \u0026#34;home\u0026#34; \u0026#34;mails\u0026#34;) # lokalny katalog do kt√≥rego trafi backup DSTDIR=\u0026#34;/srv/backup/server\u0026#34; # maksymalny czas przechowywania backup√≥w wyra≈ºony w dniach MAXAGE=33 # dodatkowe opcje dla rsynca (--verbose mo≈ºna zastƒÖpiƒá --quiet # ≈Çatwiej zauwa≈ºyƒá wtedy b≈Çƒôdy), mo≈ºna w≈ÇƒÖczyƒá kompresje # w przypadku synchronizacji ze zdalnej lokalizacji OPTIONS=\u0026#34;--del --verbose\u0026#34; # plik z has≈Çem dla rsync\u0026#39;a - zabezpieczenie marne ale w izolowanej # sieci dopuszczalne PASS=\u0026#34;/root/.rsync-passwd\u0026#34; DATE=`date +\u0026#39;%Y.%m.%d\u0026#39;` YESTERDAY=`ls -1 $DSTDIR | sort | tail -n1` LINKDEST=$DSTDIR/$YESTERDAY # prosty mechanizm lock\u0026#39;a by uniemo≈ºliwiƒá wielokrotne uruchomienie # skryptu, np. w sytuacji gdy nie zdƒÖ≈ºy wykonaƒá siƒô pe≈Çny backup # przed kolejnym wywo≈Çaniem if [ -f \u0026#34;/tmp/server_sync\u0026#34; -o -f \u0026#34;/tmp/server_sync_block\u0026#34; ]; then echo \u0026#34;Another sync is still running!\u0026#34; exit 1 fi touch /tmp/server_sync # wykonujemy kolejno kopie for DIR in ${BACKUPDIRS[@]}; do mkdir -p $DSTDIR/$DATE/$DIR # sprawdzenie czy mamy wcze≈õniejszy backup, # jak mamy to robimy snapshot if [ -d \u0026#34;$DSTDIR/$YESTERDAY/$DIR\u0026#34; ]; then rsync -a --link-dest=$LINKDEST/$DIR \\ backup@$RHOST::archive/$DIR/ \\ $DSTDIR/$DATE/$DIR/ \\ --password-file=$PASS $OPTIONS else # jak nie mamy to robimy nowy rsync -a backup@$RHOST::archive/$DIR/ \\ $DSTDIR/$DATE/$DIR/ \\ --password-file=$PASS $OPTIONS fi # sprawdzenie czy synchronizacja siƒô uda≈Ça # je≈õli siƒô nie uda≈Ça to mo≈ºemy chcieƒá skasowaƒá niedoko≈Ñczony # backup by kolejny nie musia≈Ç byƒá \u0026#34;prawie pe≈Çnym\u0026#34; # plik /tmp/server_sync_block trzeba skasowaƒá rƒôcznie if [ $? -ne 0 -a $? -ne 24 ]; then echo \u0026#34;Something was wrong becase rsync return $?\u0026#34; touch /tmp/server_sync_block exit 2 fi done # zwalnianie lock\u0026#39;a rm -f /tmp/server_sync # kasowanie najstarszych backup√≥w find $DSTDIR -maxdepth 1 -type d -mtime +$MAXAGE \\ -print0 | xargs -0 -r rm -r ","permalink":"https://timor.site/2011/12/automatyczne-backupy-w-stylu-snapshot-z-rsynciem/","summary":"Mo≈ºna znale≈∫ƒá wiele tutoriali jakimi narzƒôdziami wykonywaƒá backupy. W wiƒôkszo≈õci przypadk√≥w absolutnie wystarczajƒÖcy oka≈ºe siƒô flexbackup. Bardziej wymagajƒÖcy wykorzystajƒÖ BackupPC, Baculƒô lub Amandƒô. Narzƒôdzia te pozwalajƒÖ wykonaƒá kopie pe≈Çne, r√≥≈ºnicowe, przyrostowe - kompresujƒÖc je dla zaoszczƒôdzenia miejsca.\nWszystko fajnie - ale problemy pojawiajƒÖ siƒô przy dostƒôpie do tych danych. ≈ªeby odzyskaƒá plik zmodyfikowany dzisiaj trzeba rozpakowaƒá najpierw kopiƒô pe≈ÇnƒÖ, potem r√≥≈ºnicowƒÖ, przyrostowƒÖ by wreszcie wyciƒÖgnƒÖƒá plik z wczoraj\u0026hellip; hmm ten te≈º jest skopany.","title":"Automatyczne backupy w stylu snapshot z rsync‚Äôiem"},{"content":"Zdarzy≈Ço mi siƒô bawiƒá sprzƒôtowymi Firewallami firmy Fortigate - chcƒÖc sprawdziƒá dzia≈Çanie pewnych funkcji potrzebowa≈Çem uruchomiƒá dwa/trzy pude≈Çka na osobnych ≈ÇƒÖczach. Pomys≈Ç polega≈Ç na pr√≥bie zmuszenia pude≈Çek do wsp√≥≈Çpracy z modemem iPlus na USB.\nDrugim fajnym zastosowaniem tego triku jest mo≈ºliwo≈õƒá wykorzystania iPlusa jako \u0026ldquo;zapasowego ≈ÇƒÖcza\u0026rdquo; w przypadku awarii g≈Ç√≥wnego.\nDziƒôki pomocy in≈ºyniera Fortigate szybko uda≈Ço mi siƒô zebraƒá potrzebne do dzia≈Çania parametry, kt√≥re nale≈ºy uruchomiƒá poprzez command line (telnet/ssh).\nJe≈ºeli chcemy uruchomiƒá modem dla konkretnego vdom\u0026rsquo;u to nale≈ºy go najpierw wybraƒá, np:\nconfig vdom edit root Je≈õli nie korzystamy z vdom\u0026rsquo;√≥w to krok ten mo≈ºemy pominƒÖƒá. Dalej:\nconfig system modem set status enable set pin-init at+cpin=xxxx set auto-dial enable set idle-timer 6 set redial 2 set phone1 *99# set username1 ppp set passwd1 ppp set extra-init1 at+cgdcont=1,\\\u0026#34;IP\\\u0026#34;,\\\u0026#34;\u0026lt;a href=\u0026#34;http://www.plusgsm.pl%5C\u0026#34;\u0026gt;www.plusgsm.pl\\\u0026lt;/a\u0026gt;\u0026#34; end Przy czym w opcji pin-init zamiast xxxx nale≈ºy podaƒá PIN karty SIM.\nNa koniec potrzebny jest restart urzƒÖdzenia:\nexecute reboot Po restarcie bƒôdziemy mieƒá do dyspozycji nowy interfejs (modem). Automatycznie zostanie ustawione¬†dns proxy.\nTestowa≈Çem to na modemach Huawei E156G i Huawei E173 i r√≥≈ºnych urzƒÖdzeniach FortiGate z firmware w wersji MR2.\n","permalink":"https://timor.site/2011/12/konfiguracja-modemu-usb-iplus-na-urzadzeniach-fortigate/","summary":"Zdarzy≈Ço mi siƒô bawiƒá sprzƒôtowymi Firewallami firmy Fortigate - chcƒÖc sprawdziƒá dzia≈Çanie pewnych funkcji potrzebowa≈Çem uruchomiƒá dwa/trzy pude≈Çka na osobnych ≈ÇƒÖczach. Pomys≈Ç polega≈Ç na pr√≥bie zmuszenia pude≈Çek do wsp√≥≈Çpracy z modemem iPlus na USB.\nDrugim fajnym zastosowaniem tego triku jest mo≈ºliwo≈õƒá wykorzystania iPlusa jako \u0026ldquo;zapasowego ≈ÇƒÖcza\u0026rdquo; w przypadku awarii g≈Ç√≥wnego.\nDziƒôki pomocy in≈ºyniera Fortigate szybko uda≈Ço mi siƒô zebraƒá potrzebne do dzia≈Çania parametry, kt√≥re nale≈ºy uruchomiƒá poprzez command line (telnet/ssh).","title":"Konfiguracja modemu USB iPlus na urzƒÖdzeniach FortiGate"},{"content":"Wcze≈õniej czy p√≥≈∫niej zawsze pojawia¬†siƒô potrzeba zoptymalizowania naszej bazy MySQL. Przedstawiƒô kilka zmian w konfiguracji, kt√≥re powinny zwiƒôkszyƒá wydajno≈õƒá w wiƒôkszo≈õci przypadk√≥w.\nMyISAM -¬†key_buffer_size NajprostszƒÖ optymalizacjƒÖ baz/tabel z mechanizmem MyISAM jest odpowiednie dobranie bufora na cache dla kluczy i indeks√≥w (dane nigdy nie sƒÖ cachowane). Poni≈ºsze zapytanie pozwala oszacowaƒá zalecany rozmiar cache\u0026rsquo;u:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.4999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_key_buffer_size FROM (SELECT LEAST(POWER(2,32),KBS1) KBS FROM (SELECT SUM(index_length) KBS1 FROM information_schema.tables WHERE engine=\u0026#39;MyISAM\u0026#39; AND table_schema NOT IN (\u0026#39;information_schema\u0026#39;,\u0026#39;mysql\u0026#39;)) AA ) A, (SELECT 2 PowerOf1024) B; Wynik okre≈õla zalecany rozmiar bufora (parametr key_buffer_size w pliku /etc/mysql/my.cnf) dla bie≈ºƒÖcego stanu bazy - warto ciut dodaƒá na zapas. Na systemach 32 bitowych parametr¬†key_buffer_size mo≈ºe przyjmowaƒá maksymalnie 4GB, na 64 bitowych maksymalnie 8GB.\n[mysqld] key_buffer_size=xxxM InnoDB -¬†innodb_buffer_pool_size W przypadku InnoDB cachowane mogƒÖ byƒá zar√≥wno dane jak i klucze/indeksy a rozmiar bufora okre≈õla parametr¬†innodb_buffer_pool_size. ZalecanƒÖ minimalnƒÖ warto≈õƒá tego parametru mo≈ºemy oszacowaƒá zapytaniem:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.49999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_innodb_buffer_pool_size FROM (SELECT SUM(data_length+index_length) KBS FROM information_schema.tables WHERE engine=\u0026#39;InnoDB\u0026#39;) A, (SELECT 2 PowerOf1024) B; W przypadku InnoDB po zmianie warto≈õci¬†innodb_buffer_pool_size konieczne jest r√≥wnie≈º ustawienie¬†innodb_log_file_size na 25% warto≈õci¬†innodb_buffer_pool_size lub 2047M (nale≈ºy wybraƒá mniejszƒÖ z warto≈õci). By wygenerowaƒá pliki log o nowych rozmiarach musimy postƒôpowaƒá wed≈Çug poni≈ºszej instrukcji:\nDopisujemy w pliku my.cnf parametry: [mysqld] innodb_buffer_pool_size=xxxM innodb_log_file_size=25% xxxM lub 2047M Wy≈ÇƒÖczamy bazƒô: invoke-rc.d mysql stop Kasujemy obecnie pliki log: rm /var/lib/mysql/ib_logfile[01] Uruchamiamy bazƒô: invoke-rc.d mysql start Po starcie bazy zostanƒÖ wygenerowane nowe pliki log o nowych rozmiarach. InnoDB - kompaktowanie plik√≥w W domy≈õlnej konfiguracji MySQL dla baz InnoDB (na Debianie na bank, przypuszczam ≈ºe na innych distro jest podobnie) wszystkie tabele, indeksy, metadane tabel i inne dane dotyczƒÖce table InnoDB przechowywane sƒÖ w jednym pliku:¬†/var/lib/mysql/ibdata1\nNie jest to optymalne ustawienie szczeg√≥lnie gdy mamy du≈ºo baz i o znacznych rozmiarach - wykonywanie wielu r√≥wnoczesnych operacji na jednym gigantycznym pliku potrafi mocno przymuliƒá.\nPr√≥ba kompaktowania/optymalizowania tabel InnoDB nie powoduje zmniejszenia tego pliku - bo gdy pr√≥bujemy optymalizowaƒá tabele InnoDB powoduje to:\nu≈Ço≈ºenie danych i indeks√≥w wewnƒÖtrz pliku ibdata1 w spos√≥b ciƒÖg≈Çy, wzrost rozmiaru pliku ibdata1 poniewa≈º powy≈ºsze dane dopisywane sƒÖ na jego ko≈Ñcu. Niewiele os√≥b spodziewa siƒô takiego rezultatu.¬†Mo≈ºna zmniejszyƒá rozmiar tego pliku wy≈ÇƒÖczajƒÖc dane tabel i ich indeksy do osobnych plik√≥w ale proces ten wymaga pe≈Çnego backupu bazy i jej odtworzenia, wiƒÖ≈ºe siƒô wiƒôc z chwilowym (a w przypadku du≈ºych baz - d≈Çu≈ºszym) przestojem.\nRobimy pe≈Çny backup bazy, np. poleceniem: mysqldump --all-databases --single-transaction -uroot -p \u0026gt; my-dump.sql (dump\u0026rsquo;a mo≈ºna dodatkowo skompresowaƒá gzipem dodajƒÖc pipe\u0026rsquo;a - przyspieszy to odzyskiwanie przez zmniejszenie ilo≈õci danych potrzebnych do odczytania z dysku)\nKasujemy wszystkie bazy z wyjƒÖtkiem schematu mysql (de facto powinno wystarczyƒá skasowanie i odzyskanie tylko baz korzystajƒÖcych z tabel InnoDB)\nWy≈ÇƒÖczamy us≈Çugƒô:\ninvoke-rc.d mysql stop Dodajemy w pliku /etc/mysql/my.cnf parametry: [mysqld] innodb_file_per_table innodb_log_file_size=25% xG innodb_buffer_pool_size=xG pierwsza opcja powoduje w≈Ça≈õciwe rozdzielenie tabel InnoDB do r√≥≈ºnych plik√≥w (dodanie tej opcji na serwerze na kt√≥rym znajdujƒÖ siƒô bazy InnoDB spowoduje ich uszkodzenie - odradzam), drugƒÖ i trzeciƒÖ linijkƒô konfigurujemy wed≈Çug wcze≈õniejszej instrukcji o innodb_buffer_pool_size, mo≈ºemy te≈º na czas przywracania danych dodaƒá opcjƒô bulk_insert_buffer_size=256M, skr√≥ci to czas potrzebny na przywr√≥cenie bazy. Kasujemy pliki: ibdata1, ib_logfile0 and ib_logfile1: rm /var/lib/mysql/ibdata1 rm /var/lib/mysql/ib_logfile[01] Uruchamiamy serwer MySQL: invoke-rc.d mysql start Przywracamy wszystkie bazy z dump\u0026rsquo;a: cat¬†my-dump.sql | mysql -uroot -p Plik ibdata1 uro≈õnie ale od tej pory bƒôdzie zawieraƒá wy≈ÇƒÖcznie metadane tabel a poszczeg√≥lne tabele i indeksy bƒôdƒÖ przechowywane w osobnych plikach, np. tabela.ibd. Teraz optymalizowanie tabel InnoDB bƒôdzie powodowaƒá zmniejszenie rozmiar√≥w plik√≥w *.ibd a plik ibdata1 nie bƒôdzie tak obciƒÖ≈ºony.\nZmiana metody flush\u0026rsquo;a W niekt√≥rych przypadkach ustawienie opcji¬†innodb_flush_method na warto≈õƒá O_DIRECT mo≈ºe poprawiƒá wydajno≈õƒá, choƒá w innych wydajno≈õƒá mo≈ºe siƒô pogorszyƒá¬†(na forach sugerowano wystƒôpowanie problemu na dedykowanych macierzach). Mo≈ºna bezpiecznie w≈ÇƒÖczyƒá tƒô opcjƒô i wykonaƒá kilka bechmark√≥w:\n[mysqld] innodb_flush_method=O_DIRECT Cache wynik√≥w zapyta≈Ñ Sprawd≈∫my najpierw czy cachowanie jest w≈ÇƒÖczone (u mnie by≈Ço to domy≈õlne ustawienie), wydajemy zapytanie:\nSHOW VARIABLES LIKE \u0026#39;query_cache_type\u0026#39;; Mo≈ºliwe sƒÖ 3 ustawienia:\nON (query_cache_type = 1) - cachowanie wszystkich zapyta≈Ñ, OFF¬†(query_cache_type = 0) - cachowanie na ≈ºƒÖdanie, DEMAND¬†(query_cache_type = 2) - cachowanie wy≈ÇƒÖczone. W przypadku opcji DEMAND cachowanie jest w≈ÇƒÖczane je≈ºeli po SELECT\u0026rsquo;cie dodamy¬†SQL_CACHE. Mi osobi≈õcie najbardziej odpowiada opcja z cachowaniem wszystkiego.\nNastƒôpnie nale≈ºy ustawiƒá w pliku my.cnf poni≈ºsze zmienne wed≈Çug potrzeb:\nquery_cache_limit = 2M query_cache_size = 32M Pierwsza opcja ustala maksymalny rozmiar pojedynczego zapytanie, kt√≥re bƒôdzie cachowane - zapytania wiƒôksze nie bƒôdƒÖ cachowane. Druga opcja ustala rozmiar ca≈Çego bufora na cache (przewa≈ºnie wiƒôcej dzia≈Ça lepiej).\nMyISAM - unikanie repair with keycache Gdy nasza baza uro≈õnie i bƒôdziemy mieƒá w niej tabele o rozmiarze przekraczajƒÖcym 2GB to da siƒô zauwa≈ºyƒá ≈ºe pewne operacja jak np. zak≈Çadanie indeksu, optymalizacja, naprawa - trwajƒÖ cholernie d≈Çugo. Mo≈ºna to szczeg√≥lnie odczuƒá w≈Ça≈õnie w momencie przekraczania rozmiaru 2GB i tak utworzenie indeksu na tabeli o rozmiarze 1,9GB trwa powiedzmy kilkana≈õcie/kilkadziesiƒÖt minut, a ta sama operacja na bazie o rozmiarze 2,1GB mo≈ºe zajƒÖƒá nawet kilka godzin.\nPrzyczynƒô ≈Çatwo namierzyƒá obserwujƒÖc wynik polecenia:\nSHOW PROCESSLIST; w trakcie operacji na \u0026ldquo;ma≈Çej\u0026rdquo; i \u0026ldquo;du≈ºej\u0026rdquo; tabeli. \u0026ldquo;Ma≈Ça\u0026rdquo; zatrzymuje siƒô na d≈Çu≈ºej na operacji Repair By Sorting, a \u0026ldquo;du≈ºa\u0026rdquo; kona godzinami na Repair With Keycache. W≈Ça≈õnie r√≥≈ºnica w dzia≈Çaniu obu mechanizm√≥w sortowania daje w ko≈õƒá:\nrepair by sorting - wykorzystuje do sortowania wiele plik√≥w tymczasowych i wymaga sporo wolnego miejsca w katalogu ustawionym w opcji tmpdir (domy≈õlnie ustawionej na /tmp) - je≈ºeli miejsca bƒôdzie za ma≈Ço to wybierany bƒôdzie mechanizm \u0026ldquo;repair with keycache\u0026rdquo;, repair with keycache - wykorzystuje do sortowania bardzo ma≈Çy bufor (u mnie 8MB), jest ok 10~20 krotnie wolniejszy ni≈º \u0026ldquo;repair by sorting\u0026rdquo; a do tego tworzy mniej optymalne indeksy. O tym kt√≥ry z mechanizm√≥w zostanie wybrany decyduje opcja myisam_max_sort_file_size - zmienna ta ma domy≈õlnie warto≈õƒá 2GB i w≈Ça≈õnie dlatego problemy pojawiajƒÖ siƒô po przekroczeniu tego rozmiaru. Proponujƒô ustawiƒá jƒÖ sporo powy≈ºej rozmiaru najwiƒôkszych tablic - oczywi≈õcie je≈õli miejsce w temp\u0026rsquo;ie pozwoli na to, np:\nmyisam_max_sort_file_size=8GB Przy takim ustawieniu warto mieƒá w /tmp minimum drugie tyle wolnego miejsca.\n≈πr√≥d≈Ço http://dba.stackexchange.com/questions/3163/mysql-5-1-innodb-configuration-24gb-ram-bi-xeon-high-load\n","permalink":"https://timor.site/2011/12/mysql-proste-metody-optymalizacji/","summary":"Wcze≈õniej czy p√≥≈∫niej zawsze pojawia¬†siƒô potrzeba zoptymalizowania naszej bazy MySQL. Przedstawiƒô kilka zmian w konfiguracji, kt√≥re powinny zwiƒôkszyƒá wydajno≈õƒá w wiƒôkszo≈õci przypadk√≥w.\nMyISAM -¬†key_buffer_size NajprostszƒÖ optymalizacjƒÖ baz/tabel z mechanizmem MyISAM jest odpowiednie dobranie bufora na cache dla kluczy i indeks√≥w (dane nigdy nie sƒÖ cachowane). Poni≈ºsze zapytanie pozwala oszacowaƒá zalecany rozmiar cache\u0026rsquo;u:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.4999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_key_buffer_size FROM (SELECT LEAST(POWER(2,32),KBS1) KBS FROM (SELECT SUM(index_length) KBS1 FROM information_schema.","title":"MySQL - Proste metody optymalizacji"},{"content":"Domy≈õlna konfiguracja fail2ban\u0026rsquo;a (na Debianie) nie zawiera regu≈Ç pozwalajƒÖcych na blokowanie pr√≥b w≈Çama≈Ñ na skrzynki POP/IMAP dla dovecota (no chyba ≈ºe korzystamy z saslauthd). Mo≈ºna szybko utworzyƒá w≈Çasny zestaw filtr√≥w co przedstawiƒô poni≈ºej.\nTworzymy plik:¬†/etc/fail2ban/filter.d/dovecot.conf\n[Definition] failregex = (?: pop3-login|imap-login): .*(?:Authentication failure|Aborted login \\(auth failed|Aborted login \\(tried to use disabled|Disconnected \\(auth failed|Aborted login \\(\\d+ authentication attempts).*rip=(?P\u0026lt;host\u0026gt;\\S*),.* ignoreregex = P√≥≈∫niej dopisujemy na ko≈Ñcu pliku:¬†/etc/fail2ban/jail.conf\n[dovecot] enabled = true filter = dovecot port = pop3,pop3s,imap,imaps logpath = /var/log/mail.log maxretry = 20 # te dwa poni≈ºej wedle uznania - ja mam dobrze ustawione default\u0026#39;y #findtime = 1200 #bantime = 1200 Zosta≈Ço zrestartowaƒá fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart Tip na bazie dokumentacji:¬†http://wiki.dovecot.org/HowTo/Fail2Ban\n","permalink":"https://timor.site/2011/11/fail2ban-regulki-dla-dovecota/","summary":"Domy≈õlna konfiguracja fail2ban\u0026rsquo;a (na Debianie) nie zawiera regu≈Ç pozwalajƒÖcych na blokowanie pr√≥b w≈Çama≈Ñ na skrzynki POP/IMAP dla dovecota (no chyba ≈ºe korzystamy z saslauthd). Mo≈ºna szybko utworzyƒá w≈Çasny zestaw filtr√≥w co przedstawiƒô poni≈ºej.\nTworzymy plik:¬†/etc/fail2ban/filter.d/dovecot.conf\n[Definition] failregex = (?: pop3-login|imap-login): .*(?:Authentication failure|Aborted login \\(auth failed|Aborted login \\(tried to use disabled|Disconnected \\(auth failed|Aborted login \\(\\d+ authentication attempts).*rip=(?P\u0026lt;host\u0026gt;\\S*),.* ignoreregex = P√≥≈∫niej dopisujemy na ko≈Ñcu pliku:¬†/etc/fail2ban/jail.conf\n[dovecot] enabled = true filter = dovecot port = pop3,pop3s,imap,imaps logpath = /var/log/mail.","title":"fail2ban - regu≈Çki dla dovecot‚Äôa"},{"content":"Gdy ju≈º ustawimy reverse proxy przed Apache szybko mo≈ºna zauwa≈ºyƒá ≈ºe w logach zamiast adres√≥w IP zdalnych u≈ºytkownik√≥w pojawia siƒô tylko jeden adres: adres naszego proxy. R√≥wnie≈º z poziomu php\u0026rsquo;a jako adres klienta widaƒá IP naszego proxy.\nBy poradziƒá sobie z tym problemem trzeba na serwerze reverse proxy ustawiƒá przekazywanie informacji o oryginalnym adresie IP klienta w nag≈Ç√≥wku X-Forwarded-For. W przypadku gdy reverse proxy dzia≈Ça na nginx\u0026rsquo;e wystarczy dodaƒá taki wpis:\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; Teraz w trzeba zainstalowaƒá modu≈Ç mod_rpaf dla Apachego, kt√≥ry to zajmie siƒô interpretacjƒÖ nag≈Ç√≥wka i podmianƒÖ IP proxy prawdziwym IP. Na Debianie wystarczy wpisaƒá:\napt-get install libapache2-mod-rpaf Po instalacji nale≈ºy w pliku¬†/etc/apache2/mods-available/rpaf.conf w opcji¬†RPAFproxy_ips dopisaƒá adresy IP serwer√≥w proxy, np (oczywi≈õcie wpisz swoje adresy):\nRPAFproxy_ips 127.0.0.1 10.24.0.5 Wa≈ºne by by≈Çy to zaufane adresy IP - bo w tym miejscu pozwalamy by z tych lokalizacji mo≈ºliwe by≈Ço nadpisanie adresu IP np. w logach. Je≈ºeli pozwolimy na modyfikacjƒô adres√≥w IP z zewnƒÖtrz to atakujƒÖcy mo≈ºe wykorzystaƒá to by nadpisaƒá sw√≥j prawdziwy adres fa≈Çszywym.\nPozosta≈Ço uruchomiƒá modu≈Ç i zrestartowaƒá Apachego aby go za≈Çadowa≈Ç:\na2enmod rpaf invoke-rc.d apache2 restart Teraz zar√≥wno w logach Apache\u0026rsquo;go jak i skryptach PHP\u0026rsquo;a bƒôdzie przekazywane rzeczywiste IP u≈ºytkownika.\n","permalink":"https://timor.site/2011/11/x-forwarded-for-mod_rpaf-logowanie-rzeczywistych-adresow-ip-na-apache-za-reverse-proxy/","summary":"Gdy ju≈º ustawimy reverse proxy przed Apache szybko mo≈ºna zauwa≈ºyƒá ≈ºe w logach zamiast adres√≥w IP zdalnych u≈ºytkownik√≥w pojawia siƒô tylko jeden adres: adres naszego proxy. R√≥wnie≈º z poziomu php\u0026rsquo;a jako adres klienta widaƒá IP naszego proxy.\nBy poradziƒá sobie z tym problemem trzeba na serwerze reverse proxy ustawiƒá przekazywanie informacji o oryginalnym adresie IP klienta w nag≈Ç√≥wku X-Forwarded-For. W przypadku gdy reverse proxy dzia≈Ça na nginx\u0026rsquo;e wystarczy dodaƒá taki wpis:","title":"X-Forwarded-For + mod_rpaf - logowanie rzeczywistych adres√≥w IP na Apache za reverse proxy"},{"content":"Przez pewien czas korzysta≈Çem z eAcceleratora do przyspieszenia dzia≈Çania stron pisanych w PHP\u0026rsquo;ie ale czasem bywa≈Ç niestabilny. Aktualizacje pojawia≈Çy siƒô rzadko a od czasu do czasu miewa≈Çem problemy ze stabilno≈õciƒÖ tej wtyczki na kilku bardziej skomplikowanych aplikacjach. Zdarza≈Ço siƒô ≈ºe pomimo zmiany kodu w skrypcie php, eAccelerator serwowa≈Ç wciƒÖ≈º stary plik - konieczny by≈Ç restart Apache\u0026rsquo;go by wszystko dzia≈Ça≈Ço jak trzeba.\nZaczƒÖ≈Çem szukaƒá alternatywy i trafi≈Çem na dwa modu≈Çy:\nAPC (czyli Alternative PHP Cache), kt√≥ry ma byƒá nawet domy≈õlnie wbudowany w PHP od wersji 5.4, XCache - projekt rozwijany przez jednego z programist√≥w lighttpd. By≈Çem ciekaw wydajno≈õci poszczeg√≥lnych rozwiƒÖza≈Ñ wzglƒôdem siebie, wiƒôc przygotowa≈Çem ma≈Çe ≈õrodowisko testowe sk≈ÇadajƒÖce siƒô z 4 maszyn wirtualnych (dzia≈ÇajƒÖcych pod kontrolƒÖ VirtualBox\u0026rsquo;a):\nApache 2.2 + PHP 5.3 Apache 2.2 + PHP 5.3 + eAccelerator 0.9.6.1 (instrukcja instalacji tutaj) Apache 2.2 + PHP 5.3 + APC 3.1.3p1 (pod Debianem paczka php-apc) Apache 2.2 + PHP 5.3 + XCache 1.3.0 (pod Debianem paczka php5-xcache) Najpierw przygotowa≈Çem pierwszƒÖ maszynƒô wraz z konfiguracjƒÖ MySQL\u0026rsquo;a i domy≈õlnƒÖ instalacjƒÖ WordPress\u0026rsquo;a 3.2 by test by≈Ç w miarƒô miarodajny. Kolejne maszynki to klony tej pierwszej, plus zainstalowane i domy≈õlnie skonfigurowane kolejne rozszerzenia. Ka≈ºdemu z rozszerze≈Ñ przyzna≈Çem 32 MB pamiƒôci na cache.\nMetodyka testu Do automatyzacji testu pos≈Çu≈ºy≈Ç mi skrypt w bash\u0026rsquo;u uruchamiajƒÖcy ab dla 1000 zapyta≈Ñ z kolejno rosnƒÖcƒÖ liczbƒÖ r√≥wnoleg≈Çych po≈ÇƒÖcze≈Ñ. Pozwoli to na por√≥wnanie wydajno≈õci optymalizator√≥w przy mniejszym i wiƒôkszym obciƒÖ≈ºeniu. Uruchomienie testu dla czystej instalacji bez dodatku poka≈ºe jak du≈ºy przyrost wydajno≈õci daje siƒô uzyskaƒá.\nJako systemy testowe pos≈Çu≈ºy≈Çy mi wirtualki uruchomione pod kontrolƒÖ VirtualBOX\u0026rsquo;a z zainstalowanym aktualnym Debianem Squeeze. Przydzieli≈Çem im po 1 GB RAM\u0026rsquo;u i 2 rdzenie CPU. Wirtualki te raczej nie sƒÖ highend\u0026rsquo;em ale do og√≥lnego por√≥wnania optymalizator√≥w bƒôdƒÖ w zupe≈Çno≈õci wystarczajƒÖce.\nWyniki Ju≈º na pierwszy rzut oka widaƒá ≈ºe op≈Çaca siƒô zainstalowaƒá dowolny optymalizator bo ich wydajno≈õƒá jest zbli≈ºona a w stosunku do czystej instalacji pozwalajƒÖ obs≈Çu≈ºyƒá prawie czterokrotnie wiƒôkszy ruch. Przy czym system bez opcode cacher\u0026rsquo;a nie pokona≈Ç granicy 70 zapyta≈Ñ na sekundƒô - zaczƒÖ≈Ç swapowaƒá i nie uko≈Ñczy≈Ç kolejnych test√≥w.\nPoni≈ºej wykres przedstawiajƒÖcy ilo≈õƒá obs≈Çugiwanych ≈ºƒÖda≈Ñ w zale≈ºno≈õci od ilo≈õci r√≥wnoczesnych po≈ÇƒÖcze≈Ñ: I jeszcze jeden wykres, na kt√≥rym por√≥wna≈Çem wydajno≈õƒá poszczeg√≥lnych optymalizator√≥w wzglƒôdem czystego PHP\u0026rsquo;a Wychodzi na to ≈ºe przez wiƒôkszo≈õƒá testu eAccelerator by≈Ç najszybszy, gdzieniegdzie przeplatajƒÖc siƒô z APC. XCache nieznacznie ale na ca≈Çej d≈Çugo≈õci poni≈ºej dw√≥ch wcze≈õniejszych. Ca≈Ço≈õciowe r√≥≈ºnice pomiƒôdzy optymalizatorami przewa≈ºnie nie przekracza≈Çy 3 zapyta≈Ñ/sekundƒô - wiƒôc r√≥≈ºnice pomiƒôdzy nimi sƒÖ rzƒôdu 1~2%. Mo≈ºna na tej podstawie wywnioskowaƒá ≈ºe wydajno≈õƒá jest tak zbli≈ºona i≈º nie powinna byƒá jedynym kryterium wyboru optymalizatora dla naszego systemu.\nPoni≈ºej postaram siƒô zebraƒá subiektywne oceny poszczeg√≥lnych rozwiƒÖza≈Ñ by dostarczyƒá dodatkowych argument√≥w.\neAccelerator eAccelerator by≈Ç najszybszy w te≈õcie ale miewa≈Çem z nim problemy (kilka razy ale\u0026hellip;) stƒÖd nie jest moim faworytem.\nZalety:\nzdecydowanie najszybszy, jest stosunkowo aktywnie rozwijany, do≈õƒá stabilny, wbudowany encoder i dekoder skrypt√≥w (do dystrybucji kodu bez ≈∫r√≥de≈Ç w postaci skompilowanej). Wady:\nbrak paczek w repozytoriach Debiana - rƒôczna kompilacja nie jest ciƒô≈ºka ale gdy trzeba go utrzymaƒá na 30 serwerach to przestaje byƒá zabawnie, pomimo ≈ºe pojawiajƒÖ siƒô nowe wersje to ostatnio mia≈Çem problemy z pobraniem ich ze strony projektu - szukanie paczek \u0026ldquo;gdzie≈õ\u0026rdquo; na sieci nie wydaje mi siƒô bezpieczne, mia≈Çem problem z pewnƒÖ du≈ºƒÖ aplikacjƒÖ, nie dzia≈Ça≈Ça stabilnie pod eAcceleratorem, eAccelerator powsta≈Ç na bazie kodu¬†Turck MMCache (ten nie jest ju≈º rozwijany) -istniejƒÖ pewne wƒÖtpliwo≈õci licencyjne co do jego kodu\u0026hellip; PHP APC APC pod wzglƒôdem wydajno≈õci nieznacznie ustƒôpuje eAcceleratorowi. CiekawƒÖ funkcjƒÖ udostƒôpnianƒÖ przez APC jest obs≈Çuga¬†RFC1867 (File Upload Progress hook handler). Jest te≈º pewna potwierdzona plotka m√≥wiƒÖca o w≈ÇƒÖczeniu kodu APC do PHP\u0026rsquo;a 6. Teoretycznie je≈ºeli przesiƒÖdziemy siƒô ju≈º teraz na APC to p√≥≈∫niej powinno p√≥j≈õƒá ≈Çatwiej\u0026hellip;\nZalety:\ndo≈õƒá szybki, aktywnie rozwijany, bardzo stabilny, dostƒôpna paczka w repozytoriach Debiana (i z tego co wiem na wielu innych systemach te≈º przewa≈ºnie wystarczajƒÖ domy≈õlne repozytoria) obs≈Çuga RFC1867 (upload progress), zostanie w≈ÇƒÖczony do PHP od wersji 5.4, APC udostƒôpnia API umo≈ºliwiajƒÖce tworzenie w≈Çasnych obiekt√≥w w pamiƒôci cache wsp√≥≈Çdzielonych pomiƒôdzy zapytaniami np. by nie pobieraƒá za ka≈ºdym razem w≈Ça≈õciwo≈õci profilu z bazy, listy u≈ºytkownik√≥w, itp (co≈õ w stylu memcached). dostƒôpny jest ze skryptem apc.php, kt√≥ry pozwala zarzƒÖdzaƒá obiektami w cache, czy≈õciƒá itp. Wady:\npodobno bywa problematyczny w konfiguracji z fast-cgi (choƒá u mnie dzia≈Ça), przy mocno zapchanym cache\u0026rsquo;u czyszczenie go potrafi≈Ço siƒô zwiesiƒá. XCache Ostatni projekt rozwijany jest przez jednego z programist√≥w lighttpd. W chwili obecnej wydaje siƒô byƒá do≈õƒá dojrza≈Çym i wystarczajƒÖco stabilnym do produkcyjnego u≈ºycia. Choƒá gdy pr√≥bowa≈Çem z niego korzystaƒá jaki≈õ rok/dwa temu to mia≈Çem sporo losowych pad√≥w - niezale≈ºnych od obciƒÖ≈ºenia serwera.\nZalety:\nstabilny, aktywnie rozwijany z kilkoma ga≈Çƒôziami (stable/unstable/devel) - mo≈ºemy wybraƒá czy potrzebujemy funkcji czy stabilno≈õci. Wady:\nnieznacznie, ale jednak najni≈ºsza wydajno≈õƒá, mia≈Çem z nim ma≈Ço styczno≈õci a szybko zrazi≈Çem siƒô do kiepskiej stabilno≈õci - obecnie wydaje siƒô ≈ºe nie stanowi to problemu. Podsumowanie i m√≥j wyb√≥r Do testu celowo wybra≈Çem WordPress\u0026rsquo;a jako do≈õƒá du≈ºy i wystarczajƒÖco skomplikowany projekt - je≈õli on bƒôdzie dzia≈Çaƒá stabilnie to wiƒôkszo≈õƒá mniejszych te≈º powinna\u0026hellip; Ku mojemu zaskoczeniu ≈ºaden z optymalizator√≥w nie sypnƒÖ≈Ç b≈Çƒôdami. Dziwi≈Ço mnie to bo jeszcze jaki≈õ czas temu eAccelerator czasami losowo mi siƒô sypa≈Ç - dzia≈Ça≈Ç przez tydzie≈Ñ i nagle zgon w sobotƒô po po≈Çudniu\u0026hellip; P√≥≈∫niej pr√≥bowa≈Çem XCache i by≈Ço podobnie\u0026hellip; tylko gorzej bo problemy wystƒôpowa≈Çy czƒô≈õciej. APC testowa≈Çem jako ostatnie ale w wykorzystywanych przeze mnie aplikacjach zachowywa≈Ç siƒô bardzo stabilnie i przewidywalni. Jedyny problem z wieszaniem siƒô podczas czyszczenia/usuwania elementu z cache\u0026rsquo;u mo≈ºna obej≈õƒá stosunkowo szybkim restartem Apachego - skuteczne i efekt ten sam üòÉ Na jednym z serwer√≥w testujƒô APC w trybie fast-cgi od oko≈Ço dw√≥ch miesiƒôcy i jak na razie nie mogƒô narzekaƒá (mo≈ºe w wolnej chwili uzupe≈Çniƒô to zestawienie o testy w trybie fast-cgi).\nObecnie w wiƒôkszo≈õci administrowanych przeze mnie serwer√≥w z PHP\u0026rsquo;em standardowo instalujƒô APC. Wyb√≥r jest dla mnie tym bardziej oczywisty ≈ºe paczka ta jest dostƒôpna w standardowych repozytoriach (≈Çatwo≈õƒá aktualizacji itd) - nie ma zatem potrzeby jak w przypadku eAcceleratora instalowania wielu paczek z zale≈ºno≈õciami by m√≥c skompilowaƒá 1 modu≈Ç.\nDodatkowƒÖ zaletƒÖ jest fakt ≈ºe APC ma byƒá standardowo wbudowany w kolejne wersje PHP\u0026rsquo;a - je≈ºeli w rozwijanych aplikacjach ju≈º teraz zwr√≥ci siƒô uwagƒô na integracjƒô z tym rozwiƒÖzaniem to w przysz≈Ço≈õci migracja nie powinna przysporzyƒá problem√≥w.\nJe≈ºeli siƒô wahasz - wybierz APC. Je≈ºeli w Twoim ≈õrodowisku oka≈ºe siƒô niestabilne zawsze mo≈ºesz spr√≥bowaƒá dw√≥ch pozosta≈Çych rozwiƒÖza≈Ñ.\nPrzydatne linki (czƒô≈õƒá potwierdza moje obserwacje, sƒÖ te≈º testy z drupalem):\nhttp://php.net/manual/en/book.apc.php http://xcache.lighttpd.net/ http://eaccelerator.net/¬†(w chwili pisanie strona nie dzia≈Ça≈Ça üòÉ)\nhttp://www.ducea.com/2006/10/30/php-accelerators/ http://2bits.com/articles/benchmarking-drupal-with-php-op-code-caches-apc-eaccelerator-and-xcache-compared.html http://2bits.com/articles/benchmarking-apc-vs-eaccelerator-using-drupal.html http://hostingfu.com/article/increasing-php-application-performance-xcache\n","permalink":"https://timor.site/2011/11/porownanie-optymalizatorow-php-eaccelerator-php-apc-xcache/","summary":"Przez pewien czas korzysta≈Çem z eAcceleratora do przyspieszenia dzia≈Çania stron pisanych w PHP\u0026rsquo;ie ale czasem bywa≈Ç niestabilny. Aktualizacje pojawia≈Çy siƒô rzadko a od czasu do czasu miewa≈Çem problemy ze stabilno≈õciƒÖ tej wtyczki na kilku bardziej skomplikowanych aplikacjach. Zdarza≈Ço siƒô ≈ºe pomimo zmiany kodu w skrypcie php, eAccelerator serwowa≈Ç wciƒÖ≈º stary plik - konieczny by≈Ç restart Apache\u0026rsquo;go by wszystko dzia≈Ça≈Ço jak trzeba.\nZaczƒÖ≈Çem szukaƒá alternatywy i trafi≈Çem na dwa modu≈Çy:\nAPC (czyli Alternative PHP Cache), kt√≥ry ma byƒá nawet domy≈õlnie wbudowany w PHP od wersji 5.","title":"Por√≥wnanie optymalizator√≥w PHP - eAccelerator, PHP APC, XCache"},{"content":"Administrowa≈Çem do tej pory g≈Ç√≥wnie darmowymi distro, ale gdzie≈õ tam ukradkiem wkrad≈Ço siƒô kilka \u0026ldquo;siusiak√≥w\u0026rdquo; (aka SUSE Linux Enterprise Server). ≈ªy≈Çem w utopijnym przekonaniu ≈ºe skoro siƒô za nie p≈Çaci to powinno siƒô z nimi ≈Çatwiej wsp√≥≈Çpracowaƒá\u0026hellip; w przypadku instalacji aktualizacji (a w szczeg√≥lno≈õci SP) nie by≈Ço to a≈º takie proste.\nPrzywyk≈Çem w darmowych dystrybucjach ≈ºe gdy pojawia≈Ça siƒô nowszƒÖ \u0026ldquo;wiƒôksza wersja\u0026rdquo; to po prostu mo≈ºna by≈Ço jednym poleceniem zaktualizowaƒá wszystkie pakiety. ≈πr√≥d≈Ça aktualizowa≈Çy siƒô automatycznie (lub prawie automatycznie) - p√≥≈∫niej trzeba by≈Ço po≈Çataƒá ewentualne zmiany w plikach konfiguracyjnych. W SUSE jest ciut inaczej\u0026hellip; üòâ\nInstalacja SP1 na SLES\u0026rsquo;ie 11 Instrukcja jest dla SLES\u0026rsquo;a¬†jedenastki (o ile pamiƒôtam dziesiƒÖtkƒô aktualizowa≈Ço siƒô inaczej) i Service Pack\u0026rsquo;a 1 - ale powinna zadzia≈Çaƒá r√≥wnie≈º w przypadku ka≈ºdego kolejnego SP. Zaczynamy!\nZ root\u0026rsquo;a uruchamiamy polecenia:\nzypper ref -s zypper up -t patch zypper up -t patch (Nie pomyli≈Çem siƒô - drugie polecenie nale≈ºy uruchomiƒá dwa razy - SIC!)\nPierwsze polecenie od≈õwie≈ºy informacje o dostƒôpnych us≈Çugach i repozytoriach.\nDrugie polecenie zainstaluje aktualizacje dla program√≥w zarzƒÖdzajƒÖcych paczkami w systemi, a kolejne wywo≈Çanie powinno zainstalowaƒá pozosta≈Çe dostƒôpne aktualizacje. Podw√≥jne wywo≈Çanie¬†zypper up -t patch znajduje siƒô w oficjalnej instrukcji - w nieoficjalnej znalezionej w sieci proponowano by uruchamiaƒá to polecenie do puki nie bƒôdzie mia≈Ço ju≈º nic wiƒôcej do zaktualizowania.\nPo wydaniu powy≈ºszych polece≈Ñ, w systemie (a dok≈Çadnie w plikach¬†/etc/products.d/*.prod) powinny pojawiƒá siƒô informacje o dostƒôpnych pakietach narzƒôdzi migracyjnych. By je wylistowaƒá nale≈ºy wydaƒá polecenie:\ngrep \u0026#39;\u0026lt;product\u0026gt;\u0026#39; /etc/products.d/*.prod U mnie da≈Ço to taki wynik:\n\u0026lt;product\u0026gt;sle-sdk-SP1-migration\u0026lt;/product\u0026gt; \u0026lt;product\u0026gt;SUSE_SLES-SP1-migration\u0026lt;/product\u0026gt; Je≈ºeli u Ciebie to polecenie nic nie zwr√≥ci≈Ço tzn. ≈ºe nie ma dostƒôpnych aktualizacji lub ≈ºe zbyt ma≈Ço razy uruchomiono zypper up -t patch üòÉ\nPowy≈ºsze \u0026ldquo;produkty\u0026rdquo; - nale≈ºy zainstalowaƒá poleceniem:\nzypper in -t product sle-sdk-SP1-migration SUSE_SLES-SP1-migration Aby zaktualizowaƒá system musimy mieƒá dostƒôp do podstawowego repozytorium z nowszymi wersjami pakiet√≥w - uzyskamy go rejestrujƒÖc siƒô:\nsuse_register -d 2 -L /root/.suse_register.log Je≈ºeli nie zapomnieli≈õmy o przed≈Çu≈ºeniu licencji i rejestracja przebieg≈Ça pomy≈õlnie to mo≈ºemy od≈õwie≈ºyƒá zawarto≈õƒá nowych repozytori√≥w i us≈Çug:\nzypper ref -s Teraz wylistujmy dostƒôpne repozytoria, poleceniem:\nzypper lr BARDZO WA≈ªNE: musimy wy≈ÇƒÖczyƒá stare repozytoria (dla systemu bez SP) i w≈ÇƒÖczyƒá nowe repozytoria dla systemu z SP1 - je≈ºeli tylko w≈ÇƒÖczymy nowe repozytoria to po kolejnej aktualizacji systemu mogƒÖ zainstalowaƒá siƒô paczki w starszych wersjach rozwalajƒÖc system! Sprawdza≈Çem osobi≈õcie i rzeczywi≈õcie tak jest üòÉ\nW≈ÇƒÖczanie/wy≈ÇƒÖczanie repozytori√≥w umo≈ºliwiajƒÖ polecenia:\nzypper mr -disable repozytorium_do_wylaczenia zypper mr -enable repozytorium_do_wlaczenia Dopiero teraz system jest gotowy do aktualizacji, kt√≥rƒÖ przeprowadzamy poleceniem (je≈ºeli instalujemy zdalnie warto odpaliƒá je spod screen‚Äòa):\nzypper dup Zypper zapyta czy chcemy usunƒÖƒá zainstalowane wcze≈õniej \u0026ldquo;produkty migracyjne\u0026rdquo; i zaktualizowaƒá pozosta≈Çe pakiety - nale≈ºy potwierdziƒá (oczywi≈õcie je≈õli jeste≈õmy absolutnie pewni i mamy backup üòÉ ).\nZN√ìW WA≈ªNE: po zako≈Ñczonej aktualizacji nale≈ºy ponownie zarejestrowaƒá siusiaka aby usunƒÖƒá repozytoria z aktualizacjami dla czystej wersji 11 i zastƒÖpiƒá je repami dla wersji z SP 1:\nsuse_register -d 2 -L /root/.suse_register.log Teraz mo≈ºemy zrestartowaƒá system by prze≈Çadowa≈Ço siƒô jajko i wszystkie us≈Çugi - je≈ºeli wszystko posz≈Ço po naszej my≈õli to powita nas SLES 11 SP 1.\nPrawda ≈ºe proste a ca≈Çy proces wrƒôcz oczywisty?\n","permalink":"https://timor.site/2011/10/sles-11-instalacja-service-packa/","summary":"Administrowa≈Çem do tej pory g≈Ç√≥wnie darmowymi distro, ale gdzie≈õ tam ukradkiem wkrad≈Ço siƒô kilka \u0026ldquo;siusiak√≥w\u0026rdquo; (aka SUSE Linux Enterprise Server). ≈ªy≈Çem w utopijnym przekonaniu ≈ºe skoro siƒô za nie p≈Çaci to powinno siƒô z nimi ≈Çatwiej wsp√≥≈Çpracowaƒá\u0026hellip; w przypadku instalacji aktualizacji (a w szczeg√≥lno≈õci SP) nie by≈Ço to a≈º takie proste.\nPrzywyk≈Çem w darmowych dystrybucjach ≈ºe gdy pojawia≈Ça siƒô nowszƒÖ \u0026ldquo;wiƒôksza wersja\u0026rdquo; to po prostu mo≈ºna by≈Ço jednym poleceniem zaktualizowaƒá wszystkie pakiety.","title":"SLES 11 - instalacja Service Pack‚Äôa"},{"content":"Bardzo czƒôsto konfigurujƒÖc us≈Çugi dostƒôpne publicznie po≈õwiƒôca siƒô sporo czasu na maksymalne zwiƒôkszenie bezpiecze≈Ñstwa przez \u0026ldquo;dopieszczanie\u0026rdquo; konfiguracji (certyfikaty z mocnym szyfrowaniem, ochronƒô pewnych stron has≈Çem, dostƒôp do SSH tylko kluczami, itd.) ale ca≈Çkowicie pomija siƒô przygotowanie systemu aktywnie monitorujƒÖcego b≈Çƒôdne pr√≥by autoryzacji. Oczywi≈õcie nie mo≈ºna umniejszaƒá wagi pierwszego z wymienionych etap√≥w ale zdecydowanie nie powinno pomijaƒá siƒô te≈º tego drugiego. Przecie≈º ka≈ºdy admin chcia≈Çby wiedzieƒá gdy kto≈õ pr√≥buje w≈Çamaƒá siƒô na jego serwer (FTP, HTTP, SSH, itp.) - tylko ilu z Nas zadaje sobie trud by uruchomiƒá taki system?\nWielu z nas uwa≈ºa ≈ºe przygotowanie takiego mechanizmu jest zbyt pracoch≈Çonne, trudne, nie teraz, nie mam czasu, itd\u0026hellip;. Zapominamy przy tym ≈ºe gotowy system zostanie oddany w rƒôce u≈ºyszkodnik√≥w a Ci na pewno bƒôdƒÖ z niego korzystaƒá wbrew wszelkim zasadom bezpiecze≈Ñstwa üòÉ\nNa przyk≈Çad taki Roman z loginem romek ustawi has≈Ço do poczty abc123 - wirusy i boty zamiast pr√≥bowaƒá ≈Çamaƒá has≈Ço jednego usera bardzo czƒôsto pr√≥bujƒÖ wbiƒá siƒô na popularne loginy wykorzystujƒÖc proste has≈Ça - zaskakujƒÖce jak czƒôsto siƒô im to udaje. A p√≥≈∫niej Roman ma pretensje do admina ≈ºe jego znajomi dostali po 10000 maili z wirusami\u0026hellip;\nTymczasem w wielu przypadkach wystarczy fail2ban w praktycznie podstawowej konfiguracji by ochroniƒá typowe us≈Çugi przed:\natakami brute force jak w powy≈ºszym przyk≈Çadnie (SSH, Apache, vsftpd, proftpd, Postfix, SASL Auth, etc), atakiem pocztowego bot netu (blokowanie host√≥w generujƒÖcych du≈ºƒÖ ilo≈õƒá b≈Çƒôd√≥w), z≈Çymi crawler\u0026rsquo;ami czy narzƒôdziami skanujƒÖcymi strony WWW, ataki flood na Bind DNS. Narzƒôdzie napisane jest w Perl\u0026rsquo;u i dzia≈Ça jako demon zapisujƒÖc dane z syslog\u0026rsquo;a w ma≈Çych paczkach i okresowo uruchamiajƒÖc testy sprawdzajƒÖce - wp≈Çywa to korzystnie na wydajno≈õƒá (bo nie musi analizowaƒá ca≈Çodniowych log√≥w jak prymitywniejsze narzƒôdzia). Modu≈Çowa konfiguracja pozwala ≈Çatwo dodaƒá filtry (wykrywajƒÖce niezdefiniowane w standardzie zdarzenia) lub akcje (np. zamiast wycinaƒá spam bota przez iptables mo≈ºna go wrzuciƒá do naszego prywatnego RBL\u0026rsquo;a). W innym po≈õcie poda≈Çem przyk≈Çad dodania regu≈Ç dla dovecot\u0026rsquo;a.\nPomimo i≈º narzƒôdzie wydaje siƒô zbytnio nie obciƒÖ≈ºaƒá systemu to raczej nie odwa≈ºy≈Çbym siƒô go zainstalowaƒá na mocno obciƒÖ≈ºonym serwerze, gdzie generujƒÖ siƒô miliony linii log√≥w dziennie. W takiej sytuacji potrzebne jest inne rozwiƒÖzanie.\nInstalacja W moim przypadku na Debianie leci to typowo:\napt-get install fail2ban Aplikacjƒô mo≈ºna te≈º do≈õƒá ≈Çatwo zainstalowaƒá ze ≈∫r√≥de≈Ç - zalezno≈õci nie ma zbyt du≈ºo.\nKonfiguracja Gdy ju≈º zainstalujemy fail2ban\u0026rsquo;a otwieramy g≈Ç√≥wny plik konfiguracyjny /etc/fail2ban/jail.conf. IdƒÖc od poczƒÖtku warto zainteresowaƒá siƒô opcjami:\n# poni≈ºej nale≈ºy umie≈õciƒá listƒô adres√≥w IP, sieci CIDR, z kt√≥rych # chcemy ignorowaƒá zagro≈ºenia ignoreip = 127.0.0.1 10.3.4.0/24 # domy≈õlny czas BAN\u0026#39;a - mo≈ºna nadpisaƒá w konfiguracji danej us≈Çugi bantime = 36000 # domy≈õlna ilo≈õƒá wykrytych akcji, po kt√≥rych zostanie dojdzie # do BAN\u0026#39;a maxretry = 3 # adres osoby, kt√≥ra ma byƒá informowana o na≈Ço≈ºeniu/zdjƒôciu BAN\u0026#39;a destemail = moj@mail.pl # domy≈õlna akcja, predefiniowane sƒÖ 3 mo≈ºliwo≈õci: # action_ - tylko BAN # action_mw - BAN i powiadomienie mailem z danymi WHOIS # action_mwl - jak wy≈ºej plus linie z loga action = %(action_mwl)s Dalej w konfigu znajdujƒÖ siƒô przygotowane definicje poszczeg√≥lnych us≈Çug i ich konfiguracja, np. SSH:\n[ssh] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 5 bantime = 3600 [ssh-ddos] enabled = true port = ssh filter = sshd-ddos logpath = /var/log/auth.log maxretry = 6 Jak wspomina≈Çem wcze≈õniej w danym bloku mo≈ºna wpisaƒá inne ni≈º domy≈õlne warto≈õci dla opcji bantime i maxretry. Opcja filter wskazuje na nazwƒô pliku z definicjƒÖ filtru, np:¬†/etc/fail2ban/filter.d/sshd.conf. W tej lokalizacji mo≈ºemy dodawaƒá te≈º w≈Çasne filtry, trzeba tylko znaƒá wyra≈ºenia regularne. W opcji port mo≈ºemy zmieniƒá port, na kt√≥rym dzia≈Ça us≈Çuga (by banowanie dzia≈Ça≈Ço), korzystajƒÖc przy tym z nazw dostƒôpnych w pliku /etc/services lub bezpo≈õrednio wpisujƒÖc numer portu.\nDomy≈õlnie nie jest w≈ÇƒÖczona ochrona ≈ºadnej z us≈Çug. By jƒÖ w≈ÇƒÖczyƒá nale≈ºy zmieniƒá w stosownym bloku:\nenabled = false na:\nenabled = true i prze≈Çadowaƒá fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart I na poczƒÖtek wystarczy. W kilku banalnych krokach uruchomili≈õmy system monitorujƒÖcy logi i blokujƒÖcy pr√≥by w≈Çama≈Ñ.\nOczywi≈õcie w≈ÇƒÖczajƒÖc ochronƒô danej us≈Çugi dobrze zapoznaƒá siƒô z regu≈Çami zapisanymi w danym filtrze - by nie byƒá zaskoczonym gdy sypnie mailami üòâ\nWy≈ÇƒÖczenie powiadamiania o starcie fail2ban\u0026rsquo;a Je≈ºeli w≈ÇƒÖczymy powiadamianie mailem o banach jako gratis fail2ban bƒôdzie powiadamiaƒá nas mailem o w≈Çaczeniu i wy≈ÇƒÖczeniu ochrony dla ka≈ºdej z us≈Çug - na wypadek gdyby kto≈õ bez naszej wiedzy zechcia≈Ç go wy≈ÇƒÖczyƒá. Dla wielu mo≈ºe to byƒá irytujƒÖce zachowanie.\nBy wy≈ÇƒÖczyƒá powiadomienia trzeba w¬†pliku akcji - u mnie w:¬†/etc/fail2ban/action.d/mail-whois-lines.conf¬†wyczy≈õciƒá opcje actionstart i actionstop by wyglƒÖda≈Çy jak poni≈ºej:\nactionstop = actioncheck = ","permalink":"https://timor.site/2011/10/ochrona-uslug-przed-atakami-brute-force-z-fail2banem/","summary":"Bardzo czƒôsto konfigurujƒÖc us≈Çugi dostƒôpne publicznie po≈õwiƒôca siƒô sporo czasu na maksymalne zwiƒôkszenie bezpiecze≈Ñstwa przez \u0026ldquo;dopieszczanie\u0026rdquo; konfiguracji (certyfikaty z mocnym szyfrowaniem, ochronƒô pewnych stron has≈Çem, dostƒôp do SSH tylko kluczami, itd.) ale ca≈Çkowicie pomija siƒô przygotowanie systemu aktywnie monitorujƒÖcego b≈Çƒôdne pr√≥by autoryzacji. Oczywi≈õcie nie mo≈ºna umniejszaƒá wagi pierwszego z wymienionych etap√≥w ale zdecydowanie nie powinno pomijaƒá siƒô te≈º tego drugiego. Przecie≈º ka≈ºdy admin chcia≈Çby wiedzieƒá gdy kto≈õ pr√≥buje w≈Çamaƒá siƒô na jego serwer (FTP, HTTP, SSH, itp.","title":"Ochrona us≈Çug przed atakami brute force z fail2ban‚Äôem"},{"content":"Je≈ºeli administrujesz nawet niedu≈ºym serwerem pocztowym na pewno masz ≈õwiadomo≈õƒá, ≈ºe nie jeste≈õ w stanie monitorowaƒá log√≥w na bie≈ºƒÖco. Ciƒô≈ºko jest wy≈Çapaƒá np. problem w komunikacji z pewnƒÖ domenƒÖ. Ciƒô≈ºko te≈º oszacowaƒá skalƒô ruchu na serwerze zar√≥wno pod kƒÖtem ilo≈õci jak i wolumenu maili. Trudno wybraƒá domeny, dla kt√≥rych warto by zrezygnowaƒá z greylistingu, itd, itp\u0026hellip;\nNa szczƒô≈õcie dostƒôpne jest narzƒôdzie pflogsumm, kt√≥re wygeneruje nam do≈õƒá wyczerpujƒÖce statystyki z log√≥w postfix\u0026rsquo;a. Bardzo przydatne przy codziennym przeglƒÖdzie \u0026ldquo;stanu zdrowia\u0026rdquo; serwera pocztowego.\nPrzyk≈Çadowy wycinek statystyk z pewnego ma≈Çego serwerka prezentuje siƒô tak:\nPostfix log summaries for Jul 4 Grand Totals ------------ messages 1158 received 1261 delivered 0 forwarded 5 deferred (50 deferrals) 2 bounced 392 rejected (23%) 0 reject warnings 0 held 0 discarded (0%) 164898k bytes received 242985k bytes delivered 201 senders 77 sending hosts/domains 354 recipients 51 recipient hosts/domains Per-Hour Traffic Summary time received delivered deferred bounced rejected -------------------------------------------------------------------- 0000-0100 26 28 2 0 7 0100-0200 14 18 3 0 10 0200-0300 4 4 1 0 8 0300-0400 6 6 1 0 8 0400-0500 4 4 0 0 8 0500-0600 2 2 1 0 9 0600-0700 8 8 1 0 9 0700-0800 16 18 1 0 10 0800-0900 58 60 1 0 8 0900-1000 104 110 5 0 17 1000-1100 132 152 2 0 18 1100-1200 106 106 1 0 31 1200-1300 64 70 2 0 9 1300-1400 112 132 2 0 14 1400-1500 98 106 1 0 78 1500-1600 86 88 2 0 32 1600-1700 56 56 3 0 23 1700-1800 58 77 5 2 19 1800-1900 36 36 3 0 16 1900-2000 26 26 2 0 24 2000-2100 48 50 3 0 9 2100-2200 32 42 2 0 10 2200-2300 34 34 3 0 10 2300-2400 28 28 3 0 5 ... Host/Domain Summary: Message Delivery sent cnt bytes defers avg dly max dly host/domain -------- ------- ------- ------- ------- ----------- 132 5688k 0 1.7 s 11.0 s gmail.com 104 2633k 0 4.6 s 2.8 m wp.pl 68 1525k 0 1.3 s 9.4 s interia.pl 42 744k 21 1.1 s 83.6 h o2.pl 30 89891 0 0.7 s 2.6 s op.pl 29 6677k 1 16.1 s 7.4 m poczta.onet.pl 26 540k 0 1.9 s 6.7 s poczta.fm ... Host/Domain Summary: Messages Received msg cnt bytes host/domain -------- ------- ----------- 50 4142k gmail.com 46 491259 facebookmail.com 38 1446k wp.pl 22 13520k interia.pl 14 675k o2.pl 10 105377 poczta.fm 10 57713 hotmail.com ... i du≈ºo wiƒôcej... Instalacja na Debianie:\napt-get install pflogsumm Testowo polecenie mo≈ºna uruchomiƒá w nastƒôpujƒÖcy spos√≥b:\nsudo pflogsumm -i -d yesterday /var/log/mail.log /var/log/mail.log.1 W moim przypadku, logi przewijam codziennie ok 2:00 w nocy, dlatego podajƒô dwie ≈õcie≈ºki do plik√≥w log (bie≈ºƒÖcego i wczorajszego) by mi te dwie godzinki nie umknƒô≈Çy üòâ\nPowy≈ºsze polecenie wypisze na standardowe wyj≈õcie statystyki w postaci ≈Çadnie sformatowanych tekstowych tabel. Warto przyjrzeƒá siƒô innym parametrom polecenia - mo≈ºna dziƒôki nim zrezygnowaƒá ze statystyk, kt√≥re nas nie interesujƒÖ, bƒÖd≈∫ zmieniƒá domy≈õlnƒÖ kolejno≈õƒá.\nTeraz warto uruchomiƒá okresowe raportowanie. Edytujemy crona:\nsudo crontab -e Na generowanie statystyk warto wybraƒá godzinƒô mniejszego obciƒÖ≈ºenia serwera (@daily oznacza p√≥≈Çnoc), bo proces ich przygotowania do≈õƒá mocno obciƒÖ≈ºy CPU. Wpisujemy polecenie wraz z interesujƒÖcymi nas parametrami:\n@daily /usr/sbin/pflogsumm -i --problems_first --no_bounce_detail \\ --no_deferral_detail -d yesterday \\ /var/log/mail.log /var/log/mail.log.1 | \\ mail -e -s \u0026#34;Statystyki poczty na `uname -n`\u0026#34; postmaster Kolejnego dnia powinni≈õmy otrzymaƒá nasze statystyki.\n","permalink":"https://timor.site/2011/09/pflogsum-statystyki-poczty-dla-postfixa/","summary":"Je≈ºeli administrujesz nawet niedu≈ºym serwerem pocztowym na pewno masz ≈õwiadomo≈õƒá, ≈ºe nie jeste≈õ w stanie monitorowaƒá log√≥w na bie≈ºƒÖco. Ciƒô≈ºko jest wy≈Çapaƒá np. problem w komunikacji z pewnƒÖ domenƒÖ. Ciƒô≈ºko te≈º oszacowaƒá skalƒô ruchu na serwerze zar√≥wno pod kƒÖtem ilo≈õci jak i wolumenu maili. Trudno wybraƒá domeny, dla kt√≥rych warto by zrezygnowaƒá z greylistingu, itd, itp\u0026hellip;\nNa szczƒô≈õcie dostƒôpne jest narzƒôdzie pflogsumm, kt√≥re wygeneruje nam do≈õƒá wyczerpujƒÖce statystyki z log√≥w postfix\u0026rsquo;a.","title":"pflogsumm - statystyki poczty dla postfix‚Äôa"},{"content":"Mia≈Çem ostatnio dziwnƒÖ przygodƒô: pewien serwer do backupu gdzie lƒÖduje du≈ºo ma≈Çych plik√≥w i dodatkowo tworzonych jest sporo hardlink√≥w zaliczy≈Ç pada. Co prawda stara≈Çem siƒô go grzecznie po≈Ço≈ºyƒá z pomocƒÖ Magic SysRq ale poniewa≈º nie wiedzia≈Çem co by≈Ço przyczynƒÖ awarii fsck wydawa≈Ç siƒô wskazany.\nPodczas pr√≥by uruchomienia fsck.ext4 na systemie plik√≥w o rozmiarze ok 14TB z kilkuset milionami plik√≥w po kilkudziesiƒôciu sekundach otrzymywa≈Çem komunikat:\nB≈ÇƒÖd podczas przydzielania struktury icount: Memory allocation failed\nGooglajƒÖc dowiedzia≈Çem siƒô ≈ºe problem jest znany i wystƒôpuje podczas sprawdzania bardzo du≈ºych system√≥w plik√≥w z du≈ºƒÖ ilo≈õciƒÖ hardlink√≥w. To akurat idealnie m√≥j przypadek\u0026hellip; Przewa≈ºnie zdarza siƒô to na systemach 32 bitowych gdy fsck pr√≥buje zaalokowaƒá powy≈ºej 2GB pamiƒôci. Jest to g√≥rny limit mo≈ºliwej do wykorzystania przez pojedynczy proces pamiƒôci dla architektury 32 bitowej - nie mo≈ºna go przeskoczyƒá nawet stosujƒÖc PAE. Ale m√≥j system jest 64 bitowy, 8GB RAM\u0026rsquo;u, 8GB swap\u0026rsquo;a - nie jest dobrze je≈õli brakuje pamiƒôci ;-/\nZgodnie z sugestiami Teo Tso by zmniejszyƒá zapotrzebowanie fsck na pamiƒôƒá mo≈ºna zmusiƒá go by informacje o inodach i w≈Ça≈õciwo≈õciach katalog√≥w przechowywa≈Ç w tymczasowym katalogu. By wskazaƒá katalog tymczasowy nale≈ºy utworzyƒá plik: /etc/e2fsck.conf z zawarto≈õciƒÖ:\n[scratch_files] directory = /var/cache/e2fsck Katalog trzeba utworzyƒá rƒôcznie:\nmkdir /var/cache/e2fsck Warto zadbaƒá o kilka/kilkana≈õcie gigabajt√≥w wolnego miejsca w tym katalogu - a najlepiej na czas sprawdzania podmontowaƒá jaki≈õ zas√≥b z fizycznie innego dysku ni≈º sprawdzany. Ja wykorzysta≈Çem w tym celu¬†kilku gigabajtowego¬†pen drive\u0026rsquo;a.\nTeraz mo≈ºna odpaliƒá fsck\u0026rsquo;a (odpalajƒÖc zdalnie warto zrobiƒá to w screen\u0026rsquo;ie):\nfsck.ext4 -f -C0 /dev/md1 Parametr -f wymusi sprawdzenie, a -C0 wy≈õwietli pasek postƒôpu co przy d≈Çugim sprawdzeniu da nam jakie≈õ wyobra≈ºenie postƒôpu. Tak uruchomiony fsck dzia≈Ça zdecydowanie wolniej ni≈º przy domy≈õlnych ustawieniach ale przynajmniej powinien siƒô wykonaƒá.\n# fsck.ext4 -fD -C0 /dev/md1 e2fsck 1.41.12 (17-May-2010) Przebieg 1: Sprawdzanie i-wƒôz≈Ç√≥w, blok√≥w i rozmiar√≥w /dev/md1: |=========== | 19.4% Opis podobnego problemu ze wsparciem Teo Tso:¬†http://www.linux-archive.org/ext3-users/103464-2gb-memory-limit-running-fsck-6tb-device.html\n","permalink":"https://timor.site/2011/09/fsck-ext4-blad-podczas-przydzielania-struktury-icount-memory-allocation-failed/","summary":"Mia≈Çem ostatnio dziwnƒÖ przygodƒô: pewien serwer do backupu gdzie lƒÖduje du≈ºo ma≈Çych plik√≥w i dodatkowo tworzonych jest sporo hardlink√≥w zaliczy≈Ç pada. Co prawda stara≈Çem siƒô go grzecznie po≈Ço≈ºyƒá z pomocƒÖ Magic SysRq ale poniewa≈º nie wiedzia≈Çem co by≈Ço przyczynƒÖ awarii fsck wydawa≈Ç siƒô wskazany.\nPodczas pr√≥by uruchomienia fsck.ext4 na systemie plik√≥w o rozmiarze ok 14TB z kilkuset milionami plik√≥w po kilkudziesiƒôciu sekundach otrzymywa≈Çem komunikat:\nB≈ÇƒÖd podczas przydzielania struktury icount: Memory allocation failed","title":"fsck.ext4 - B≈ÇƒÖd podczas przydzielania struktury icount: Memory allocation failed"},{"content":"Pomimo i≈º Linux uchodzi za stabilne ≈õrodowisko to raz na jaki≈õ czas trafi siƒô ciƒô≈ºka zwiecha - z powodu przeciƒÖ≈ºenia, awarii sprzƒôtu\u0026hellip; nieistotne\u0026hellip;\nZa≈Ç√≥≈ºmy ≈ºe licho wziƒô≈Ço za cel g≈Ç√≥wny serwer plik√≥w lub bazƒô danych dla wielu, wielu stron internetowych. Dostaƒá siƒô po ssh nie mo≈ºemy bo lecƒÖ timeout\u0026rsquo;y, a siedzƒÖc bezpo≈õrednio przy klawiaturze konsola nie reaguje. Mimo to co≈õ ostro daje po dyskach, wiƒôc ewentualny twardy reset to na bank utrata czƒô≈õci plik√≥w\u0026hellip; je≈õli system po nim w og√≥le wstanie\u0026hellip; üòë\nJe≈õli powy≈ºsza historyjka wyglƒÖda znajomo to zdecydowanie warto czytaƒá dalej.\nW wielu dystrybucjach kernel standardowo jest skompilowany z opcjƒÖ¬†CONFIG_MAGIC_SYSRQ - opcja opisana jest jako szczeg√≥lnie przydatna dla developer√≥w jƒÖdra ale i nam mo≈ºe siƒô przys≈Çu≈ºyƒá jako ostatnia deska ratunku przed twardym resetem.\nAby wywo≈Çaƒá funkcjƒô trzeba przytrzymaƒá na klawiaturze: Lewy ALT + PrintScrn/SysRq i przycisk okre≈õlajƒÖcy funkcjƒô. Kilka z mo≈ºliwych funkcji to:\nr - prze≈ÇƒÖcz klawiaturƒô z trybu RAW do XLATE (w wolnym t≈Çumaczeniu: odzyskaj obs≈Çugƒô klawiatury od X\u0026rsquo;√≥w), e - wy≈õlij SIGTERM do wszystkich proces√≥w z wyjƒÖtkiem init\u0026rsquo;a, i - wy≈õlij SIGKILL do wszystkich proces√≥w z wyjƒÖtkiem init\u0026rsquo;a, s - wywo≈Çaj sync dla wszystkich zamontowanych zasob√≥w (czyli zapisz wszystkie niezapisane dotƒÖd transakcje dyskowe), u - przemontuj wszystkie zamontowane zasoby to trybu tylko do odczytu, b - natychmiast uruchom ponownie system, bez odmontowania partycji i bez synchronizacji dysk√≥w. To tylko czƒô≈õƒá funkcji, ale wykonanie ich w kolejno≈õci w jakiej zosta≈Çy tu wypisane (czyli reisub) powinno skutkowaƒá STOSUNKOWO bezpiecznym resetem, po kt√≥rym nawet nie powinno byƒá potrzebne skanowanie dysk√≥w fsck\u0026rsquo;iem. Ka≈ºda z funkcji potrzebuje kilka/kilkana≈õcie sekund na wykonanie (szczeg√≥lnie warto zaczekaƒá by po sync\u0026rsquo;u przesta≈Ça migaƒá kontrolka aktywno≈õci dysk√≥w twardych) - nie warto siƒô spieszyƒá.¬†Po ostatnim wywo≈Çaniu system powinien siƒô zrestartowaƒá - zdarzy≈Ço mi siƒô tylko kilka razy by procedura ta zawiod≈Ça.\nMetodƒô mo≈ºna te≈º wykorzystaƒá dla serwer√≥w zdalnych, kt√≥re np. z jakiego≈õ powodu nie chcƒÖ siƒô zrestartowaƒá w standardowy spos√≥b. Mo≈ºna to zrobiƒá wysy≈ÇajƒÖc poszczeg√≥lne polecenia do¬†/proc/sysrq-trigger, np. tak:\necho b \u0026gt; /proc/sysrq-trigger Na zdalnych maszynach i VPS\u0026rsquo;ach wygodniejsze mo≈ºe byƒá skonfigurowanie demona sysrqd.\nDla zainteresowanych, wiƒôcej funkcji Magic SysRq (g≈Ç√≥wnie diagnostycznych) i dok≈Çadniejszy opis mo≈ºna znale≈∫ƒá na stronie wiki.\n","permalink":"https://timor.site/2011/09/magic-sysrq-bezpieczny-reset-linuxa/","summary":"Pomimo i≈º Linux uchodzi za stabilne ≈õrodowisko to raz na jaki≈õ czas trafi siƒô ciƒô≈ºka zwiecha - z powodu przeciƒÖ≈ºenia, awarii sprzƒôtu\u0026hellip; nieistotne\u0026hellip;\nZa≈Ç√≥≈ºmy ≈ºe licho wziƒô≈Ço za cel g≈Ç√≥wny serwer plik√≥w lub bazƒô danych dla wielu, wielu stron internetowych. Dostaƒá siƒô po ssh nie mo≈ºemy bo lecƒÖ timeout\u0026rsquo;y, a siedzƒÖc bezpo≈õrednio przy klawiaturze konsola nie reaguje. Mimo to co≈õ ostro daje po dyskach, wiƒôc ewentualny twardy reset to na bank utrata czƒô≈õci plik√≥w\u0026hellip; je≈õli system po nim w og√≥le wstanie\u0026hellip; üòë","title":"Magic SysRq - bezpieczny reset Linux‚Äôa"},{"content":"Wielu administrator√≥w gdy zaczyna swojƒÖ przygodƒô zarzƒÖdza jednƒÖ/dwoma maszynami\u0026hellip; Po pewnym czasie jest ich ju≈º kilka\u0026hellip; W kt√≥rym≈õ momencie dostrzega siƒô zalety wirtualizacji i na kilku maszynach fizycznych dzia≈Ça kilkana≈õcie czy kilkadziesiƒÖt maszyn wirtualnych. W takiej sytuacji pobieranie aktualizacji dla wszystkich maszyn potrafi mocno zabiƒá ≈ÇƒÖcze.\nI w tym momencie zaczynamy siƒô zastanawiaƒá czy mo≈ºe nie warto by≈Çoby zrobiƒá w≈Çasnego mirror\u0026rsquo;a paczek dla naszego ulubionego distro\u0026hellip; do prywatnego u≈ºytku\u0026hellip; synchronizowanego w nocy by nikomu nie przeszkadzaƒá\u0026hellip; i dostƒôpnego nawet gdy bƒôdziemy offline\u0026hellip; Zaczynamy liczyƒá miejsce i okazuje siƒô ≈ºe repozytorium Debiana dla architektury i386 to prawie 60GB (sic!), no ale mamy kilka maszynek z arch amd64 i tutaj te≈º prawie 60GB - auƒá. W tym miejscu wielu dochodzi do wniosku ≈ºe to jeszcze nie pora na w≈Çasnego mirror\u0026rsquo;a üòÉ\nRozwiƒÖzaniem tego dylematu jest wykorzystanie approx\u0026rsquo;a, kt√≥ry dzia≈Ça jako cachujƒÖce proxy dla repozytori√≥w Debiania i Ubuntu. Po≈õredniczy on w pobieraniu plik√≥w zapisujƒÖc kopiƒô ka≈ºdego w lokalnym cache\u0026rsquo;u. Czyli pierwsze pobranie zasysa paczkƒô z sieci, a kolejne odwo≈Çania do approx\u0026rsquo;a serwujƒÖ ju≈º tƒô pobranƒÖ kopiƒô. Ponadto przyjemne jest zachowanie approx\u0026rsquo;a, kt√≥ry samodzielnie utrzymuje \u0026ldquo;czysto≈õƒá\u0026rdquo; cachu usuwajƒÖc starsze paczki i aktualizujƒÖc nowe. Druga wielka zaleta tej aplikacji to lokalne sk≈Çadowanie tylko tych paczek kt√≥re pobieramy - bo ma≈Ço kto potrzebuje ca≈Çego repozytorium - drastycznie zmniejsza to rozmiar takiego mirror\u0026rsquo;a. U mnie mirror ma raptem kilkaset megabajt√≥w.\nInstalacja i konfiguracja Poniewa≈º paczka jest w domy≈õlnych repozytoriach wystarczy:\napt-get install approx i po chwili mamy dzia≈ÇajƒÖce proxy.\nDomy≈õlnie pliki sk≈Çadowane sƒÖ w: /var/cache/approx - warto zadbaƒá o odrobinƒô wolnego miejsca w tej lokalizacji.\nNale≈ºy jeszcze skonfigurowaƒá odpowiednie repozytoria w pliku konfiguracyjnym: /etc/approx/approx.conf¬†- u mnie wyglƒÖda on tak:\ndebian http://ftp.pl.debian.org/debian security http://security.debian.org/debian-security volatile http://volatile.debian.org/debian-volatile backports http://backports.debian.org/debian-backports # The following are the default parameter values, so there is # no need to uncomment them unless you want a different value. # See approx.conf(5) for details. $interface any $port 8080 $max_wait 30 #$max_rate unlimited #$user approx #$group approx #$syslog daemon #$pdiffs true #$verbose false #$debug true Po instalacji approx dzia≈Ça na porcie 8080 o czym trzeba pamiƒôtaƒá przy podawaniu URL\u0026rsquo;i w sources.list - mi to odpowiada≈Ço bo na porcie 80-tym mam odpalony serwer WWW pod jakie≈õ ≈õmieci\u0026hellip; Ale Wam mo≈ºe odpowiadaƒá uruchomienie approx\u0026rsquo;a na 80-tce.\nTeraz restartujemy approx\u0026rsquo;a by prze≈Çadowaƒá konfiguracjƒô:\ninvoke-rc.d approx restart Zosta≈Ço skonfigurowaƒá plik /etc/apt/sources.list tak by wskazywa≈Ç na nasze proxy. Nale≈ºy to zrobiƒá na ka≈ºdej maszynce w naszej sieci by ca≈Ça zabawa mia≈Ça sens.\nW przypadku Lenny\u0026rsquo;ego powinien on wyglƒÖdaƒá mniej wiƒôcej tak:\ndeb http://approx.costam.pl:8080/debian lenny main non-free contrib deb-src http://approx.costam.pl:8080/debian lenny main non-free contrib deb http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb-src http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb-src http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb http://approx.costam.pl:8080/backports lenny-backports-sloppy main W przypadku Squeeze\u0026rsquo;a tak:\ndeb http://approx.costam.pl:8080/debian squeeze main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze main non-free contrib deb http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb-src http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb http://approx.costam.pl:8080/backports squeeze-backports main No i na koniec sprawdzamy czy wszystko dzia≈Ça:\napt-get update Mo≈ºemy spr√≥bowaƒá co≈õ zainstalowaƒá i zobaczyƒá jak siƒô spisze proxy.\n","permalink":"https://timor.site/2011/09/approx-cachujace-proxy-dla-repozytoriow-debiana/","summary":"Wielu administrator√≥w gdy zaczyna swojƒÖ przygodƒô zarzƒÖdza jednƒÖ/dwoma maszynami\u0026hellip; Po pewnym czasie jest ich ju≈º kilka\u0026hellip; W kt√≥rym≈õ momencie dostrzega siƒô zalety wirtualizacji i na kilku maszynach fizycznych dzia≈Ça kilkana≈õcie czy kilkadziesiƒÖt maszyn wirtualnych. W takiej sytuacji pobieranie aktualizacji dla wszystkich maszyn potrafi mocno zabiƒá ≈ÇƒÖcze.\nI w tym momencie zaczynamy siƒô zastanawiaƒá czy mo≈ºe nie warto by≈Çoby zrobiƒá w≈Çasnego mirror\u0026rsquo;a paczek dla naszego ulubionego distro\u0026hellip; do prywatnego u≈ºytku\u0026hellip; synchronizowanego w nocy by nikomu nie przeszkadzaƒá\u0026hellip; i dostƒôpnego nawet gdy bƒôdziemy offline\u0026hellip; Zaczynamy liczyƒá miejsce i okazuje siƒô ≈ºe repozytorium Debiana dla architektury i386 to prawie 60GB (sic!","title":"approx - cachujƒÖce proxy dla repozytori√≥w Debiana"},{"content":"Linux bardzo agresywnie wykorzystuje wolnƒÖ pamiƒôƒá RAM do buforowania danych odczytywanych z dysk√≥w (inode\u0026rsquo;√≥w, plik√≥w, itd\u0026hellip;). Ma to niebagatelny wp≈Çyw na zwiƒôkszenie szybko≈õci uruchamiania program√≥w kt√≥re ju≈º raz zosta≈Çy uruchomione. Jednak nie zawsze jest to po≈ºƒÖdane zachowanie, np. testujƒÖc szybko≈õƒá uruchomienia/wykonywania tworzonej przez nas aplikacji - buforowanie zmienia czas ≈Çadowania aplikacji przy kolejnych uruchomieniach.¬†Dobrze by≈Çoby m√≥c wymusiƒá zwolnienie bufor√≥w by ka≈ºdy start programu mia≈Ç por√≥wnywalne \u0026ldquo;warunki startowe\u0026rdquo;.\nNa szczƒô≈õcie mo≈ºna to zrobiƒá w prosty spos√≥b:\nsync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches Polecenie to zwolni niewykorzystywany cache pliku stronicowania, katalog√≥w i inod√≥w.¬†Wcze≈õniejsze uruchomienie¬†sync¬†pozwala zwolniƒá wiƒôkszƒÖ ilo≈õƒá bufor√≥w przez wymuszenie zapisania otwartych plik√≥w.\nInne mo≈ºliwe warianty to:\nzwolnienie cache pliku stronicowania: sync \u0026amp;\u0026amp; echo 1 \u0026gt; /proc/sys/vm/drop_caches zwolnienie cache cache katalog√≥w i inod√≥w: sync \u0026amp;\u0026amp; echo 2 \u0026gt; /proc/sys/vm/drop_caches Opcja ta dostƒôpna jest w jajkach od wersji 2.6.16.\n","permalink":"https://timor.site/2011/09/wymuszenie-zwolnienia-pamieci-buforow-dyskowych-na-linuxie/","summary":"Linux bardzo agresywnie wykorzystuje wolnƒÖ pamiƒôƒá RAM do buforowania danych odczytywanych z dysk√≥w (inode\u0026rsquo;√≥w, plik√≥w, itd\u0026hellip;). Ma to niebagatelny wp≈Çyw na zwiƒôkszenie szybko≈õci uruchamiania program√≥w kt√≥re ju≈º raz zosta≈Çy uruchomione. Jednak nie zawsze jest to po≈ºƒÖdane zachowanie, np. testujƒÖc szybko≈õƒá uruchomienia/wykonywania tworzonej przez nas aplikacji - buforowanie zmienia czas ≈Çadowania aplikacji przy kolejnych uruchomieniach.¬†Dobrze by≈Çoby m√≥c wymusiƒá zwolnienie bufor√≥w by ka≈ºdy start programu mia≈Ç por√≥wnywalne \u0026ldquo;warunki startowe\u0026rdquo;.\nNa szczƒô≈õcie mo≈ºna to zrobiƒá w prosty spos√≥b:","title":"Wymuszenie zwolnienia pamiƒôci bufor√≥w dyskowych na Linux‚Äôie"},{"content":"Od jakiego≈õ czasu dostƒôpny jest w sieci skrypt slowloris.pl¬†pozwalajƒÖcy z pojedynczego komputera wykonaƒá atak DOS na zdalny serwer WWW. Atak polega na uruchomieniu wielu r√≥wnoczesnych sesji i bardzo wolnym wysy≈Çaniu komunikat√≥w HTTP. AtakujƒÖcy udaje \u0026ldquo;klienta z wolnym ≈ÇƒÖczem\u0026rdquo; r√≥wnocze≈õnie uruchamiajƒÖc kolejne sesje by po pewnym czasie zajƒÖƒá wszystkie dostƒôpne. Serwer WWW przestaje wtedy odpowiadaƒá na zapytania od innych klient√≥w. Dodatkowo na ≈∫le wyskalowanych serwerach du≈ºa liczba proces√≥w Apachego mo≈ºe spowodowaƒá swapowanie i b≈Çƒôdy braku pamiƒôci.\nW zale≈ºno≈õci od wydajno≈õci atakowanej maszyny by doprowadziƒá do jej zablokowania potrzeba przewa≈ºnie od kilkunastu do kilkudziesiƒôciu sekund. Tak przeprowadzony atak DOS nie wymaga botnetu czy super wydajnego sprzƒôtu - zwyk≈Çy lapciak w zupe≈Çno≈õci wystarczy.\nNa chwilƒô obecnƒÖ sƒÖ ju≈º co najmniej dwie metody ochrony Apachego przed takim atakiem: mod_antiloris i¬†mod_reqtimeout. Pierwszy dostƒôpny jako modu≈Ç do rƒôcznej kompilacji dla starszych wersji Apache (np. w Debianie Lennym). Drugi dostƒôpny jest w standardzie od wersji 2.2.15 (np. w Debianie Squeeze).\nmod_antiloris pozwala na limitowanie ilo≈õci r√≥wnoczesnych sesji dla zdalnego klienta. W przypadku przekroczenia dozwolonej liczby zrywane jest po≈ÇƒÖczenie. Ma to swoje wady, np. gdy z naszego serwera WWW korzysta jaka≈õ du≈ºa NAT\u0026rsquo;owana sieƒá to przy wiƒôkszej liczbie po≈ÇƒÖcze≈Ñ zostanƒÖ zablokowani \u0026ldquo;dobrzy\u0026rdquo; klienci. Trudne jest te≈º w≈Ça≈õciwe ustawienie maksymalnej liczby po≈ÇƒÖcze≈Ñ - domy≈õlnie ustawiona jest warto≈õƒá 5. Ale niekt√≥re przeglƒÖdarki (lub wtyczki do nich) podnoszƒÖ limit r√≥wnoczesnych po≈ÇƒÖcze≈Ñ per serwer do 8.\nmod_reqtimeout pozwala na okre≈õlenie po jakim czasie przerwaƒá po≈ÇƒÖczenie w przypadku nie utrzymywania wystarczajƒÖcego przep≈Çywu danych. SkracajƒÖc - pozwala precyzyjnie wyciƒÖƒá \u0026ldquo;wolnych\u0026rdquo; klient√≥w. Modu≈Ç ten monitoruje ka≈ºdƒÖ sesjƒô z osobna przez co nie ma zagro≈ºenia blokowania sieci NAT czy \u0026ldquo;agresywnie\u0026rdquo; ustawionych przeglƒÖdarek.\nInstalacja i konfiguracja mod_antiloris¬†(Apache do wersji 2.2.14) Najpierw musimy pobraƒá potrzebne zale≈ºno≈õci:\napt-get install gcc apache2-prefork-dev P√≥≈∫niej pobieramy mod\u0026rsquo;a i rozpakowujemy:\nwget \u0026#34;ftp://ftp.monshouwer.eu/pub/linux/mod_antiloris/mod_antiloris-0.4.tar.bz2\u0026#34; tar xvf mod_antiloris-0.4.tar.bz2 cd mod_antiloris-0.4 Je≈ºeli mamy takƒÖ potrzebƒô mo≈ºemy wyedytowaƒá plik mod_antiloris.c i podnie≈õƒá limit po≈ÇƒÖcze≈Ñ:\n#define antiloris_MAX_PER_IP 5 Do kompilacji modu≈Çu wykorzystamy narzƒôdzie apxs2, kt√≥re skompiluje modu≈Ç jako dynamicznie ≈Çadowalny:\napxs2 -c mod_antiloris.c Teraz mo≈ºemy skopiowaƒá modu≈Ç do katalogu z innymi modu≈Çami:\nsudo cp .libs/mod_antiloris.so /usr/lib/apache2/modules/mod_antiloris.so Musimy te≈º utworzyƒá plik konfiguracyjny, kt√≥ry bƒôdzie ≈Çadowaƒá mod\u0026rsquo;a:\nsudo su -c \u0026#34;echo \u0026#39;LoadModule antiloris_module /usr/lib/apache2/modules/mod_antiloris.so\u0026#39; \u0026gt; /etc/apache2/mods-available/antiloris.load\u0026#34; W≈ÇƒÖczamy modu≈Ç:\nsudo a2enmod antiloris Na koniec musimy prze≈Çadowaƒá Apache\u0026rsquo;go:\nsudo service apache2 reload Instalacja i konfiguracja mod_reqtimeout (Apache od wersji 2.2.15) Poniewa≈º modu≈Ç jest dostƒôpny wystarczy go uaktywniƒá i prze≈Çadowaƒá Apachego:\nsudo a2enmod reqtimeout sudo service apache2 reload Domy≈õlnie w Squeeze dostƒôpny jest plik konfiguracyjny z poni≈ºszymi warto≈õciami:\nRequestReadTimeout header=20-40,minrate=500 RequestReadTimeout body=10,minrate=500 Pierwsza opcja oznacza: zezwalaj na wysy≈Çanie zapytania przez co najmniej 20 sekund i zwiƒôkszaj limit do maksymalnie 40 sekund po 1 sekundzie za ka≈ºde przes≈Çane 500 bajt√≥w.\nDruga opcja oznacza: zezwalaj na pobieranie przez co najmniej 10 sekund i zwiƒôkszaj limit czasu w niesko≈Ñczono≈õƒá po 1 sekundzie za ka≈ºde pobrane 500 bajt√≥w.\nDomy≈õlne ustawienia sƒÖ sensowne i powinny wystarczyƒá w wiƒôkszo≈õci przypadk√≥w. Dodatkowe dopieszczenie tych opcji mo≈ºe byƒá potrzebne na serwerach wysy≈ÇajƒÖcych bƒÖd≈∫ odbierajƒÖcych do≈õƒá du≈ºe pliki lub w przypadku \u0026ldquo;spersonalizowanych\u0026rdquo; atak√≥w DOS.\nTest dzia≈Çania Skoro mamy ju≈º \u0026ldquo;tarczƒô\u0026rdquo; warto sprawdziƒá czy dzia≈Ça. W tym celu pobieramy slowlorisa:\nwget \u0026#34;http://ha.ckers.org/slowloris/slowloris.pl\u0026#34; chmod +x slowloris.pl I mo≈ºemy uruchomiƒá test:\nperl slowloris.pl -dns twojserwer.pl -port 80 -timeout 1 -num 300 -cache Je≈ºeli po dw√≥ch minutach (w zale≈ºno≈õci od konfiguracji sprzƒôtowej serwera) strona odpowiada i mo≈ºna siƒô na niƒÖ bez problem√≥w dostaƒá to znaczy ≈ºe nasz wysi≈Çek siƒô op≈Çaci≈Ç.\nDalszym krokami wartymi podjƒôcia mo≈ºe byƒá analiza log√≥w np. z u≈ºyciem fail2ban i wycinanie na firewallu bardziej uciƒÖ≈ºliwych go≈õci.\n","permalink":"https://timor.site/2011/09/zabezpieczenie-apachego-na-debianie-przed-slowlorisem/","summary":"Od jakiego≈õ czasu dostƒôpny jest w sieci skrypt slowloris.pl¬†pozwalajƒÖcy z pojedynczego komputera wykonaƒá atak DOS na zdalny serwer WWW. Atak polega na uruchomieniu wielu r√≥wnoczesnych sesji i bardzo wolnym wysy≈Çaniu komunikat√≥w HTTP. AtakujƒÖcy udaje \u0026ldquo;klienta z wolnym ≈ÇƒÖczem\u0026rdquo; r√≥wnocze≈õnie uruchamiajƒÖc kolejne sesje by po pewnym czasie zajƒÖƒá wszystkie dostƒôpne. Serwer WWW przestaje wtedy odpowiadaƒá na zapytania od innych klient√≥w. Dodatkowo na ≈∫le wyskalowanych serwerach du≈ºa liczba proces√≥w Apachego mo≈ºe spowodowaƒá swapowanie i b≈Çƒôdy braku pamiƒôci.","title":"Zabezpieczenie Apachego na Debianie przed slowloris‚Äôem"},{"content":"Na jednym z serwer√≥w zauwa≈ºy≈Çem dziwny wzrost obciƒÖ≈ºenia. Tzw. LOAD od kilku dni po woli r√≥s≈Ç. top pokazywa≈Ç ≈ºe dwa rdzenie CPU czekajƒÖ na dane z dysku - tzw. io wait na poziomie 80~90% ale ≈ºaden proces w znaczƒÖcym stopniu nie obciƒÖ≈ºa≈Ç CPU.\nJest kilka narzƒôdzi (iostat, wmstat), kt√≥re pozwalajƒÖ monitorowaƒá obciƒÖ≈ºenie dysk√≥w ale ja nie szuka≈Çem informacji czy i w jakim stopniu dyski sƒÖ obciƒÖ≈ºone - wiedzia≈Çem ≈ºe sƒÖ. Chcia≈Çem dowiedzieƒá siƒô kt√≥ry proces generuje to obciƒÖ≈ºenie - by m√≥c go ubiƒá üòÉ\nPrzydatny okaza≈Ç siƒô programik iotop - kt√≥ry dzia≈Ça jak top ale sortuje procesy w zale≈ºno≈õci od generowanego przez nie obciƒÖ≈ºenia dysk√≥w - w≈Ça≈õnie tego szuka≈Çem:\nProgram jest w standardowych repozytoriach Debiana i mo≈ºna go zainstalowaƒá w ten spos√≥b:\napt-get install iotop ","permalink":"https://timor.site/2011/09/sprawdzenie-ktory-proces-obciaza-dyski/","summary":"Na jednym z serwer√≥w zauwa≈ºy≈Çem dziwny wzrost obciƒÖ≈ºenia. Tzw. LOAD od kilku dni po woli r√≥s≈Ç. top pokazywa≈Ç ≈ºe dwa rdzenie CPU czekajƒÖ na dane z dysku - tzw. io wait na poziomie 80~90% ale ≈ºaden proces w znaczƒÖcym stopniu nie obciƒÖ≈ºa≈Ç CPU.\nJest kilka narzƒôdzi (iostat, wmstat), kt√≥re pozwalajƒÖ monitorowaƒá obciƒÖ≈ºenie dysk√≥w ale ja nie szuka≈Çem informacji czy i w jakim stopniu dyski sƒÖ obciƒÖ≈ºone - wiedzia≈Çem ≈ºe sƒÖ. Chcia≈Çem dowiedzieƒá siƒô kt√≥ry proces generuje to obciƒÖ≈ºenie - by m√≥c go ubiƒá üòÉ","title":"Sprawdzenie kt√≥ry proces obciƒÖ≈ºa dyski"},{"content":"Onego czasu pr√≥bowa≈Çem znale≈∫ƒá co≈õ co u≈Çatwi≈Çoby mi rysowanie prostych wykres√≥w w¬†PHP\u0026rsquo;ie inaczej ni≈º z¬†palca w¬†GD. Kumpel poleci≈Ç mi¬†JPGraph.\nJPGraph to ≈õwietna sprawa, do generowania statystyk jak chocia≈ºby na mojej¬†stronie, ale biblioteka potrafi du≈ºo wiƒôcej\u0026hellip;\nZa≈Ç√≥≈ºmy, ≈ºe ze stronki zbieramy do bazy takie rzeczy jak: datƒô, adres IP, ilo≈õƒá po≈ÇƒÖcze≈Ñ z¬†tego adresu. Prosta tabela (przyk≈Çad w¬†Postgre SQL\u0026rsquo;u):\nCREATE TABLE wizyty ( pid serial NOT NULL, \u0026#34;data\u0026#34; date NOT NULL DEFAULT (\u0026#39;now\u0026#39;::text)::date, odslony integer NOT NULL DEFAULT 1, CONSTRAINT visits_pkey PRIMARY KEY (id) ); Dane z¬†takiej tabeli mo≈ºna ≈Çatwo wyciƒÖgnƒÖƒá jednym select\u0026rsquo;em:\nSELECT date_part(\u0026#39;day\u0026#39;, \u0026#34;data\u0026#34;) AS x, sum(odslony) AS y FROM wizyty WHERE date \u0026gt; (\u0026#39;today\u0026#39;::date - \u0026#39;30 days\u0026#39;::interval) GROUP BY \u0026#34;data\u0026#34; ORDER BY \u0026#34;data\u0026#34; LIMIT 30 Powy≈ºsze zapytanie wyciƒÖgnie nam z¬†kolumny z¬†datƒÖ tylko dzie≈Ñ miesiƒÖca, kt√≥ry pos≈Çu≈ºy za etykietƒô (o≈õ X) oraz sumƒô ods≈Çon z¬†danego dnia (o≈õ Y), posortowane wed≈Çug daty i¬†tylko z¬†ostanich 30 dni.\nW zale≈ºno≈õci od sposobu dostƒôpu i¬†pobierania wynik√≥w, trzeba je ≈Çadnie zapisaƒá w¬†dw√≥ch tablicach. Ja to robiƒô w¬†prostej pƒôtli:\nforeach ($query-\u0026gt;result() as $row) { $x[] = $row-\u0026gt;x; $y[] = $row-\u0026gt;y; } MajƒÖc dane zosta≈Ça zabawa z¬†ustawieniem oczekiwanych opcji wykresu:\n/* bez tego ani rusz */ include_once \u0026#39;jpgraph/jpgraph.php\u0026#39;; include_once \u0026#39;jpgraph/jpgraph_bar.php\u0026#39;; /* szeroko≈õƒá */ $width = 400; /* wysoko≈õƒá */ $height = 300; $graph = new Graph($width, $height); /* o≈õ X tekst, o≈õ Y warto≈õci ca≈Çkowite */ $graph-\u0026gt;SetScale(\u0026#34;textint\u0026#34;); /* kolorowa ramka */ $graph-\u0026gt;SetFrame(true, \u0026#39;#222222\u0026#39;); $graph-\u0026gt;SetMarginColor(\u0026#39;#222222\u0026#39;); /* tytu≈Ç wykresu, trochƒô konfiguracji font√≥w i kolor√≥w */ $graph-\u0026gt;title-\u0026gt;Set(\u0026#39;Sumaryczna liczba ods≈Çon\u0026#39;); $graph-\u0026gt;title-\u0026gt;SetFont(FF_VERDANA,FS_BOLD,10); $graph-\u0026gt;title-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* podobnie≈º jak powy≈ºej dla osi X */ $graph-\u0026gt;xaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;xaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;yaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;yaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* dodatkowe ozdobniki ;) */ $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SupressZeroLabel(false); $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;xaxis-\u0026gt;SetTickLabels($x); /* genrujemy w≈Ça≈õciwy wykres, w tym przypadku s≈Çupkowy */ $bplot = new BarPlot($y); $bplot-\u0026gt;SetWidth(0.6); $bplot-\u0026gt;SetFillColor(\u0026#39;#D01A71@0.25\u0026#39;); $bplot-\u0026gt;SetColor(\u0026#39;gray\u0026#39;); /* dodajemy wykres do obiektu $graph (kt√≥ry to mo≈ºe pomie≈õciƒá i wy≈õwietliƒá wiele wykres√≥w, co z resztƒÖ pokazane jest poni≈ºej) */ $graph-\u0026gt;Add($bplot); /* teraz wykres liniowy */ $lplot = new LinePlot($y); /* z wype≈Çnieniem pod wykresem */ $lplot-\u0026gt;SetFillColor(\u0026#39;skyblue@0.5\u0026#39;); $lplot-\u0026gt;SetColor(\u0026#39;navy@0.7\u0026#39;); $lplot-\u0026gt;SetBarCenter(); /* typ znacznika na ≈ÇƒÖcznikach pomiƒôdzy kolejnymi s≈Çupkami */ $lplot-\u0026gt;mark-\u0026gt;SetType(MARK_SQUARE); $lplot-\u0026gt;mark-\u0026gt;SetColor(\u0026#39;blue@0.5\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetFillColor(\u0026#39;lightblue\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetSize(6); /* wrzucamy kolejny wykres do obrazu */ $graph-\u0026gt;Add($lplot); /* a teraz magia... */ $graph-\u0026gt;Stroke(); Co to robi\u0026hellip; Ano dok≈Çadnie taki wykres jak ten poni≈ºej:\nCo prawda ten wykres nie odpowiada w¬†100% podanemu ≈∫r√≥d≈Çu. To statystyka z¬†mojej strony, kt√≥ra niebieskƒÖ liniƒÖ zaznacza volumen pobranych danych - przyk≈Çad jest nieco uproszczony.\nNa pierwszy rzut oka powy≈ºszy skrypt wydaje siƒô do≈õƒá d≈Çugi i¬†jest tam du≈ºo nie do ko≈Ñca jasnych opcji, ale najfajniejsze jest to, ≈ºe tak na prawdƒô nie trzeba siƒô tego uczyƒá. Biblioteka domy≈õlnie dostarczana jest z¬†masƒÖ przyk≈Çad√≥w. Wystarczy wybraƒá wykres podobny do tego, kt√≥ry sami chcemy zrobiƒá - skopiowaƒá wiƒôkszo≈õƒá kodu. Znale≈∫ƒá inne wykresy, kt√≥rych cechy chcemy powieliƒá i¬†po kilku pr√≥bach skleimy takiego Frankenstein\u0026rsquo;a jakiego ≈õwiat nie widzia≈Ç wcze≈õniej ;-D\nM√≥j wykres akurat ma trochƒô zmieniony kolor t≈Ça ramki i¬†fonty, tak aby lepiej wkomponowa≈Ç siƒô w¬†mojƒÖ stronƒô - reszta bazuje na dw√≥ch przyk≈Çadach z¬†dokumentacji. Na zrobienie pierwszego wykresu z¬†wykorzystaniem¬†JPGraph¬†potrzebowa≈Çem ok. godziny. Teraz wystarcza mi 10 minut üòÉ\n","permalink":"https://timor.site/2011/08/jpgraph-wykresy-z-phpa/","summary":"Onego czasu pr√≥bowa≈Çem znale≈∫ƒá co≈õ co u≈Çatwi≈Çoby mi rysowanie prostych wykres√≥w w¬†PHP\u0026rsquo;ie inaczej ni≈º z¬†palca w¬†GD. Kumpel poleci≈Ç mi¬†JPGraph.\nJPGraph to ≈õwietna sprawa, do generowania statystyk jak chocia≈ºby na mojej¬†stronie, ale biblioteka potrafi du≈ºo wiƒôcej\u0026hellip;\nZa≈Ç√≥≈ºmy, ≈ºe ze stronki zbieramy do bazy takie rzeczy jak: datƒô, adres IP, ilo≈õƒá po≈ÇƒÖcze≈Ñ z¬†tego adresu. Prosta tabela (przyk≈Çad w¬†Postgre SQL\u0026rsquo;u):\nCREATE TABLE wizyty ( pid serial NOT NULL, \u0026#34;data\u0026#34; date NOT NULL DEFAULT (\u0026#39;now\u0026#39;::text)::date, odslony integer NOT NULL DEFAULT 1, CONSTRAINT visits_pkey PRIMARY KEY (id) ); Dane z¬†takiej tabeli mo≈ºna ≈Çatwo wyciƒÖgnƒÖƒá jednym select\u0026rsquo;em:","title":"JPGraph, wykresy z PHP‚Äôa"},{"content":"Tak siƒô sk≈Çada, ≈ºe¬†Debian¬†ze wzglƒôdu na stosunkowo rzadkie wydawanie kolejnych wersji szybko staje siƒô niezbyt ≈õwie≈ºy a dostƒôpne w¬†nim pakiety czƒôsto nie spe≈ÇaniajƒÖ naszych oczekiwa≈Ñ. Nie ma najnowszej wersji¬†Subversion\u0026hellip; Nie ma¬†mod_security¬†itd, itp\u0026hellip;\nRozwiƒÖzaniem tego problemu mo≈ºe byƒá instalacja pakiet√≥w z¬†testowej ga≈Çƒôzi ale mo≈ºna polec na zale≈ºno≈õciach. Mo≈ºna te≈º kompilowaƒá ze ≈∫r√≥de≈Ç\u0026hellip; Tak czy siak w obu przypadkach aktualizacja i¬†utrzymanie tak zmodyfikowanego systemu by≈Çoby jak wrz√≥d na zadku.\nNa szczƒô≈õcie jest prostsze rozwiƒÖzanie. System backport√≥w¬†- czyli repozytorium dostarczajƒÖce mo≈ºliwie najnowsze wersje pakiet√≥w dla ga≈Çƒôzi stabilnej. DodajƒÖc jedno ≈∫r√≥d≈Ço mo≈ºna zainstalowaƒá¬†subversion,¬†mod_security¬†i¬†inne, a¬†przy tym r√≥wnocze≈õnie nie rozwaliƒá sobie systemu.\nKonfiguracja - Squeeze Najpierw trzeba dodaƒá dodatkowe ≈∫r√≥d≈Ço pakiet√≥w:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ squeeze-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Konfiguracja - Lenny Najpierw trzeba dodaƒá dodatkowe ≈∫r√≥d≈Ço pakiet√≥w:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ lenny-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Ponadto w Lennym dostƒôpne jest drugie repozytorium backport√≥w tzw. lenny-backports-sloppy - to repozytorium nie gwarantuje bezproblemowej aktualizacji do Squeeze\u0026rsquo;a ale powinny siƒô tam znale≈∫ƒá nowsze wersje pakiet√≥w ni≈º w przypadku podstawowego repo.\nDodatkowym krokiem w przypadku Lennego, aczkolwiek zalecanym jest ustawienie tzw. pinningu dla backport√≥w, aby pakiety zainstalowane z¬†nich by≈Çy aktualizowane. Robimy to wklepujƒÖc:\necho \u0026#34;Package: *\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin: release a=lenny-backports\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin-Priority: 200\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences Od≈õwie≈ºanie repozytori√≥w Teraz od≈õwie≈ºamy repozytoria:\napt-get update Instalacja pakiet√≥w z backport√≥w Instalacja pakiet√≥w z¬†backport√≥w wymaga wymuszenia ich u≈ºycia, dziƒôki czemu jedynie wybrane przez nas pakiety zostanƒÖ zainstalowane w¬†nowszych wersjach. Robimy to przyk≈Çadowo tak:\napt-get -t squeeze-backports install subversion lub\napt-get -t lenny-backports install subversion To tyle. Mo≈ºemy korzystaƒá z aktualnych wersji paczek.\n","permalink":"https://timor.site/2011/08/konfiguracja-backportow-na-debianie/","summary":"Tak siƒô sk≈Çada, ≈ºe¬†Debian¬†ze wzglƒôdu na stosunkowo rzadkie wydawanie kolejnych wersji szybko staje siƒô niezbyt ≈õwie≈ºy a dostƒôpne w¬†nim pakiety czƒôsto nie spe≈ÇaniajƒÖ naszych oczekiwa≈Ñ. Nie ma najnowszej wersji¬†Subversion\u0026hellip; Nie ma¬†mod_security¬†itd, itp\u0026hellip;\nRozwiƒÖzaniem tego problemu mo≈ºe byƒá instalacja pakiet√≥w z¬†testowej ga≈Çƒôzi ale mo≈ºna polec na zale≈ºno≈õciach. Mo≈ºna te≈º kompilowaƒá ze ≈∫r√≥de≈Ç\u0026hellip; Tak czy siak w obu przypadkach aktualizacja i¬†utrzymanie tak zmodyfikowanego systemu by≈Çoby jak wrz√≥d na zadku.","title":"Konfiguracja backport√≥w na Debianie"},{"content":"Przy okazji wykonywania kilku drobnych optymalizacji swojej stronki natknƒÖ≈Çem siƒô na¬†eAccelerator\u0026rsquo;a. Ciekawy projekt, kt√≥ry w¬†sposobie dzia≈Çania przypomina Zend Optimizer\u0026rsquo;a ale ma jednƒÖ zasadniczƒÖ zaletƒô - jest darmowy üòÉ\nNiestety nie ma go w¬†repozytoriach Debiana, wiƒôc trzeba go sobie skompilowaƒá - ca≈Çy proces jest do≈õƒá prosty. Zaczynamy od pobrania naj≈õwie≈ºszej paczki, obecnie jest to wersja 0.9.5.3:\nPobierz eAccelerator¬†(ostatnio mia≈Çem problem z tym linkiem wiƒôc proponujƒô pogooglaƒá)\nPobieramy i¬†rozpakowujemy pliki:\ntar xvfj eaccelerator-0.9.5.3.tar.bz2 cd eaccelerator-0.9.5.3 Do kompilacji¬†eAccelerator\u0026rsquo;a potrzebujemy paru paczek, kt√≥re mo≈ºemy zainstalowaƒá tak:\napt-get install build-essential php5-dev W katalogu ze ≈∫r√≥d≈Çami klepiemy (prawie standardowo):\nphpize ./configure make make install No i¬†eAccelerator¬†jest ju≈º zainstalowany w¬†naszym systemie. Pozosta≈Ço wygenerowanie konfiguracji¬†PHP¬†aby modu≈Ç by≈Ç automatycznie ≈Çadowany oraz ustawienie podstawowych parametr√≥w konfiguracyjnych.\nW¬†przypadku Debiana domy≈õlna konfiguracja modu≈Ç√≥w¬†PHP¬†przechowywana jest w¬†katalogu/etc/php5/conf.d¬†tam te≈º zapiszemy naszƒÖ konfiguracjƒô jako¬†eaccelerator.ini. Tworzymy plik naszym ulubionym edytorem:\nvim /etc/php5/conf.d/eaccelerator.ini W pliku wpisujemy:\nextension=\u0026#34;eaccelerator.so\u0026#34; eaccelerator.shm_size=\u0026#34;128\u0026#34; eaccelerator.cache_dir=\u0026#34;/tmp/eaccelerator\u0026#34; eaccelerator.enable=\u0026#34;1\u0026#34; eaccelerator.optimizer=\u0026#34;1\u0026#34; eaccelerator.check_mtime=\u0026#34;1\u0026#34; eaccelerator.debug=\u0026#34;0\u0026#34; eaccelerator.filter=\u0026#34;\u0026#34; eaccelerator.shm_max=\u0026#34;0\u0026#34; eaccelerator.shm_ttl=\u0026#34;0\u0026#34; eaccelerator.shm_prune_period=\u0026#34;0\u0026#34; eaccelerator.shm_only=\u0026#34;0\u0026#34; eaccelerator.compress=\u0026#34;1\u0026#34; eaccelerator.compress_level=\u0026#34;9\u0026#34; extension¬†- okre≈õla plik modu≈Çu, czasami mo≈ºe byƒá konieczne podanie pe≈Çnej ≈õcie≈ºki,\neaccelerator.shm_size¬†- okre≈õla ilo≈õƒá pamiƒôci wsp√≥≈Çdzielonej, kt√≥rƒÖ¬†eAccelerator¬†rezerwuje do przechowywani skompilowanych skrypt√≥w. Ja da≈Çem 128MB bo m√≥j serwerek ma 2GB RAM-u, ale przypuszczam ≈ºe dla mniej obciƒÖ≈ºonych maszyn wystarczy 16~32MB. NadajƒÖc temu parametrowi warto≈õƒá 0¬†ufamy programistom üòâ\neaccelerator.cache_dir¬†- ustawiamy katalog, w¬†kt√≥rym bƒôdzie zapisywane to co nie bƒôdzie siƒô ju≈º mie≈õciƒá w¬†pamiƒôci wsp√≥≈Çdzielonej. Ja mam akurat sporƒÖ osobnƒÖ partycjƒô na¬†/tmp, ale r√≥wnie dobrze nadaje siƒô¬†/var/cache/eaccelerator¬†czy¬†/var/tmp/eaccelerator¬†- ustawiamy jak nam wygodnie,\nSkoro ju≈º piszƒô o¬†katalogu na skrypty to warto go utworzyƒá i¬†przypisaƒá mu odpowiednie uprawnienia (ja ustawiam je tak aby tylko¬†Apache¬†mia≈Ç dostƒôp):\nmkdir /tmp/eaccelerator chown -R www-data:www-data /tmp/eaccelerator chmod 0770 /tmp/eaccelerator No to jeszcze restart¬†Apache\u0026rsquo;a i¬†mo≈ºemy sprawdzaƒá jak dzia≈Ça:\ninvoke-rc.d apache2 restart Najszybszym sposobem sprawdzenia czy modu≈Ç siƒô ≈Çaduje jest sprawdzenie wyniku poleceniaphp -v¬†- powinno to wyglƒÖdaƒá jak poni≈ºej:\nphp -v Zend Engine v2.2.0, Copyright (c) 1998-2008 Zend Technologies with eAccelerator v0.9.5.3, Copyright (c) 2004-2006 eAccelerator, \\ by eAccelerator Podobny wynik mo≈ºna uzyskaƒá z¬†pomocƒÖ funkcji¬†phpinfo()¬†i¬†prostego skryput¬†PHP¬†do jej wywo≈Çania.\nCzy warto? Na koniec trochƒô benchmark√≥w aby sprawdziƒá czy ca≈Ça zabawa jest warta ≈õwieczki üòÉ\nWykorzystam standardowe narzƒôdzie dostƒôpne z¬†Apache\u0026rsquo;m:¬†ab¬†(Apache benchmark):\nab -n 1000 -c 10 http://mojastrona.pl/ U mnie da≈Ço to nastƒôpujƒÖce wyniki - z¬†wy≈ÇƒÖczonym¬†eAccelerator\u0026rsquo;em:\nConcurrency Level: 10 Time taken for tests: 8.648 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12836000 bytes HTML transferred: 12612000 bytes Requests per second: 115.64 [#/sec] (mean) Time per request: 86.477 [ms] (mean) Time per request: 8.648 [ms] (mean, across all concurrent requests) Transfer rate: 1449.54 [Kbytes/sec] received Oraz z¬†w≈ÇƒÖczonym:\nConcurrency Level: 10 Time taken for tests: 3.663 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12840344 bytes HTML transferred: 12616120 bytes Requests per second: 272.97 [#/sec] (mean) Time per request: 36.634 [ms] (mean) Time per request: 3.663 [ms] (mean, across all concurrent requests) Transfer rate: 3422.90 [Kbytes/sec] received Wynik jest ca≈Çkiem niez≈Çy, ponad dwa razy szybciej. A¬†w niekt√≥rych przypadkach udaje siƒô wyciƒÖgnƒÖƒá wiƒôcej (nawet 5~10 krotnie). Tak, czy siak - warto!\n","permalink":"https://timor.site/2011/08/optymalizacja-php-z-eacceleratorem/","summary":"Przy okazji wykonywania kilku drobnych optymalizacji swojej stronki natknƒÖ≈Çem siƒô na¬†eAccelerator\u0026rsquo;a. Ciekawy projekt, kt√≥ry w¬†sposobie dzia≈Çania przypomina Zend Optimizer\u0026rsquo;a ale ma jednƒÖ zasadniczƒÖ zaletƒô - jest darmowy üòÉ\nNiestety nie ma go w¬†repozytoriach Debiana, wiƒôc trzeba go sobie skompilowaƒá - ca≈Çy proces jest do≈õƒá prosty. Zaczynamy od pobrania naj≈õwie≈ºszej paczki, obecnie jest to wersja 0.9.5.3:\nPobierz eAccelerator¬†(ostatnio mia≈Çem problem z tym linkiem wiƒôc proponujƒô pogooglaƒá)\nPobieramy i¬†rozpakowujemy pliki:","title":"Optymalizacja PHP z eAccelerator‚Äôem"},{"content":"Co prawda na swojej stronie zrobi≈Çem kilka podstawowych statystyk i¬†co≈õ tam sobie logujƒô do bazy danych, ale gdyby siƒô chwilƒô zastanowiƒá to przecie≈º to samo robi serwer¬†www¬†- wrzuca do log√≥w ka≈ºde zapytanie¬†HTTP, kod b≈Çƒôdu, nazwƒô agenta, itd. Dublowanie tych danych nie jest najbardziej optymalne.\nStƒÖd te≈º chwilƒô pogoogla≈Çem i¬†znalaz≈Çem ≈õwietny¬†Open Source\u0026rsquo;owy projekt:¬†AWStats, kt√≥ry jest webowym analizatorem log√≥w dla serwer√≥w¬†HTTP,¬†FTP¬†i¬†SMTP.\nInstalacja i konfiguracja Najpierw instalacja, na moim Debianie leci to tak:\nsudo apt-get install awstats Teraz trzeba siƒô chwilƒô zastanowiƒá nad konfiguracjƒÖ serwera i¬†celem, kt√≥ry chcemy osiƒÖgnƒÖƒá:\nczy staty bƒôdƒÖ dostƒôpne publicznie? czy tylko dla ograniczonego grona zainsteresowanych (np. w¬†pewnej sieci)? a mo≈ºe zabezpieczenie has≈Çem? WiedzƒÖc, ≈ºe¬†AWStats¬†dzia≈ÇajƒÖ jako skrypt¬†CGI¬†wystawianie takiego serwisu \u0026ldquo;na ≈õwiat\u0026rdquo; nie wydaje mi siƒô bezpiecznym rozwiƒÖzaniem. Wolƒô np. skonfigurowaƒá serwis tak aby by≈Ç dostƒôpny tylko w¬†LAN\u0026rsquo;ie, gdzie mam wiƒôkszƒÖ w≈Çadzƒô i¬†szybciej poradzƒô sobie z¬†namierzeniem i¬†zablokowaniem ewentualnego napastnika üòâ\nOpiszƒô tylko dwa pierwsze przypadki (jak kto≈õ chce has≈Ço to szybko znajdzie jak je ustawiƒá) - pierwsza dla leniwych, druga dla ambitnych üòâ\nPrzygotowania Bez wzglƒôdu na wybranƒÖ metodƒô konfiguracji (leniwƒÖ, bƒÖd≈∫ nie) do dzia≈Çania serwisu potrzebny jest w≈ÇƒÖczony w¬†Apache¬†modu≈Ç¬†CGI. Jest tak w¬†domy≈õlnej konfiguracji ale je≈õli nie masz pewno≈õci to odpal:\na2enmod cgi Musi te≈º byƒá zdefiniowany katalog ze skryptami¬†CGI¬†z¬†uaktywnionƒÖ interpretacjƒÖ¬†CGI¬†- domy≈õlnie w¬†pliku /etc/apache2/sites-available w¬†pliku default jest poni≈ºsza konfiguracja:\nScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; Na potrzeby metody leniwej jest to wystarczajƒÖca konfiguracja, dla ambitnych zalecam trochƒô inne umiejscowienie tego kodu.\nMetoda dla leniwych Poniewa≈º pakiet¬†AWStats¬†instaluje siƒô przyk≈ÇadowƒÖ konfiguracjƒÖ mo≈ºna bardzo szybko uruchomiƒá staty, wystarczy skopiowaƒá jeden plik:\ncp /usr/share/doc/awstats/examples/apache.conf /etc/apache2/conf.d/awstats No i¬†tyle üòâ\nMetoda dla ambitnych Ambitnym zalecam nieco innƒÖ konfiguracjƒô: z¬†ograniczeniem dostƒôpu do statystyk wy≈ÇƒÖcznie z LAN\u0026rsquo;u i¬†tak samo z¬†dostƒôpem do skrypt√≥w¬†CGI. W¬†moim przypadku ≈ºaden z¬†wystawianych przezemnie serwis√≥w nie korzysta z¬†CGI, wiƒôc udostƒôpnianie tych skrypt√≥w wszystkim \u0026ldquo;zainteresowanym\u0026rdquo; nie ma sensu.\nProponujƒô wykorzystaƒá taki lub podobny plik konfiguracyjny dla hosta serwujƒÖcego statystyki:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName staty.domena.pl ServerAdmin webmaster@domena.pl DocumentRoot /var/www/stats/ \u0026lt;Directory /\u0026gt; Options FollowSymLinks AllowOverride None \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/www/stats/\u0026gt; Options -Indexes FollowSymLinks MultiViews AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/lib/awstats\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; Alias /awstats-icon/ /usr/share/awstats/icon/ \u0026lt;Directory /usr/share/awstats/icon\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ErrorLog /var/log/apache2/error.log LogLevel warn CustomLog /var/log/apache2/access.log combined \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On RewriteCond $1 !^$ RewriteCond %{REQUEST_URI} !.*cgi-bin RewriteCond %{REQUEST_URI} !.*awstats.pl RewriteRule /(.*)/? /cgi-bin/awstats.pl?config=$1 [PT] RewriteRule ^/awstats.pl(.*?) /cgi-bin/awstats.pl$1 [QSA,R,L] \u0026lt;/IfModule\u0026gt; \u0026lt;/VirtualHost\u0026gt; Z wa≈ºnych rzeczy do personalizacji:\nServerName - wpisz swojƒÖ nazwƒô serwisu, Allow from 192.168.1.0/24 - zamie≈Ñ na adres/adresy, kt√≥re Tobie odpowiadajƒÖ, mod_rewrite - ostatnich kilka linijek wykorzystuje mod_rewrite do uproszczenia odwo≈Ça≈Ñ do statystyk, wystarczy wtedy wpisaƒá adres np. tak: http://staty.domena.pl/nazwa.domeny.ktora.nas.interesuje.pl plik zapisujemy jako¬†/etc/apache2/sites-available/awstats Teraz zosta≈Ço nam uaktywnienie site\u0026rsquo;a:\na2ensite awstats Czƒô≈õƒá wsp√≥lna konfiguracji Teraz tworzymy katalog na skrypt, kt√≥ry wypisze nam dostƒôpne statystyki:\nmkdir /var/www/stats chown -R www-data:www-data /var/www/stats/ W≈Ça≈õciwa konfiguracja AWStats Pliki konfiguracyjne¬†AWStats¬†znajdujƒÖ siƒô w¬†katalogu¬†/etc/awstats. Jest ich dok≈Çadnie 2¬†szt.:\nawstats.conf awstats.conf.local Plik¬†awstats.conf zawiera przyk≈ÇadowƒÖ konfiguracjƒô wystarczajƒÖcƒÖ do odpalenia statystyk dla pojedynczego hosta. Z¬†kolei plik¬†awstats.conf.local jest miejscem gdzie mo≈ºna wrzuciƒá wsp√≥lnƒÖ konfiguracjƒô dla kilku plik√≥w host√≥w.\nJe≈ºeli mamy wiele host√≥w (a taki przypadek tutaj omawiam) to wygodniej bƒôdzie nam wrzuciƒá ca≈Çy plik¬†awstats.conf¬†do¬†awstats.conf.local¬†i¬†w kolejnych plikach konfiguracyjnych zmieniaƒá tylko parametry rozr√≥≈ºniajƒÖce poszczeg√≥lne hosty. Robimy wiƒôc tak:\nmv /etc/awstats/awstats.conf.local /etc/awstats/awstats.conf.local.orig mv /etc/awstats/awstats.conf /etc/awstats/awstats.conf.local Teraz musimy zmieniƒá kilka linijek w¬†pliku¬†awstats.conf.local:\n# musimy odszukaƒá i zakomentowaƒá poni≈ºsze linie LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; SiteDomain=\u0026#34;\u0026#34; HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # w ten spos√≥b #LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; #SiteDomain=\u0026#34;\u0026#34; #HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; #Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # dodatkowo odszukujemy liniƒô LogFormat=4 # i zamieniamy na LogFormat=1 Teraz mo≈ºemy utworzyƒá pliki konfiguracyjne dla naszych vhost√≥w raptem w¬†kilku linijkach, np.:\nLogFile=\u0026#34;/var/log/apache2/access.log\u0026#34; SiteDomain=\u0026#34;domena.pl\u0026#34; HostAliases=\u0026#34;www.domena.pl\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; Oczywi≈õcie trzeba wpisaƒá w≈ÇasnƒÖ lokalizacjƒô pliku access.log. W¬†powy≈ºszym przypadku jest to lokalizacja domy≈õlna, wsp√≥lna dla wszystkich vhost√≥w - rozr√≥≈ºnienie ruchu do poszczeg√≥lnych vhost√≥w nastƒôpuje dziƒôki podaniu parametr√≥w SiteDomain (podstawowej domeny danej strony) oraz HostAliases (innych domen wskazujƒÖcych na tego samego vhosta).\nOstatnim elementem jest za≈Çadowanie pliku ze wsp√≥lnƒÖ konfiguracjƒÖ.\nZmiana uprawnie≈Ñ do log√≥w Aby umo≈ºliwiƒá dostƒôp¬†AWStats¬†do log√≥w serwera¬†Apache¬†musimy wykonaƒá dwie czynno≈õci. Na poczƒÖtek zmiana atrybutu dla aktualnego pliku log:\nchmod o+r /var/log/apache2/access.log P√≥≈∫niej musimy zadbaƒá aby logi po rotacji przez logrotate r√≥wnie≈º zachowywa≈Çy atrybuty, oraz aby przed rotacjƒÖ¬†AWStats¬†wygenerowa≈Ço statystyki, kt√≥rych nie zebra≈Ço wcze≈õniej. W¬†tym celu zmieniamy plik¬†/etc/logrotate.d/apache2¬†tak by wyglƒÖda≈Ç jak poni≈ºej:\n/var/log/apache2/*.log { weekly missingok rotate 52 compress delaycompress notifempty create 644 root adm sharedscripts prerotate /usr/share/doc/awstats/examples/awstats-update endscript postrotate if [ -f /var/run/apache2.pid ]; then /etc/init.d/apache2 restart \u0026gt; /dev/null fi endscript } Prze≈Çadowanie konfiguracji Apache Po tych wszystkich zmianach w¬†konfiguracji musimy zrestartowaƒá Apache:\ninvoke-rc.d apache2 restart Testy konfiguracji Aby sprawdziƒá konfiguracjƒô¬†AWStats¬†spr√≥bujemy wej≈õƒá na stronƒô:http://staty.domena.pl/cgi-bin/awstats.pl?config=domena.pl\nJe≈ºeli na ≈ºadnym z¬†etap√≥w nie pope≈Çnili≈õmy b≈Çƒôdu to naszym oczom powinny ukazaƒá siƒô \u0026ldquo;wspania≈Çe i¬†upragnione statystyki\u0026rdquo; üòÉ\nJedynƒÖ delikatnƒÖ wadƒÖ¬†awstats¬†jest \u0026ldquo;brzydki\u0026rdquo; i¬†d≈Çugi link z¬†cgi¬†w¬†≈õrodku\u0026hellip; Nieco mnie to irytowa≈Ço, wiƒôc przysiad≈Çem chwilƒô przy mod_rewrite i¬†przygotowa≈Çem regu≈Çki (by≈Çy podane w¬†konfiguracji dla ambitnych), kt√≥re pozwalajƒÖ rozpoczƒÖƒá przeglƒÖdanie statystyk z¬†uproszczonego linku postaci:\nhttp://staty.domena.pl/domena.pl\nProste, czyste i¬†klarowne, bez zbƒôdnych ≈õmieci.\nCo prawda koniec tutora, ale nie koniec samej konfiguracji - proponujƒô aby przejrzeƒá przyk≈Çadowy plik z¬†konfiguracjƒÖ i¬†zapoznaƒá siƒô z¬†zawartymi tam opcjami.\n","permalink":"https://timor.site/2011/08/statystyki-odwiedzin-dla-wielu-serwisow-z-awstats/","summary":"Co prawda na swojej stronie zrobi≈Çem kilka podstawowych statystyk i¬†co≈õ tam sobie logujƒô do bazy danych, ale gdyby siƒô chwilƒô zastanowiƒá to przecie≈º to samo robi serwer¬†www¬†- wrzuca do log√≥w ka≈ºde zapytanie¬†HTTP, kod b≈Çƒôdu, nazwƒô agenta, itd. Dublowanie tych danych nie jest najbardziej optymalne.\nStƒÖd te≈º chwilƒô pogoogla≈Çem i¬†znalaz≈Çem ≈õwietny¬†Open Source\u0026rsquo;owy projekt:¬†AWStats, kt√≥ry jest webowym analizatorem log√≥w dla serwer√≥w¬†HTTP,¬†FTP¬†i¬†SMTP.","title":"Statystyki odwiedzin dla wielu serwis√≥w z AWStats"},{"content":"Certyfikaty oparte o¬†SSL stanowiƒÖ obecnie podstawƒô bezpiecze≈Ñstwa wielu us≈Çug sieciowych zaczynajƒÖc od¬†HTTP, przez POPS, IMAPS, itd\u0026hellip; Niestety zakupienie certyfikatu w¬†organizacjach jak VeriSgin czy Thawte jest do≈õƒá kosztowe, a¬†je≈ºeli potrzebujemy kilka certyfikat√≥w to czƒôsto na lokalne potrzeby jest to po prostu nie op≈Çacalne.\nPostaram siƒô przedstawiƒá wersjƒô \u0026ldquo;ekonomicznƒÖ\u0026rdquo; certyfikacji üòÉ\nGenerowanie Certificate Signing Request Pierwszym etapem generowania certyfikatu jest przygotowanie¬†Certificate Signing Request, czyli czego≈õ w¬†rodzaju \u0026ldquo;pro≈õby\u0026rdquo; o¬†certyfikat. Nasza \u0026ldquo;pro≈õba\u0026rdquo; po podpisaniu przez centrum autoryzacyjne stanie siƒô certyfikatem.\nDo wygenerowania Request\u0026rsquo;u potrzebny jest najpierw klucz prywatny, kt√≥ry generujemy np. tak:\nopenssl genrsa -aes256 -out priv.key 4096 Powy≈ºsze polecenie poprosi nas dwa razy o¬†has≈Ço, kt√≥re trzeba zapamiƒôtaƒá (albo zapisaƒá i¬†zamknƒÖƒá w¬†sejfie) - zalecam wykorzystanie pseudolosowego ciƒÖgu znak√≥w o¬†d≈Çugo≈õci minimum 10 znak√≥w.\nPo genrsa mo≈ºemy u≈ºyƒá kilku opcji wybierajƒÖc w¬†ten spos√≥b algorytm szyfrujƒÖcy - do wyboru sƒÖ: des, des3, aes128, aes192, aes256. Ja wybra≈Çem 256-bitowego¬†AES\u0026rsquo;a - najmocniejszy z¬†tych algorytm√≥w.\nOstatni parametr do d≈Çugo≈õƒá klucza. Mo≈ºna u≈ºyƒá innej warto≈õci np. 1024. Klucz wygeneruje siƒô szybciej ale bƒôdzie prostszy do z≈Çamania. Z¬†moich do≈õwiadcze≈Ñ wynika, ≈ºe niekt√≥re aplikacje mogƒÖ mieƒá problem z¬†obs≈ÇugƒÖ d≈Çugiego klucza (np. jak u≈ºyty w¬†tym przyk≈Çadzie), wtedy u≈ºycie mniejszej warto≈õci mo≈ºe byƒá konieczne (lepsze s≈Çabsze zabezpieczenie ni≈º ≈ºadne).\nWarto ograniczyƒá uprawnienia do wygenerowanego klucza prywatnego tak by tylko w≈Ça≈õciciel mia≈Ç do niego dostƒôp:\nchmod 400 priv.key Przy okazji tworzenia klucza prywatnego warto wspomnieƒá, ≈ºe je≈ºeli bƒôdziemy chcieli go u≈ºyƒá w¬†tak przygotowanej postaci np. w¬†Apache\u0026rsquo;m to przy ka≈ºdym starcie¬†Apache¬†bƒôdzie nas prosiƒá o¬†podanie has≈Ça odbezpieczajƒÖcego klucz prywatny. Niby to bezpieczne ale z¬†drugiej strony przez takie zabezpieczenie¬†Apache¬†nie podniesie siƒô samodzielnie np. po awarii. RozwiƒÖzaniem jest przygotowanie odszyfrowanej wersji klucza prywatnego, z¬†kt√≥rej bƒôdzie korzysta≈Ç¬†Apache. Robi siƒô to tak:\nopenssl rsa -in priv.key -out priv.unsecure.key chmod 400 priv.unsecure.key Klucz priv.unsecure.key podany w¬†konfiguracji programu nie bƒôdzie wymaga≈Ç has≈Ça. Koniecznie nale≈ºy uniemo≈ºliwiƒá dostƒôp do tego pliku wszystkim z¬†wyjƒÖtkiem root\u0026rsquo;a!\nopenssl req -new -key priv.key -out request.csr Co dalej? Ok. Mamy ju≈º request\u0026rsquo;a, warto w¬†tym miejscu wspomnieƒá co mo≈ºemy z¬†nim zrobiƒá:\nMo≈ºemy taki plik przes≈Çaƒá do centrum autoryzacji (CA) celem podpisania i¬†wygenerowania certyfiaktu. Klucze publiczne¬†CA¬†takich jak Thawte, VeriSign, itp sƒÖ do≈ÇƒÖczane do przeglƒÖdarek internetowych, OS\u0026rsquo;√≥w, etc. dziƒôki temu strony internetowe (bƒÖd≈∫ inne us≈Çugi) zabezpieczone takimi certyfikatami weryfikujƒÖ siƒô bez ≈ºadnej dodatkowej akcji ze strony u≈ºytkownika. Warto wziƒÖƒá pod uwagƒô to rozwiƒÖzanie, je≈ºeli np. chcemy zabezpieczyƒá sklep internetowy - my odpu≈õcimy to rozwiƒÖzanie ze wzglƒôdu na koszty üòÉ DrugƒÖ opcjƒÖ jest mo≈ºliwo≈õƒá samodzielnego podpisania certyfikatu tworzƒÖc tzw. SelfSigned Certificate. To zalecana opcja je≈ºeli mamy tylko jednƒÖ stronƒô/us≈Çugƒô. U≈ºytkownik raz doda sobie nasz certyfikat jako zaufany i¬†bƒôdzie m√≥g≈Ç korzystaƒá z¬†szyfrowania do woli. Metoda ta robi siƒô problematyczna gdy trzeba zarzƒÖdzaƒá wiƒôkszƒÖ liczbƒÖ certyfikat√≥w. Ostatnia opcja to utworzenie w≈Çasnego¬†CA¬†(czyli specjalnego certyfikatu), kt√≥rym bƒôdziemy podpisywaƒá wygenerowane przez nas Requesty. Ma to tƒô zaletƒô, ≈ºe wystarczy rozdystrybuowaƒá klucz publiczny¬†CA¬†i¬†bƒôdzie to wystarczajƒÖce do weryfikacji wszystkich naszych certyfikat√≥w. Tworzenie certyfikat√≥w SelfSigned Zaczniemy od prostszej wersji czyli podpiszemy nasz request wygenerowanym przez nas wcze≈õniej kluczem prywatnym tworzac tzw. SelfSigned Certificate. Aby to zrobiƒá potrzebne jest takie polecenie:\nopenssl x509 -req -days 365 -in request.csr \\ -signkey priv.key -out certificate.crt Tak oto wygenerowali≈õmy certyfikat certificate.crt wa≈ºny przez 365 dni (tutaj dowolno≈õƒá ustawie≈Ñ ale 1¬†rok to sensowny okres).\nW≈Ça≈õciwo≈õci certyfikatu mo≈ºna sprawdziƒá np. tak:\nopenssl x509 -noout -text -in certificate.crt Tworzenie w≈Çasnego CA Najpierw nale≈ºy utworzyƒá klucz prywatny naszego¬†CA, robi siƒô to dok≈Çadnie tak samo jak w¬†przypadku generowanego wcze≈õniej klucza prywatnego dla serwera. P√≥≈∫niej musimy wygenerowaƒá requesta (analogicznie jak w¬†przypadku certyfikatu SelfSigned). Czyli dwa polecenia:\nopenssl genrsa -aes256 -out ca.key 4096 openssl req -new -x509 -days 365 \\ -key ca.key -out ca.crt Skoro mamy ju≈º certyfikat naszego¬†CA, mo≈ºemy podpisaƒá wcze≈õniej wygenerowany Request dla serwera:\nopenssl x509 -req -days 365 -in request.csr -CA ca.crt \\ -CAkey ca.key -set_serial 01 -out certificate.crt Og√≥lnie podpisywanie przebiega podobnie jak przy certyfikatach SelfSigned ale jak widaƒá wykorzystywane sƒÖ klucze naszego¬†CA¬†i¬†pojawia siƒô nowy parametr: set_serial, kt√≥rego warto≈õƒá musi byƒá inna dla ka≈ºdego podpisanego przez to¬†CA¬†certyfikatu. Najpro≈õciej aby serial przyjmowa≈Ç numer kolejno wygenerowanego certyfikatu.\nJe≈ºeli wiemy, ≈ºe bƒôdziemy generowaƒá wiƒôcej tego typu certyfikat√≥w warto przygotowaƒá sobie skrypt, kt√≥ry zadba o¬†prawid≈ÇowƒÖ warto≈õƒá pola serial.\nPodsumowanie Certyfikaty przygotowane w¬†ten spos√≥b mo≈ºna wrzuciƒá do¬†Apache, postfix\u0026rsquo;a, itd\u0026hellip; i¬†w ten spos√≥b szyfrujƒÖc ruch.\n","permalink":"https://timor.site/2011/08/certyfikaty-selfsigned/","summary":"Certyfikaty oparte o¬†SSL stanowiƒÖ obecnie podstawƒô bezpiecze≈Ñstwa wielu us≈Çug sieciowych zaczynajƒÖc od¬†HTTP, przez POPS, IMAPS, itd\u0026hellip; Niestety zakupienie certyfikatu w¬†organizacjach jak VeriSgin czy Thawte jest do≈õƒá kosztowe, a¬†je≈ºeli potrzebujemy kilka certyfikat√≥w to czƒôsto na lokalne potrzeby jest to po prostu nie op≈Çacalne.\nPostaram siƒô przedstawiƒá wersjƒô \u0026ldquo;ekonomicznƒÖ\u0026rdquo; certyfikacji üòÉ\nGenerowanie Certificate Signing Request Pierwszym etapem generowania certyfikatu jest przygotowanie¬†Certificate Signing Request, czyli czego≈õ w¬†rodzaju \u0026ldquo;pro≈õby\u0026rdquo; o¬†certyfikat.","title":"Certyfikaty SelfSigned"},{"content":"M√≥j serwer pocztowy dzia≈Ça od jakiego≈õ czasu na dynamicznym IP (dobre bo tanie\u0026hellip;) i¬†przewa≈ºnie nie ma z¬†tym problem√≥w. Postara≈Çem siƒô jak mog≈Çem ustawiajƒÖc SPF\u0026rsquo;a i¬†DomainKeys aby uwiarygodniƒá go u¬†wiƒôkszych dostawc√≥w poczty.\nNiestety wszystko to diabli biorƒÖ w¬†momencie gdy wygasa mi leasse¬†DHCP¬†i¬†dostajƒô nowe IP po jakim≈õ spamerze/zombiaku. Wisi takie w¬†2-3 wiƒôkszych¬†RBL\u0026rsquo;ach i¬†o dostarczaniu poczty mo≈ºna zapomnieƒá. Mi≈Ço gdy jeszcze zdalny¬†MTA¬†zechce odes≈Çaƒá zwrotkƒô \u0026ldquo;zr√≥bta co≈õ bo wisisz w¬†RBL\u0026rsquo;u takim a¬†takim\u0026hellip;\u0026rdquo;, ale zdecydowania niefajnie gdy wysy≈Çasz pocztƒô a¬†ona od razu leci do /dev/null rblcheck Poszpera≈Çem trochƒô i¬†znalaz≈Çem fajne narzƒôdzie aka¬†rblcheck, kt√≥re sprawdza domy≈õlnie kilka¬†RBL\u0026rsquo;i. Mo≈ºna te≈º dodaƒá kolejne jako parametry. WyglƒÖda to mniej wiƒôcej tak:\n$ rblcheck 89.76.116.114 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net Jak widaƒá IP wisi w¬†jednym z¬†RBL\u0026rsquo;i.\nMo≈ºna odpytaƒá¬†RBL\u0026rsquo;e o¬†konkretny pow√≥d znalezienia siƒô na li≈õcie (o ile funkcja taka jest obs≈Çugiwana):\n$ rblcheck 89.76.116.114 -t 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net: \\ Dynamic IP Addresses See: http://www.sorbs.net/lookup.shtml?89.76.116.114 W tym przypadku nie by≈Ço siƒô czym przejmowaƒá. Ta lista zawiera wszystkie IP dynamiczne, wiƒôc nic dziwnego ≈ºe nasze dynamiczne te≈º tam jest. Nawet je≈ºeli kto≈õ bƒôdzie korzystaƒá z¬†tej listy to raczej nie na zasadzie odcinania siƒô od dynamicznych IP, ale w¬†ramach punktowania wiarygodno≈õci danego adresu. Z¬†naszej strony za bardzo nie jeste≈õmy w¬†stanie nic z¬†tym zrobiƒá (bo nie zamierzamy dop≈Çaciƒá za statyczne IP), wiƒôc tak musi byƒá.\nAutomatyzacja Najpierw zainstalowa≈Çem¬†rblcheck‚Äòa:\nsudo apt-get install rblcheck P√≥≈∫niej w¬†/usr/local/sbin/check_rbls.sh¬†utworzy≈Çem skrypt sprawdzajƒÖcy kilka interesujƒÖcych mnie¬†RBL\u0026rsquo;i:\n#!/bin/bash IP=`host mojadomena.pl | head -n1 | awk \u0026#39;{print $4}\u0026#39;` RBLCHECK=\u0026#34;rblcheck -t -s dul.dnsbl.sorbs.net \\ -s abuse.rfc-ignorant.org \\ -s postmaster.rfc-ignorant.org \\ -s dsn.rfc-ignorant.org \\ -s ix.dnsbl.manitu.net \\ -s rhsbl.ahbl.org\u0026#34; $RBLCHECK $IP | awk \u0026#39;{if($2 != \u0026#34;not\u0026#34;) print $0 }\u0026#39; Skrypt ten podlinkowa≈Çem aby uruchamia≈Ç siƒô co godzinƒô:\nln -s /usr/local/sbin/check_rbls.sh /etc/cron.hourly/ Dziƒôki temu prostemu skryptowie je≈ºeli dostanƒô IP wiszƒÖce w¬†tych kilku¬†RBL\u0026rsquo;ach to stosunkowo szybko (maksymalnie w¬†ciƒÖgu godziny) siƒô o¬†tym dowiem. Je≈ºeli natomiast IP bƒôdzie czyste to nie bƒôdƒô dostawaƒá zbƒôdnych maili. Do tego listƒô sprawdzanych¬†RBL\u0026rsquo;i mo≈ºna bardzo ≈Çatwo powiƒôkszyƒá o¬†kolejne w¬†razie takiej potrzeby.\n","permalink":"https://timor.site/2011/08/dynamiczne-ip-i-rble/","summary":"M√≥j serwer pocztowy dzia≈Ça od jakiego≈õ czasu na dynamicznym IP (dobre bo tanie\u0026hellip;) i¬†przewa≈ºnie nie ma z¬†tym problem√≥w. Postara≈Çem siƒô jak mog≈Çem ustawiajƒÖc SPF\u0026rsquo;a i¬†DomainKeys aby uwiarygodniƒá go u¬†wiƒôkszych dostawc√≥w poczty.\nNiestety wszystko to diabli biorƒÖ w¬†momencie gdy wygasa mi leasse¬†DHCP¬†i¬†dostajƒô nowe IP po jakim≈õ spamerze/zombiaku. Wisi takie w¬†2-3 wiƒôkszych¬†RBL\u0026rsquo;ach i¬†o dostarczaniu poczty mo≈ºna zapomnieƒá. Mi≈Ço gdy jeszcze zdalny¬†MTA¬†zechce odes≈Çaƒá zwrotkƒô \u0026ldquo;zr√≥bta co≈õ bo wisisz w¬†RBL\u0026rsquo;u takim a¬†takim\u0026hellip;\u0026rdquo;, ale zdecydowania niefajnie gdy wysy≈Çasz pocztƒô a¬†ona od razu leci do /dev/null rblcheck Poszpera≈Çem trochƒô i¬†znalaz≈Çem fajne narzƒôdzie aka¬†rblcheck, kt√≥re sprawdza domy≈õlnie kilka¬†RBL\u0026rsquo;i.","title":"Dynamiczne IP i RBL‚Äôe"},{"content":"Klastrowanie to mo≈ºe zbyt dumnie powiedziane. RozwiƒÖzanie to wyszuka≈Çem gdy chcƒÖc skonfigurowaƒá dwa serwery¬†apache¬†do wsp√≥≈Çpracy na rzecz jednego serwisu okaza≈Ço siƒô, ≈ºe sejse trzymane sƒÖ tylko przez jeden serwer a¬†drugi nic o¬†nich nie wie. To oczywi≈õcie nie pozwala≈Ço na prawid≈Çowe dzia≈Çanie jakiegokolwiek serwisu korzystajƒÖcego z¬†sesji.\nPomys≈Ç jest taki, ≈ºe zastƒôpujemy domy≈õny mechanizm przechowywania sesji w¬†plikach na dysku mechanizmem memcache. Poniewa≈º¬†memcached¬†dzia≈Ça jako us≈Çuga sieciowa, r√≥≈ºne serwery mogƒÖ siƒô odwo≈Çywaƒá do puli¬†memcached¬†i¬†odczytywaƒá zapisane w¬†niej dane. W¬†przypadku sesji - nie jest wa≈ºne, kto jƒÖ utworzy≈Ç - bo po jej wys≈Çaniu do puli¬†memcached¬†staje siƒô dostƒôpna dla wszystkich klient√≥w¬†php¬†z¬†niej korzystajƒÖcych.\nJednym z¬†pierwszych pyta≈Ñ nasuwajƒÖcych siƒô do takiej kofiguracji jest:¬†a co je≈õli serwer¬†memcached¬†padnie?¬†W¬†chwili gdy wiele serwer√≥w¬†apache¬†zale≈ºy od jednego serwera¬†memcached¬†jego awaria unieruchamia kaskadowo wszystkie.\nDlatego wykorzysta≈Çem konfiguracjƒô z¬†dwoma serwerami¬†memcached. Gdy¬†php¬†zapisuje dane w¬†puli¬†memcached¬†dane sƒÖ wysy≈Çane do wszystkich podanych serwer√≥w (w tym przypadku dw√≥ch). A¬†odczytywanie polega na odpytaniu pierwszego podanego serwera, a¬†je≈õli to siƒô nie uda to drugiego. Uk≈Çad nie jest idealny (jak mamy dwa dzia≈ÇajƒÖce serwery to a≈º siƒô prosi o¬†loadbalancing) ale zmniejsza prawdopodobie≈Ñstwo, ≈ºe awaria pojedynczego elementu po≈Ço≈ºy wszystko.\nZ moim problemem mo≈ºa by≈Ço sobie poradziƒá te≈º inaczej, np. zmieniajƒÖc mechanizm sesji na stronie na taki, kt√≥ry korzysta z¬†bazy danych. O¬†ile w¬†przypadku jednego serwisu nie jest to du≈ºy k≈Çopot, to ju≈º przy kilku/kilkunastu by≈Çoby to ju≈º spore przedsiƒôwziƒôcie.\nWa≈ºnƒÖ zaletƒÖ tego rozwiƒÖzanie jest fakt, ≈ºe nie sƒÖ wymagane ≈ºadne zmiany w¬†istniejƒÖcych serwisach. Po zmianie mechanizmu w¬†konfiguracji¬†php¬†wszystko powinno dzia≈Çaƒá bez zmian.\nInstalacja/Konfiguracja Najpierw trzeba zainstalowaƒá i¬†uruchomiƒá¬†memcached¬†oraz rozszerzenie¬†php5-memcachedo¬†php\u0026rsquo;a, kt√≥re da nam mo≈ºliwo≈õƒá korzystania z¬†niego. U¬†mnie robi siƒô to tak:\napt-get install memcached php5-memcache Teraz trzeba by wyedytowaƒá konfiguracjƒô¬†/etc/memcached.conf. Poni≈ºej wycinek z¬†tego pliku z¬†opcjami, kt√≥re nale≈ºy ustawiƒá:\n# Maksymalna warto≈õƒá pamiƒôci w MB jaka mo≈ºe byƒá # wykorzystana przez demona. # Warto dostosowaƒá do swoich potrzeb. -m 128 # Interfejs, na kt√≥rym nas≈Çuchiwaƒá bƒôdzie us≈Çuga. # Ja dla wygody wybiorƒô wszystkie :) -l 0.0.0.0 # mo≈ºna te≈º dostosowaƒá port do nas≈Çuchiwania -p 11211 # u≈ºytkownika nieuprzywilejowanego # (memcached domy≈õlnie startuje jako root) -u nobody Wprowadzamy i¬†zapisujemy zmiany. Trzeba zrestartowaƒá serwer¬†memcached¬†uruchamiajƒÖc:\ninvoke-rc.d memcached restart Do tego momentu musimy powt√≥rzyƒá konfiguracjƒô na drugiej maszynie (bo inaczej po co klastrowaƒá sesje).\nTeraz trzeba skonfiguraowaƒá¬†php\u0026rsquo;a aby zamiast u≈ºywaƒá sesji zapisywanych do plik√≥w, korzysta≈Ç z¬†memcached. Edytujemy¬†php.ini, u¬†mnie akurat w¬†lokalizacji:/etc/php5/apache2/php.ini. Odszukujemy opcje:\nsession.save_handler = files ;session.save_path = /var/lib/php5 I zamieniamy na:\nsession.save_handler = memcache ; adresy oczywi≈õcie nale≈ºy dostosowaƒá do w≈Çasnych ustawie≈Ñ session.save_path = \u0026#34;tcp://localhost:11211, tcp://remotehost:11211\u0026#34; W kolejnym kroku edytujemy plik konfiguracyjny rozszerzenia¬†php\u0026rsquo;a dla memcache w¬†/etc/php5/conf.d/memcache.ini¬†dodajƒÖc takie ustawienia:\nextension=memcache.so [memcache] memcache.dbpath=\u0026#34;/var/lib/memcache\u0026#34; memcache.maxreclevel=0 memcache.maxfiles=0 memcache.archivememlim=0 memcache.maxfilesize=0 memcache.maxratio=0 ; to jedyna wymagana opcja - resztƒô mo≈ºna dostosowaƒá pod siebie ; albo zostawiƒá domy≈õlnie memcache.allow_failover=1 memcache.max_failover_attempts=20 memcache.default_port=11211 memcache.chunk_size=8192 memcache.hash_strategy=standard memcache.hash_function=\u0026#34;crc32\u0026#34; Wiƒôcej po poszczeg√≥lnych opcjach mo≈ºna siƒô dowiedzieƒá z¬†dokumentacji php.\nTest Je≈ºeli zrobili≈õmy wszystko jak trzeba to sesje powinny zapisywaƒá siƒô z¬†pamiƒôci¬†memcachedi¬†dystrybuowaƒá na wszystkie wpisane w¬†polu save_path serwery. Mo≈ºemy to sprawdziƒá wykorzystujƒÖc np. taki skrypt:\n\u0026lt;?php session_start(); print \u0026#34;Opcja save_handler: \u0026#34; . ini_get(\u0026#34;session.save_handler\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; print \u0026#34;Opcja save_path: \u0026#34; . ini_get(\u0026#34;session.save_path\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; if(isset($_SESSION[\u0026#39;testowa\u0026#39;])) { print \u0026#34;Testowa sesja jest ju≈º ustawiona: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } else { $_SESSION[\u0026#39;testowa\u0026#39;] = \u0026#34;i wyglƒÖda, ≈ºe dzia≈Ça dobrze\u0026#34;; print \u0026#34;Ustawiamy testowƒÖ sesjƒÖ warto≈õciƒÖ: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } ?\u0026gt; Wystarczy od≈õwie≈ºyƒá skrypt kilka razy. Za pierwszym razem sesja zostanie ustawiona a¬†kolejne od≈õwie≈ºenia bƒôdƒÖ ju≈º zwraca≈Çy jej warto≈õƒá.\nProponujƒô te≈º wy≈ÇƒÖczyƒá jeden z¬†serwer√≥w¬†memcached¬†aby sprawdziƒá czy¬†php¬†poprawnie odwo≈Ça siƒô do drugiego serwera.\n","permalink":"https://timor.site/2011/08/klastrowanie-sesji-php-z-memcached/","summary":"Klastrowanie to mo≈ºe zbyt dumnie powiedziane. RozwiƒÖzanie to wyszuka≈Çem gdy chcƒÖc skonfigurowaƒá dwa serwery¬†apache¬†do wsp√≥≈Çpracy na rzecz jednego serwisu okaza≈Ço siƒô, ≈ºe sejse trzymane sƒÖ tylko przez jeden serwer a¬†drugi nic o¬†nich nie wie. To oczywi≈õcie nie pozwala≈Ço na prawid≈Çowe dzia≈Çanie jakiegokolwiek serwisu korzystajƒÖcego z¬†sesji.\nPomys≈Ç jest taki, ≈ºe zastƒôpujemy domy≈õny mechanizm przechowywania sesji w¬†plikach na dysku mechanizmem memcache. Poniewa≈º¬†memcached¬†dzia≈Ça jako us≈Çuga sieciowa, r√≥≈ºne serwery mogƒÖ siƒô odwo≈Çywaƒá do puli¬†memcached¬†i¬†odczytywaƒá zapisane w¬†niej dane.","title":"Klastrowanie sesji PHP z memcached"},{"content":"Instalacja serwera¬†MySQL¬†na Debianie jest niezwykle prosta i¬†sprowadza siƒô do jednego polecenia:\nsudo apt-get install mysql-server Polecenie to zainstaluje i¬†uruchomi us≈Çugƒô serwerowƒÖ¬†MySQL. W¬†czasie instalacji bƒôdziemy proszeni o¬†podanie has≈Ça dla root\u0026rsquo;a (kt√≥re oczywi≈õcie dobrze jest zapamiƒôtaƒá bƒÖd≈∫ zapisaƒá).\nTak zainstalowana baza nas≈Çuchuje na lokalnym porcie (localhost:3306) umo≈ºliwiajƒÖƒá dostƒôp wy≈ÇƒÖcznie root\u0026rsquo;owi. Jest to bardzo bezpieczna konfiguracja\u0026hellip; Ale je≈õli nie mamy zamiaru na tej samej maszynie instalowaƒá oprogramowania zarzƒÖdzajƒÖcego to nie zawsze jest to wygodne, tym bardziej gdy przyk≈Çadowo mamy dzia≈ÇajƒÖcego phpmyadmin\u0026rsquo;a na jakim≈õ serwerze¬†www. W¬†takim przypadku pierwszƒÖ rzeczƒÖ, kt√≥rƒÖ robiƒô jest udostƒôpnienie dostƒôpu zdalnego dla root\u0026rsquo;a. Warto zaznaczyƒá ≈ºe uprawnienia root\u0026rsquo;a mo≈ºna nadaƒá dowolnemu u≈ºytkownikowi (np. romanowi) co jest du≈ºo bezpieczniejszƒÖ konfiguracjƒÖ ni≈º dzia≈Çanie bezpo≈õrednio na koncie root\u0026rsquo;a (kt√≥rego nazwa jest powszechnie znana).\nDostƒôp zdalny dla root\u0026rsquo;a Aby umo≈ºliwiƒá zdalne zalogowanie siƒô do bazy z¬†uprawnieniami root\u0026rsquo;a trzeba ustawiƒá odpowiednie GRANT\u0026rsquo;y, robimy to tak:\nmysql -u root -p mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;roman\u0026#39;@\u0026#39;%\u0026#39; \u0026gt; IDENTIFIED BY \u0026#39;haslo dla zdalnego roota\u0026#39; WITH GRANT OPTION; mysql\u0026gt; FLUSH PRIVILEGES; mysql\u0026gt; exit Pierwsze polecenie po≈ÇƒÖczy nas z¬†bazƒÖ proszƒÖc o¬†has≈Ço podane w¬†czasie instalacji.\nKolejne to polecenia¬†SQL\u0026rsquo;owe, kt√≥re pozwalajƒÖ u≈ºytkownikowi ‚Äòroman\u0026rsquo; ≈ÇƒÖczƒÖcemu siƒô z¬†hosta ‚Äò%\u0026rsquo; (dowolnego) identyfikujƒÖcemu siƒô has≈Çem ‚Äòhaslo dla zdalnego roota\u0026rsquo;. Je≈ºeli chcemy ograniczyƒá dostƒôp do tylko jednego zdalnego adresu to zamiast znaku procenta wpisujemy ten adres IP. Tak dodany u≈ºytkownik ma te≈º prawo nadawania uprawnie≈Ñ (GRANT OPTION). Polecenie¬†FLUSH PRIVILEGES¬†prze≈Çadowuje uprawnienia - umo≈ºliwiajƒÖc logowanie z¬†podanymi wcze≈õniej uprawnieniami.\nPozosta≈Ço nam zmieniƒá ustawienia serwera tak aby nas≈Çuchiwa≈Ç nie tylko na localho≈õcie. W¬†tym celu edytujemy plik¬†/etc/mysql/my.cnf:\nsudo vim /etc/mysql/my.cnf Odszukujemy nastƒôpujƒÖcƒÖ liniƒô:\nbind-address = 127.0.0.1 Liniƒô tƒô mo≈ºemy zakomentowaƒá co bƒôdzie skutkowaƒá nas≈Çuchiwaniem przez serwer na wszystkich skonfigurowanych adresach IP (taki sam efekt da wpisanie w¬†polu adresu 0.0.0.0). Mo≈ºna te≈º wpisaƒá tylko jeden adres IP w¬†przypadku gdy na serwerze jest ich kilka i¬†nie chcemy aby serwer by≈Ç dostƒôpny na wszystkich.\nOstatnim krokiem jest zrestartowania serwera¬†MySQL¬†aby zadzia≈Ça≈Çy wprowadzone w¬†pliku konfiguracyjnym zmiany. Mo≈ºna to zrobiƒá tak:\ninvoke-rc.d mysql restart Je≈ºeli w≈Ça≈õnie za≈Ço≈ºy≈Çe≈õ i udostƒôpni≈Çe≈õ nowy serwer bazodanowy MySQL to oszczƒôd≈∫ sobie pracy w przysz≈Ço≈õci i od razu ustaw przechowywanie tabel InnoDB w osobnych plikach.\n","permalink":"https://timor.site/2011/08/mysql-na-szybko/","summary":"Instalacja serwera¬†MySQL¬†na Debianie jest niezwykle prosta i¬†sprowadza siƒô do jednego polecenia:\nsudo apt-get install mysql-server Polecenie to zainstaluje i¬†uruchomi us≈Çugƒô serwerowƒÖ¬†MySQL. W¬†czasie instalacji bƒôdziemy proszeni o¬†podanie has≈Ça dla root\u0026rsquo;a (kt√≥re oczywi≈õcie dobrze jest zapamiƒôtaƒá bƒÖd≈∫ zapisaƒá).\nTak zainstalowana baza nas≈Çuchuje na lokalnym porcie (localhost:3306) umo≈ºliwiajƒÖƒá dostƒôp wy≈ÇƒÖcznie root\u0026rsquo;owi. Jest to bardzo bezpieczna konfiguracja\u0026hellip; Ale je≈õli nie mamy zamiaru na tej samej maszynie instalowaƒá oprogramowania zarzƒÖdzajƒÖcego to nie zawsze jest to wygodne, tym bardziej gdy przyk≈Çadowo mamy dzia≈ÇajƒÖcego phpmyadmin\u0026rsquo;a na jakim≈õ serwerze¬†www.","title":"MySQL - dostƒôp zdalny na szybko"},{"content":"Wiƒôkszo≈õƒá system√≥w plik√≥w w¬†linuksie pozwala na ustawienie quoty na dw√≥ch poziomach: na u≈ºytkownika lub na grupƒô u≈ºytkownik√≥w. W¬†wielu przypadkach taki podzia≈Ç jest sensowny i¬†wystarczajƒÖcy. Ale zdarzajƒÖ siƒô scenariusze, w¬†kt√≥rych to za ma≈Ço.\nDobrym przyk≈Çadem jest serwer¬†FTP¬†z¬†wirtualnymi kontami u≈ºytkownik√≥w. Czyli us≈Çuga serwera dzia≈Ça jako pewien nieuprzywilejowany u≈ºytkownik systemowy (przewa≈ºnie ftp) przypisany do nieuprzywilejowanej grupy (np. nogroup). Konta u≈ºytkownik√≥w serwera¬†FTP¬†sƒÖ zdefiniowane w¬†bazie danych lub serwerze¬†LDAP. Takie konta nazywa siƒô wirtualnymi poniewa≈º po autoryzacji w¬†pewnym systemie (bazie danych,¬†LDAP\u0026rsquo;ie) dzia≈ÇajƒÖ z¬†uprawnieniami pewnego systemowego konta (w tym przypadku ftp) - nie ma wiƒôc odzwierciedlenia pomiƒôdzy u≈ºytkownikami korzystajƒÖcymi z¬†ftp¬†a¬†kontami systemowymi.\nProblemem w¬†takim uk≈Çadzie jest to, ≈ºe nie mo≈ºna rozr√≥≈ºniƒá poszczeg√≥lnych u≈ºytkownik√≥w wirtualnych, poniewa≈º wszystkie operacje sƒÖ wykonywane przez systemowego u≈ºytkownika¬†ftp¬†i¬†to on jest w≈Ça≈õcicielem wszystich plik√≥w utworzonych przez u≈ºytkownik√≥w wirtualnych.\nRozwiƒÖzania sƒÖ dwa:\nsewer¬†FTP¬†ma zaimplementowanƒÖ obs≈Çugƒô quot dla u≈ºytkownik√≥w wirtualnych, z¬†czego mo≈ºemy skorzystaƒá, tworzymy quotƒô na katalog domowy u≈ºytkownika wirtualnego. Mnie bardziej odpowiada≈Ça druga metoda.\nO ile mi wiadomo w¬†ext3¬†nie mo≈ºna tworzyƒá quoty per katalog, na moje szczƒôscie takƒÖ mo≈ºliwo≈õƒá oferuje¬†xfs, z¬†kt√≥rego korzysta≈Çem.\nJe≈ºeli jeszcze nie masz dysku sformatowanego jako¬†xfs¬†to jest to dobry moment aby to zrobiƒá:\n# sdaX zastap nazwa urzadzenia ktore TY chcesz sformatowac mkfs.xfs /dev/sdaX # aby dzialala quoata per katalog trzeba zamonowac # dysk z opcja prjquota mount /dev/sdaX /mnt/ftp -o prjquota Logika quot per katalog w¬†xfs\u0026rsquo;ie jest taka, ≈ºe najpierw tworzymy projekt z¬†powiƒÖzanym z¬†nim id, a¬†p√≥≈∫niej wiƒÖ≈ºemy poprzez id projekt z¬†katalogiem. Przez co quota na≈Ço≈ºona na projekt ma zastosowanie do katalogu. Konfiguracja projekt√≥w i¬†przypisanych im katalog√≥w znajduje siƒô w¬†dw√≥ch plikach: /etc/projid i¬†/etc/projects.\nUtw√≥rzymy teraz projekt i¬†konfiguracjƒô dla niego:\n# nazwa projektu i jego id - w naszym przypadku # konto uzytkownika romana :) echo ftproman:10 \u0026gt;\u0026gt; /etc/projid # projektowi z id 10 przypisujemy katalog /mnt/ftp/roman echo 10:/mnt/ftp/roman \u0026gt;\u0026gt; /etc/projects Pozosta≈Ço uruchomiƒá quotƒô wpisujƒÖƒá komendy:\n# wiazemy projekt z punktem montowania xfs_quota -x -c \u0026#39;poject -s ftproman\u0026#39; /mnt/ftp # ustawiamy wlasiwa quote 1 gigabajt dla projektu xfs_quota -x -c \u0026#39;limit -p bhard=1g ftproman\u0026#39; /mnt/ftp To tyle - quota na katalog jest za≈Ço≈ºona i¬†dzia≈Ça. Aby zobaczyƒá jaka czƒô≈õƒá miejsca jest ju≈º wykorzystana wystarczy uruchomiƒá polecenie:\nxfs_quota -x -c \u0026#39;report -h /mnt/ftp\u0026#39; Project quota on /mnt/ftp (/dev/sda3) Blocks Project ID Used Soft Hard Warn/Grace ---------- --------------------------------- ftproman 1,5M 0 1G 00 [------] Dla kolejnych u≈ºytkownik√≥w musimy tworzyƒá kolejne wpisy.\n","permalink":"https://timor.site/2011/08/quota-na-katalog-w-xfsie/","summary":"Wiƒôkszo≈õƒá system√≥w plik√≥w w¬†linuksie pozwala na ustawienie quoty na dw√≥ch poziomach: na u≈ºytkownika lub na grupƒô u≈ºytkownik√≥w. W¬†wielu przypadkach taki podzia≈Ç jest sensowny i¬†wystarczajƒÖcy. Ale zdarzajƒÖ siƒô scenariusze, w¬†kt√≥rych to za ma≈Ço.\nDobrym przyk≈Çadem jest serwer¬†FTP¬†z¬†wirtualnymi kontami u≈ºytkownik√≥w. Czyli us≈Çuga serwera dzia≈Ça jako pewien nieuprzywilejowany u≈ºytkownik systemowy (przewa≈ºnie ftp) przypisany do nieuprzywilejowanej grupy (np. nogroup). Konta u≈ºytkownik√≥w serwera¬†FTP¬†sƒÖ zdefiniowane w¬†bazie danych lub serwerze¬†LDAP.","title":"Quota na katalog w XFS‚Äôie"},{"content":"Je≈ºeli tu zaglƒÖdasz pewnie zdarzy≈Ço Ci siƒô kiedy≈õ, ≈ºe przyk≈Çadowo wygrzebujesz jaki≈õ stary serwer i¬†nie masz pojƒôcia co na nim by≈Ço, ani do czego s≈Çu≈ºy≈Ço, czy jeszcze dzia≈Ça\u0026hellip; Albo jeszcze inaczej - serwer dzia≈Ça≈Ç tak d≈Çugo, ≈ºe wszystkie osoby znajƒÖce has≈Ço na root\u0026rsquo;a przesz≈Çy na emeryturƒô lub zmar≈Çy\u0026hellip; Nieistotne üòÉ\nJest pewna prosta sztuczka, pozwalajƒÖca wbiƒá siƒô na konto root\u0026rsquo;a nie znajƒÖc has≈Ça - dajƒÖc nam mo≈ºliwo≈õƒá jego zmiany. Potrzebne¬†dwa restarty¬†ale za to nie trzeba korzystaƒá z¬†≈ºadnychlive cd.\nNa poczƒÖtek zmuszamy serwer do restartu - mieƒá nadziejƒô, ≈ºe maszyna obs≈Çuguje ACPI i¬†delikatne wci≈õniƒôcie przycisku¬†power¬†subtelnie jƒÖ wy≈ÇƒÖczy. Je≈õli to nie zadzia≈Ça to kojarzƒÖ mi siƒô tylko brzydkie rzeczy üòÉ Gdy po restarcie za≈Çaduje siƒô¬†grub¬†na domy≈õlnej opcji bootowania wybieramy edycjƒô wciskajƒÖc \u0026ldquo;e\u0026rdquo;. Wybieramy liniƒô zaczynajƒÖcƒÖ siƒô od¬†kernel¬†i¬†zn√≥w wybieramy edycjƒô wciskajƒÖc \u0026ldquo;e\u0026rdquo;. Je≈ºeli znajduje siƒô tam parametr¬†ro¬†to zastƒôpujemy go¬†rw¬†i¬†dopisujemy na ko≈Ñcu¬†init=/bin/bash Wbijamy¬†\u0026ldquo;enter\u0026rdquo;¬†zapisujƒÖc zmieniony wiersz. Bootujemy siƒô z¬†tak zmienionej konfiguracji wciskajƒÖc \u0026ldquo;b\u0026rdquo;. Po chwili system zamiast wystartowaƒá init\u0026rsquo;a i¬†uruchamiaƒá us≈Çugi, lƒÖduje w¬†bash\u0026rsquo;u z¬†uprawnieniami root\u0026rsquo;a. A¬†skoro mamy root\u0026rsquo;a to mo≈ºemy wpisaƒá¬†passwd¬†i¬†zmieniƒá rootowi has≈Ço üòÉ Teraz ju≈º tylko reboot i¬†startujemy system normalnie - has≈Ço root\u0026rsquo;a powinno dzia≈Çaƒá. Niestety ta prosta sztuczka nie dzia≈Ça na wszystkich¬†linux\u0026rsquo;ach - szczeg√≥lnie tych wykorzystujƒÖcych initramfs-tools. Na tych systemach trzeba ciut wiƒôcej pokombinowaƒá ale przynajmniej ma siƒô jaki≈õ punkt wyj≈õcia.\n","permalink":"https://timor.site/2011/08/wlam-na-lokalne-konto-roota/","summary":"Je≈ºeli tu zaglƒÖdasz pewnie zdarzy≈Ço Ci siƒô kiedy≈õ, ≈ºe przyk≈Çadowo wygrzebujesz jaki≈õ stary serwer i¬†nie masz pojƒôcia co na nim by≈Ço, ani do czego s≈Çu≈ºy≈Ço, czy jeszcze dzia≈Ça\u0026hellip; Albo jeszcze inaczej - serwer dzia≈Ça≈Ç tak d≈Çugo, ≈ºe wszystkie osoby znajƒÖce has≈Ço na root\u0026rsquo;a przesz≈Çy na emeryturƒô lub zmar≈Çy\u0026hellip; Nieistotne üòÉ\nJest pewna prosta sztuczka, pozwalajƒÖca wbiƒá siƒô na konto root\u0026rsquo;a nie znajƒÖc has≈Ça - dajƒÖc nam mo≈ºliwo≈õƒá jego zmiany. Potrzebne¬†dwa restarty¬†ale za to nie trzeba korzystaƒá z¬†≈ºadnychlive cd.","title":"W≈Çam na lokalne konto root‚Äôa"},{"content":"Kiedy≈õ potrzebowa≈Çem w¬†ramach testu obciƒÖ≈ºeniowego wys≈Çaƒá du≈ºo wiadomo≈õci z¬†za≈ÇƒÖcznikami. Chcia≈Çem to zrobiƒá na szybko z¬†shell\u0026rsquo;a i¬†tutaj chwilƒô musia≈Çem pogooglaƒá aby znale≈∫ƒá dzia≈ÇajƒÖce polecenie. To co znalaz≈Çem wyglƒÖda tak:\n(echo \u0026#34;testowa wiadomosc\u0026#34;; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test\u0026#34; testowy@mail.pl WiedzƒÖc ju≈º jak wysy≈Çaƒá maile z¬†za≈ÇƒÖcznikami, ma≈Çy mail bombing mog≈Çem zrobiƒá tak:\nfor i in `seq 1 100`; do (cat tekst.txt; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test $i\u0026#34; testowy@mail.pl; done ","permalink":"https://timor.site/2011/08/wysylanie-zalacznikow-poleceniem-mail/","summary":"Kiedy≈õ potrzebowa≈Çem w¬†ramach testu obciƒÖ≈ºeniowego wys≈Çaƒá du≈ºo wiadomo≈õci z¬†za≈ÇƒÖcznikami. Chcia≈Çem to zrobiƒá na szybko z¬†shell\u0026rsquo;a i¬†tutaj chwilƒô musia≈Çem pogooglaƒá aby znale≈∫ƒá dzia≈ÇajƒÖce polecenie. To co znalaz≈Çem wyglƒÖda tak:\n(echo \u0026#34;testowa wiadomosc\u0026#34;; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test\u0026#34; testowy@mail.pl WiedzƒÖc ju≈º jak wysy≈Çaƒá maile z¬†za≈ÇƒÖcznikami, ma≈Çy mail bombing mog≈Çem zrobiƒá tak:\nfor i in `seq 1 100`; do (cat tekst.txt; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test $i\u0026#34; testowy@mail.","title":"Wysy≈Çanie za≈ÇƒÖcznik√≥w poleceniem mail"},{"content":"My name is Tomasz GƒÖgor, for some people known as TiMoR.\nI\u0026rsquo;m passionate day to day DevOps practitioner. I enjoy anything Linux driven, especially containerised web services. Security freak who loves to work with, managing, scaling Cloud systems.\nI\u0026rsquo;m generalist, touched many, strange topic across my career. I used to speak with DBAs about advantages of one DB engine over the another and how to tune them for better performance. With Network guys, it happen to me to debug IKE proto by raw packets. Somehow (don\u0026rsquo;t know how it happen), people know me as Java performance tuning specialist :)\nI\u0026rsquo;m not a regular developer, but it happen to me to write smaller and bigger apps in Python/Ruby/Groovy/Perl. I love Makefiles for simple day to day automation. I\u0026rsquo;m addicted to shell, it\u0026rsquo;s one of main reasons I consider myself Windows incompatible (no Bash/Zsh). I have voodoo-magic-script for almost anything. I automate a lot, either with code or just playing with another IoT thingie.\nAll pages on my site are licensed under CC BY-SA 4.0.\n","permalink":"https://timor.site/about/","summary":"My name is Tomasz GƒÖgor, for some people known as TiMoR.\nI\u0026rsquo;m passionate day to day DevOps practitioner. I enjoy anything Linux driven, especially containerised web services. Security freak who loves to work with, managing, scaling Cloud systems.\nI\u0026rsquo;m generalist, touched many, strange topic across my career. I used to speak with DBAs about advantages of one DB engine over the another and how to tune them for better performance. With Network guys, it happen to me to debug IKE proto by raw packets.","title":"Few words about me"},{"content":" Rzecz o istocie informatykiAlgorytmika\nAuthors: David Harel, Yishai Feldman\nempik.com ","permalink":"https://timor.site/books/2010/algorytmika/","summary":" Rzecz o istocie informatykiAlgorytmika\nAuthors: David Harel, Yishai Feldman\nempik.com ","title":"Rzecz o istocie informatyki"},{"content":" Jeszcze kr√≥tsza historia czasuAuthor: Stephen Hawking\namazon.pl ","permalink":"https://timor.site/books/2009/krotka-historia-czasu/","summary":" Jeszcze kr√≥tsza historia czasuAuthor: Stephen Hawking\namazon.pl ","title":"Jeszcze kr√≥tsza historia czasu"},{"content":" Pere≈Çki oprogramowaniaSeria Klasyka informatyki\nAuthor: Jon Bentley\nhelion.pl ","permalink":"https://timor.site/books/2009/perelki-oprogramowania/","summary":" Pere≈Çki oprogramowaniaSeria Klasyka informatyki\nAuthor: Jon Bentley\nhelion.pl ","title":"Pere≈Çki oprogramowania"},{"content":" ZarzƒÖdzanie czasemStrategie dla administrator√≥w system√≥w\nAuthor: Thomas Limoncelli\nhelion.pl ","permalink":"https://timor.site/books/2008/zarzadzanie-czasem/","summary":" ZarzƒÖdzanie czasemStrategie dla administrator√≥w system√≥w\nAuthor: Thomas Limoncelli\nhelion.pl ","title":"ZarzƒÖdzanie czasem"},{"content":" G≈ÇƒôbiaAuthor: Lincoln Child\namazon.com ","permalink":"https://timor.site/books/2008/glebia/","summary":" G≈ÇƒôbiaAuthor: Lincoln Child\namazon.com ","title":"G≈Çƒôbia"},{"content":" Bezpiecze≈Ñstwo sieciNarzƒôdzia\nAuthors: Nitesh Dhanjani, Justin Clarke\nhelion.pl ","permalink":"https://timor.site/books/2007/bezpieczenstwo-sieci/","summary":" Bezpiecze≈Ñstwo sieciNarzƒôdzia\nAuthors: Nitesh Dhanjani, Justin Clarke\nhelion.pl ","title":"Bezpiecze≈Ñstwo sieci"},{"content":" Kurs Szybkiego CzytaniaAuthor: Jamruszkiewicz Jolanta\nempik.com ","permalink":"https://timor.site/books/2006/kurs-szybkiego-czytania/","summary":" Kurs Szybkiego CzytaniaAuthor: Jamruszkiewicz Jolanta\nempik.com ","title":"Kurs Szybkiego Czytania"},{"content":" W≈Çadca Pier≈õcieniDru≈ºyna pier≈õcienia. Tom 1\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://timor.site/books/2005/lotr1/","summary":" W≈Çadca Pier≈õcieniDru≈ºyna pier≈õcienia. Tom 1\nAuthor: J. R. R. Tolkien\nempik.com ","title":"W≈Çadca Pier≈õcieni"},{"content":" W≈Çadca Pier≈õcieniDwie wie≈ºe. Tom 2\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://timor.site/books/2005/lotr2/","summary":" W≈Çadca Pier≈õcieniDwie wie≈ºe. Tom 2\nAuthor: J. R. R. Tolkien\nempik.com ","title":"W≈Çadca Pier≈õcieni"},{"content":" W≈Çadca Pier≈õcieniPowr√≥t kr√≥la. Tom 3\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://timor.site/books/2005/lotr3/","summary":" W≈Çadca Pier≈õcieniPowr√≥t kr√≥la. Tom 3\nAuthor: J. R. R. Tolkien\nempik.com ","title":"W≈Çadca Pier≈õcieni"},{"content":" Kuku≈Çcze jajoEkscytujƒÖce ≈Çowy na nieuchwytnego hackera\nAuthor: Clifford Stoll\nlubimyczytac.plamazon.pl ","permalink":"https://timor.site/books/2000/kukulcze-jajo/","summary":" Kuku≈Çcze jajoEkscytujƒÖce ≈Çowy na nieuchwytnego hackera\nAuthor: Clifford Stoll\nlubimyczytac.plamazon.pl ","title":"Kuku≈Çcze jajo"},{"content":"","permalink":"https://timor.site/bookshelf/","summary":"","title":"Bookshelf"}]