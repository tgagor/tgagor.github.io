[{"content":" TL;DR If you don\u0026rsquo;t care about my reasoning, just jump to the description of installation and usage . Click here for demo .\nI try to read at least one book each month. It\u0026rsquo;s not much, but that\u0026rsquo;s what I can afford. I read a lot of technical books, building my technical skill set, but a lot of random topics from business, psychology or fantasy.\nIt happen to me many times that when I\u0026rsquo;ve been asked about specific book, I couldn\u0026rsquo;t remember \u0026ldquo;Where did I read it?\u0026rdquo; or \u0026ldquo;What was the book title?\u0026rdquo;. Some day I accidentally passed through Dave\u0026rsquo;s Rupert blog and his Bookshelf\u0026thinsp; external link and I loved it! I have a blog, so great place to collect and share, with easy access from anywhere - why not to use it?\nI didn\u0026rsquo;t want to add all those books manually as it felt a little bit an overkill, so I was looking for some kind of social network about books, where I could add books easily and just auto-generate the listing on my blog via API calls. I found GoodReads\u0026thinsp; external link , which was looking very promising especially that they documented their API1. There was just a little but\u0026hellip;\nI\u0026rsquo;ve been looking through few other alternatives, but didn\u0026rsquo;t found a good replacement \u0026#x1f626;\nThen, I have to add it to my blog. There were two ways I could do it:\nCreate a blog post per book and extract them on one sub-page. Use Hugo\u0026rsquo;s data templates2, so store Books metadata in YAML/JSON and generate page from it. Second idea was tempting, but what if I\u0026rsquo;d like to add a book review or short summary of it\u0026rsquo;s content? It\u0026rsquo;s just better to go with the blog post per book. It won\u0026rsquo;t take too much time to add it and I will have some obligation to write summary. It was decided.\nBookshelf Theme installation and usage It requires original PaperMod theme\u0026thinsp; external link , install it first\u0026thinsp; external link .\nClone PaperMod-bookshelf repo\u0026thinsp; external link to your project:\nas submodule (recommended) Add bookshelf theme to the Hugo site cd your-site-location git submodule add --depth=1 \\ https://github.com/tgagor/hugo-PaperMod-bookshelf.git \\ themes/PaperMod-bookshelf You might need to initialize any uninitialized submodules in the repository and then recursively update all submodules to their latest versions to make it fully available:\nUpdate git submodules git submodule update --init --recursive or by just cloning Add bookshelf theme to the Hugo site cd your-site-location git clone --depth=1 \\ https://github.com/tgagor/hugo-PaperMod-bookshelf.git \\ themes/PaperMod-bookshelf To activate the bookshelf, update config.yaml or hugo.yaml:\nUpdate hugo.yaml theme: - PaperMod - PaperMod-bookshelf Original theme should be first, followed up by extension.\nCreate a page where books will be listed, for example content/bookshelf.md with such content: --- title: My Bookshelf layout: bookshelf url: /bookshelf --- Of course you can amend the url and title. layout field is what identifies how it should be filled, so have to stay like that.\nAdd first book, by calling: Add example book for testing hugo new -k book books/2024/my-first-book Run your page and check /bookshelf URL, usually: http://localhost:1313/bookshelf\u0026thinsp; external link If you\u0026rsquo;re unable to get it working, check exampleSite\u0026thinsp; external link directory.\nAdding books Theme rely on posts in the content/books directory, to keep them outside of the typical posts. You might prefer other location.\nTo add new book, just create a new blog post of kind book:\nAdd book entry hugo new -k book books/2024/new-book It will generate \u0026ldquo;a blog post\u0026rdquo; describing the book, with additional fields which are required to make the mechanism work. They are:\ntitle: Book\u0026#39;s title sub_title: Book\u0026#39;s sub title date: Date when you read the book book_authors: - A list of - All the authors categories: # used for finding the books among other posts - Book tags: # categories of books, use/extend as you want - Biography - Business - Fantasy - Finance - Graphic Novel - Leadership - Management - Mystery - Politics - Pop-sci - Psychology - Security - Sci-fi - Science - Sociology - Technology - Trading book_rating: 5 # your raging 1-5 stars Archetype adds a shortcode book call, which I preferred over typical book cover. It add\u0026rsquo;s a nice header with book\u0026rsquo;s cover and can provide links to few websites (reviews, affiliate links, whatever).\nRest is up to you. You can write a review or whatever \u0026#x1f609;\nConsiderations on configuration Configuration related to the cover in the original theme might impact how books are presented. My config looks like that:\nparams: cover: responsiveImages: true hidden: false # hide everywhere but not in structured data hiddenInList: false # hide on list pages and home If you use hiddenInSingle: false, then book posts would always show a huge cover image as a cover. I didn\u0026rsquo;t like it. It\u0026rsquo;s best to leave this setting not configured.\nThen to hide the covers for books I hide them in every post by:\ncover: hidden: true That\u0026rsquo;s my preference, feel free to play settings according to your needs.\nFor more details, check the repo3.\nEnjoyed? https://www.goodreads.com/api\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gohugo.io/templates/data-templates/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tgagor/hugo-PaperMod-bookshelf\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/04/create-a-virtual-bookshelf-with-hugo-and-papermod/","summary":"TL;DR If you don\u0026rsquo;t care about my reasoning, just jump to the description of installation and usage . Click here for demo .\nI try to read at least one book each month. It\u0026rsquo;s not much, but that\u0026rsquo;s what I can afford. I read a lot of technical books, building my technical skill set, but a lot of random topics from business, psychology or fantasy.\nIt happen to me many times that when I\u0026rsquo;ve been asked about specific book, I couldn\u0026rsquo;t remember \u0026ldquo;Where did I read it?","title":"Create a Virtual Bookshelf with Hugo and PaperMod"},{"content":" The Culture MapBreaking Through the Invisible Boundaries of Global Business\nAuthor: Erin Meyer\namazon.plempik.com Still reading\u0026hellip; but already like it!\n","permalink":"https://gagor.pro/books/2024/culture-map/","summary":"The Culture MapBreaking Through the Invisible Boundaries of Global Business\nAuthor: Erin Meyer\namazon.plempik.com Still reading\u0026hellip; but already like it!","title":"The Culture Map"},{"content":" RewiredThe McKinsey Guide to Outcompeting in the Age of Digital and AI\nAuthors: Eric Lamarre, Kate Smaje, Rodney Zemmel\namazon.plamazon.de I\u0026rsquo;ve heard a lot of strange things about McKinsey, starting with this parody commercial:\nThen few more facts, but still provided in an amusing way by the John Oliver:\nIt\u0026rsquo;s not just jokes than worry me, but also some news I\u0026rsquo;ve read:\nthey had no issues to consult both companies and their regulators at the same time1, they work with tobbaco companies and health industry suggesting ways to profit companies, that might be bad for their clients2, they\u0026rsquo;re behind OxyContin (opioid) sales turbocharge, which resulted in hundreds of people dead of overdose3, having relationships with authoritarian regimes and more. Anyway, I decided to read it with open mind.\nMy impression is that they fill the nitche of high level executives (or polititians) seeking for \u0026ldquo;a trusted advice\u0026rdquo;. Anything, so they could say loud: \u0026ldquo;we performed a deep analysis, supported by world best experts to ensure that\u0026hellip; bla bla\u0026rdquo;. Because it costs millions, it have to be true - right?\nMy first impression - this book is not yet for me. I work on different level, more with the people (at the bottom) and not that much on the company\u0026rsquo;s strategy. It\u0026rsquo;s all about the strategy. Find an initiative big enough to generate 20~30% rise in the revenue (or EBIT) within next 18 months. You need 3~5 such initiatives. They require that much of the budget\u0026hellip; etc. Step by step, what to do, what should work and what might not. It\u0026rsquo;s like a \u0026ldquo;Leading through company\u0026rsquo;s technology reorganisation for dummies\u0026rdquo; \u0026#x1f923;\nIf you accidentally became a CEO, CFO or CTO - then should read it. It seriously might help you.\nThe book is interesting, but I\u0026rsquo;m not yet ready to read it. It was enlightning experience to see how company works from this perspective. I might get back to it in a while.\nhttps://www.wgbh.org/news/national/2022-10-03/how-mckinsey-cashed-in-by-consulting-for-both-companies-and-their-regulators\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.nytimes.com/2022/09/29/business/mckinsey-tobacco-juul-opioids.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.nytimes.com/2024/04/24/business/mckinsey-criminal-investigation.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/books/2024/rewired/","summary":"RewiredThe McKinsey Guide to Outcompeting in the Age of Digital and AI\nAuthors: Eric Lamarre, Kate Smaje, Rodney Zemmel\namazon.plamazon.de I\u0026rsquo;ve heard a lot of strange things about McKinsey, starting with this parody commercial:\nThen few more facts, but still provided in an amusing way by the John Oliver:\nIt\u0026rsquo;s not just jokes than worry me, but also some news I\u0026rsquo;ve read:\nthey had no issues to consult both companies and their regulators at the same time1, they work with tobbaco companies and health industry suggesting ways to profit companies, that might be bad for their clients2, they\u0026rsquo;re behind OxyContin (opioid) sales turbocharge, which resulted in hundreds of people dead of overdose3, having relationships with authoritarian regimes and more.","title":"Rewired"},{"content":"New system, old problems. Getting Oracle Instant Client to work was a trouble 10 years ago and it\u0026rsquo;s not different today \u0026#x1f923;\nThere are two ways to install it on Ubuntu/Debian. First is \u0026ldquo;recommended\u0026rdquo;, but boring. Second is \u0026ldquo;crazy\u0026rdquo;, but have some benefits. As I\u0026rsquo;m doing it mostly in Docker images, that\u0026rsquo;s how I will present it.\nFirst, check for up to date versions\u0026thinsp; external link and links on Oracle\u0026rsquo;s official site.\nInstallation from ZIP package (recommended) For non-RPM based distributions recommended way to install Oracle Instant Client is via ZIP package1. I do it more or less like below:\nZIP Dockerfile example FROM ubuntu:24.04 ENV OIC_VERSION 21.13.0.0.0 ENV ORACLE_HOME /usr/lib/oracle/21/client64 ENV PATH $ORACLE_HOME/lib:$PATH RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ curl \\ unzip \\ libaio1t64 \\ libnsl2 \u0026amp;\u0026amp; \\ mkdir -p /opt/oracle \u0026amp;\u0026amp; \\ curl -fsSLo instantclient-basic-linux.x64-${OIC_VERSION}dbru.zip \\ https://download.oracle.com/otn_software/linux/instantclient/2113000/instantclient-basic-linux.x64-${OIC_VERSION}dbru.zip \u0026amp;\u0026amp; \\ unzip instantclient-basic-linux.x64-${OIC_VERSION}dbru.zip -d /opt/oracle \u0026amp;\u0026amp; \\ curl -fsSLo instantclient-sqlplus-linux.x64-${OIC_VERSION}dbru.zip \\ https://download.oracle.com/otn_software/linux/instantclient/2113000/instantclient-sqlplus-linux.x64-${OIC_VERSION}dbru.zip \u0026amp;\u0026amp; \\ unzip instantclient-sqlplus-linux.x64-${OIC_VERSION}dbru.zip -d /opt/oracle \u0026amp;\u0026amp; \\ rm -f instantclient*.zip \u0026amp;\u0026amp; \\ # 21 from version export OIC_MAJOR=$(printf $OIC_VERSION | cut -d. -f1) \u0026amp;\u0026amp; \\ echo $OIC_MAJOR \u0026amp;\u0026amp; \\ # 13 from version export OIC_MINOR=$(printf $OIC_VERSION | cut -d. -f2) \u0026amp;\u0026amp; \\ echo $OIC_MINOR \u0026amp;\u0026amp; \\ # put it where you normally expect it mkdir -p $ORACLE_HOME \u0026amp;\u0026amp; \\ ln -s /opt/oracle/instantclient_${OIC_MAJOR}_${OIC_MINOR} $ORACLE_HOME/lib \u0026amp;\u0026amp; \\ # we might beed to manually fix some lib paths ln -s /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1 \u0026amp;\u0026amp; \\ # update ld.so cache echo $ORACLE_HOME/lib \u0026gt; /etc/ld.so.conf.d/oracle-instantclient.conf \u0026amp;\u0026amp; \\ ldconfig \u0026amp;\u0026amp; \\ # remove packages we don\u0026#39;t need anymore apt-get purge -y \\ curl \\ unzip \u0026amp;\u0026amp; \\ apt-get autoremove -y \u0026amp;\u0026amp; \\ # let test if it works sqlplus -version There are few weird looking things here.\nZIP packages provides binaries and libraries in the same directory, usually named instantclient_XX_YY. To make binaries (eg. sqlplus) available for execution, we can either add this whole directory to the PATH or symlink binaries one by one\u0026hellip; PATH wins because it\u0026rsquo;s simpler, but I don\u0026rsquo;t like it. Feels dirty. ZIP packages provide few symlinks for libraries, but you still might need to add few more to get it working. I don\u0026rsquo;t like to manually amend stuff directly in the /usr/lib. There\u0026rsquo;s a /usr/local/lib for that. You have to remember about ls.so cache. Overwriting LD_LIBRARY_PATH especially in Docker containers might cause troubles to load libraries from other locations. Adding proper config in /etc/ld.so.conf.d saves a lot of pain. Installation from RPM (yes, on Ubuntu!) It\u0026rsquo;s one of those crazy ideas, but surprisingly it have non obvious benefits. RPM packages have better structured file localizations (separate bin and lib). DEBs generated from RPM packages can be uploaded to the Nexus/Artifactory which would save developers trouble to fetch proper binaries each time.\nWarning\nThe only reason for me to go this way is to upload DEB packages to the central repo that I\u0026rsquo;m in control of. If you don\u0026rsquo;t have it, go with the ZIP packages.\nLet me demonstrate how to achieve it with a Docker builder pattern.\nRPM Dockerfile example FROM ubuntu:24.04 as builder WORKDIR /tmp RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y alien ENV OIC_VERSION 21.13.0.0.0-1 RUN curl -fsSLo oracle-instantclient-basic-${OIC_VERSION}.el8.x86_64.rpm \\ https://download.oracle.com/otn_software/linux/instantclient/2113000/oracle-instantclient-basic-${OIC_VERSION}.el8.x86_64.rpm \u0026amp;\u0026amp; \\ curl -fsSLo oracle-instantclient-sqlplus-${OIC_VERSION}.el8.x86_64.rpm \\ https://download.oracle.com/otn_software/linux/instantclient/2113000/oracle-instantclient-sqlplus-${OIC_VERSION}.el8.x86_64.rpm RUN alien *.rpm FROM ubuntu:24.04 # so I won\u0026#39;t need to purge them manually VOLUME /var/lib/apt/lists /var/cache/apt/archives ENV ORACLE_HOME /usr/lib/oracle/21/client64 ENV PATH $ORACLE_HOME/bin:$PATH COPY --from=builder /tmp/*.deb /tmp/ RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ libaio1t64 \\ libnsl2 \u0026amp;\u0026amp; \\ dpkg -i /tmp/oracle*.deb \u0026amp;\u0026amp; \\ rm -f /tmp/oracle*.deb \u0026amp;\u0026amp; \\ # we might beed to manually fix some lib paths ln -s /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1 \u0026amp;\u0026amp; \\ # let test if it works sqlplus -version If with ZIP package there were \u0026ldquo;few weird things\u0026rdquo;, there\u0026rsquo;s much more weird stuff here \u0026#x1f923;\nWarning\nYou have to carefully select the version of RPM to be binary compatible with your distribution. Check for glibc version or just experiment.\nWhy to even go this path? Because if you\u0026rsquo;re able to provide packages from central repository, the whole things became much simpler for users, less vulnerable to the outside changes (like Oracle changing/dropping URLs). Just take a look:\nCentral repo example FROM ubuntu:24.04 # so I won\u0026#39;t need to purge them manually VOLUME /var/lib/apt/lists /var/cache/apt/archives ENV ORACLE_HOME /usr/lib/oracle/21/client64 ENV PATH $ORACLE_HOME/bin:$PATH RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ oracle-instantclient-basic_21.13.0.0.0 \\ oracle-instantclient-sqlplus_21.13.0.0.0 \\ libaio1t64 \\ libnsl2 \u0026amp;\u0026amp; \\ ln -s /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1 \u0026amp;\u0026amp; \\ sqlplus -version And that\u0026rsquo;s it! It\u0026rsquo;s about half of the size comparing to the first example with ZIP packages, but still crazy \u0026#x1f609;\nHaving environment prepared like that, you can install now Python with cx-oracle2 or even better oracledb3.\nEnjoyed? https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html#ic_x64_inst\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://oracle.github.io/python-cx_Oracle/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://oracle.github.io/python-oracledb/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/04/installing-oracle-instant-client-on-ubuntu-24.04/","summary":"New system, old problems. Getting Oracle Instant Client to work was a trouble 10 years ago and it\u0026rsquo;s not different today \u0026#x1f923;\nThere are two ways to install it on Ubuntu/Debian. First is \u0026ldquo;recommended\u0026rdquo;, but boring. Second is \u0026ldquo;crazy\u0026rdquo;, but have some benefits. As I\u0026rsquo;m doing it mostly in Docker images, that\u0026rsquo;s how I will present it.\nFirst, check for up to date versions\u0026thinsp; external link and links on Oracle\u0026rsquo;s official site.","title":"Installing Oracle Instant Client on Ubuntu 24.04"},{"content":"Following my recent SEO issues , I\u0026rsquo;m actively exploring ways to bolster my website\u0026rsquo;s ranking. One aspect I\u0026rsquo;m keen to address is expediting Search Engine indexing for pages that have been altered or updated. This is precisely the purpose of Sitemaps, a feature fully supported by Hugo1. However, there\u0026rsquo;s a limitation in how Hugo handles this by default. It sets the lastmod parameter to either the page\u0026rsquo;s creation time or the last build time. What I aim to achieve is a clear separation between creation and modification dates. This would signal to Search Engines to focus on scrutinizing only the \u0026ldquo;changed\u0026rdquo; posts, expediting their reevaluation—an approach commonplace in the Wordpress realm but more intricate with Hugo.\nFortunately, I stumbled upon the lastmod field that can be added to a post\u0026rsquo;s front matter. While this allows manual updating on a per-post basis, relying on memory might be fallible. Given that my pages are stored in a Git repository, I sought a solution to automate this process with some kind of pre-commit or post commit hook, but I discovered that Hugo offers just that \u0026#x1f913; 2\nTo enable this feature, simply add the following line to your hugo.yaml configuration:\nhugo.yaml enableGitInfo: true However, there\u0026rsquo;s a caveat: this feature requires full Git history; shallow clones won\u0026rsquo;t suffice.\nFor those employing Github Actions for their build pipeline, adjusting the checkout action is necessary. Set fetch-depth to 0 to ensure the full repository history is fetched:\nGithub Actions unshallow - uses: actions/checkout@v4 with: submodules: true fetch-depth: 0 As for myself, having recently migrated to Cloudflare Pages 3, I had to tweak the build command accordingly:\nCloudflare Pages build command git fetch --unshallow \u0026amp;\u0026amp; hugo --minify This same approach can be applied to other CI/CD configurations seamlessly.\nEnjoyed? https://gohugo.io/methods/page/sitemap/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gohugo.io/methods/page/gitinfo/#last-modified-date\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gohugo.io/methods/page/gitinfo/#hosting-considerations\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/03/automatically-setting-lastmod-in-hugo-pages-with-git-modification-date/","summary":"Following my recent SEO issues , I\u0026rsquo;m actively exploring ways to bolster my website\u0026rsquo;s ranking. One aspect I\u0026rsquo;m keen to address is expediting Search Engine indexing for pages that have been altered or updated. This is precisely the purpose of Sitemaps, a feature fully supported by Hugo1. However, there\u0026rsquo;s a limitation in how Hugo handles this by default. It sets the lastmod parameter to either the page\u0026rsquo;s creation time or the last build time.","title":"Automatically setting 'lastmod' in Hugo pages with Git modification date"},{"content":"Around 3 years ago I\u0026rsquo;ve checked the age of various base images available on Docker Hub. Curiosity recently got the better of me, prompting another investigation into the current state of affairs.\nSince then, there have been significant changes:\nCentOS 8 has reached its end of life\u0026thinsp; external link , prompting a shift to CentOS 8 Stream\u0026thinsp; external link , which is also nearing its demise \u0026#x1f923; OpenJDK images have been deprecated\u0026thinsp; external link and no longer receiving updates. They recommend to switch to alternatives such as: Amazon\u0026rsquo;s Corretto or Eclipse Temurin (formely AdoptOpenJDK). Let\u0026rsquo;s delve into the findings:\nImage Creation date Age(in days) Packagesto upgrade centos:7 2021-09-15 921 51 quay.io/centos/centos:centos7 2020-11-14 1226 51 quay.io/centos/centos:stream8 2024-03-19 5 2 quay.io/centos/centos:stream9 2024-03-19 5 0 debian:10 2024-03-12 12 0 debian:11 2024-03-12 12 0 debian:12 2024-03-12 12 0 ubuntu:18.04 2023-05-30 299 1 ubuntu:20.04 2024-02-16 36 0 ubuntu:22.04 2024-02-27 26 2 ubuntu:24.04 2024-02-25 28 10 alpine:3.17 2024-01-27 57 1 alpine:3.18 2024-01-27 57 1 alpine:3.19 2024-01-27 57 1 node:16 2023-09-08 198 78 node:18 2024-03-12 12 0 node:20 2024-03-12 12 0 node:21 2024-03-12 12 0 openjdk:8 2022-08-02 600 45 openjdk:11 2022-08-02 600 45 openjdk:17 2022-04-27 696 67 openjdk:21 2023-09-22 184 41 amazoncorretto:8 2024-03-16 8 0 amazoncorretto:11 2024-03-16 8 0 amazoncorretto:17 2024-03-16 8 1 amazoncorretto:21 2024-03-16 8 0 eclipse-temurin:8 2024-03-06 18 4 eclipse-temurin:11 2024-03-06 18 4 eclipse-temurin:17 2024-03-06 18 4 eclipse-temurin:21 2024-03-06 18 4 Surprisingly, the results were not as dire as expected. Modern OS versions tend to be reasonably up-to-date. This marks a positive change from the past, where CentOS images were both popular and outdated.\nHowever, it\u0026rsquo;s worth noting that many users may still be unaware of the age of certain images, such as openjdk, potentially leading to issues down the line. A search on GitHub reveals\u0026thinsp; external link nearly 300k Dockerfiles referencing FROM openjdk.\nHow did I collect those numbers? I wrote a small bash script. Use it as a base to check images of your interest.\nEnjoyed? ","permalink":"https://gagor.pro/2024/03/how-old-are-official-docker-images-2024-edition/","summary":"Around 3 years ago I\u0026rsquo;ve checked the age of various base images available on Docker Hub. Curiosity recently got the better of me, prompting another investigation into the current state of affairs.\nSince then, there have been significant changes:\nCentOS 8 has reached its end of life\u0026thinsp; external link , prompting a shift to CentOS 8 Stream\u0026thinsp; external link , which is also nearing its demise \u0026#x1f923; OpenJDK images have been deprecated\u0026thinsp; external link and no longer receiving updates.","title":"How old are Official Docker images? 2024 Edition"},{"content":"I was a big fan of the Atom1 code editor, so when it was discontinued and replaced by M$\u0026rsquo;s VS Code2, I was both sad and dissapointed. While I can admit that VS Code is a solid editor, my personal preference always leaned towards Atom.\nHowever, over time, Atom started to feel sluggish. I collected numerous plugins over the years and it began to weigh heavily on performance, not just during startup but also during regular usage. Typing slow response was especially quite irritating.\nJust recently, I stumbled upon Zed3. Built in Rust and designed with a focus on speed, Zed intrigued me with the promise of recapturing some of Atom\u0026rsquo;s essence. While it\u0026rsquo;s not quite ready to replace my heavily customized VS Code setup, I\u0026rsquo;m keeping a close eye on its development.\nIn conclusion, while I mourn the loss of Atom, I\u0026rsquo;m hopeful that Zed might just be the editor to reignite that same sense of joy and efficiency in coding.\nTo make it usable for me, I had to change few config options:\nMy Zed\u0026#39;s config { \u0026#34;theme\u0026#34;: \u0026#34;Ayu Dark\u0026#34;, // I like Monokai, but it\u0026#39;s not yet there \u0026#34;base_keymap\u0026#34;: \u0026#34;Atom\u0026#34;, // no need to change habbits \u0026#34;ui_font_size\u0026#34;: 16, \u0026#34;buffer_font_size\u0026#34;: 16, \u0026#34;auto_update\u0026#34;: false, // I use Homebrew, so don\u0026#39;t need that \u0026#34;autosave\u0026#34;: \u0026#34;on_focus_change\u0026#34;, // I\u0026#39;m lazy and get used to that \u0026#34;language_overrides\u0026#34;: { \u0026#34;Markdown\u0026#34;: { \u0026#34;tab_size\u0026#34;: 2 } } } Config is located in ~/.config/zed/settings.json, but you can also open it quickly with Cmd + , shortcut. More configuration options can be found in docs4.\nHave fun!\nhttps://atom-editor.cc\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://code.visualstudio.com\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zed.dev\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zed.dev/docs/configuring-zed\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/03/zed-code-editor-as-an-atom-reincarnation/","summary":"I was a big fan of the Atom1 code editor, so when it was discontinued and replaced by M$\u0026rsquo;s VS Code2, I was both sad and dissapointed. While I can admit that VS Code is a solid editor, my personal preference always leaned towards Atom.\nHowever, over time, Atom started to feel sluggish. I collected numerous plugins over the years and it began to weigh heavily on performance, not just during startup but also during regular usage.","title":"Zed Code Editor as an Atom reincarnation"},{"content":"When it comes to building Docker images, adhering to the \u0026ldquo;Filesystem Hierarchy Standard\u0026rdquo;12 can greatly enhance the organization and maintainability of your containers. Unfortunately, it\u0026rsquo;s not uncommon to encounter Docker images where files are haphazardly scattered across directories, leading to confusion and unnecessary complications. Let\u0026rsquo;s delve into some best practices to ensure your Dockerfiles follow the FHS guidelines, thus avoiding common pitfalls and streamlining your container development process.\nBelow you can find the most important directories, from the perspective of Docker images. /dev or /root rarely are useful here.\ngraph LR root[\"root /\"] --\u003e bin[\"/bin\"] root --\u003e sbin[\"/sbin\"] root --\u003e etc[\"/etc\"] root --\u003e home[\"/home\"] root --\u003e usr[\"/usr\"] root --\u003e opt[\"/opt\"] root --\u003e srv[\"/srv\"] root --\u003e tmp[\"/tmp\"] root --\u003e var[\"/var\"] usr --\u003e local[\"/local\"] local --\u003e local_bin[\"/bin\"] local --\u003e local_sbin[\"/sbin\"] local --\u003e share[\"/share\"] share --\u003e java[\"/java\"] opt --\u003e app[\"/your-app-name\"] var --\u003e var_lib[\"/lib\"] var --\u003e var_cache[\"/cache\"] var --\u003e var_log[\"/log\"] var --\u003e var_tmp[\"/tmp\"] Organize scripts One frequent oversight is the placement of scripts and binaries within Docker images. Instead of cluttering the root directory (/) or other unconventional locations, adhere to the FHS guidelines:\nScripts: Store your scripts in /usr/local/bin or /usr/local/sbin for root-owned scripts. Placing scripts here eliminates the need to manipulate the PATH environment variable manually and ensures they\u0026rsquo;re easily accessible.\nDockerfile COPY script.sh /usr/local/bin/ Handle Java binaries properly Binaries and Artifacts: If your Docker image includes binary artifacts like Java jar packages or shared libraries, designate appropriate directories such as /opt/your-app-name or /usr/local/share/java. This practice maintains clarity and consistency within your image structure.\nDockerfile COPY your-app.jar /opt/your-app-name/ Handling Web Files for Web Servers When configuring web servers like Nginx or Apache within Docker images, it\u0026rsquo;s crucial to adhere to FHS principles for storing web files. Here\u0026rsquo;s how you can effectively manage web content:\nNginx: For Nginx, follow the convention of placing web files in /usr/share/nginx/html. This directory is the default location where Nginx looks for static web content. By adhering to this standard, you ensure seamless integration with Nginx configurations.\nDockerfile COPY --chown=nginx:nginx index.html /usr/share/nginx/html/ Apache: Similarly, Apache web server deployments should utilize /var/www/html for storing web files. This directory is Apache\u0026rsquo;s default document root, simplifying the setup and maintenance of your Apache-powered containers.\nDockerfile COPY --chown=www-data:www-data index.html /var/www/html/ Handling caches or temp files Writing files inside of container is generally a bad idea. Layered file systems like aufs or overlay provide poor performance, so if you really need to write files inside container, write them to volumes. First setup any permissions, then make them volumes \u0026ldquo;and freeze\u0026rdquo; them in this state. Volumes are bind mounted to the local file system on hosts so they provide much better performance.\nTemporary files: There are two most common locations for temporary files.\nDockerfile VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;] Cache: Set permissions before volume creation, because later it\u0026rsquo;s not persisting. Remember that those files are stored on local file system, so set reasonable limits. When container will be terminated, volume will be left, so your cluster should have some cleanup job configured to purge them.\nDockerfile RUN mkdir -p /var/cache/nginx \u0026amp;\u0026amp; \\ chown -R nginx:nginx /var/cache/nginx VOLUME [\u0026#34;/var/cache/nginx\u0026#34;] Summary By following these guidelines, you just make your life easier. Many things could just work out of the box, leading to smoother development and deployment workflows.\nhttps://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/03/best-practices-for-writing-dockerfiles-follow-filesystem-hierarchy-standard/","summary":"When it comes to building Docker images, adhering to the \u0026ldquo;Filesystem Hierarchy Standard\u0026rdquo;12 can greatly enhance the organization and maintainability of your containers. Unfortunately, it\u0026rsquo;s not uncommon to encounter Docker images where files are haphazardly scattered across directories, leading to confusion and unnecessary complications. Let\u0026rsquo;s delve into some best practices to ensure your Dockerfiles follow the FHS guidelines, thus avoiding common pitfalls and streamlining your container development process.\nBelow you can find the most important directories, from the perspective of Docker images.","title":"Best practices for writing Dockerfiles - Follow \"Filesystem Hierarchy Standard\""},{"content":"Introduction Being the owner of this blog, I often find joy in revisiting past endeavors that may no longer be part of my daily work. From fine-tuning website performance to delving into the intricacies of SEO, I relish the opportunity to explore various aspects of digital craftsmanship.\nOne aspect that particularly piques my interest is finding creative ways to monetize my blog without resorting to intrusive advertisements. As a steadfast supporter of Open Source principles, I value the ability for people to express their appreciation through voluntary contributions or tips, on their own terms. This is where platforms like Ko-fi come into play.\nIn this blog post, I\u0026rsquo;d like to share a collection of custom shortcodes tailored for Hugo static site generator, designed to seamlessly integrate Ko-fi\u0026thinsp; external link buttons and widgets into any blog post. With easy configuration and a range of options to choose from, these shortcodes offer a hassle-free solution for bloggers seeking alternative monetization avenues.\nLet\u0026rsquo;s dive into how you can leverage these shortcodes to enhance your blog and cultivate support from your audience.\nOf course if you enjoy it, \u0026#x2615; me!\nInstallation and configuration First, add shortcode theme to your site:\nAdd kofi shortcodes to Hugo site cd your-site-location git submodule add --depth=1 \\ https://github.com/tgagor/hugo-shortcode-kofi.git \\ themes/hugo-shortcode-kofi All shortcodes rely on one global parameter, which should be set up in your config.yaml or hugo.yaml:\nUpdate hugo.yaml theme: - YourThemeOfChoice - hugo-shortcode-kofi params: kofi: username: your-name Thanks to that, you can just use shortcodes without the need to each time provide the user parameter. For more details, check repo\u0026thinsp; external link .\nGeneric parameters Parameter name Default value Description user your Ko-fi user name, tries to use params.kofi.username first text \u0026quot;Support Me on Ko-fi\u0026quot; text shown on the button alt_text \u0026quot;Buy Me a Coffee at ko-fi.com\u0026quot; text shown if image can\u0026rsquo;t be loaded color \u0026quot;#29abe0\u0026quot; hex color of the button (if applicable) style \u0026quot;blue\u0026quot; style of the button (if applicable) height 36 height in pixels (if applicable) Demos Animated buttons Those look nice and have animated mugs, but I saw them being blocked by uBlock Origin.\nAnimated button Shortcode {{\u0026lt; kofi/button \u0026gt;}} or {{\u0026lt; kofi/button color=\u0026quot;#13C3FF\u0026quot; \u0026gt;}} {{\u0026lt; kofi/button color=\u0026quot;#FF5E5B\u0026quot; \u0026gt;}} {{\u0026lt; kofi/button color=\u0026quot;#434b57\u0026quot; \u0026gt;}} Static images Image style 1 Shortcode {{\u0026lt; kofi/image1 \u0026gt;}} or {{\u0026lt; kofi/image1 style=\u0026quot;blue\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image1 style=\u0026quot;grey\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image1 style=\u0026quot;red\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image1 style=\u0026quot;green\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image1 style=\u0026quot;dark\u0026quot; \u0026gt;}} Image style 2 Shortcode {{\u0026lt; kofi/image2 \u0026gt;}} or {{\u0026lt; kofi/image2 style=\u0026quot;blue\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image2 style=\u0026quot;red\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image2 style=\u0026quot;stroke\u0026quot; \u0026gt;}} {{\u0026lt; kofi/image2 style=\u0026quot;dark\u0026quot; \u0026gt;}} Logotypes Logos Shortcode {{\u0026lt; kofi/logo \u0026gt;}} or {{\u0026lt; kofi/logo style=\u0026quot;logo\u0026quot; \u0026gt;}} {{\u0026lt; kofi/logo style=\u0026quot;mug\u0026quot; \u0026gt;}} {{\u0026lt; kofi/logo style=\u0026quot;pixel\u0026quot; \u0026gt;}} Badges Badges Shortcode {{\u0026lt; kofi/badge \u0026gt;}} or {{\u0026lt; kofi/badge style=white \u0026gt;}} {{\u0026lt; kofi/badge style=dark \u0026gt;}} {{\u0026lt; kofi/badge style=bg-white \u0026gt;}} {{\u0026lt; kofi/badge style=bg-dark \u0026gt;}} Dynamic widgets Widgets Shortcode check left-bottom corner {{\u0026lt; kofi/floating-button \u0026gt;}} {{\u0026lt; kofi/donation-panel \u0026gt;}} Custom images You can also use your custom images:\n{{\u0026lt; kofi/custom height=200 url=\u0026#34;https://media.giphy.com/media/kgKFcQk6oa1WIdHNSl/giphy.gif\u0026#34; \u0026gt;}} For inspiration check here:\nhttps://more.ko-fi.com/brand-assets\u0026thinsp; external link https://giphy.com/Kofi_button\u0026thinsp; external link ","permalink":"https://gagor.pro/2024/03/monetize-your-blog-with-ko-fi-shortcodes-for-hugo-sites/","summary":"Introduction Being the owner of this blog, I often find joy in revisiting past endeavors that may no longer be part of my daily work. From fine-tuning website performance to delving into the intricacies of SEO, I relish the opportunity to explore various aspects of digital craftsmanship.\nOne aspect that particularly piques my interest is finding creative ways to monetize my blog without resorting to intrusive advertisements. As a steadfast supporter of Open Source principles, I value the ability for people to express their appreciation through voluntary contributions or tips, on their own terms.","title":"Monetize your blog with Ko-fi shortcodes for Hugo sites"},{"content":"I rely heavily on shell usage. For over 15 years, I was a devoted user of Bash until I discovered Zsh and made the switch. One thing that remained constant throughout my transition was my configuration to maintain an extensive commands history. This setup is replicated across all my systems. When you spend a lot of time coding, you often find yourself repeating commands or running variations of them. Remembering all these commands can be challenging, but with resources like Google and ChatGPT, it\u0026rsquo;s not always necessary. However, if you find yourself frequently searching for specific commands, it can be time-consuming and distracting. That\u0026rsquo;s where having parts of the command at your fingertips becomes invaluable. With a quick press of Ctrl+r and a little scanning, you can find what you need.\nLet\u0026rsquo;s consider an example: Suppose you need to rename all *.md files in the current directory to *.en.md because you have to add multilingual translations to your 100 articles.\nI recall that I used the rename command for this task, but I don\u0026rsquo;t quite remember how. So, I press Ctrl+r, start typing rename, and there it is:\nrename \u0026#39;s/\\.md$/\\.en\\.md/\u0026#39; *.md It took me just a few seconds.\nWhile Bash allows you to recall only the latest command and requires multiple Ctrl+r presses to eventually find what you need, you can use fzf, which provides a fuzzy-find algorithm to search through the entire history. It allows you to find the desired command quickly using the \u0026#x2b06;\u0026#xfe0f; and \u0026#x2b07;\u0026#xfe0f; arrow keys. It\u0026rsquo;s even faster.\nMy extensive history configuration for Zsh My ~/.zshrc # Set the maximum number of lines to keep in the history file export HISTSIZE=1000000 # Set the number of history entries to save to the history file export SAVEHIST=2000000 # Records the timestamp along with the history entry setopt EXTENDED_HISTORY # Causes duplicates to be removed in the order they appear in the history file setopt HIST_EXPIRE_DUPS_FIRST # Prevents duplicate commands from being recorded in the history setopt HIST_IGNORE_DUPS # Deletes the old recorded entry if a new entry is a duplicate setopt HIST_IGNORE_ALL_DUPS # When searching the history, it won\u0026#39;t display a previously found line setopt HIST_FIND_NO_DUPS # Commands starting with a space won\u0026#39;t be recorded in the history setopt HIST_IGNORE_SPACE # Appends each new history line to the history file immediately setopt INC_APPEND_HISTORY # When saving the history to a file, duplicate entries won\u0026#39;t be written setopt HIST_SAVE_NO_DUPS # Shares command history between all sessions setopt SHARE_HISTORY # Prevents commands from being immediately executed when retrieved from the history using event designators unsetopt HIST_VERIFY My extensive history configuration for Bash (when I still used it) My ~/.bash_profile # Set the control behavior for the history file to ignore and erase duplicates export HISTCONTROL=ignoredups:erasedups # Append to the history file, don\u0026#39;t overwrite it shopt -s histappend # Set the maximum number of lines to keep in the history file export HISTSIZE=1000000 # Set the maximum number of lines in the history file to keep on disk export HISTFILESIZE=2000000 Enhance performance with fzf Install fzf on MacOS brew install fzf /opt/homebrew/opt/fzf/install By installing fzf, your default Ctrl+r shortcut will be replaced with fzf. You can also use other shortcuts like Alt+c or Ctrl+t. For tips on fzf usage, check out this article\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2024/03/unlocking-efficiency-the-power-of-extensive-command-history-in-bash-and-zsh/","summary":"I rely heavily on shell usage. For over 15 years, I was a devoted user of Bash until I discovered Zsh and made the switch. One thing that remained constant throughout my transition was my configuration to maintain an extensive commands history. This setup is replicated across all my systems. When you spend a lot of time coding, you often find yourself repeating commands or running variations of them. Remembering all these commands can be challenging, but with resources like Google and ChatGPT, it\u0026rsquo;s not always necessary.","title":"Unlocking Efficiency - The power of extensive command history in Bash and Zsh"},{"content":"I used to utilize GitHub Pages to serve static content for my blog. I secured it behind Cloudflare to employ a custom domain and automate HTTPS certificate management. Additionally, I utilized a few Page Rules to implement redirects:\nFrom www.* to non-www, From HTTP to HTTPS, And for some SEO renaming. Unfortunately, in the Free plan from Cloudflare, you are limited to:\n3 Page Rules (with simple glob matching), 10 Transformation Rules (no regex rules), 10 Redirect Rules (no regex). I made use of a combination of these rules, but due to these limitations, I couldn\u0026rsquo;t meet all of my requirements. I have few hundreds of broken URLs reported by Google Web Console. Google still remembers that I migrated from Wordpress and attempts to index some paths I no longer host. Although I attempted to utilize Hugo\u0026rsquo;s aliases 1, it only facilitates 1-to-1 matching. I\u0026rsquo;m unable to address this issue within these constraints.\nOn the flip side, Cloudflare Page supports the _redirects file, which can host 2000 static and 100 dynamic redirects, totaling up to 2100 redirects 2. That\u0026rsquo;s more than what I need.\nI decided to transition static file hosting to Cloudflare Pages3. There\u0026rsquo;s a helpful article describing the configuration\u0026thinsp; external link , and the process is quite straightforward.\nNow, what I needed was an easy way to generate this _redirects file.\nGenerating _redirects within Hugo Basic syntax of _redirects files is simple 4:\n[source] [destination] [code?] I\u0026rsquo;d like to generate it automatically from Hugo\u0026rsquo;s aliases, then add few more. For that let create a new template file under layouts/_default/home._redirects with the following content:\nMinimum layouts/_default/home._redirects content {{- range .Site.Pages -}} {{- if .Aliases -}} {{- $new := .RelPermalink -}} {{- range .Page.Aliases }}{{ . }} {{ $new }} 301 {{ end -}} {{- end -}} {{- end -}} At the end of this file, you can add your own custom redirects. For me, these include taxonomy changes:\n/tag/* /tags/:splat 301 /category/* /categories/:splat 301 /main/* / 301 /author/* /authors/:splat 301 Now, in the config.yaml or hugo.yaml, add the configuration to handle the new file type:\nExtend Hugo\u0026#39;s config with this mediaTypes: text/redirects: outputFormats: _redirect: name: _redirects mediaType: text/redirects baseName: _redirects isPlainText: true rel: alternate isHTML: false noUgly: true permalinkable: false That\u0026rsquo;s it. Deploy/rebuild your website at Cloudflare and check if redirects work as expected.\nI must admit it\u0026rsquo;s not my original idea — I found it in the sources\u0026thinsp; external link of russ.foo\u0026thinsp; external link \u0026rsquo;s blog. Check it out, there are more fancy modifications.\nEnjoyed? https://gohugo.io/methods/page/aliases/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developers.cloudflare.com/pages/platform/limits/#redirects\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developers.cloudflare.com/pages/framework-guides/deploy-a-hugo-site\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developers.cloudflare.com/pages/configuration/redirects/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/03/optimizing-hugo-static-site-redirects-with-cloudflare-pages/","summary":"I used to utilize GitHub Pages to serve static content for my blog. I secured it behind Cloudflare to employ a custom domain and automate HTTPS certificate management. Additionally, I utilized a few Page Rules to implement redirects:\nFrom www.* to non-www, From HTTP to HTTPS, And for some SEO renaming. Unfortunately, in the Free plan from Cloudflare, you are limited to:\n3 Page Rules (with simple glob matching), 10 Transformation Rules (no regex rules), 10 Redirect Rules (no regex).","title":"Optimizing Hugo static site redirects with Cloudflare Pages"},{"content":"Towards the end of the year, I received a notification reminding me to renew the domain for my blog. The domain in question was timor.site. This was the first invoice I had received from GoDaddy since they acquired Uniregistry, and to my dismay, it was significantly more expensive than what I had been accustomed to paying. With the year drawing to a close, I was reluctant to invest too much time or money into the renewal process. Frustrated by the cost, I decided to redirect my blog to another domain I owned: gagor.pl (where this blog was actually running in the past).\nHowever, I hadn\u0026rsquo;t anticipated the impact this move would have on my blog\u0026rsquo;s SEO. Despite redirecting all the content without making any changes, there were noticeable differences in the results and traffic.\nLet\u0026rsquo;s examine the raw data. The old domain, timor.site, appeared in search results over 150 times within roughly a month and I distinctly recall seeing a significant decrease in traffic in this time.\ntimor.site search result statistics\nOn the other hand, the new (technically old) domain, gagor.pl, garnered approximately 200 impressions over two months. During this period, one of the pages went viral, resulting in a considerable influx of traffic.\ngagor.pl search result statistics\nI also conducted an analysis of domain rank and other SEO metrics. Surprisingly, my gagor.pl domain boasted a rank of 4.6, whereas timor.site lagged far behind at 1.3. Despite its lower rank, the latter domain attracted more traffic. This served as a reminder of why I had initially opted for an international domain—to enhance its recognition on a global scale. However, the bilingual nature of my content, with occasional posts in Polish, could potentially confuse some of my followers.\nIt appears that the time has come to move forward and transform my site into a multilingual, with content segregated by language. This strategy should improve its international visibility and better align with the SEO requirements of my website.\nOf course, this transition won\u0026rsquo;t be without its challenges—implementing multilingual support will necessitate a considerable number of redirects. But as they say, progress often comes at a cost.\n","permalink":"https://gagor.pro/2024/03/seo-kicks-my-stats/","summary":"Towards the end of the year, I received a notification reminding me to renew the domain for my blog. The domain in question was timor.site. This was the first invoice I had received from GoDaddy since they acquired Uniregistry, and to my dismay, it was significantly more expensive than what I had been accustomed to paying. With the year drawing to a close, I was reluctant to invest too much time or money into the renewal process.","title":"SEO kicks my stats"},{"content":"I\u0026rsquo;ve been writing recently about best practices for patching and deprecating Docker images , but today I want to show how to automate a huge part of this process.\nYou might already hear about Dependabot1, it\u0026rsquo;s a Github\u0026rsquo;s way to notify developers about security vulnerabilities in their projects. Renovate2 is similar tool3, but doesn\u0026rsquo;t require Github. For my professional work I use Bitbucket, so Renovate feels more universal as can be used anywhere.\nWhat\u0026rsquo;s the main problem here? When we write Dockerfiles, we might install system packages (this is easy to update), but sometimes we have to fetch something from Github or Maven. Those binaries might not have a specific repository and updating them is a manual work. Variety of tools and ways of doing it, makes the whole thing a complex task.\nThat\u0026rsquo;s where Renovate steps in. It provides a variety of \u0026ldquo;managers\u0026rdquo;\u0026thinsp; external link which can automatically determine package versions from known sources (docker, maven, npm, pypi and many more). Those can automatically make patching easier. There\u0026rsquo;s also special manager, called regex which allows to make impossible a possible, by finding dependencies for custom links. Let me show you how.\nHow Renovate Bot can help us? Without much configuration, Renovate can automatically detect changes in Dockerfiles and propose updates of parent image versions to their latest. That\u0026rsquo;s not much and if you \u0026ldquo;pin\u0026rdquo; parent images loosely:\nBase image pinning example FROM alpine:3 ... then it\u0026rsquo;s not really useful.\nIt\u0026rsquo;s getting harder when you want to keep up to date tools downloaded from different sources like with curl or git. For that, we have to prepare a syntax a little bit. Take a look at two examples:\nDownloading NVM with git. Git fetching example # renovate: datasource=github-releases depName=nvm-sh/nvm ENV NVM_VERSION v0.39.3 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y git \u0026amp;\u0026amp; \\ git clone \\ --depth 1 \\ --branch $NVM_VERSION \\ https://github.com/nvm-sh/nvm.git ... Downloading minimum init binary with curl. Curl fetching example # renovate: datasource=github-releases depName=krallin/tini ARG TINI_VERSION=v0.18.0 RUN curl -fsSLo /usr/local/sbin/tini https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-amd64 \u0026amp;\u0026amp; \\ curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-amd64.sha256sum | sha256sum -c \u0026amp;\u0026amp; \\ chmod +x /usr/local/sbin/tini \u0026amp;\u0026amp; \\ tini -v ... Important part here is to use ENV or ARG and expose the version we need in single place. We use it to refer to the app version when installing.\nAnother important thing are # renovate comments. They provide metadata, pointing what kind of datasource 4 and dependency name to use, for example:\nmaven -\u0026gt; # renovate: datasource=maven depName=com.google.code.gson:gson github-releases -\u0026gt; # renovate: datasource=github-releases depName=nvm-sh/nvm docker -\u0026gt; # renovate: datasource=docker depName=tomcat check Renovate\u0026rsquo;s documentation\u0026thinsp; external link for even more sources. Renovate do not understand our comments yet, so let teach it \u0026#x1f609;\nRenovate config.js example module.exports = { // check for more here: https://docs.renovatebot.com/modules/platform/ platform: \u0026#34;github\u0026#34;, gitAuthor: \u0026#34;renovate \u0026lt;renovate@example.com\u0026gt;\u0026#34;, forkProcessing: \u0026#34;enabled\u0026#34;, // renovate usually requires a branch or a config file in the repo, // I run it from separate project, to scan multiple Docker images repositories requireConfig: \u0026#34;optional\u0026#34;, onboarding: false, // to match those repositories, we have to discover them autodiscover: true, autodiscoverFilter: [ \u0026#34;tgagor/docker-*\u0026#34; ], // we will only use regex module in the example // feel free to add more as you need: https://docs.renovatebot.com/modules/manager/ enabledManagers: [\u0026#34;custom.regex\u0026#34;], // we\u0026#39;re adding custom regex matcher // more examples here: https://docs.renovatebot.com/modules/manager/regex/ customManagers: [ { customType: \u0026#34;regex\u0026#34;, fileMatch: [\u0026#34;/Dockerfile$\u0026#34;], matchStrings: [ \u0026#34;#\\\\s*renovate:\\\\s*datasource=(?\u0026lt;datasource\u0026gt;.*?)\\\\s+depName=(?\u0026lt;depName\u0026gt;.*?)(\\\\s+versioning=(?\u0026lt;versioning\u0026gt;.*?))?\\\\s*(ENV|ARG)\\\\s+[A-Z0-9_]+_VERSION[= ]?[\\\u0026#34;\u0026#39;]?(?\u0026lt;currentValue\u0026gt;.*?)[\\\u0026#34;\u0026#39;]?\\\\s\u0026#34;, ], // datasourceTemplate: \u0026#34;{{{datasource}}}\u0026#34;, // depNameTemplate: \u0026#34;{{{depName}}}\u0026#34;, // versioningTemplate: \u0026#34;{{#if versioning}}{{{versioning}}}{{else}}semver-coerced{{/if}}\u0026#34; }, ], // here we can add per package exceptions packageRules: [ // I keep separate image per major version of Node // so I wan\u0026#39;t to disable propositions for major upgrades { matchDatasources: [\u0026#34;node-version\u0026#34;], matchUpdateTypes: [\u0026#34;major\u0026#34;], enabled: false }, // similar for Python, I want to be informed only about patch // level updates { matchDatasources: [\u0026#34;docker\u0026#34;], matchUpdateTypes: [\u0026#34;major\u0026#34;, \u0026#34;minor\u0026#34;], matchPackagePatterns: [\u0026#34;python\u0026#34;], enabled: false }, // let enforce to use our maven repo cache // useful if you use Nexus or Artifactory { matchDatasources: [\u0026#34;maven\u0026#34;], registryUrls: [\u0026#34;https://use.custom.maven.repo\u0026#34;], }, ], // Git-related customizations group: { commitMessageTopic: \u0026#34;feat(renovate): upgrading dependencies\u0026#34;, }, branchPrefix: \u0026#34;renovate/\u0026#34;, }; The most important part in the config is the custom regex matcher in the customManagers section. It extracts metadata from our comments, and use them to query defined datasources for new version, so Renovate could propose us updated. If we have test images deeply, we can even configure it to auto merge changes if build is passing.\nWe can also use packageRules to define some custom rules on per package basis or group changes of specific type to not be flooded by PRs.\nI\u0026rsquo;m really fascinated by this tool. It makes boring and annoying task so easy. I can add it to the cron, run monthly or weekly and never by late with the updates again.\nhttps://docs.github.com/en/code-security/dependabot\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/renovatebot/renovate\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.renovatebot.com/bot-comparison/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.renovatebot.com/modules/datasource/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/03/automatic-updates-of-docker-images-with-renovate-bot/","summary":"I\u0026rsquo;ve been writing recently about best practices for patching and deprecating Docker images , but today I want to show how to automate a huge part of this process.\nYou might already hear about Dependabot1, it\u0026rsquo;s a Github\u0026rsquo;s way to notify developers about security vulnerabilities in their projects. Renovate2 is similar tool3, but doesn\u0026rsquo;t require Github. For my professional work I use Bitbucket, so Renovate feels more universal as can be used anywhere.","title":"Automatic updates of Docker images with Renovate Bot"},{"content":" Błękitna kropkaCzłowiek i jego przyszłość w kosmosie\nAuthor: Carl Sagan\nempik.comamazon.de I love science and I\u0026rsquo;m passionate about cosmos since I was 5. One of my favourite books from the childhood was the one about Universe - with color pictures. This one is a sweet reminder of those ancient times and have even better pictures!\nThere is an interesting reference to the deities and their relation to the planets. Their impact on humanities history, then a swift dive into the place of the God in \u0026ldquo;our\u0026rdquo; universe. Statistical considerations of human being \u0026ldquo;the choosen one\u0026rdquo; by God. I didn\u0026rsquo;t expect it in a book like that, but it resonated the strings inside me. I really enjoyed this part.\nThere\u0026rsquo;s also a lot informations about the Voyager missions, pictures, results, conclusion.\nIt\u0026rsquo;s a must read for any Cosmos/Uniwerse knowledge fan.\n","permalink":"https://gagor.pro/books/2024/blekina-kropka/","summary":"Błękitna kropkaCzłowiek i jego przyszłość w kosmosie\nAuthor: Carl Sagan\nempik.comamazon.de I love science and I\u0026rsquo;m passionate about cosmos since I was 5. One of my favourite books from the childhood was the one about Universe - with color pictures. This one is a sweet reminder of those ancient times and have even better pictures!\nThere is an interesting reference to the deities and their relation to the planets. Their impact on humanities history, then a swift dive into the place of the God in \u0026ldquo;our\u0026rdquo; universe.","title":"Błękitna kropka"},{"content":" ŚlepowidzenieAuthor: Peter Watts\namazon.plempik.com Still reading\u0026hellip;\n","permalink":"https://gagor.pro/books/2024/slepowidzenie/","summary":"ŚlepowidzenieAuthor: Peter Watts\namazon.plempik.com Still reading\u0026hellip;","title":"Ślepowidzenie"},{"content":"I used Chart.js charts on one of my Hugo blogs for few years already. Recently I needed to add few diagrams and I started glueing them in Chart.js but then I found that Hugo supports GoAT and Mermaid diagrams\u0026thinsp; external link . They\u0026rsquo;re not working out of the box, but it\u0026rsquo;s easy to extend. Much easier than my custom shortcodes.\nI tried to follow up the instruction1 but failed. After few minutes I found slightly simpler solution. Just create file: layouts/_default/_markup/render-codeblock-mermaid.html and fill it with:\nlayouts/_default/_markup/render-codeblock-mermaid.html \u0026lt;pre class=\u0026#34;mermaid\u0026#34;\u0026gt; {{- .Inner | safeHTML }} \u0026lt;/pre\u0026gt; {{- if not (.Page.Store.Get \u0026#34;hasMermaid\u0026#34;) -}} {{- .Page.Store.Set \u0026#34;hasMermaid\u0026#34; true -}} {{- $theme := \u0026#34;neutral\u0026#34; -}} {{- if eq site.Params.defaultTheme \u0026#34;dark\u0026#34; -}} {{- $theme = \u0026#34;dark\u0026#34; -}} {{- end -}} \u0026lt;script type=\u0026#34;module\u0026#34;\u0026gt; import mermaid from \u0026#39;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs\u0026#39;; mermaid.initialize({ startOnLoad: true, theme: \u0026#39;{{ $theme }}\u0026#39; }); \u0026lt;/script\u0026gt; {{- end -}} The best thing about this code is that it will load Javascript only once per web page. The only thing to add Mermaid diagram to my pages I have to call it like:\nExample diagram ```mermaid flowchart LR A -- text --\u0026gt; B -- text2 --\u0026gt; C ``` Just past it in the Markdown file and you will get:\nflowchart LR A -- text --\u003e B -- text2 --\u003e C Nice and simple as ABC \u0026#x1f603;\nhttps://gohugo.io/content-management/diagrams/#mermaid-diagrams\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/02/adding-mermaid-diagrams-to-hugo/","summary":"I used Chart.js charts on one of my Hugo blogs for few years already. Recently I needed to add few diagrams and I started glueing them in Chart.js but then I found that Hugo supports GoAT and Mermaid diagrams\u0026thinsp; external link . They\u0026rsquo;re not working out of the box, but it\u0026rsquo;s easy to extend. Much easier than my custom shortcodes.\nI tried to follow up the instruction1 but failed. After few minutes I found slightly simpler solution.","title":"Adding Mermaid diagrams to Hugo"},{"content":"First contact with make When I was invited for my first job interview in the IT, I\u0026rsquo;ve been asked such question:\nHow would you typically build a program from sources, what commands will you use?\nI answered:\nIt\u0026rsquo;s obvious:\n./configure make make install Those times belong to the past now and nowadays not many programmers use GNU Make1. Try asking this question and you will see disgust at best.\nFor many it\u0026rsquo;s the fist contact with make and often the last one, but not for me \u0026#x1f609;\nWhat make does? Let build a base line. make orchestrates tasks based on dependencies, executing commands to generate target files and keep them up to date efficiently. It streamlines software compilation and project management.\nAs simple as it is, it does few things pretty well:\ndetects changes in files (source -\u0026gt; binary), manages dependencies, manages default values for variables much easier than Bash, allows to build in parallel, OS detection, on my system binary is only 16kB in size, is available on any OS, and much more! My second contact with make - Postfix The second non obvious use of make I stood, was the way to refresh Postfix\u0026rsquo;s map files. With Postfix servers, you\u0026rsquo;ve been usually writing a bunch of text files like aliases, transports, etc, which have to be indexed to the binary Berkley DB format.\n/etc/postfix/main.cf alias_maps = hash:/etc/postfix/aliases To \u0026ldquo;generate\u0026rdquo; Berkley DB file you have to run:\nGenerate aliases database postalias aliases Which was producing aliases.db file. Other types of files required to use postmap command to generate them.\nThe more complicated the Postfix configuration, the more files you had. If you missed to update one of map files, your configuration wasn\u0026rsquo;t effective and you could spend hours debugging: why this f\u0026hellip; alias do not work?\nThat\u0026rsquo;s where make comes handy 2. You can just drop the file like this:\ncat Makefile .PHONY: reload all: aliases.db access.db virtual.db reload aliases.db: aliases postalias aliases access.db: access postmap access virtual.db: virtual postmap virtual reload: postfix reload Let me explain what happens here.\n.PHONY: reload: This line declares the target reload as a phony target 3. Phony targets are ones that do not represent actual files. This is typically used for targets that don\u0026rsquo;t produce output files, such as clean, all, etc. It ensures that even if a file named reload exists in the directory, the reload target will still be executed.\nall: aliases.db access.db virtual.db reload: This line specifies that when you run make all, it will generate the files aliases.db, access.db, and virtual.db, and then execute the reload target. As all target is the default, it\u0026rsquo;s enough to just run make.\naliases.db: aliases: This line specifies that aliases.db depends on the file aliases. If aliases file is newer than aliases.db or aliases.db doesn\u0026rsquo;t exist, the commands listed below will be executed.\npostalias aliases: This line is the command to generate the aliases.db file from the aliases file using the postalias command. postalias is a command used in Postfix to create or update the alias database.\nSimilarly, access.db: access and virtual.db: virtual are rules to generate access.db and virtual.db files from access and virtual files respectively using the postmap command.\nreload:: This line declares the reload target. When you run make reload, it will execute the command listed below.\npostfix reload: This line is the command to reload the Postfix service. It tells Postfix to reload its configuration, applying any changes that may have been made.\nSumming up, when you run make, it will generate or update the necessary database files for Postfix configuration (aliases.db, access.db, virtual.db) and then reload the Postfix service.\nNow you won\u0026rsquo;t make mistakes again.\nI know that today many of you would say: \u0026ldquo;just use Ansible dude!\u0026rdquo; But at that time, there was no Ansible yet. I didn\u0026rsquo;t use this pattern for years now, so let check more up to date usage examples.\nUse of make in Python projects I love Python for it\u0026rsquo;s simplicity\u0026hellip; at least when it comes to coding, because when you start managing dependencies, it\u0026rsquo;s getting tricky. What do you use: raw dependencies.txt or rather Poetry\u0026thinsp; external link or Pipenv\u0026thinsp; external link ? Do you use system Python or maybe pyenv\u0026thinsp; external link ?\nMy answer: it depends \u0026#x1f603;\nFor simple projects, I usually just use pip. Sometimes even without requirements.txt files, just listing in the README what to install. But the more projects I wrote, the harder it is to remember how to test them. Again, that\u0026rsquo;s where Make comes handy.\nSimple Makefile for Python projects .PHONY: requirements test .venv: python3 -m venv .venv requirements: source .venv/bin/activate \u0026amp;\u0026amp; \\ python3 -m pip install -r requirements.txt \u0026amp;\u0026amp; \\ python3 -m pip install pytest test: .venv requirements dev-requirements source .venv/bin/activate \u0026amp;\u0026amp; \\ pytest What happens here?\n.PHONY: requirements test: Declares requirements and test as phony targets to ensure they are always executed regardless of file existence.\n.venv:: Creates a Python virtual environment named .venv if it doesn\u0026rsquo;t already exist.\nrequirements:: Installs Python packages listed in requirements.txt into the virtual environment created earlier. Additionally, it installs the pytest package globally.\ntest: .venv requirements dev-requirements: Sets up dependencies for testing, including the virtual environment and specified requirements. Then, it activates the virtual environment and runs the tests using pytest.\nAlternative config for Poetry, might look more or less like that: Simple Makefile for Python projects .PHONY: requirements test requirements: poetry install test: requirements poetry run pytest When I see Makefile in a Python project, I can blindly run make test and it will do what I expect -\u0026gt; run tests. Whatever it requires to configure or run, it will just happen.\nmake in Terraform projects Similar situation to Python, I have with Terraform\u0026thinsp; external link projects. In simple project you just need:\nTypical Terraform flow terraform init terraform plan terraform apply But what if you use different accounts for PROD and DEV environments? What if you need to fetch latest version of modules?\nI have a Makefile for this too.\nTerraform Makefile .PHONY: init SHELL=/bin/bash # those variables you should initialize outside of this script # and export, Make will just set then based on what you will # have set in your environment. You can use for eg. `aws sts` AWS_ACCESS_KEY_ID ?= AWS_SECRET_ACCESS_KEY ?= AWS_REGION ?= \u0026#34;us-west-2\u0026#34; # dev by default ENVIRONMENT ?= dev STATE_FILE_BUCKET ?= s3-bucket-$(AWS_ACCESS_KEY_ID)-$(ENVIRONMENT)-terraform-state STATE_FILE_KEY ?= state/some_service/$(ENVIRONMENT)/terraform.tfstate # make some variable available in Terraform export TF_VAR_something ?= something1 export TF_VAR_something_else ?= something-else .terraform: terraform init \\ -reconfigure \\ -backend-config=\u0026#39;key=$(STATE_FILE_KEY)\u0026#39; \\ -backend-config=\u0026#39;bucket=$(STATE_FILE_BUCKET)\u0026#39; \\ -var-file=environments/$(ENVIRONMENT)/variables.tfvars \\ -out terraform.plan terraform get # this will switch Terraform version to the one that your project needs # https://github.com/tfutils/tfenv init: .terraform tfenv install plan: init terraform plan apply: plan terraform apply \\ -auto-approve \\ terraform.plan destroy: terraform destroy \\ -auto-approve \\ -var-file=environments/$(ENVIRONMENT)/variables.tfvars dev-plan: export AWS_ACCESS_KEY_ID=dev-key dev-plan: plan dev-apply: export AWS_ACCESS_KEY_ID=dev-key dev-apply: apply dev-destroy: export AWS_ACCESS_KEY_ID=dev-key dev-destroy: destroy prod-plan: export AWS_ACCESS_KEY_ID=prod-key prod-plan: plan prod-apply: export AWS_ACCESS_KEY_ID=prod-key prod-apply: apply clean: @rm -rf .terraform/modules @rm -f terraform.* This file expects a directory structure like\nDirectory structure $ tree example/ . ├── main.tf ├── variables.tf ├── provider.tf ├── backend.tf ├── outputs.tf ├── ... ├── environments/ │ ├── dev │ │ ├── variables.tfars │ ├── prod/ │ │ ├── variables.tfars │ ├── .../ Backends configuration in the backend.tf file can be just basic:\nbackend.tf terraform { backend \u0026#34;s3\u0026#34; { region = \u0026#34;us-west-2\u0026#34; encrypt = true } } Rest of parameters are provided in the Makefile - it\u0026rsquo;s called partial backend configuration4. This configuration allows me to use same codebase for all the environments. All customizations have to be listed as variables in variables.tfvars files. It can be easily extended to suport 4 or 6 environments and the only think I need to remember is:\nmake dev-plan make dev-apply make for Hugo blogging Even for blogging with Hugo I have a Makefile5 that I use across multiple sites. It\u0026rsquo;s simplifying some of the steps, that I won\u0026rsquo;t need to remember them.\nMakefile for Hugo BASEDIR=$(CURDIR) INPUTDIR=$(BASEDIR)/content STATICDIR=$(BASEDIR)/static OUTPUTDIR=$(BASEDIR)/public RESOURCESDIR=$(BASEDIR)/resources PORT=1313 FTP_HOST=localhost FTP_USER=anonymous FTP_TARGET_DIR=/ SSH_HOST=vc1 SSH_PORT=22 SSH_USER=root SSH_TARGET_DIR=/var/www/hugo SSH_CHOWN=33:33 S3_BUCKET=my_s3_bucket CLOUDFILES_USERNAME=my_rackspace_username CLOUDFILES_API_KEY=my_rackspace_api_key CLOUDFILES_CONTAINER=my_cloudfiles_container DROPBOX_DIR=~/Dropbox/Public/ GITHUB_PAGES_BRANCH=gh-pages all: html publish: html gzip_static rsync_upload help: @echo \u0026#39;Makefile for a hugo Web site \u0026#39; @echo \u0026#39; \u0026#39; @echo \u0026#39;Usage: \u0026#39; @echo \u0026#39; make html (re)generate the web site \u0026#39; @echo \u0026#39; make clean remove the generated files \u0026#39; @echo \u0026#39; make publish generate using production settings \u0026#39; @echo \u0026#39; make server [PORT=1313] serve site at http://localhost:1313\u0026#39; @echo \u0026#39; make ssh_upload upload the web site via SSH \u0026#39; @echo \u0026#39; make rsync_upload upload the web site via rsync+ssh \u0026#39; @echo \u0026#39; make dropbox_upload upload the web site via Dropbox \u0026#39; @echo \u0026#39; make ftp_upload upload the web site via FTP \u0026#39; @echo \u0026#39; make s3_upload upload the web site via S3 \u0026#39; @echo \u0026#39; make cf_upload upload the web site via Cloud Files\u0026#39; @echo \u0026#39; make github upload the web site via gh-pages \u0026#39; @echo \u0026#39; \u0026#39; html: clean hugo --minify clean: [ ! -d $(OUTPUTDIR) ] || rm -rf $(OUTPUTDIR) \u0026amp;\u0026amp; mkdir -p $(OUTPUTDIR) \u0026amp;\u0026amp; touch $(OUTPUTDIR)/.placeholder rm -rf $(RESOURCESDIR) server: ifdef PORT hugo server -D -p $(PORT) --disableFastRender --buildExpired --buildFuture else hugo server -D --disableFastRender --buildExpired --buildFuture endif generate: clean cd $(BASEDIR); hugo check_urls: @cd /tmp; wget -r --spider http://localhost:$(PORT) 2\u0026gt;\u0026amp;1 | grep -B 2 \u0026#34;404 Not Found\u0026#34; | grep http:// | cut -d \u0026#34; \u0026#34; -f 4 | sort -u markdownlint: @docker run --rm -ti -v ${PWD}:/data:ro markdownlint/markdownlint content gzip_static: for pattern in \u0026#34;*.js\u0026#34; \u0026#34;*.json\u0026#34; \u0026#34;*.css\u0026#34; \u0026#34;*.htm\u0026#34; \u0026#34;*.html\u0026#34; \u0026#34;*.xml\u0026#34;; do \\ find $(OUTPUTDIR) -iname $$pattern -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39;; \\ done optimize_images: find $(STATICDIR) -mtime -7 -iname *.png -print | parallel optipng -quiet -preserve -o7 find $(INPUTDIR) -mtime -7 -iname *.png -print | parallel optipng -quiet -preserve -o7 find $(STATICDIR) -mtime -7 -iname *.jpg -print | parallel jpegtran -optimize -progressive -copy none -outfile \u0026#34;{}\u0026#34; \u0026#34;{}\u0026#34; find $(INPUTDIR) -mtime -7 -iname *.jpg -print | parallel jpegtran -optimize -progressive -copy none -outfile \u0026#34;{}\u0026#34; \u0026#34;{}\u0026#34; ssh_upload: generate scp -P $(SSH_PORT) -r $(OUTPUTDIR)/* $(SSH_USER)@$(SSH_HOST):$(SSH_TARGET_DIR) rsync_upload: generate gzip_static ifdef SSH_CHOWN rsync -e \u0026#34;ssh -p $(SSH_PORT)\u0026#34; -P -avh --delete $(OUTPUTDIR)/ $(SSH_USER)@$(SSH_HOST):$(SSH_TARGET_DIR) --chown $(SSH_CHOWN) else rsync -e \u0026#34;ssh -p $(SSH_PORT)\u0026#34; -P -avh --delete $(OUTPUTDIR)/ $(SSH_USER)@$(SSH_HOST):$(SSH_TARGET_DIR) endif dropbox_upload: generate cp -r $(OUTPUTDIR)/* $(DROPBOX_DIR) ftp_upload: generate lftp ftp://$(FTP_USER)@$(FTP_HOST) -e \u0026#34;mirror -R $(OUTPUTDIR) $(FTP_TARGET_DIR) ; quit\u0026#34; s3_upload: generate s3cmd sync $(OUTPUTDIR)/ s3://$(S3_BUCKET) --acl-public --delete-removed --guess-mime-type cf_upload: generate cd $(OUTPUTDIR) \u0026amp;\u0026amp; swift -v -A https://auth.api.rackspacecloud.com/v1.0 -U $(CLOUDFILES_USERNAME) -K $(CLOUDFILES_API_KEY) upload -c $(CLOUDFILES_CONTAINER) . github: generate # ghp-import -m \u0026#34;Generate Hugo site\u0026#34; -b $(GITHUB_PAGES_BRANCH) $(OUTPUTDIR) # git push origin $(GITHUB_PAGES_BRANCH) cd $(OUTPUTDIR) git add --all git commit -m \u0026#34;Update\u0026#34; git push .PHONY: all html help clean generate server ssh_upload rsync_upload dropbox_upload ftp_upload s3_upload cf_upload github This Makefile is actually an extension\u0026thinsp; external link of one dedicated Pelican\u0026thinsp; external link static page generator\nSummary There are many creative ways to use Makefiles to automate and simplify daily tasks. Tool is small and simple, available on any platform (even on Windows via WSL or Cygwin\u0026thinsp; external link ). Many of my colleagues considered this tool an \u0026ldquo;old school\u0026rdquo; or \u0026ldquo;obsolete\u0026rdquo; initially, but they eventually fall under impression of the recipes simplicity and now just replicate them all around.\nI hope, I will also impress you \u0026#x1f609; Good luck and happy automating!\nEnjoyed? https://www.gnu.org/software/make/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.postfix.org/DATABASE_README.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.gnu.org/software/make/manual/html_node/Phony-Targets.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developer.hashicorp.com/terraform/language/settings/backends/configuration#partial-configuration\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/getpelican/pelican-blog/blob/main/Makefile\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/02/how-i-stopped-worrying-and-loved-makefiles/","summary":"First contact with make When I was invited for my first job interview in the IT, I\u0026rsquo;ve been asked such question:\nHow would you typically build a program from sources, what commands will you use?\nI answered:\nIt\u0026rsquo;s obvious:\n./configure make make install Those times belong to the past now and nowadays not many programmers use GNU Make1. Try asking this question and you will see disgust at best.\nFor many it\u0026rsquo;s the fist contact with make and often the last one, but not for me \u0026#x1f609;","title":"How I stopped worrying and loved Makefiles"},{"content":"Intro One of the biggest benefits of Docker images is their immutability. Once they\u0026rsquo;re built, they don\u0026rsquo;t change. Built once, would work forever\u0026hellip; That\u0026rsquo;s how nightmares of security guys starts \u0026#x1f923;\nWe have then two contradictory concepts:\nflowchart LR id1(Keep it stable) \u003c---\u003e id2(Keep is up to date and secure) For day to day work, usually first concept wins. You want your builds stable and try to avoid tempting distractions of upgrading log4j to latest version\u0026hellip; Who knows what might break. That\u0026rsquo;s fine, makes sense.\nWhen is then the good moment to upgrade?\nIt usually happens once a year, maybe once a 3 years, maybe when we need feature or want to add Java 21 to our CV. Generally speaking \u0026ldquo;it\u0026rsquo;s a special event\u0026rdquo;. For some developers it\u0026rsquo;s once in a lifetime experience. Taking into account how long they would stay in a company, they might not be there next time it happens.\nThere are obvious security risks of not performing updates, we saw it multiple times: Log4Shell\u0026thinsp; external link , POODLE\u0026thinsp; external link , Shellshock\u0026thinsp; external link , Heartbleed\u0026thinsp; external link .\nQuestion is: How to maintain updates, allowing developer to work?\nLet my share how we do it.\nDeprecate before deleting By deprecation we mean: mark as outdated, unsupported, smelly and crappy. It\u0026rsquo;s working still (on best effort) but will be gone soon. We usually point what should be used instead.\nDeprecation is a process which we perform for old Docker images, that we would like to drop or outdated tools which we don\u0026rsquo;t have resources to support anymore.\nTip\nWe do not delete anything instantly! We first notify about deprecation and notify, that breaking change will be applied on next quarter.\nHow to deprecate Docker image? It\u0026rsquo;s simple! Just drop something like this to your docker file:\nDeprecate Docker image ... ONBUILD RUN echo \u0026lt;\u0026lt; EOF ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⣤⣤⣤⣀⣀⣀⣀⣀⣀⣀⣀⣠⣀⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣶⠟⠉⠉⠉⠋⠋⠙⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠋⠛⢙⠙⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁⣠⣾⠟⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠻⣷⡄⠙⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⡟⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣦⡀⠙⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⣦⡀⠙⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡀⠙⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡈⢹⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⣿⡇⠀⠀⢀⣠⣤⣤⡀⠀⢠⣤⣤⣤⣤⣤⣤⣤⠀⠀⢀⣤⣤⣄⡀⠀⠀⣠⣤⣤⣤⣤⣀⠀⠀⠀⢹⡇⢰⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢿⡇⠀⣴⡟⢻⣭⣍⢻⣷⣸⣯⣍⡍⢫⣍⣉⣿⡄⣾⠟⣿⣩⣉⠻⣦⠀⣿⡏⢹⣏⣉⡛⣿⣄⠀⢸⡇⠨⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⣿⠀⣿⡉⣿⣷⣿⡟⠛⢻⣟⢘⣿⠉⠉⢸⡏⢠⣿⠙⣿⠀⣿⡇⣿⡇⢸⡟⢻⡇⠘⣿⠀⢸⡇⢨⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⣿⣆⠸⣷⡉⠉⠉⠁⠀⢸⡯⠰⣿⠀⠀⢸⡇⠸⣿⠀⣿⠀⣿⡇⣿⡇⢸⣇⢸⣧⠀⣿⠀⢸⡇⠐⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⠈⢻⣦⡈⠻⣦⡀⠀⠀⢸⣇⠀⣿⠀⠀⢸⡇⠈⣿⠀⣿⠀⣿⡇⣿⣇⠸⠿⡿⣣⣼⡟⠀⢸⡇⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⢁⣀⣀⣈⢿⣆⠘⣿⡆⠀⢸⡗⠀⣿⠀⠀⢸⡇⠀⣿⠀⣿⠀⣿⡇⣿⡟⢻⡿⠾⠿⠋⠀⠀⢸⡇⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⣿⠙⣿⣁⣿⠇⣿⡇⠀⢸⡧⠀⣿⠀⠀⢸⣧⠐⣿⣀⣿⠀⣿⡇⣿⡇⢸⣿⠀⠀⠀⠀⠀⢸⡇⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⢸⡇⠀⢿⣧⣙⡛⣋⣴⣿⠁⠀⢸⣿⣰⣿⠀⠀⠀⢿⣆⣙⢛⣋⣴⡟⠁⣿⣇⣸⣿⠀⠀⠀⠀⠀⢸⡇⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀⣸⡇⠀⠀⠉⠛⠛⠛⠉⠀⠀⠀⠈⠛⠛⠛⠀⠀⠀⠀⠙⠛⠛⠛⠉⠀⠀⠙⠛⠛⠋⠀⠀⠀⠀⠀⣸⡇⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣄⠘⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠁⣰⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡀⠙⢷⣆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡀⠹⢿⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡀⠻⣷⣆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⡈⠻⣷⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣴⡿⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣦⣀⣀⣀⣀⣀⣈⣉⣉⣀⣀⣀⣀⣁⣁⣀⣀⣀⣀⣀⣠⣼⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠛⠛⠛⠛⠛⣿⡟⠛⠛⠛⢛⣿⡟⠛⠛⠛⠛⠛⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⣼⡆⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡅⠀⠀⣿⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡂⠀⠀⣿⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡅⠀⠀⣿⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⣿⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⣿⡇⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡅⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⠶⠶⠶⠶⢾⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤ ⣘⠀⠀⣙⡋⣛⣋⢋⢛⣛⣟⡏⣛⣛⣙⣻⡟⡋⢛⢉⣿⡄⣀⣀⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⢀⢀⣀⡀⠀⠀⢀⢀⣀⡀⣈⣉⣉⣉⣉⣉⣉⣙⣙⣙⣛⣉⣉⣙ This Docker image is deprecated and will be soon deleted! Please use XYZ instead. In case of issues, contact: #best-team-ever best-team-ever@corpo.net EOF This command will print our clear message when people will try to use it.\nTip\nIf you\u0026rsquo;re a really bad person and afraid that developers don\u0026rsquo;t read build logs, you can extend command above with 5 min sleep. They will see for sure and will be more motivated to change \u0026#x1f609;\nMake upgrades predictable We seriously upgrade our base images every quarter. By seriously, I mean we\u0026rsquo;re touching everything possible to upgrade. We pin (hardcode) versions for all the changes, to ensure stable rebuilds. That\u0026rsquo;s usually a week of work for the team and then we drop a bomb by releasing those updates\u0026hellip;\ngraph TB A(Upgrade everything that can be upgraded) --\u003e B(Introduce new images) B --\u003e C(Deprecate outdated) C --\u003e D(Delete previously deprecated) D -- repeat\nquarterly --\u003e A Make upgrades visible Before we push upgrades or even before we start working on them, we\u0026rsquo;re performing a quick research on what we might be delivering:\ncheck new versions available (new Java, new Python, etc), check what we will drop (deprecated last time), check what we might change breaking contract and causing potential configuration issues. Usually it takes around 30~60 minutes to quickly scan what we planned to do last quarter, what will change right now. We use Semantic Versioning1, so we focus here on MAJOR and MINOR changes.\nWe send a quick mail with the summary of changes to all the IT people. Week or two later changes are deployed. It\u0026rsquo;s not much time you might say, that\u0026rsquo;s true, but it\u0026rsquo;s not our first notification. After we release upgrade we send more detailed mail with:\nall the changes that happen, all the deprecations (more on that later), plans for bigger changes for next quarter. sequenceDiagram Sprint start -\u003e\u003e Quarterly patching: Send email Note right of Sprint start: at beginning of the sprint,\nshort reminder,\nraw plans Quarterly patching -\u003e\u003e Sprint end: Send mail again Note left of Sprint end: detailed\nnew,\nupdated,\ndeprecated,\ndropped Info\nLast part of this message is important! We notify 3 months in advance about changes, that we\u0026rsquo;re already aware or plan.\nThis makes big difference, because dev teams know what will happen in 3 months from now. They can start prepare and plan time for the change. They can own it!\nIs it enough to upgrade quarterly? No, it\u0026rsquo;s not.That\u0026rsquo;s a bare minimum. If you do just that, you will be vulnerable just a few weeks after the Quarterly patching.\nWe try to limit, big breaking changes and upgrades to those quarterly events, but we still perform smaller upgrades between those big events.\nIt happen twofold:\nWe rebuild our base images monthly.\nEach rebuild will update OS system packages and install recent updates. This makes images bigger, but also us safer and is as simple as adding:\nRUN apt update \u0026amp;\u0026amp; apt dist-upgrade -y to our top level base images.\nThose are unattended and completely automated events. No big changes happen here, so they rarely cause trouble.\nWe usually work on Docker images between upgrades.\nAdding new features or fixing bugs. Those are usually MINOR or PATCH level changes, so they don\u0026rsquo;t cause much fuss.\nEach our change, triggers action from pt. 1 - upgrading OS packages.\ngraph LR A((Monthly rebuild\\nor on-demand)) --\u003e B(Upgrade OS packages) --\u003e C((Push)) Ask devs to use latest Docker image tag by default Christ, latest! Seriously?\nYes. We maintain our own base images and we encourage developers to rely on the latest tag by default. They can \u0026ldquo;fallback\u0026rdquo; to the tagged version if they face issues. This usually follows up with a ticket to check what\u0026rsquo;s wrong and get back \u0026ldquo;to the bright side\u0026rdquo; in next sprint.\nThis way they use recent images most of the time and when they\u0026rsquo;re not - it\u0026rsquo;s their responsibility to get back on the latest.\nWe (as a team) do not provide help/support if someone use non latest image version.\nEnsure latest tags are stable - test Docker images! We prepared a testing framework that allows us to test Docker images after they\u0026rsquo;re build. We used for that Pytest\u0026thinsp; external link and Testinfra\u0026thinsp; external link . Together with some bash in Makefile, we\u0026rsquo;re listing all the images and checking if they behave how we expect, if they start, run as non-root, etc.\nTestinfra allows to easily check any condition\u0026thinsp; external link . Pytest spins Docker image per set of tests. For around 200 images that we maintain, we run more than 1500 tests. Since we started doing this, it\u0026rsquo;s really hard to break basic functionality of images. This allows us to push harder with confidence that latest versions are stable.\nSum it up What was the impact of implementation of this process?\nThere are many positive aspects of it:\nScheduled upgrades: Changes are communicated in advance, reducing the need for immediate requests. Stability through testing: Rigorous testing minimizes the risk of disruptions during upgrades. Informed stakeholders: Developers and Product Owners receive early notifications, enabling better planning. Efficient deprecation process: Notices allow for timely reactions and signal the seriousness of image changes. Reduced change requests: Teams anticipate updates within the quarterly cycle, reducing the frequency of requests. Seamless bug fixes: Bug fixes and new features are implemented between major upgrades without causing disruptions. Clear deletion timeline: Deprecated images are scheduled for deletion in the following quarter, accelerating legacy removal. Collaborative issue resolution: Complaints prompt discussions and options for resolution, fostering collaboration. Flexibility in usage: Teams have the autonomy to choose whether to stay on older image versions if needed. Updates have to be delivered frequently - it have to be boring process, that every body expect. It is for us \u0026#x1f913;\nEnjoyed? https://semver.org/spec/v2.0.0.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/02/keeping-docker-afloat-best-practices-for-patching-and-deprecating-images/","summary":"Intro One of the biggest benefits of Docker images is their immutability. Once they\u0026rsquo;re built, they don\u0026rsquo;t change. Built once, would work forever\u0026hellip; That\u0026rsquo;s how nightmares of security guys starts \u0026#x1f923;\nWe have then two contradictory concepts:\nflowchart LR id1(Keep it stable) \u003c---\u003e id2(Keep is up to date and secure) For day to day work, usually first concept wins. You want your builds stable and try to avoid tempting distractions of upgrading log4j to latest version\u0026hellip; Who knows what might break.","title":"Keeping Docker afloat - Best practices for patching and deprecating images"},{"content":"Working with Docker environments amid diverse architectures, like Apple\u0026rsquo;s arm64 and x86-64/AMD64, presents challenges. I\u0026rsquo;ve encountered the clash between my Mac\u0026rsquo;s M1 arm64 architecture and my x86-centric server workloads. The solution? Just use DOCKER_DEFAULT_PLATFORM 1 2.\nJust run in the terminal:\nEnforce platform for all commands export DOCKER_DEFAULT_PLATFORM=linux/amd64 With this command, Docker enforces x86 architecture by default on commands supporting --platform parameter, streamlining workflows and sparing the need for repetitive --platform specifications.\nFor specific commands requiring alternate platforms, it\u0026rsquo;s fine to use it like:\nEnforce platform for all commands docker run --rm -ti --platform linux/arm64 ubuntu:22.04 uname -a docker run --rm -ti --platform linux/amd64 ubuntu:22.04 uname -a But by setting DOCKER_DEFAULT_PLATFORM, working with Docker environments becomes intuitive, allowing me to focus on building and deploying instead of fighting with my hardware.\nInfo\nOf course it\u0026rsquo;s slower than running on native platform (because of virtualization), but I prefer to wait a little bit longer over fixing silly bugs, which I wouldn\u0026rsquo;t have on proper architecture.\nhttps://github.com/rancher-sandbox/rancher-desktop/issues/2339\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.docker.com/engine/reference/commandline/cli/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/02/how-to-run-x86-64-docker-images-on-apples-macbook-with-m1/m2/m3-cpu/","summary":"Working with Docker environments amid diverse architectures, like Apple\u0026rsquo;s arm64 and x86-64/AMD64, presents challenges. I\u0026rsquo;ve encountered the clash between my Mac\u0026rsquo;s M1 arm64 architecture and my x86-centric server workloads. The solution? Just use DOCKER_DEFAULT_PLATFORM 1 2.\nJust run in the terminal:\nEnforce platform for all commands export DOCKER_DEFAULT_PLATFORM=linux/amd64 With this command, Docker enforces x86 architecture by default on commands supporting --platform parameter, streamlining workflows and sparing the need for repetitive --platform specifications.","title":"How to run x86-64 Docker images on Apple's MacBook with M1/M2/M3 CPU"},{"content":" Extreme OwnershipHow U.S. Navy SEALs Lead and Win\nAuthors: Jocko Willink, Leif Babin\namazon.pl The book is a compelling and practical leadership guide co-authored by Jocko Willink and Leif Babin, both former Navy SEAL officers. The book revolves around the central theme of taking absolute responsibility for every aspect of one\u0026rsquo;s life and leadership role. Through gripping combat stories from their SEAL experiences, the authors illustrate key principles such as accountability, decisiveness, and humility.\nThe book\u0026rsquo;s strength lies in its straightforward and actionable advice, providing readers with a roadmap for effective leadership in any context. Willink and Babin emphasize the importance of ownership, urging leaders to embrace challenges, learn from mistakes, and empower their teams. The real-world examples make the concepts tangible and relatable, making it a valuable read for leaders in various fields.\nWhile the militaristic tone may not resonate with everyone, the underlying principles are universally applicable. \u0026ldquo;Extreme Ownership\u0026rdquo; is a motivational and instructive read that encourages readers to adopt a mindset of extreme accountability, leading to personal and professional success. Whether you\u0026rsquo;re a business executive or an aspiring leader, this book offers valuable insights into effective leadership and achieving success in challenging environments.\n","permalink":"https://gagor.pro/books/2024/extreme-ownership/","summary":"Extreme OwnershipHow U.S. Navy SEALs Lead and Win\nAuthors: Jocko Willink, Leif Babin\namazon.pl The book is a compelling and practical leadership guide co-authored by Jocko Willink and Leif Babin, both former Navy SEAL officers. The book revolves around the central theme of taking absolute responsibility for every aspect of one\u0026rsquo;s life and leadership role. Through gripping combat stories from their SEAL experiences, the authors illustrate key principles such as accountability, decisiveness, and humility.","title":"Extreme Ownership"},{"content":"I love blogging with Hugo\u0026thinsp; external link and I have two blogs already that use it. The good thing about static sites is that you have all the data in the files. You can optimize them locally, batch process, amend, etc. Powerful templating engine allows to quickly pre fill documents in the format I like.\nI have some steps in the Makefile for things like image optimization, but I often don\u0026rsquo;t remember to run them \u0026#x1f603;\nThat\u0026rsquo;s where pre-commit1 comes to help, triggering linters, file syntax checks, optimizers.\nInstall hooks in the git repo by calling: Install pre-commit hooks pre-commit install Create config file called .pre-commit-config.yaml.\nFill it with hooks2 that will make you life easier:\n.pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v4.5.0 hooks: - id: check-merge-conflict - id: check-yaml - id: check-json - id: check-toml - id: end-of-file-fixer - id: mixed-line-ending args: [--fix=auto] - id: check-added-large-files - id: trailing-whitespace args: [--markdown-linebreak-ext=md] - id: pretty-format-json args: [--autofix, --indent=4, --no-sort-keys] - repo: https://github.com/boidolr/pre-commit-text rev: v1.2.13 hooks: - id: pretty-format-yaml args: [--preserve-quotes] # Optimize images for size - repo: https://github.com/boidolr/pre-commit-images rev: v1.5.1 hooks: - id: optimize-avif - id: optimize-jpg - id: optimize-png - id: optimize-svg - id: optimize-webp # Strip EXIF data from images - repo: https://github.com/stefmolin/exif-stripper rev: 0.1.2 hooks: - id: strip-exif (Optional) Run pre-commit manually on all files to update \u0026ldquo;old\u0026rdquo; files in one shot: Run pre-commit pre-commit run -a With this config, every time I commit, my files will be cleaned and optimized.\nhttps://pre-commit.com\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pre-commit.com/hooks.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/my-pre-commit-config-for-hugo-blog/","summary":"I love blogging with Hugo\u0026thinsp; external link and I have two blogs already that use it. The good thing about static sites is that you have all the data in the files. You can optimize them locally, batch process, amend, etc. Powerful templating engine allows to quickly pre fill documents in the format I like.\nI have some steps in the Makefile for things like image optimization, but I often don\u0026rsquo;t remember to run them \u0026#x1f603;","title":"My pre-commit config for Hugo blog"},{"content":" TL;DR If you\u0026rsquo;re not interested in the \u0026ldquo;story of my life\u0026rdquo;, go directly to \u0026ldquo;Tuning PipeWire \u0026rdquo; section.\nI\u0026rsquo;m not an audiophile, but I spent whole days in the headphones and I like when sound sounds good. I like slight bass boost, which adds this kick to the melody, but won\u0026rsquo;t overwhelm me after an hour of listening. I like when high tones are clear, but I get annoyed if they\u0026rsquo;re too strong. And finally, I hate all cracks, hisses or electric circuit noises.\nMy hardware I\u0026rsquo;ve collected a bunch of these over the years \u0026#x1f604;\nAsus Xonar DX Sampling Sampling rate SNR 24 bit 192kHz 116 dB I have it for around 10 years. It\u0026rsquo;s great. Well supported by Linux. Not supported by Windows 11, as it\u0026rsquo;s too old.\nThere\u0026rsquo;s one thing that irritates me - Intel cards used to automatically detect when headphone was plugged to the front panel and automatically switching audio to it, Xonar do not have it \u0026#x1f622;\nROG SupremeFX aka Realtek ALC4080 + Savitech SV3H712 AMP Sampling Sampling rate SNR 32 bit 384kHz 120 dB It\u0026rsquo;s integrated audio on my motherboard. It\u0026rsquo;s sometimes referred as a \u0026ldquo;premium audio\u0026rdquo;. The way it\u0026rsquo;s installed, pretends USB connection, to avoid any noises that could go via circuit (all digital). To be honest, it\u0026rsquo;s pretty decent - no cracks or strange noises, but Linux support was \u0026#x1f4a9; for quite a long time. S/PDIF didn\u0026rsquo;t work well until I upgraded kernel to 6.5. Even now, it happen that sound card is loaded but it\u0026rsquo;s not giving any output via S/PDIF until a restart.\nFiiO K5 Pro Sampling Sampling rate SNR 32 bit 768kHz 115 dB That\u0026rsquo;s my recent Xmas gift. Works well on Linux and MacOS. Provides more power, than I need with any of my headphones.\nLegacy of the PulseAudio PulseAudio was blamed for high latency and some \u0026ldquo;non optimal\u0026rdquo; decisions. Default configuration was enforcing resampling to 44.1 kHz always, where many media now use 48 kHz. This means that audio was resampled most of the time, even when my sound card supports higher sample rates. I wanted to avoid that and it\u0026rsquo;s well described how to achieve it on the Delightly Linux\u0026thinsp; external link 1 2 blog.\nRecent systems switch to new, better PipeWire audio system by default. I\u0026rsquo;d like to be sure, that I still get the best from my hardware. Not that I could hear the difference\u0026hellip; \u0026#x1f603; Just for the sake of doing it the right way!\nI\u0026rsquo;ve been looking for a good description on \u0026ldquo;how to tune it\u0026rdquo; and I failed. Arch\u0026rsquo;s Wiki3 is usually a good source, but there\u0026rsquo;s no direct description regarding quality. I also found some Reddit posts 4 5, but nothing step by step. Let me share what I found.\nTuning PipeWire PipeWire runs as a user-space service and it can use configs from per-system or per-user configs. That\u0026rsquo;s how it look like:\n/usr/share/pipewire - default configs, /etc/pipewire - system wide configs, to override defaults, ~/.config/pipewire - per-use configs, can use: full file names like ~/.config/pipewire/pipewire.conf subsection replacements ~/.config/pipewire/pipewire.conf.d/ (same pattern works in /etc/pipewire too) It\u0026rsquo;s easier to play in user\u0026rsquo;s home dir, but I prefer to have it in the system space. As I found quite a lot of differences in those config files between Ubuntu versions, I like \u0026ldquo;subsection\u0026rdquo; approach as it allows to overwrite only the parts I want, leaving rest on defaults.\nDefault parameters After system installation my /etc/pipewire was just empty. I was wondering where can I check the defaults and I found it, under:\nCheck default configs ls -1 /usr/share/pipewire/ client.conf client-rt.conf filter-chain filter-chain.conf jack.conf minimal.conf pipewire-aes67.conf pipewire-avb.conf pipewire.conf pipewire-pulse.conf It\u0026rsquo;s a good source of documentation and examples.\nSet client resample quality When resampling happens, let it happen with the best possible quality. Look for resample.quality in the client.conf file, uncomment it and set to 10:\nConfigure resampling mkdir -p ~/.config/pipewire/client.conf.d cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.config/pipewire/client.conf.d/resample-quality.conf stream.properties = { resample.quality = 10 } EOF And another one for Pulse Audio:\nConfigure resampling mkdir -p ~/.config/pipewire/pipewire-pulse.conf.d/ cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.config/pipewire/pipewire-pulse.conf.d/resample-quality.conf stream.properties = { resample.quality = 10 } EOF DSP configuration That\u0026rsquo;s my configuration I use:\nConfigure DSP mkdir -p ~/.config/pipewire/pipewire.conf.d cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.config/pipewire/pipewire.conf.d/dsp-defaults.conf context.properties = { ## Properties for the DSP configuration. default.clock.rate = 48000 default.clock.allowed-rates = [ 44100 48000 88200 96000 176400 192000 352800 384000 705600 768000 ] #default.clock.quantum = 1024 default.clock.min-quantum = 32 default.clock.max-quantum = 8192 #default.clock.quantum-limit = 8192 } EOF Let see what happen here:\ndefault.clock.rate - PipeWire has one global sample rate used in the processing pipeline. All signals are converted to this sample rate and then converted to the sample rate of the device. That said, if you set it too low, it won\u0026rsquo;t use capabilities of your device. If you set it higher that the sources you use, it will be resampling to higher frequencies, which will just waste CPU time.\ndefault.clock.allowed-rates - This one lists the frequencies that are supported by your sound device. By default it\u0026rsquo;s set to [ 48000 ], which quite often will resample audio. To allow use of other frequencies, I had to manually list them.\nAlsa monitor config Theoretically: it specifies the supported input/output audio formats for ALSA devices. In this case, it is set to \u0026ldquo;all,\u0026rdquo; indicating that PipeWire should support all available output audio formats. This includes various audio formats such as PCM (Pulse Code Modulation), AC3 (Dolby Digital), DTS (Digital Theater Systems), and more.\nConfigure Alsa #cat \u0026lt;\u0026lt; EOF | sudo tee -a /etc/pipewire/alsa-monitor.conf cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.config/pipewire/alsa-monitor.conf [alsa-monitor] output_formats = all input_formats = all EOF Info\nI\u0026rsquo;m having hard time to recall where I found this and why I needed it? I will updated when I figure it out.\nReload PipeWire configuration To reload configuration files after the change, you can restart PipeWire like this:\nRestart PipeWire systemctl --user restart pipewire.service systemctl --user restart pipewire-pulse.service Warning\nSome applications (eg. Spotify) might not detect new sinks and will stay \u0026ldquo;muted\u0026rdquo;. Restart them to get the sound back.\nFor testing, command line tools like mpv or aplay are more reliable.\nHow to check if it works? Warning\nRestart PipeWire first!\nWe can check specific files effective configuration by calling:\nCheck config $ pw-config -n pipewire.conf list ... ], \u0026#34;config.name.d\u0026#34;: \u0026#34;pipewire.conf.d\u0026#34;, \u0026#34;override.1.0.config.path\u0026#34;: \u0026#34;/home/timor/.config/pipewire/pipewire.conf.d/dsp-defaults.conf\u0026#34;, \u0026#34;override.1.0.config.name\u0026#34;: \u0026#34;dsp-defaults.conf\u0026#34;, \u0026#34;override.1.0.context.properties\u0026#34;: { ## Properties for the DSP configuration. default.clock.rate = 48000 default.clock.allowed-rates = [ 44100 48000 88200 96000 176400 192000 352800 384000 705600 768000 ] #default.clock.quantum = 1024 default.clock.min-quantum = 32 default.clock.max-quantum = 8192 #default.clock.quantum-limit = 8192 } or\nCheck config pw-config -n client.conf list ... \u0026#34;config.name.d\u0026#34;: \u0026#34;client.conf.d\u0026#34;, \u0026#34;override.1.0.config.path\u0026#34;: \u0026#34;/home/timor/.config/pipewire/client.conf.d/resample-quality.conf\u0026#34;, \u0026#34;override.1.0.config.name\u0026#34;: \u0026#34;resample-quality.conf\u0026#34;, \u0026#34;override.1.0.stream.properties\u0026#34;: { resample.quality = 10 } Check config pw-config -n pipewire-pulse.conf list ... \u0026#34;config.name.d\u0026#34;: \u0026#34;pipewire-pulse.conf.d\u0026#34;, \u0026#34;override.1.0.config.path\u0026#34;: \u0026#34;/home/timor/.config/pipewire/pipewire-pulse.conf.d/resample-quality.conf\u0026#34;, \u0026#34;override.1.0.config.name\u0026#34;: \u0026#34;resample-quality.conf\u0026#34;, \u0026#34;override.1.0.stream.properties\u0026#34;: { resample.quality = 10 } If all looks good, we should check if it\u0026rsquo;s working as we want by calling pw-top:\nAlternatively we can watch directly Alsa\u0026rsquo;s outputs:\nCheck raw Alsa\u0026#39;s hardware params watch -n1 cat /proc/asound/card*/pcm*/sub0/hw_params Is it wroth it? Probably not.\nOne difference I found between PipeWire and PulseAudio is some kind of delay, before the sink is closed. For example, if I start Spotify which will create sink with 44.1 kHz sampling rate and enable Youtube - it will keep 44.1 kHz despite Youtube working with 48 kHz. You\u0026rsquo;re usually not listening two sources at the same time, but it behaves the same even if I shut down Spotify and start Youtube \u0026ldquo;too fast\u0026rdquo;. It\u0026rsquo;s at least bunch of seconds, I didn\u0026rsquo;t found if it\u0026rsquo;s configurable yet.\nWith PulseAudio, it was instant. When I was checking PCM outputs, they were just switching instantly. With PipeWire, if I won\u0026rsquo;t keep OS silent for a while, it might start with much lower sample rates. You know, 44.1 kHz is more than I can hear, but it\u0026rsquo;s just disappointing and feels like a regression.\nI can see also other \u0026ldquo;silly\u0026rdquo; behaviors of PipeWire. At lest 2 of my cards support 32 bit sampling, but even if source use 32bit, Alsa usually use just 24. Again, it\u0026rsquo;s nothing I could hear, but if hardware is capable of more, why not to use it?\nThose are things I feel missing right now, but maybe it\u0026rsquo;s something that might be solved with more configs \u0026#x1f609;\nI can\u0026rsquo;t really hear the difference. I stayed with 192 kHz sampling rate as a default for few days. I felt rather tired, when listening too long and I couldn\u0026rsquo;t name what was the reason. 95% of the audio I listen comes from 2 sources:\nSpotify (44.1 kHz) Youtube (48 kHz) For me this means that the default of 48 kHz as a sampling rate is just fine. Spotify gets resampled up, which should be less harmful than resampling to the lower frequencies. That\u0026rsquo;s why I stayed there.\nTODO Force Alsa to use 32 bit sampling by default. My cards support it but I only see S24LE going to the output. Same like with sampling rate, why to convert if I should be able to just drop it on the hardware? 6\nI heard about Harman curve and I\u0026rsquo;d like to play with it. I have relatively good headphones: Bayerdynamic DT 990 Pro 250 Ohm. They sound quite well and don\u0026rsquo;t make me feel tired - I guess it\u0026rsquo;s a matter of open design. Still I think they\u0026rsquo;re strengthening higher frequencies. I\u0026rsquo;d like to use AutoEQ profile, to setup my headphones according to the \u0026ldquo;recommended\u0026rdquo; way. But I don\u0026rsquo;t like to do it with external equalizer app - PipeWire should be able to handle that, I just don\u0026rsquo;t know yet how :) 7 8\nI\u0026rsquo;ve read some day that the more I age, the less frequencies I can hear. Which means my kids might be hearing some noises which I won\u0026rsquo;t be able. There are even Ringtones for teenagers that rely on this phenomena. Teachers can\u0026rsquo;t hear them. 9 10 11\nhttps://delightlylinux.wordpress.com/2017/01/13/experience-better-sound-in-linux-with-the-asus-xonar-dx-sound-card/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://delightlylinux.wordpress.com/2017/01/23/asus-xonar-dsx-and-linux/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/PipeWire\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gitlab.freedesktop.org/pipewire/pipewire/-/wikis/Configuration?version_id=25749f548c1e2fddd9e1678d9b7e57ebfcae3cf2\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.reddit.com/r/Fedora/comments/xd8tg7/audio_quality_with_pipewire_dac/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/PipeWire#High_latency_with_USB_DACs_(e.g._Schiit_DACs)\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://headphonesaddict.com/harman-curve/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://autoeq.app/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.sciencedirect.com/topics/medicine-and-dentistry/high-frequency-hearing-loss\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.signia.net/en/service/hearing-test/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.classicfm.com/discover-music/how-high-can-you-hear-video-frequency-hearing-test/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/tuning-pipewire-for-best-audio-quality-on-ubuntu/","summary":"TL;DR If you\u0026rsquo;re not interested in the \u0026ldquo;story of my life\u0026rdquo;, go directly to \u0026ldquo;Tuning PipeWire \u0026rdquo; section.\nI\u0026rsquo;m not an audiophile, but I spent whole days in the headphones and I like when sound sounds good. I like slight bass boost, which adds this kick to the melody, but won\u0026rsquo;t overwhelm me after an hour of listening. I like when high tones are clear, but I get annoyed if they\u0026rsquo;re too strong.","title":"Tuning PipeWire for best audio quality on Ubuntu"},{"content":"I use Git a lot, even writing this article i will commit text few times. There\u0026rsquo;s a set of aliases I rely on daily and they\u0026rsquo;re first I add in new place.\nSome Git commands are unnecessarily verbose. You can make your life much easier with bash-completions, but if you write it tens of times per day, it\u0026rsquo;s anyway a lot of typing\u0026hellip; and I\u0026rsquo;m a lazy man \u0026#x1f604;\nSimple status/log checks git s s = status --short --branch --untracked-files Shows a short, branch-focused status with untracked files.\nExample git s git tags tags = !sh -c \u0026#39;git tag -n1 | sort -V\u0026#39; Lists tags along with their annotations, sorted by the version.\nExample git tags git graph graph = log --graph --oneline --all Displays a compact graph of the commit history.\nExample git graph git tree tree = log --graph --decorate --pretty=oneline --abbrev-commit Displays a tree-like view of the commit history with decorations.\nExample git tree Saving progress, committing I rarely use git commit -a -m .... I have simpler ways to do it\ngit jira jira = !\u0026#34;f() { git rev-parse --abbrev-ref HEAD | sed -n -E \u0026#39;s#^(feature|(bug|hot)-?(fix)?)/([A-Z]+-[0-9]+)[^a-zA-Z0-9].*#\\\\4#p\u0026#39; ; }; f\u0026#34; Extracts the Jira ticket number from the current branch name. I use it to feed git cm, as some of the projects I work require Jira ticket ID on every commit.\nExample git switch -c feature/ABC-123-awesome-stuff git jira ABC-123 git cm # cm = !git add -A \u0026amp;\u0026amp; git commit -m # cm = !\u0026#34;f() { if echo \\\u0026#34;$1\\\u0026#34; | egrep -q \u0026#39;^[A-Z]+-[0-9]+ \u0026#39;; then git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$1\\\u0026#34;; else JIRA=$(git jira); if [ -z \u0026#34;$JIRA\u0026#34; ]; then echo \u0026gt;\u0026amp;2 \u0026#39;#### Start message with Jira ticket number! ####\u0026#39;; exit 1; else git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$JIRA $1\\\u0026#34;; fi; fi; }; f\u0026#34; cm = !\u0026#34;f() { SCOPE=\u0026#39;\u0026#39;; JIRA=\u0026#39;\u0026#39;; if echo \\\u0026#34;$1\\\u0026#34; | egrep -q -i \u0026#39;^(BUILD|REVISION|PATCH|MINOR|MAJOR) \u0026#39;; then SCOPE=$(echo \\\u0026#34;$1\\\u0026#34; | sed -n -E \u0026#39;s#^(BUILD|REVISION|PATCH|MINOR|MAJOR).*#\\\\1#p\u0026#39;); fi; if echo \\\u0026#34;$1\\\u0026#34; | egrep -q \u0026#39;^((BUILD|REVISION|PATCH|MINOR|MAJOR)[ ]+)?[A-Z]+-[0-9]+ \u0026#39;; then JIRA=$(echo \\\u0026#34;$1\\\u0026#34; | sed -n -E \u0026#39;s#^((BUILD|REVISION|PATCH|MINOR|MAJOR)[ ]+)?([A-Z]+-[0-9]+).*#\\\\3#p\u0026#39;); else JIRA=$(git jira); if [ -z \u0026#34;$JIRA\u0026#34; ]; then echo \u0026gt;\u0026amp;2 \u0026#39;#### Start message with Jira ticket number! ####\u0026#39;; exit 1; fi; fi; COMMIT_MESSAGE=$(echo \\\u0026#34;$1\\\u0026#34; | sed -n -E \u0026#39;s#^((BUILD|REVISION|PATCH|MINOR|MAJOR)[ ]+)?([A-Z]+-[0-9]+)?[ ]?##p\u0026#39;); if [ -z \u0026#34;$SCOPE\u0026#34; ]; then git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$JIRA $COMMIT_MESSAGE\\\u0026#34;; else git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$SCOPE $JIRA $COMMIT_MESSAGE\\\u0026#34;; fi; }; f\u0026#34; Commits changes with an optional Jira ticket number and scope in the message. I started with the first one, but as some corpo requirements forced me to prefix each message with a ticket ID, I hacked it \u0026#x1f604;\nExample git cm \u0026#34;feat: implement new feature\u0026#34; git save save = !git add -A \u0026amp;\u0026amp; git commit -m \u0026#34;SAVEPOINT\u0026#34; Adds all changes and commits with a \u0026ldquo;SAVEPOINT\u0026rdquo; message.\nExample git save git wip wip = commit -am \u0026#34;WIP\u0026#34; Commits all changes with a \u0026ldquo;WIP\u0026rdquo; (Work In Progress) message.\nExample git wip Branch management and dirty reverts git co co = checkout Shortcut for git checkout. I prefer git switch recently.\nExample git co feature-branch git cob cob = checkout -b Creates a new branch and checks it out. To be honest, I recently more often use git switch -c.\nExample git cob new-feature git undo undo = reset HEAD~1 --mixed Undoes the last commit, keeping changes in the working directory.\nExample git undo git amend amend = commit -a --amend Amends the last commit with any changes in the working directory.\nExample git amend git wipe wipe = !git add -A \u0026amp;\u0026amp; git commit -qm \u0026#39;SAVEPOINT before WIPE\u0026#39; \u0026amp;\u0026amp; git reset HEAD~1 --hard Creates a \u0026lsquo;SAVEPOINT\u0026rsquo; commit before resetting the branch to the previous commit.\nExample git wipe git dropmerged dropmerged = \u0026#34;!git branch --merged | grep -v \u0026#39;* \u0026#39; | xargs git branch -d; git checkout -q master \u0026amp;\u0026amp; git for-each-ref refs/heads/ \\\u0026#34;--format=%(refname:short)\\\u0026#34; | while read branch; do mergeBase=$(git merge-base master $branch) \u0026amp;\u0026amp; [[ $(git cherry master $(git commit-tree $(git rev-parse \\\u0026#34;$branch^{tree}\\\u0026#34;) -p $mergeBase -m _)) == \\\u0026#34;-\\\u0026#34;* ]] \u0026amp;\u0026amp; git branch -D $branch; done\u0026#34; Deletes branches that have been merged into master and have no unmerged changes. It can even recognize squash merges.\nI love this one. I often have 5+ branches on different projects and when I get back to them, I don\u0026rsquo;t remember which stuff is still useful. I call this command and only uncommitted stuff is left.\nExample git dropmerged Bitbucket helpers I work with Bitbucket a lot, so it\u0026rsquo;s helpful to jump from the code into repo or pull requests page quickly.\ngit bb bb = ! open $(git config --get remote.origin.url | sed \u0026#39;s/^ssh:\\\\/\\\\/git@\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\).git/https:\\\\/\\\\/\\\\1\\\\/projects\\\\/\\\\2\\\\/repos\\\\/\\\\3\\\\/browse/\u0026#39;) Opens Bitbucket repository in the default web browser.\nExample git bb git prs prs = ! open $(git config --get remote.origin.url | sed \u0026#39;s/^ssh:\\\\/\\\\/git@\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\).git/https:\\\\/\\\\/\\\\1\\\\/projects\\\\/\\\\2\\\\/repos\\\\/\\\\3\\\\/pull-requests/\u0026#39;) Opens Bitbucket Pull Requests page in the default web browser.\nExample git prs Voodoo/Black magic Those are helpful in some situations, but might be dangerous or made your palls angry. You\u0026rsquo;ve been warned \u0026#x1f609;\ngit authorupdate authorupdate = !git filter-branch -f --env-filter \u0026#34;GIT_AUTHOR_NAME=\u0026#39;My Name\u0026#39;; GIT_AUTHOR_EMAIL=\u0026#39;my-mail@github.com\u0026#39;; GIT_COMMITTER_NAME=\u0026#39;My Name\u0026#39;; GIT_COMMITTER_EMAIL=\u0026#39;my-mail@github.com\u0026#39;;\u0026#34; HEAD \u0026amp;\u0026amp; git push origin master --force Updates the author information for all commits and force-pushes to master.\nOne time I wanted to unify commits on one repo, that I\u0026rsquo;ve done from multiple computers with different names and emails. This allowed me to achieve this.\nYes, it allows to overwrite with any name. That\u0026rsquo;s why it\u0026rsquo;s worth to sign your commits \u0026#x1f604;\nExample git authorupdate My whole .gitconfig [alias]\u0026rsquo;es section For those as lazy as I\u0026rsquo;m, my config file. Just edit your ~/.gitconfig file and add aliases you like.\ncat ~/.gitconfig [alias] retag = \u0026#34;!f() { git tag -d $1 \u0026amp;\u0026amp; git push origin :refs/tags/$1 \u0026amp;\u0026amp; git tag -a $1 -m \\\u0026#34;$1\\\u0026#34; \u0026amp;\u0026amp; git push \u0026amp;\u0026amp; git push --tags ; }; f\u0026#34; s = status --short --branch --untracked-files tags = !sh -c \u0026#39;git tag -n1 | sort -V\u0026#39; co = checkout cob = checkout -b jira = !\u0026#34;f() { git rev-parse --abbrev-ref HEAD | sed -n -E \u0026#39;s#^(feature|(bug|hot)-?(fix)?)/([A-Z]+-[0-9]+)[^a-zA-Z0-9].*#\\\\4#p\u0026#39; ; }; f\u0026#34; cm = !git add -A \u0026amp;\u0026amp; git commit -m cm = !\u0026#34;f() { if echo \\\u0026#34;$1\\\u0026#34; | egrep -q \u0026#39;^[A-Z]+-[0-9]+ \u0026#39;; then git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$1\\\u0026#34;; else JIRA=$(git jira); if [ -z \u0026#34;$JIRA\u0026#34; ]; then echo \u0026gt;\u0026amp;2 \u0026#39;#### Start message with Jira ticket number! ####\u0026#39;; exit 1; else git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$JIRA $1\\\u0026#34;; fi; fi; }; f\u0026#34; save = !git add -A \u0026amp;\u0026amp; git commit -m \u0026#39;SAVEPOINT\u0026#39; wip = commit -am \u0026#34;WIP\u0026#34; undo = reset HEAD~1 --mixed amend = commit -a --amend wipe = !git add -A \u0026amp;\u0026amp; git commit -qm \u0026#39;SAVEPOINT before WIPE\u0026#39; \u0026amp;\u0026amp; git reset HEAD~1 --hard graph = log --graph --oneline --all tree = log --graph --decorate --pretty=oneline --abbrev-commit authorupdate = !git filter-branch -f --env-filter \u0026#34;GIT_AUTHOR_NAME=\u0026#39;My Name\u0026#39;; GIT_AUTHOR_EMAIL=\u0026#39;my-mail@github.com\u0026#39;; GIT_COMMITTER_NAME=\u0026#39;My Name\u0026#39;; GIT_COMMITTER_EMAIL=\u0026#39;my-mail@github.com\u0026#39;;\u0026#34; HEAD \u0026amp;\u0026amp; git push origin master --force dropmerged = \u0026#34;!git branch --merged | grep -v \u0026#39;* \u0026#39; | xargs git branch -d; git checkout -q master \u0026amp;\u0026amp; git for-each-ref refs/heads/ \\\u0026#34;--format=%(refname:short)\\\u0026#34; | while read branch; do mergeBase=$(git merge-base master $branch) \u0026amp;\u0026amp; [[ $(git cherry master $(git commit-tree $(git rev-parse \\\u0026#34;$branch^{tree}\\\u0026#34;) -p $mergeBase -m _)) == \\\u0026#34;-\\\u0026#34;* ]] \u0026amp;\u0026amp; git branch -D $branch; done\u0026#34; bb = ! open $(git config --get remote.origin.url | sed \u0026#39;s/^ssh:\\\\/\\\\/git@\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\).git/https:\\\\/\\\\/\\\\1\\\\/projects\\\\/\\\\2\\\\/repos\\\\/\\\\3\\\\/browse/\u0026#39;) prs = ! open $(git config --get remote.origin.url | sed \u0026#39;s/^ssh:\\\\/\\\\/git@\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\)\\\\/\\\\([^\\\\/]*\\\\).git/https:\\\\/\\\\/\\\\1\\\\/projects\\\\/\\\\2\\\\/repos\\\\/\\\\3\\\\/pull-requests/\u0026#39;) Feel free to share in comments if you have done it better, or which might be a nice addition to my collection.\n","permalink":"https://gagor.pro/2024/01/git-hacks-a-set-of-my-favorite-git-aliases/","summary":"I use Git a lot, even writing this article i will commit text few times. There\u0026rsquo;s a set of aliases I rely on daily and they\u0026rsquo;re first I add in new place.\nSome Git commands are unnecessarily verbose. You can make your life much easier with bash-completions, but if you write it tens of times per day, it\u0026rsquo;s anyway a lot of typing\u0026hellip; and I\u0026rsquo;m a lazy man \u0026#x1f604;\nSimple status/log checks git s s = status --short --branch --untracked-files Shows a short, branch-focused status with untracked files.","title":"Git hacks - a set of my favorite git aliases"},{"content":"One day, I was looking for some gains to improve the startup time for Jenkins agents. We run them as containers and because images are quite big, I was thinking about cutting the size, by cutting less frequently used features. I was looking for the metrics I could use to decide which changes are most valuable. I could think about two: download time and startup time. Together they combine to the gap between the request to start agent and the moment you can start to use it. Sadly it wasn\u0026rsquo;t that easy to measure that and collect statistics. It\u0026rsquo;s also not that practical, when you\u0026rsquo;re developing improvement as you can\u0026rsquo;t see the difference.\nI started thinking about alternative metrics and I quickly figured out, that I can check Docker image compressed size. It would be proportional to the download time. I can also compare compressed size with uncompressed size as it will show how much data have to be extracted, being proportional to the startup time. Although those KPIs are not as good, they\u0026rsquo;re fairy ok to use during development.\nThen I found, it\u0026rsquo;s not that easy to guess compressed size of Docker image. docker image ls shows extracted size. docker inspect shows compressed size per layer. Luckily I found1 nice bash function that can make the whole process easy:\nSimple function to guess compressed size of image dockersize() { docker manifest inspect -v \u0026#34;$1\u0026#34; | jq -c \u0026#39;if type == \u0026#34;array\u0026#34; then .[] else . end\u0026#39; | jq -r \u0026#39;[ ( .Descriptor.platform | [ .os, .architecture, .variant, .\u0026#34;os.version\u0026#34; ] | del(..|nulls) | join(\u0026#34;/\u0026#34;) ), ( [ .SchemaV2Manifest.layers[].size ] | add ) ] | join(\u0026#34; \u0026#34;)\u0026#39; | numfmt --to iec --format \u0026#39;%.2f\u0026#39; --field 2 | column -t ; } Now it\u0026rsquo;s easy to imagine download time impact and gains from my work:\nUsage of dockersize $ docker pull alpine $ docker pull python $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE python latest 33039c2f184f 5 weeks ago 1.02GB alpine latest 1dc785547989 7 weeks ago 7.73MB $ dockersize alpine linux/amd64 3.26M linux/arm/v6 3.02M linux/arm/v7 2.79M linux/arm64/v8 3.20M linux/386 3.10M linux/ppc64le 3.21M linux/s390x 3.10M $ dockersize python linux/amd64 362.96M linux/arm/v5 330.63M linux/arm/v7 316.31M linux/arm64/v8 353.81M linux/386 365.13M linux/ppc64le 377.20M linux/s390x 333.43M windows/amd64/10.0.20348.2227 1.83G windows/amd64/10.0.17763.5329 1.99G https://gist.github.com/MichaelSimons/fb588539dcefd9b5fdf45ba04c302db6?permalink_comment_id=4243739#gistcomment-4243739\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/checking-compressed-size-of-docker-image/","summary":"One day, I was looking for some gains to improve the startup time for Jenkins agents. We run them as containers and because images are quite big, I was thinking about cutting the size, by cutting less frequently used features. I was looking for the metrics I could use to decide which changes are most valuable. I could think about two: download time and startup time. Together they combine to the gap between the request to start agent and the moment you can start to use it.","title":"Checking compressed size of Docker image"},{"content":"A little bit of sour-sweet memories I\u0026rsquo;ve been in many job interviews. Few as a candidate but many more as a Technical Leader or Hiring Manager. There are many questions I\u0026rsquo;ve been asked and many tests I passed. I remember one test, where I had to install and configure Apache Tomcat on a Virtual Machine, under Windows (which I don\u0026rsquo;t use for years), without access to internet. There were more traps there, like some files in the locations where Tomcat is normally installed were blocked for write with chattr immutable attribute. I was able to configure the application to run, but then I asked what\u0026rsquo;s the point of this exercise? Main explanation was to \u0026ldquo;understand my knowledge\u0026rdquo;. I asked more questions:\nHow often do you work without access to Internet? Do they look for people with encyclopedic knowledge or those that can do the work? What about topics where you have to discover how to do something, that no one else did before? Do you often work on a systems where others prepare \u0026ldquo;traps\u0026rdquo; for you? Just to cut it here. I\u0026rsquo;ve been irritated, by the test. They\u0026rsquo;ve been irritated by my questions, although they admit they touch some points.\nThey propose me a job, but I rejected. I didn\u0026rsquo;t want to work there. So how to do it better, what could be a better approach?\nReview as an interview That\u0026rsquo;s something we practice more often recently. Idea is simple, let see how candidate works in similar topics and environment we do. We prepare a repo, which is just not in the best shape (to say it gently). Actually it\u0026rsquo;s completely fine if you take one of the \u0026ldquo;skeletons in the closet\u0026rdquo;, remove sensitive stuff and throw more rubbish at the top of it like a cherry \u0026#x1f604;\nYou share this repo with candidate day or two before the interview. It should be enough time to get familiar with it, but we don\u0026rsquo;t want to make it too expensive and spend a whole week on it. We don\u0026rsquo;t expect any changes, any work in front.\nThe test question is: Check this repository and we would like to discuss what kind of improvements you would suggest?.\nIn our repo we have:\nbasic Terraform infra with unencrypted resources and few spooky policies, some Python code, some secrets committed, some comments of TODO/TBD/FIXME type, some rubbish accidentally committed, etc. Project should generally represent a landscape of technologies YOU use. There\u0026rsquo;s something for everyone and we already collected twice as much good advices, than we expected to hear \u0026#x1f604;\nIt\u0026rsquo;s more natural for people to work with such \u0026ldquo;test\u0026rdquo;. There will be people that will have a lot of Cloud improvement suggestion, but won\u0026rsquo;t touch the code, or vice versa. Some people would advice to replace Jenkinsfile with Github Actions, others would prefer to deploy with Ansible. You can guide this person through the repo, ask more targeted questions. Those are usually a good hook points to ask about specific technology experience or project\u0026rsquo;s size. You can see if they think big or small, if scale is similar to your expectations.\nI\u0026rsquo;ve read a blog1 which I read and it\u0026rsquo;s quite aligned with what we do.\nDevOps interview questions The list below is with me for many years right now, but I still use some of the questions from time to time.\nThis is the more old school way of doing interviews. I collected a bunch of good questions, that I often use as a starters, to worm up a candidate or just go one by one if candidate is too stressed to discuss freely.\nGeneral Please tell us about one of your success stories – something you’re proud of, something you did and you really enjoyed (best if this is technical story). Please tell us about one of your failure stories – something you did but you’re not proud of it. (best if this is technical story)? Tip\nThose two questions usually go in pair. We start with something nice, something that this person likes and is proud of. This opens candidate and builds confidence. It often shows what this person likes to do - so you can consider how much of this type of work you have for her/him.\nWarning\nThe best part is second question - does he/she learn anything from mistakes?\nIf you could choose anything, what your dream job would look like? What you would like to do from the morning till the evening? Tip\nListen carefully if you can offer anything close to it. People have different expectations - some want to work mostly with Cloud, others with Ansible, they either like to work with people or they don\u0026rsquo;t - if you won\u0026rsquo;t be able to fulfill those needs, (s)he would most likely leave soon.\nDo you know what the term Twelve-Factor App\u0026thinsp; external link mean? Could you tell something about it? Note\nIt\u0026rsquo;s like 12 questions, on which you can discuss long hours. If you agree or not and why?\nWhat’s the difference between Continuous Integration/Continuous Delivery/Continuous Deployment? What Configuration Management tools do you know, which will you choose and why? Attitude How lazy are you? I.e. which tasks would you not automate? Note\nAutomation is core of DevOps work. We want people who can automate anything, test anything etc.\nBut we also want people, that can can do the math and won\u0026rsquo;t spend a week to automate one-time task taking 30 min.\nHow much access should Developers have to a production system? Set out reasons why. Note\nThis is about work culture in your company. Some people wouldn\u0026rsquo;t allow developers to touch anything, others work with them as with partners.\nHave you taken part in Agile/Scrum teams? Are you comfortable suggesting improvements to an architecture? Can you provide occasions you have? How would you handle a technical disagreement with a colleague? Do you consider yourself a developer, sysadmin, or other? Which is more appealing to you? Security, Performance, Features, Reliability. list them in importance. Who should be responsible for deployments? The team are planning on deploying an application change you believe is fundamentally unsound. How do you deal with this? Development Describe Language experience. MUST have at least good knowledge of on of Python, Ruby, Java, GoLang. Just Bash doesn’t count. Do you use GIT (or other SCM) in your daily duties? Can you name and explain any GIT workflows? Talk about a reasonable size development project you\u0026rsquo;ve created. What did you use? Which Frameworks? Describe the difference between a unit tests, integration test and performance tests. Talk about the tools you would use for each. At which stage of the development would you use them? Have you ever been working with SSO software? SAML/OAuth/Open ID Connect? Have you been working with Spring and dynamic properties? SpringCloud/Consul? How do you test infrastructure deployments? How do you version infrastructure changes? Describe the concept of idempotent calls and how it applies to CM tools. Please provide Github repo. If no Github repo, why not? What kind of communication patterns/protocols in micro-services ecosystem do you know? What are the advantages of using micro-services? How can you introduce an API / contract breaking change? DBs What DB experiences do you have? What DB engines do you used? We have shared between few apps DB schema. It\u0026rsquo;s starting to be a problem. What would you suggest? Types of SQL joins? Jenkins Can you elaborate on best practices on Jenkinsfile writing? Do you know how to create Jenkins pipeline libraries? Do you know what are best practices for pipeline libraries creation? Docker What are good practices for writing Dockerfiles? What problems arrive in services that run on Docker? We have 4 different environments: DEV, QC, UAT, PROD – there are small differences between them (ex. different login/password/IP on DB server). We use Docker to prepare images with our applications. What would you suggest to do, to reflect those small differences between environments but without need to build different Docker images for them? Monitoring/Metrics Have you ever been working with ELK stack? Do you know Grafana? What about Sensu/Nagios/Zabbix? Linux How will you check who lately logged to server via SSH or by accessing machine? How will you check open connections on server? How will you check what service is running on port 2345? How many connections are opened now to port 2345? You have no space left on device error but df -h shows a lot of free space – what could go wrong? How to change number of available inodes on existing volume? Apache server is running under huge load (ex. 300 or 3000) but you could login to it and system is responsive. What could be wrong? You have a lot of Apache processes in state D – how will you kill them? How to create Zombie process? How to kill it? Is there any mechanism that should kill them automatically? CDN/WAF/LB This one is very specific, but I was looking for people with this specialization.\nHave you ever been working with Apache? mod_rewrite mod_proxy? mod_security? other modules? Are you familiar with Nginx? rewrites? proxy? NAXSI? What\u0026rsquo;s difference between rewrite and redirect? Have you ever been working on reverse proxy configuration? What is CDN, when to use it? What headers are used in HTTP for resource caching? What\u0026rsquo;s mine difference between them, which one is better? Expires vs Cache-Control https://devcenter.heroku.com/articles/increasing-application-performance-with-http-cache-headers#http-cache-headers\u0026thinsp; external link https://developer.mozilla.org/pl/docs/Web/HTTP/Headers/Cache-Control\u0026thinsp; external link https://developer.mozilla.org/pl/docs/Web/HTTP/Headers/Expires\u0026thinsp; external link Do you have any experiences with load-balancers (eg. HAProxy)? Can you list few load balancing algorithms? (random, round-robin, weighted variants, sticky) What \u0026ldquo;sticky session\u0026rdquo; mean? Do you know what Server Side Includes or Edge Server Includes? How would you build whole HTML page from multiple micro services (one page with sections from different services)? Can you explain how HTTP/HTTPS request/response look like (as deep as it\u0026rsquo;s possible)? DNS TCP handshake (or UDP) SSL - RTT-0 Nice explanation can be found here: https://blog.cloudflare.com/introducing-0-rtt/\u0026thinsp; external link What configuration would you suggest to shorten time of connection in HTTP/2? (TLS 1.3, RTT-0, session-tickets, keep-alive) https://chrlschn.dev/blog/2023/07/interviews-age-of-ai-ditch-leetcode-try-code-reviews-instead/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/how-to-interview-devops-candidates/","summary":"A little bit of sour-sweet memories I\u0026rsquo;ve been in many job interviews. Few as a candidate but many more as a Technical Leader or Hiring Manager. There are many questions I\u0026rsquo;ve been asked and many tests I passed. I remember one test, where I had to install and configure Apache Tomcat on a Virtual Machine, under Windows (which I don\u0026rsquo;t use for years), without access to internet. There were more traps there, like some files in the locations where Tomcat is normally installed were blocked for write with chattr immutable attribute.","title":"How to interview DevOps candidates?"},{"content":" Working BackwardsInsights, Stories, and Secrets from Inside Amazon\nAuthor: Bryar Colin\nempik.com Still reading\u0026hellip;\n","permalink":"https://gagor.pro/books/2024/working-backwards/","summary":"Working BackwardsInsights, Stories, and Secrets from Inside Amazon\nAuthor: Bryar Colin\nempik.com Still reading\u0026hellip;","title":"Working Backwards"},{"content":"Why? Hiring people is hard, but having a new person in a team is not a work done. It\u0026rsquo;s important to monitor how people perform during probation period and share feedback with them. Personally, I like to ask the whole team, with a bunch of open questions, which allows me to look from higher altitude on the new hire. My point of view might be biased but by checking what others think, I can fill blind spots.\nSuch questionnaire can support my decision over I want to continue to work with this person, or maybe we (as a team) saw too many red flags and it\u0026rsquo;s time to part ways here. Firing people is never a pleasant task. It\u0026rsquo;s draining and leaves you often feeling guilty. But end of Probation Period is a moment that both you and a candidate expect some kind of verification and summarization. It\u0026rsquo;s important to take this opportunity and pause for a moment, collect the feedback and share it. This allows you to clearly say, what worked, what didn\u0026rsquo;t work. It\u0026rsquo;s the best moment to point the expected direction and ensure you\u0026rsquo;re both aligned on the target.\nThere are many questions that might be asked, but I rather try to keep a list of questions short.\nWhat to ask the team for? I prefer open questions. They allow to express more concerns or share custom perspective. Marek thanks for this awesome list!\nDo you have any story with NAME HERE that you would like to share? (does not have to be work-related) How likely are you to recommend working with NAME HERE to a friend, colleague or manager? Number 1-10 or descriptive text field What NAME HERE is good at and why? (name 1 thing) What NAME HERE should stop doing and why? (name 1 thing) What NAME HERE should start doing and why? (name 1 thing) Is there anything more that you want to share anonymously with NAME HERE? Info\nYou can use Google Forms\u0026thinsp; external link or M$ Forms\u0026thinsp; external link to prepare the survey. Putting questions in a mail will work too.\nDid you fill the questionnaire? No? Then do it!\nHow to prepare for the discussion We know \u0026ldquo;Why?\u0026rdquo;, so let think about \u0026ldquo;What?\u0026rdquo;. Conducting an effective employee evaluation at the end of the probationary period can contribute to a better understanding of their strengths, areas for improvement, and defining further development directions.\nBelow are some questions you can use when gathering feedback or evaluating your judgement:\nOverall impression\nWhat is your overall impression after the employee\u0026rsquo;s probationary period? Have you noticed any positive aspects of their work? Strengths\nIn which areas did the employee excel? Have you observed any unique skills that bring value to the team? Areas for improvement\nAre there any areas where the employee can further develop? Are there specific skills that need improvement? Team collaboration\nHow does the employee integrate with the team? Do they communicate effectively and actively collaborate with other team members? Engagement and motivation\nHow would you rate the employee\u0026rsquo;s engagement in tasks? Have you noticed any signs of motivation to grow within the company? Initiative and independence\nDoes the employee take initiative and work independently, or do they require constant supervision? Development suggestions\nWhat suggestions do you have regarding the employee\u0026rsquo;s further development? Do you see the need for additional training and development? Team goals\nHow has the employee contributed to achieving the team\u0026rsquo;s common goals? Do you see them as a valuable team member in the longer term? Communication\nHow do you assess the employee\u0026rsquo;s communication? Is it clear and understandable? Are they open in sharing information and receptive to others\u0026rsquo; opinions? Summary\nAre there any additional comments you would like to share about the employee\u0026rsquo;s performance? You should answer those questions as part of preparation to the meeting. Think about positive surprises and note them. Avoid criticism! You\u0026rsquo;re giving feedback both positive and constructive (some call it negative \u0026#x1f61c;) for the better future. Name your expectations in specific situations.\nI\u0026rsquo;ve got the answers! Now what? Usually it\u0026rsquo;s enough to sort data from the team, clear repetitions, polish phrases. Now, plan 1-on-1 talk with new hire and explain what you did and why. Yes, be honest about it.\nAsk if she/he want to hear the answers. Start with positive feedback. Ask if she/he agrees, what she/he thinks about it? Listen! Ask if she/he\u0026rsquo;s ready to listen constructive feedback too? Ask if she/he agrees, what she/he thinks about it? Listen! Share your decision on the continuation of the contract. If it\u0026rsquo;s positive, ask if person agrees with it and wants to continue collaboration. If you have concerns, share them and maybe propose another time limited contract. If you want to terminate the contract, share it - with compassion. This person, might be under heavy stress so be gentle and supportive. If she/he asks why? You have the list. Warning\nIf it happen, that you have to terminate the contract it shouldn\u0026rsquo;t be your first talk on this matter with new hire. Red lights should be seen early enough to give you time to correct them BEFORE final discussion.\nFingers crossed and good luck! You can do it.\nIf you know better way or can suggest improvements, feel free and drop me a comment. I\u0026rsquo;m really curios how people approach it.\n","permalink":"https://gagor.pro/2024/01/how-to-give-probation-period-feedback/","summary":"Why? Hiring people is hard, but having a new person in a team is not a work done. It\u0026rsquo;s important to monitor how people perform during probation period and share feedback with them. Personally, I like to ask the whole team, with a bunch of open questions, which allows me to look from higher altitude on the new hire. My point of view might be biased but by checking what others think, I can fill blind spots.","title":"How to give Probation Period feedback?"},{"content":"Nazbierało mi się w domu całkiem sporo różnych roślin. Problem w tym, że nie zawsze pamiętam jak należy je pielęgnować. Rzeczy typu częstotliwość podlewania, wymagane nasłonecznienie, rodzaj nawozu, częstotliwość używania nawozu, itd., itp.\nPostanowiłem to w końcu spisać. Na razie na jednej stronie i trochę na kolanie, by pokonać opór. Zamierzam to nieco uszczegółowić i wypracować jakiś schemat podlewania/nawożenia. Myślałem też o jakiś zabawkach do mierzenia poziomu wilgotności/PH - zobaczymy.\nZestawienie Roślina Podlewanie Nasłonecznienie Temperatura Nawożenie Storczyki Raz na 7-10 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Skrzydłokwiat Co 1-2 tygodnie Jasne, filtry słoneczne 18-24°C w dzień, 15°C w nocy Co 2-3 tygodnie Fikus Co 7-14 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Cytryna/Mandarynka Co 7-10 dni Pełne nasłonecznienie 20-25°C w dzień, 15°C w nocy Co 2-4 tygodnie Awokado Co 7-10 dni Jasne, pełne nasłonecznienie 20-25°C w dzień, 15°C w nocy Co 4-6 tygodni Juka (Dracena) Co 7-14 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 4-6 tygodni Mango Co 4-7 dni Pełne nasłonecznienie 24-30°C w dzień, 18°C w nocy Co 2-4 tygodnie Grudnik (Azalia) Co 7-10 dni Jasne, zacienione 18-24°C w dzień, 15°C w nocy Co 4-6 tygodni Strelicja (Ptasi Kwiat) Co 5-7 dni Pełne nasłonecznienie 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Zamiokulkas Co 10-14 dni Jasne, umiarkowane 18-24°C Co 2-4 tygodnie Orchidea Co 7-10 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Eszynantus Co 7-10 dni Jasne, umiarkowane 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Gwiazda Betlejemska Co 7-10 dni Jasne, zacienione 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Dracena Co 7-14 dni Jasne, umiarkowane 18-24°C w dzień, 15°C w nocy Co 4-6 tygodni Palma Areka Co 7-10 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 4-6 tygodni Drzewko Bonsai Ostrokrzew Ilex (Ilex crenata) Raz na 5-7 dni Jasne, rozproszone 18-24°C w dzień, 15°C w nocy Co 2-4 tygodnie Opisy Storczyki Storczyki są dość łatwe w pielęgnacji. Ważne jest utrzymanie odpowiedniego podłoża, które powinno być lekkie i dobrze przepuszczające powietrze.\nPodlewanie: Podlewanie storczyków zależy od rodzaju storczyka i warunków otoczenia. Ogólnie unikaj nadmiernego podlewania. Substrat powinien lekko przesychać między podlewaniami.\nŚwiatło: Storczyki preferują jasne, rozproszone światło, ale unikaj bezpośredniego nasłonecznienia. Mogą rosnąć zarówno w miejscach zacienionych, jak i bardziej oświetlonych.\nTemperatura: Optimum to 18-24°C w ciągu dnia i minimalnie 15-16°C w nocy. Unikaj gwałtownych zmian temperatury.\nNawożenie: Podczas okresu wzrostu (wiosna/lato), stosuj nawozy o niższym stężeniu co 2-4 tygodnie. Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nZapewniaj odpowiednią wilgotność, zwłaszcza w okresie zimowym, używając tacki z wodą i żwiru. Jeśli storczyk nie kwitnie, sprawdź, czy otrzymuje wystarczająco światła. Ogranicz podlewanie podczas okresu spoczynku (zimą) i kontroluj stan korzeni. Warto zauważyć, że różne odmiany storczyków mogą mieć nieco różne wymagania pielęgnacyjne, więc dostosuj opiekę do konkretnego gatunku.\nOrchidea Orchidea to piękna, ale wymagająca roślina doniczkowa. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj orchideę umiarkowanie. Zaleca się podlewanie, gdy podłoże jest niemal suche. Unikaj przelania, ponieważ nadmierna wilgotność może prowadzić do zgnilizny korzeni.\nŚwiatło: Orchidea potrzebuje jasnego, ale rozproszonego światła. Unikaj bezpośredniego słońca, które może spowodować oparzenia liści. Umieść ją w miejscu o jasnym oświetleniu, ale zacienionym przed intensywnym światłem słonecznym.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Orchidea korzysta z różnicy temperatury między dniem a nocą, co pobudza kwitnienie.\nNawożenie: Stosuj nawozy dla orchidei o niższym stężeniu co 2-4 tygodnie w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nOrchidea preferuje wysoką wilgotność. Możesz umieścić doniczkę na podstawce z wilgotnym żwirem lub używać nawilżacza powietrza. Nie zanurzaj korzeni w wodzie. Podlewaj przez zanurzenie tylko na krótki okres, a następnie dobrze osusz korzenie. Gdy kwiaty zwiędną, przyciąć łodygę tuż nad jednym zielonym oczkiem, aby pobudzić nowe kwitnienie. Przesadź orchideę co 1-2 lata, gdy podłoże zacznie się rozkładać. Orchidea może być wymagająca, ale z odpowiednią opieką nagrodzi cię pięknym kwitnieniem. Dostosuj pielęgnację do konkretnego rodzaju orchidei, którą posiadasz.\nSkrzydłokwiat Skrzydłokwiat to roślina doniczkowa o dekoracyjnych liściach. Wymaga umiarkowanej pielęgnacji.\nPodlewanie: Podlewaj skrzydłokwiat, gdy wierzchnia warstwa podłoża jest sucha. Unikaj przelania. W okresie zimowym ogranicz ilość podlewanej wody.\nŚwiatło: Preferuje jasne, ale rozproszone światło. Unikaj bezpośredniego nasłonecznienia, które może spowodować uszkodzenia liści.\nTemperatura: Optymalna temperatura to 18-24°C. Nie wystawiaj na nagłe zmiany temperatury i unikaj przeciągów.\nNawożenie: W okresie wzrostu (wiosna/lato) stosuj nawozy do roślin doniczkowych co 4-6 tygodni. Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nRegularnie sprawdzaj liście pod kątem szkodników i kurzajek. Zapewniaj odpowiednią wilgotność powietrza, zwłaszcza zimą, używając nawilżacza powietrza lub umieszczając doniczkę na podstawce z wilgotnym żwirem. Przesadzaj roślinę co 1-2 lata, gdy korzenie wypełnią doniczkę. Pamiętaj, że każda roślina może mieć indywidualne potrzeby, więc bacznie obserwuj swój skrzydłokwiat i dostosuj pielęgnację w razie potrzeby.\nFikus Fikusy to rośliny doniczkowe o ozdobnych liściach. Są stosunkowo łatwe w pielęgnacji, ale wymagają pewnych warunków.\nPodlewanie: Podlewaj fikusa, gdy wierzchnia warstwa gleby jest lekko sucha. Unikaj przelania, ale nie dopuszczaj do całkowitego przesychania podłoża. Zimą zmniejsz ilość podlewanej wody.\nŚwiatło: Fikusy preferują jasne, rozproszone światło, ale tolerują półcień. Unikaj bezpośredniego słońca, które może prowadzić do oparzeń liści.\nTemperatura: Optimum to 18-24°C w ciągu dnia, a nocą nie niższa niż 15°C. Unikaj nagłych zmian temperatury.\nNawożenie: W sezonie wzrostu (wiosna/lato) nawoź fikusa raz na miesiąc. Zimą ogranicz nawożenie. Stosuj nawozy o równowadze makroelementów.\nDodatkowe wskazówki:\nRegularnie spryskuj liście wodą, aby utrzymać wilgotność. Unikaj przeciągów, które mogą prowadzić do utraty liści. Przesadzaj fikusa co 1-2 lata lub gdy korzenie wypełnią doniczkę. Pamiętaj, że istnieje wiele odmian fikusów, a niektóre mogą mieć specyficzne wymagania. Dlatego dostosuj pielęgnację do konkretnej odmiany fikusa, którą posiadasz.\nCytryna/Mandarynka Hodowla cytryny lub mandarynki w domu wymaga uwagi, ale może być satysfakcjonująca. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewanie powinno być regularne, ale unikaj przelania. Gleba powinna być wilgotna, ale nie mokra. W okresie zimowym zmniejsz ilość podlewanej wody.\nŚwiatło: Cytryny i mandarynki potrzebują dużo światła słonecznego. Umieść je w miejscu o pełnym nasłonecznieniu, najlepiej na parapecie okna.\nTemperatura: Optymalna temperatura to 20-25°C w ciągu dnia, a w nocy nie niższa niż 15°C. Unikaj nagłych zmian temperatury.\nNawożenie: Stosuj nawozy dla cytrusów co 2-4 tygodnie w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nRegularnie kontroluj roślinę pod kątem szkodników, zwłaszcza mszyc. Zabezpiecz roślinę przed przeciągami. Możesz umieścić doniczkę na tackach z wodą i kamieniami, aby utrzymać podwyższoną wilgotność. Przesadzaj roślinę co 2-3 lata, gdy korzenie wypełnią doniczkę. Pamiętaj, że cytryny i mandarynki wymagają cierpliwości, ponieważ owocowanie może nastąpić po kilku latach hodowli. Dostosuj pielęgnację do warunków panujących w twoim domu.\nAwokado Hodowla awokado w domu jest fascynująca, ale wymaga cierpliwości. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj awokado regularnie, utrzymując wilgotność gleby. Unikaj przelania i przesuszania. Woda powinna być dobrze odprowadzana.\nŚwiatło: Awokado potrzebuje jasnego, rozproszonego światła. Umieść go w miejscu o pełnym nasłonecznieniu, np. na parapecie okna.\nTemperatura: Optymalna temperatura to 20-25°C w ciągu dnia, a w nocy nie niższa niż 15°C. Awokado nie znosi mrozu.\nNawożenie: Stosuj nawozy o równowadze mikroelementów co 4-6 tygodni w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nPo osiągnięciu odpowiedniej wysokości (około 15 cm), przycięcie pędów pobudzi roślinę do rozgałęziania. Unikaj przeciągów i nagłych zmian temperatury. Możesz umieścić awokado na tacce z wodą i kamykami, aby utrzymać wilgotność. Pamiętaj, że uprawa awokado z pestki może zająć kilka lat, zanim roślina zacznie owocować. Cierpliwość i systematyczna pielęgnacja są kluczowe podczas hodowli awokado. Dostosuj opiekę do warunków panujących w twoim domu.\nMango Hodowla mango w domu jest możliwa, ale wymaga specyficznych warunków i cierpliwości. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewanie mango powinno być regularne, utrzymując wilgotność gleby. Unikaj przelania i przesuszania. Woda powinna być dobrze odprowadzana, a gleba nie powinna stać w wodzie.\nŚwiatło: Mango potrzebuje pełnego nasłonecznienia. Umieść roślinę w miejscu o intensywnym świetle słonecznym, np. na parapecie okna.\nTemperatura: Optimum to 24-30°C w ciągu dnia, a w nocy nie niższa niż 18°C. Mango jest wrażliwe na niskie temperatury, zwłaszcza poniżej 10°C.\nNawożenie: Stosuj nawozy o równowadze mikroelementów co 2-4 tygodnie w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nMango najlepiej rośnie w glebie o pH 5.5-7.5. Możesz użyć specjalnej mieszanki do uprawy roślin tropikalnych. Regularnie kontroluj roślinę pod kątem szkodników, takich jak przędziorki. Możesz umieścić doniczkę na tackach z wodą i kamieniami, aby zwiększyć wilgotność. Oczekuj, że uprawa mango z pestki może zająć kilka lat, zanim roślina osiągnie znaczny rozmiar. Mango to roślina wymagająca specyficznych warunków tropikalnych, więc dostarczenie odpowiednich warunków, zwłaszcza światła i temperatury, jest kluczowe dla jej udanej hodowli w domu.\nJuka Juka, znana również jako dracena, to roślina doniczkowa o wyrazistych, mieczowatych liściach. Jest stosunkowo łatwa w pielęgnacji.\nPodlewanie: Podlewaj jukę, gdy wierzchnia warstwa gleby jest sucha. Unikaj przelania i zbyt wilgotnego podłoża. W okresie zimowym ogranicz ilość podlewanej wody.\nŚwiatło: Juka najlepiej rośnie w jasnym, rozproszonym świetle. Może tolerować niskie poziomy światła, ale jej wzrost może zwolnić. Unikaj bezpośredniego nasłonecznienia.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Juka jest wrażliwa na nagłe zmiany temperatury.\nNawożenie: Stosuj nawozy do roślin doniczkowych co 4-6 tygodni w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nPrzesadź jukę co 2-3 lata, gdy korzenie wypełnią doniczkę. Regularnie usuwaj kurz z liści za pomocą wilgotnej gąbki lub ściereczki. Możesz spryskiwać liście wodą, aby zwiększyć wilgotność powietrza wokół rośliny. Unikaj nadmiernego przeciągu. Juka jest rośliną, która łatwo dostosowuje się do warunków domowych. Regularna, ale umiarkowana pielęgnacja sprawi, że będzie ozdobą Twojego wnętrza.\nDracena Dracena to popularna roślina doniczkowa o zróżnicowanych odmianach, charakteryzujących się pięknymi, wąskimi liśćmi. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj dracenę umiarkowanie, pozwalając wierzchniej warstwie gleby lekko przeschnąć między podlewaniami. Unikaj przelania, ponieważ nadmierne podlewanie może prowadzić do gnicia korzeni.\nŚwiatło: Dracena dobrze rośnie w umiarkowanym świetle, ale najlepiej rozwija się w jasnych warunkach. Unikaj bezpośredniego nasłonecznienia, które może powodować oparzenia liści.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Dracena jest wrażliwa na nagłe zmiany temperatury.\nNawożenie: Stosuj nawozy do roślin doniczkowych co 4-6 tygodni w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nRegularnie usuwaj kurz z liści, aby zapewnić roślinie dostęp do światła. Dracena może być wrażliwa na szkodniki, zwłaszcza przędziorki. Monitoruj roślinę pod kątem ewentualnych problemów. Nie przesadzaj draceny zbyt często, ponieważ może być wrażliwa na zmianę środowiska korzeniowego. Dracena to roślina, która dostosowuje się do różnych warunków, co sprawia, że jest doskonała do hodowli w pomieszczeniach. Regularna pielęgnacja, odpowiednie podlewanie i umiarkowane światło pomogą jej rozwijać się zdrowo i dekoracyjnie.\nPalma Areka Palma Areka, znana również jako areka lub palma paciorek, to popularna roślina doniczkowa o eleganckich liściach. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj palmę arekę regularnie, utrzymując umiarkowaną wilgotność gleby. Unikaj przelania, ale nie dopuszczaj do całkowitego przesuszenia podłoża.\nŚwiatło: Palma areka preferuje jasne, rozproszone światło, ale dobrze rośnie także w umiarkowanym świetle. Unikaj bezpośredniego słońca, zwłaszcza w gorące dni.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Palma areka nie znosi chłodzenia poniżej 10°C.\nNawożenie: Stosuj nawozy do roślin doniczkowych co 4-6 tygodni w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nPalma areka ceni sobie wysoką wilgotność powietrza. Spryskuj liście wodą lub umieść doniczkę na podstawce z wilgotnym żwirem. Regularnie usuwaj kurz z liści, aby zachować ich zdrowy wygląd. Palma areka jest wrażliwa na przeciągi. Umieść ją w miejscu, gdzie nie będzie wystawiona na nagłe zmiany temperatury. Przesadź palmę arekę co 2-3 lata, gdy korzenie wypełnią doniczkę. Palma areka to roślina, która wnosi do wnętrza elegancję i tropikalny urok. Pamiętaj o odpowiednim podlewaniu, ochronie przed przeciągami i regularnej pielęgnacji, aby cieszyć się jej zdrowym wzrostem.\nGrudnik Grudnik, znany również jako azalia, to roślina doniczkowa o bujnym kwitnieniu. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj grudnika regularnie, utrzymując umiarkowaną wilgotność gleby. Unikaj przelania, ale nie dopuszczaj do całkowitego przesychania podłoża. Woda powinna być miękka, bezapteczna.\nŚwiatło: Grudnik potrzebuje jasnego, rozproszonego światła, ale unikaj bezpośredniego słońca. Umieść go w miejscu o jasnym oświetleniu, ale chronionym przed intensywnym światłem słonecznym.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Unikaj nagłych zmian temperatury i przeciągów.\nNawożenie: Stosuj nawozy do roślin kwasolubnych (dla rododendronów i azalii) co 4-6 tygodni w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie.\nDodatkowe wskazówki:\nGrudnik lubi wilgotne otoczenie. Spryskuj delikatnie liście wodą lub umieść doniczkę na podstawce z wilgotnym żwirem. Regularnie usuwaj przekwitające kwiaty, aby pobudzić roślinę do nowego kwitnienia. Przesadź grudnika co 2-3 lata, gdy korzenie wypełnią doniczkę. Staraj się utrzymać równomierne nawilżenie, szczególnie podczas okresu kwitnienia. Grudnik to roślina, która nagradza staranność pielęgnacyjną pięknym kwitnieniem. Dostosuj opiekę do warunków panujących w twoim domu, a grudnik będzie zdrowy i efektowny.\nStrelicja Strelicja, znana również jako ptasi kwiat, to roślina doniczkowa o efektownych kwiatach. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj strelicję regularnie, utrzymując wilgotność gleby. W okresie wzrostu podlewaj obficie, a w okresie spoczynku zmniejsz ilość podlewanej wody.\nŚwiatło: Strelicja potrzebuje jasnego światła słonecznego, ale może tolerować półcień. Umieść ją w miejscu o intensywnym oświetleniu.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Unikaj nagłych zmian temperatury.\nNawożenie: Stosuj nawozy o równowadze mikroelementów co 2-4 tygodnie w okresie wzrostu (wiosna/lato). W okresie spoczynku ogranicz nawożenie.\nDodatkowe wskazówki:\nZapewnij odpowiednią wilgotność powietrza, zwłaszcza w okresie zimowym, poprzez umieszczenie doniczki na podstawce z wilgotnym żwirem. Usuwaj przekwitające kwiatostany, aby zachęcić roślinę do produkcji nowych pędów kwiatowych. Regularnie sprawdzaj roślinę pod kątem szkodników, takich jak przędziorki. Strelicja to roślina o imponujących kwiatach, jednak wymaga pewnych starań w zakresie podlewania i światła. Dostosuj pielęgnację do warunków panujących w twoim domu, aby zachować zdrowie i dekoracyjność strelicji.\nZamiokulkas Zamiokulkas, nazywany także \u0026ldquo;dolarowym drzewkiem\u0026rdquo; lub \u0026ldquo;kwiatem szczęścia\u0026rdquo;, to roślina doniczkowa o odporności na niedogodności. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Zamiokulkas jest tolerancyjny na suszę. Podlewaj umiarkowanie, pozwalając na przesuszenie wierzchniej warstwy gleby między podlewaniami. Unikaj nadmiernego podlewania.\nŚwiatło: Zamiokulkas radzi sobie zarówno w jasnym, jak i umiarkowanie oświetlonym miejscu. Jest odporny na niskie poziomy światła, ale jego wzrost może być wolniejszy w miejscach z mniejszym światłem.\nTemperatura: Optymalna temperatura to 18-24°C. Jest tolerancyjny na zmienne warunki temperaturowe, ale unikaj wystawiania go na niskie temperatury poniżej 10°C.\nNawożenie: Stosuj nawozy do roślin doniczkowych co 2-4 tygodnie tylko w okresie wzrostu (wiosna/lato). Zimą ogranicz nawożenie lub nie nawoż.\nDodatkowe wskazówki:\nZamiokulkas przechodzi okresy spoczynku, podczas których może zatrzymać wzrost. Nie martw się, jeśli zauważysz, że roślina nie rośnie przez pewien czas. Roślina jest stosunkowo odporna na szkodniki, ale regularnie sprawdzaj liście pod kątem ewentualnych problemów. Możesz przesadzić zamiokulkasa co kilka lat, gdy korzenie wypełnią doniczkę. Zamiokulkas to roślina idealna dla tych, którzy szukają rośliny łatwej w pielęgnacji. Jest trwały i elegancki, dodając urok każdemu wnętrzu.\nEszynantus Eszynantus, znany również jako kwiat kaktusa lub kwiat wigonii, to roślina doniczkowa o pięknych kwiatach. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewaj eszynantusa regularnie, utrzymując umiarkowaną wilgotność gleby. W okresie wzrostu (wiosna/lato) podlewaj obficie, ale pozwól, aby górna warstwa gleby lekko przesychała między podlewaniami. Zimą ogranicz ilość podlewanej wody.\nŚwiatło: Eszynantus preferuje jasne, rozproszone światło. Umieść go w miejscu o intensywnym, ale pośrednim oświetleniu. Unikaj bezpośredniego słońca, które może spowodować oparzenia liści.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Jest stosunkowo odporny na zmiany temperatury.\nNawożenie: Stosuj nawozy o równowadze mikroelementów co 2-4 tygodnie tylko w okresie wzrostu. Zimą ogranicz nawożenie lub nie nawoż.\nDodatkowe wskazówki:\nEszynantus kwitnie obficie w odpowiednich warunkach. Spróbuj dostarczać chłodniejsze temperatury (około 15-18°C) w nocy, aby pobudzić kwitnienie. Możesz spryskiwać roślinę wodą, aby zwiększyć wilgotność powietrza. W okresie spoczynku, czyli zimą, ogranicz podlewanie i utrzymuj chłodniejszą temperaturę, aby roślina mogła odpocząć. Eszynantus to roślina kwitnąca o dekoracyjnych kwiatach, która wymaga pewnej uwagi, ale nagradza pięknym kwitnieniem. Dostosuj pielęgnację do warunków panujących w twoim domu.\nGwiazda Betlejemska (Poinsecja) Gwiazda Betlejemska to roślina, która wymaga pewnych staranności, zwłaszcza w okresie jej kwitnienia. Oto kilka wskazówek dotyczących pielęgnacji:\nPodlewanie: Podlewanie gwiazdy betlejemskiej powinno być umiarkowane. Trzymaj podłoże stale wilgotne, ale unikaj przelania. Podczas kwitnienia dbaj o regularne podlewanie.\nŚwiatło: Gwiazda Betlejemska preferuje jasne światło, ale z umiarkowanym nasłonecznieniem. Unikaj bezpośredniego słońca, zwłaszcza w gorące dni.\nTemperatura: Optymalna temperatura to 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Unikaj nagłych zmian temperatury.\nNawożenie: Stosuj nawozy do kwiatów doniczkowych co 2-4 tygodnie w okresie wzrostu (wiosna/lato). W okresie spoczynku ogranicz nawożenie.\nDodatkowe wskazówki:\nGwiazda Betlejemska kwitnie głównie w okresie zimowym. Aby osiągnąć ponowne kwitnienie, w połowie wiosny przechodź roślinę do spoczynku, stopniowo zmniejszając podlewanie. Unikaj kontaktu z mleczem (sokiem) tej rośliny, który może powodować podrażnienia skóry i oczu u niektórych osób. W okresie spoczynku, kiedy liście opadną, ogranicz podlewanie i umieść roślinę w chłodniejszym miejscu. Gwiazda Betlejemska to roślina sezonowa, ale z odpowiednią pielęgnacją może przynieść radość swoim kolorowym kwitnieniem w okresie świątecznym.\nDrzewko Bonsai - Ostrokrzew Ilex (Ilex crenata) Pielęgnacja: Ostrokrzew Ilex, znany również jako Ilex crenata, to popularny gatunek drzewka bonsai, który charakteryzuje się drobnymi liśćmi i gęstym wzrostem. Oto kilka wskazówek dotyczących pielęgnacji tego gatunku:\nPodlewanie: Podlewanie ostrokrzewu Ilex bonsai powinno być umiarkowane. Sprawdzaj regularnie wilgotność gleby i podlewaj, gdy wierzchnia warstwa zaczyna wysychać, ale unikaj przelania. Zapewnij, aby woda nie zalegała w podstawce doniczki.\nŚwiatło: Ostrokrzew Ilex preferuje jasne, ale rozproszone światło. Umieść swoje drzewko bonsai w miejscu o jasnym oświetleniu, ale zacienionym przed bezpośrednim słońcem, zwłaszcza w gorące dni.\nTemperatura: Optymalna temperatura dla ostrokrzewu Ilex to około 18-24°C w ciągu dnia, a w nocy nie niższa niż 15°C. Jest to gatunek odporny na różnice temperatur, ale unikaj nagłych skoków temperatury.\nNawożenie: Stosuj specjalistyczne nawozy do bonsai, które zapewnią odpowiednie składniki odżywcze dla ostrokrzewu Ilex. Nawoź regularnie w okresie wzrostu, stosując się do zaleceń producenta.\nPrzycinanie: Aby utrzymać gęsty i estetyczny wzrost, regularnie przycinaj ostrokrzew Ilex. Przycinanie powinno być wykonywane ostrożnie, aby zachować naturalny kształt drzewka bonsai.\nPodłoże: Wybierz odpowiednie podłoże dla drzewka bonsai, które zapewni dobrą drenaż i retencję wilgoci. Podłoże powinno być lekkie, ale równocześnie zatrzymywać dostęp do wody i składników odżywczych.\nOstrokrzew Ilex to atrakcyjny gatunek drzewka bonsai, który może przynieść wiele satysfakcji z hodowli. Pamiętaj o regularnej pielęgnacji, odpowiednim podlewaniu i dostarczaniu odpowiedniego światła, aby zachować zdrowie i estetykę swojego drzewka bonsai.\n","permalink":"https://gagor.pro/2024/01/ladne-kwiatki-czyli-wywod-o-pielegnacji-roslin-doniczkowych/","summary":"Nazbierało mi się w domu całkiem sporo różnych roślin. Problem w tym, że nie zawsze pamiętam jak należy je pielęgnować. Rzeczy typu częstotliwość podlewania, wymagane nasłonecznienie, rodzaj nawozu, częstotliwość używania nawozu, itd., itp.\nPostanowiłem to w końcu spisać. Na razie na jednej stronie i trochę na kolanie, by pokonać opór. Zamierzam to nieco uszczegółowić i wypracować jakiś schemat podlewania/nawożenia. Myślałem też o jakiś zabawkach do mierzenia poziomu wilgotności/PH - zobaczymy.","title":"Ładne kwiatki - czyli wywód o pielęgnacji roślin doniczkowych"},{"content":" Info\nI use free tier of Cloudflare and all recommendations assume only those are available.\nWhy to cache statically generated blog? My Blog is statically generated website served from Github Pages\u0026thinsp; external link . As Github don\u0026rsquo;t allow to easily set my own domain (at least in free version), I needed some kind of proxy that:\ncan serve page from my domain, will provide valid certificate for HTTPS. Info\nI know HTTPS for static site is a non-sense, but to keep it performant with HTTP/2 and HTTP/3 - you need it.\nObviously, static site is pretty fast even with default Cloudflare configuration, but as I\u0026rsquo;m doing it for fun\u0026hellip; let squeeze it \u0026#x1f604;\nWhat Cloudflare can offer us? Cloudflare offers many options that can impact both security and performance of the website. I will go through the options I usually configure. Some of them can be easily turned on on any website, others will have more sense on rather static page.\nWarning\nPlease, don\u0026rsquo;t turn one everything at the same time.\nEnable, measure with Page Speed\u0026thinsp; external link , SSL Server Test\u0026thinsp; external link , then continue with another option.\nSSL/TLS tuning Security goes first \u0026#x1f604;\nAlways Use HTTPS Turn it ON.\nI want it, you want it. It ensures all non encrypted requests will be redirected. Same could be achieved with a Page rule, but as you can only have 3 rules - you just save one.\nHTTPS is crucial for other optimization options to work.\nHTTP Strict Transport Security (HSTS) HTTP Strict Transport Security1 (HSTS, RFC 6797) is a header which allows a website to specify and enforce security policy in client web browsers. It makes no impact for the performance, but improves security and allows to get better grade in SSL Test\u0026thinsp; external link \u0026#x1f604;\nWarning\nOne critical consideration when using HSTS on Cloudflare is that once HSTS is turned on, your website must continue to have a valid HTTPS configuration conforming with the HSTS header to avoid making the website inaccessible to users!\nI have it enabled.\nMinimum TLS Version Default is TLS 1.0, which provides the best compatibility. I wanted to kick my score in SSL Test, so I bumped it to 1.2. Most modern browsers support it.\nOpportunistic Encryption We have a rule to redirect everything to HTTPS so this one is a no-op, but might be left enabled. TLS 1.3 Turn it ON.\nThis one is important. TLS 1.3 provides much faster initial connection and 0-RTT (check later).\nSpeed \\ Optimization \\ Content Optimization Brotli Turn it ON. It does what it says.\nCloudflare Fonts It optimize font loading times for custom fonts. I don\u0026rsquo;t have them, so I don\u0026rsquo;t care. If you use custom fonts, you can try it, but measure if there\u0026rsquo;s any difference.\nEarly Hints Turn it ON. Rocket Loader Play with it :) On one page it was resulting in faster load times, but on my blog it made load times actually higher.\nAuto Minify Play with it :) It minifies text files by removing spaces, etc. As it\u0026rsquo;s done on the fly, it can slow down page load times for the first time. Follow up request should be cached by Cloudflare.\nIn my case, I minify all the files during the content generation, so I just don\u0026rsquo;t need it.\nCaching \\ Configuration Caching Level You can play with two other options on dynamically generated pages to avoid caching dynamic content.\nFor static page, Standard is fine. Browser Cache TTL It\u0026rsquo;s first place where we can enforce strong caching of our page in the user\u0026rsquo;s browser. I don\u0026rsquo;t like this option because it treat all the type files the same.\nCrawler Hints It won\u0026rsquo;t impact your page performance, but allows Cloudflare to share some of your page access statistics with Bing (which shares data with DuckDuckGo too)\nM$ call this feature IndexNow\u0026thinsp; external link .\nCaching \\ Cache Rules This is where the game starts!\nMost recommendations advice to configure Page rule and play around with Cache Level2 or Browser Cache TTL3 options. The problem is that in free version you can only have 3 rules and I already use one for redirection from HTTP to HTTPS.\nOther con of using Page rules is the way wildcard matching works there. It\u0026rsquo;s possible to use a star sign (*), to match multiple URLs. But to create rule matching image files like: JPG, JPEG, PNG, WEBP - I will need 4 rules. To handle CSS/JS and more, I will need even more. Dead end.\nWhat I wanted to achieve. My page don\u0026rsquo;t change too often\nAnd Github Pages sets cache TTL for images to only 10 days, it also do not set any cache for HTML/JS/CSS files. Github is quite reliable provider, but to improve world wide load times it\u0026rsquo;s nice to spread cache around the world and avoid requests to origin at all cost. Pages should be cached in the Cloudflare and served directly from there.\nMy target would be to:\ncache pages and static files in Cloudflare for 1 day, set browser cache for: HTML pages to 1 day CSS/JS to maybe 10-30 days (they don\u0026rsquo;t change frequently and each build provides new file) images of all sort 3-6 months (they don\u0026rsquo;t change at all, but are relatively big and it\u0026rsquo;s good to serve them from the edge locations) I found that Caching -\u0026gt; Cache Rules options, which allow to better tune the way cache works in Cloudflare. Even more, I can add 10 rules for free and within those rules I can use OR operator to make single rule to match multiple patterns. Perfect!\nCaching images rule Let tackle image caching first.\nOn specific domain, go to Caching-\u0026gt;Cache rules Create a new Cache rule. Under When incoming requests match… add: Field: URI Path Operator: ends with Value: .webp add another rule and repeat for: .jpg, .jpeg, .png, .ico and whatever more you need Expression Preview should be showing: (ends_with(http.request.uri.path, \u0026#34;.webp\u0026#34;)) or (ends_with(http.request.uri.path, \u0026#34;.png\u0026#34;)) or (ends_with(http.request.uri.path, \u0026#34;.jpg\u0026#34;)) or (ends_with(http.request.uri.path, \u0026#34;.jpeg\u0026#34;)) or (ends_with(http.request.uri.path, \u0026#34;.ico\u0026#34;)) Under Then\u0026hellip; block set Cache eligibility -\u0026gt; Eligible for cache set Edge TTL -\u0026gt; Ignore cache-control header and use this TTL under Input time-to-live (TTL) dropdown select 1 day (or more) set Browser TTL -\u0026gt; Override origin and use this TTL under Input time-to-live (TTL) dropdown select 6 months (that\u0026rsquo;s minimal to make to stop https://pagespeed.web.dev/\u0026thinsp; external link from complaining) enable Serve stale content while revalidating Hit Deploy button Check my config below: What it does? Matching URI\u0026rsquo;s by extension should be clear, then we enable cache, which we want too. Now is the interesting part.\nEdge TTL - by setting this option, we enforce edge Cloudflare servers to ignore headers returned from Github pages and store matching responses for 1 day. I could probably go with 7 or 30 days, but if I eventually change any image I won\u0026rsquo;t need to purge cache. Cloudflare will just keep files for 1 day on it\u0026rsquo;s proxies and revalidate them next day. Browser TTL - above we configure \u0026ldquo;the server side\u0026rdquo; cache. Here we tell our users browsers how to treat files from our page. At least for images it makes sense to set pretty long caching time. Google\u0026rsquo;s Page Speed test stopped complaining about too short caching time after I set it up to 6 months. Serve stale content while revalidating - my page is static. Content changes infrequently and when it does, it\u0026rsquo;s not critical if change will became visible slightly later. But with this option enabled Cloudflare will server whatever he have in their cache and if it\u0026rsquo;s expired, then it will fetch it and update cache. What\u0026rsquo;s important - user don\u0026rsquo;t need to wait. This changes time to first byte from around 1~1.5s to 0.1s. Caching HTML/CSS/JS files Caching text files differs from images. With images you want to cache them for a long time as they don\u0026rsquo;t change. If they do, they change spectacularly, usually with the new URL.\nChanges to text documents happen more often, especially on a blog - small update, fixing some typos, etc. You want them to be visible fast. Not necessarily immediately, but fast.\nLet build another rule. With CSS and JS files it\u0026rsquo;s easy - we match the end of URI Path with their extensions. Done.\nWith HTML files it\u0026rsquo;s harder, because not all page URLs end with HTML extension. It\u0026rsquo;s also not possible to match rule by Content Type (which is such a pity). But we can match a Request Header. We can use Accept header\u0026thinsp; external link as a hook, because when browsers request HTML files they usually do it like that:\nAccept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8 We can then match Accept Request Header if it contains text/html - that would be quite probable HTML file \u0026#x1f604;\nWarning\nI wouldn\u0026rsquo;t use it on any dynamically generated, production site without extensive testing. In my case a static site is safe to assume those are HTML files and nothing else.\nLet take a look how does it look completely: We can deploy the rule now.\nSummary Those are all options I use in the Cloudflare for my static pages. There are many more variations on the topic, like CLoudflare Pages\u0026thinsp; external link which might be nice alternative to Github Pages and could further improve loading speed.\nIt\u0026rsquo;s good to have a small site or blog to play around before you will turn it all ON.\nUse services like:\nPage Speed\u0026thinsp; external link SSL Server Test\u0026thinsp; external link To verify the real impact of your changes. Having RUM monitoring\u0026thinsp; external link might be even better.\nGood luck and sub zero loading times for your pages!\nEnjoyed? https://developers.cloudflare.com/ssl/edge-certificates/additional-options/http-strict-transport-security/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developers.cloudflare.com/rules/page-rules/reference/recommended-rules/#security-level-and-cache-level\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://developers.cloudflare.com/rules/page-rules/reference/recommended-rules/#edge-cache-ttl-and-browser-cache-ttl\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/maximizing-page-performance-and-security-with-cloudflare-tuning/","summary":"Info\nI use free tier of Cloudflare and all recommendations assume only those are available.\nWhy to cache statically generated blog? My Blog is statically generated website served from Github Pages\u0026thinsp; external link . As Github don\u0026rsquo;t allow to easily set my own domain (at least in free version), I needed some kind of proxy that:\ncan serve page from my domain, will provide valid certificate for HTTPS. Info","title":"Maximizing page performance and security with Cloudflare tuning"},{"content":"Almost 2 years ago I was promoted to a Team Lead position. Since then at least half of my time is dedidcated to \u0026ldquo;the people\u0026rsquo;s stuff\u0026rdquo; \u0026#x1f604;\nOn one side, I have less time to code and write about technical stuff, which sometimes makes me feel guilty (just a little bit). On the other side, I\u0026rsquo;m discovering a whole new world of possibilities to solve technical problems without touching the code (almost).\nAlthough coding and leadership are completely different topics and I feel like I\u0026rsquo;m not good enough to teach about leadersihp, I didn\u0026rsquo;t create this blog to teach.\nIn it\u0026rsquo;s beginning, my idea was to write about topics, techniques or tools, that made my life easier. Stuff that I don\u0026rsquo;t want to forget. If by any chance it could be useful to anyone - that\u0026rsquo;s just additional benefit. If I make mistakes publicly, I hope someone will let me know in comments. Those assumptions still apply today, to both tech and leadership stuff.\nThen, it\u0026rsquo;s decided. I will start writing about leadership, hiring or soft skills, which are just a managers tools.\n","permalink":"https://gagor.pro/2024/01/new-years-reflections-and-leadership-path/","summary":"Almost 2 years ago I was promoted to a Team Lead position. Since then at least half of my time is dedidcated to \u0026ldquo;the people\u0026rsquo;s stuff\u0026rdquo; \u0026#x1f604;\nOn one side, I have less time to code and write about technical stuff, which sometimes makes me feel guilty (just a little bit). On the other side, I\u0026rsquo;m discovering a whole new world of possibilities to solve technical problems without touching the code (almost).","title":"New Year's reflections and Leadership path"},{"content":"I\u0026rsquo;m a fan of Getting Things Done1 methodology and I recommend to read the book of the same title to anyone. I know, it looks like a typical, american corpo bull****, but it\u0026rsquo;s not! Wheter you\u0026rsquo;re a busy manager or a father of three - it might help you to manage things you have to do, on time and with less stress.\nI\u0026rsquo;ve been trying multiple apps to support this methodology, but eventually I stick to Google Tasks app 2. It\u0026rsquo;s simple, not to say primitive, but it integrates nicely with Gmail and Calendar and just works. At least on mobile, experience is ok. On desktop it runs as a small extension to Gmail and this sucks.\nI found a way to run it in a separate tab, which makes it much easier to review and reorganise tasks.\nJust add a bookmark in your browser (Ctry+D or Cmd+D) and call it Tasks Edit it and set URL address to: https://tasks.google.com/embed/?origin=https://mail.google.com\u0026amp;fullWidth=1\u0026amp;amp;lfhs=2 That\u0026rsquo;s it. You have now a dedicated button, that will open the Tasks in a separate browser\u0026rsquo;s tab.\nhttps://en.wikipedia.org/wiki/Getting_Things_Done\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://play.google.com/store/apps/details?id=com.google.android.apps.tasks\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2024/01/how-to-run-google-tasks-in-separate-browser-tab/","summary":"I\u0026rsquo;m a fan of Getting Things Done1 methodology and I recommend to read the book of the same title to anyone. I know, it looks like a typical, american corpo bull****, but it\u0026rsquo;s not! Wheter you\u0026rsquo;re a busy manager or a father of three - it might help you to manage things you have to do, on time and with less stress.\nI\u0026rsquo;ve been trying multiple apps to support this methodology, but eventually I stick to Google Tasks app 2.","title":"How to run Google Tasks in separate browser tab?"},{"content":"Zwift runs by default in windowed mode, with ugly start menu at bottom and title bar. It\u0026rsquo;s not a big deal, just something irritating. It might be as silly as funny, but I wasn\u0026rsquo;t able to find a good answer for it. There are instructions but, where the heck is \u0026ldquo;Settings\u0026rdquo; button? \u0026#x1f604;\nEventually, accidentaly I found it! So let me share, with pictures \u0026#x1f60e;\nFind in top-right corner icon with your points and click it From menu, choose \u0026ldquo;My settings\u0026rdquo; In new window, choose \u0026ldquo;Sound \u0026amp; Display\u0026rdquo; tab There it is! You can change \u0026ldquo;Screen Mode\u0026rdquo; -\u0026gt; \u0026ldquo;Full Screen\u0026rdquo; There are few more useful options. I perfer to ride with my playlist from Spotify so I disabled \u0026ldquo;Title Music\u0026rdquo; too.\nRide On in full screen mode!\n","permalink":"https://gagor.pro/2023/12/how-to-run-zwift-in-full-screen/","summary":"Zwift runs by default in windowed mode, with ugly start menu at bottom and title bar. It\u0026rsquo;s not a big deal, just something irritating. It might be as silly as funny, but I wasn\u0026rsquo;t able to find a good answer for it. There are instructions but, where the heck is \u0026ldquo;Settings\u0026rdquo; button? \u0026#x1f604;\nEventually, accidentaly I found it! So let me share, with pictures \u0026#x1f60e;\nFind in top-right corner icon with your points and click it From menu, choose \u0026ldquo;My settings\u0026rdquo; In new window, choose \u0026ldquo;Sound \u0026amp; Display\u0026rdquo; tab There it is!","title":"How to run Zwift in full screen"},{"content":" 48 praw władzyAuthor: Robert Greene\nempik.com Still reading\u0026hellip;\n","permalink":"https://gagor.pro/books/2023/48-praw-wladzy/","summary":"48 praw władzyAuthor: Robert Greene\nempik.com Still reading\u0026hellip;","title":"48 praw władzy"},{"content":"I\u0026rsquo;ve been thinking about switching comments system to something different on my blog. Since I moved to Hugo\u0026thinsp; external link , I used Disqus\u0026thinsp; external link . Integration was easy and it was also easy to move comments from Wordpress. Maybe the move was easy, but people somehow didn\u0026rsquo;t like the interface. I receive 6 comments since I switched from Wordpress 3 years ago \u0026#x1f604; To be honest, I don\u0026rsquo;t like it too.\nI was thinking about switch to something else, something different, but I was waiting for this punch. Until recently Disqus went too far and started showing adds on my page!\nI like in Hugo the fact that my blog have no infrastructure to handle. Despite the domain and two repos, I don\u0026rsquo;t need to keep any servers running. I want to keep it like that. There are two solutions fulfilling this demand and appeal to me:\nUtterances\u0026thinsp; external link 1\nRelies on Github Issues as a storage for comments data and Github auth for login. It\u0026rsquo;s both crazy and awesome idea at the same time. Auth via Github is fine on my technology related blog. The only thing is, that it\u0026rsquo;s quite odd to use Issues for comments and discussions\u0026hellip;\nGiscus\u0026thinsp; external link And that\u0026rsquo;s where the second app steps in. It uses recent Discussions API to store comments. I\u0026rsquo;d feel it\u0026rsquo;s a better place, but utterances is better tested.\nI planned to start with Giscus first, but I accidentally configured the other one \u0026#x1f923; Utterances looks and feels good and now I\u0026rsquo;m not sure if I will try Giscus.\nhttps://www.pugillum.com/posts/2021_11_28_github_issues_as_comments/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2023/12/changing-comments-provider-for-my-blog/","summary":"I\u0026rsquo;ve been thinking about switching comments system to something different on my blog. Since I moved to Hugo\u0026thinsp; external link , I used Disqus\u0026thinsp; external link . Integration was easy and it was also easy to move comments from Wordpress. Maybe the move was easy, but people somehow didn\u0026rsquo;t like the interface. I receive 6 comments since I switched from Wordpress 3 years ago \u0026#x1f604; To be honest, I don\u0026rsquo;t like it too.","title":"Changing comments provider for my blog"},{"content":"Today, I was trying to pull/push repos from Github and I was getting timeout errors. I use SSH for clonning and I prefer it this way over HTTPS.\nI was looking for the reason but it felt like a temporary glitch, either on Github\u0026rsquo;s or my provider\u0026rsquo;s side. I was googling for anything, that could help me and I found1 the way to use Github ssh clone/push/pull via SSH (as I want), but via port 443\u0026hellip; Simulating HTTPS traffic\u0026hellip; OK\u0026hellip;\nInfo\nThat\u0026rsquo;s actually a neat hack to pass firewall blocking SSH access, which is often practice in hotel\u0026rsquo;s wifi or other public hot spots.\nFirst, you have to add this to your ~/.ssh/config file:\n~/.ssh/config Host github.com Hostname ssh.github.com Port 443 Test if you can reach Github:\nQuick test ssh -T git@github.com Now you can push, pull and clone as usual 😎\nhttps://stackoverflow.com/a/52817036\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2023/12/use-github-with-ssh-on-port-443/","summary":"Today, I was trying to pull/push repos from Github and I was getting timeout errors. I use SSH for clonning and I prefer it this way over HTTPS.\nI was looking for the reason but it felt like a temporary glitch, either on Github\u0026rsquo;s or my provider\u0026rsquo;s side. I was googling for anything, that could help me and I found1 the way to use Github ssh clone/push/pull via SSH (as I want), but via port 443\u0026hellip; Simulating HTTPS traffic\u0026hellip; OK\u0026hellip;","title":"Use Github with SSH on port 443"},{"content":" Jak zdobyć przyjaciół i zjednać sobie ludziAuthor: Carnegie Dale\nempik.com Książka pisana w typowo amerykańskim stylu - \u0026ldquo;rób tak, a tak, a twoje życie odmieni się na zawsze\u0026hellip;\u0026rdquo; \u0026#x1f604;\nZbyt dosadne stosowanie reguł z książki może trącić manipulacją i nasz rozmówca wyczuje to. Można natomiast zastanowić się nad pewnymi błędami, które popełniając utrudniają nam kontakty z ludźmi, np. gdy to my mówimy zbyt dużo i nie dajemy innym przebić się do głosu.\nTak czy siak, uważam że książka jest o manipulacji. Owszem, jeżeli zastosujemy zasady szczerze, to w danej sytuacji może to poprawić kontakty. Jeżeli jednak zaczniemy używać pewnych technik automatycznie, to możemy wpływać na innych \u0026ldquo;intensywniej\u0026rdquo; niż wcześniej, jakbyśmy obchodzili ich firewalla \u0026#x1f604;\nTak czy siak, czytało się przyjemnie więc pozwoliłem sobie wynotować reguły, by móc czasem zerknąć na nie i sobie przypomnieć \u0026#x1f642;\nPodstawowe zasady w kontaktach z ludźmi Nie krytykuj, nie potępiaj i nie pouczaj.\nZamiast potępiać ludzi, spróbujmy ich zrozumieć. Zastanówmy się, dlaczego robią to, co robią. To korzystniejsze, rodzi sympatię, tolerancję i uprzejmość.\nSzczerze i uczciwie wyrażaj uznanie.\nPrzestań myśleć o własnych dokonaniach i własnych życzeniach. Spróbuj dostrzec zalety drugiego człowieka. Zapomnij o pochlebstwach, szczerze i uczciwie dawaj wyraz uznania.\nWzbudź w innych szczere chęci.\nKiedy wpadniemy na wspaniały pomysł, zamiast podkreślać na każdym kroku, że jest nasz, spróbujmy pozwolić innym, by uznali go za własny. Będzie im się wtedy podobał i być może zjedzą kilka porcji.\nSześć sposobów, które sprawią, że ludzie będą cię lubić Okazuj ludziom szczere zainteresowanie.\n\u0026ldquo;Interesujemy się innymi, kiedy oni interesują się nami\u0026rdquo;. Okazanie zaitneresowania - musi być szczere. Musi opłacać się nie tylko temu, kto zainteresowanie okazuje, ale i temu, kto jest jego obiektem. To działa w obie strony i obie na tym zyskują.\nUśmiechaj się.\n\u0026ldquo;Człowiek, który się nie uśmiecha, nie może otworzyć sklepu\u0026rdquo;. Twój uśmiech jest posłańcem twojej dobrej woli. Twój uśmiech rozjaśnia życie wszystkich, którzy go widzą. Dla kogoś, kto ogląda dziesiątki ponurych i zachmurzonych twarzy odwracających wzrok, twój uśmiech to promień słońca przebijający się przez chmury. Zwłaszcza gdy ten ktoś jest właśnie nękany przez szefa, klientów, nauczycieli, rodziców lub dzieci, twój uśmiech przypomini mu, że nie wszystko jest beznadziejne, że na świecie jest jeszcze radość.\nPamiętaj, że własne nazwisko to dla człowieka najsłodsze i najważniejsze spośród wszystkich słów świata.\nPowinniśmy zdawać sobie sprawę ze znaczenia imienia i nazwiska i zawsze pamietać, że są one święte i stanowia wyłączną własność człowieka, z którym rozmawiamy. Niczyją inną! Nazwisko i imię odróżnia jednostkę od innych, czyni ją jedyną pośród wszystkich ludzi. Informacja, której udzielamy, lub przedstawiana przez nas prośba nabierają specjalnej wagi, jeśli dodamy do nich nazwisko i imię naszego rozmówcy. Od kelnerki po prezesa - zapamiętane nazwisko uczyni cuda w naszych kontaktach z nimi.\nBądź dobrym słuchaczem. Zachęcaj rozmówcę, aby mówił o sobie samym.\nJeśli chcesz być dobrym rozmówcą, bądź uważnym słuchaczem. Interesuj się innymi, a wydasz się im interesujący. Zadawaj takie pytania, na które rozmówca odpowie z przyjemnością. Zachęcaj rozmówcę do mówienia o nim samym i jego dokonaniach. Pamiętaj, że ludzie, z którymi rozmawiasz, stokroć bardziej przejęci są sobą i swoimi problemami niż tobą i sprawami, które cię trapią.\nMów o tym, co interesuje twojego rozmówcę.\nRozmawianie z ludźmi w kategoriach tego, co interesuje raczej ich niż nas, opłaca się obu stronom.\nSpraw, aby twój rozmówca poczuł się ważny - i zrób to szczerze.\n\u0026ldquo;Mów ludziom o nich samych. Mów o nich samych, a będą cię słuchać godzinami\u0026rdquo;\nJak przekonać innych do Twojego sposobu myślenia Jedyny sposób, aby zwyciężyć w kłótni, to unikać jej.\nRadośnie witaj różnicę zdań. Nie reaguj instynktownie. Pohamuj swój temperament. Najpierw słuchaj. Szukaj punktów wspólnych. Bądź uczciwy. Obiecaj oponentowi, że przemyślisz jego pogląd i dokładnie go przeanalizujesz. Szczerze podziekuj oponentowi za rozmowę. Daj sobie i oponentowi czas na dokładne przemyślenie sprawy. Okaż szacunek dla poglądów rozmówcy. Nigdy nie mów mu: \u0026ldquo;Nie masz racji\u0026rdquo;.\nInnymi słowy, nie spieraj się ani z klientem, anie ze współmałżonkiem, ani z wrogiem. Nie mów im, że nie mają racji, nie sprawiaj, aby się najeżyli. Stosuj choć odrobinę dyplomacji.\nJeśli nie masz racji, przyznaj to szybko i bardzo wyraźnie.\nProszę rozważyć również i fakt, że sam nie całkiem się z tym zgadzam. Nie wszystko, co napisałem wczoraj, przemawia do mnie z taką samą siłą\u0026hellip;\nZacznij od okazania przyjaźni.\nSłońce potrafi szybciej sprawić, że zdejmiesz płaszcz, niz wiatr. A uprzejmość, przyjazne podejście i wyrazy uznania mogą skłonić ludzi do zmiany zdania szybciej niz wszystkie burze i wichury świata.\nJak najprędzej wydobądź z rozmówcy liczne \u0026ldquo;tak\u0026rdquo;.\n\u0026hellip; Zadawał pytania, na które jego rozmówca musiał odpowiedzieć twierdząco. I zdobywał odpowiedzi twierdzące jedną za drugą, aż miał za sobą całą górę przytaknięć. Zadawał pytania tak długo, dopóki jego przeciwnik, nie zdając sobie z tego sprawy, nie doszedł do konkluzji, przeciwko której gwałtownie protestowałby jeszcze kilka minut wcześniej.\nPozwól rozmówcy się wygadać.\nNawet przyjaciele wolą opowiadać nam o swoich osiągnięciach, niż słuchać, jak przechwalamy się własnymi.\nDaj rozmówcy myśleć, że twoja idea wyszła od niego.\nSzczerze próbuj przyjmować punkt widzenia twojego rozmówcy.\nJeżeli lektura tej książki sprawi choćby tyle, że zawsze będziesz się starał rozpatrywać sprawy z punktu widzenia drugiej osoby\u0026hellip; szybko przekonasz się, że stanie się ona punktem zwrotnym\u0026hellip;\nOkaż zrozumienie i współczuj z myślami i pragnieniami rozmówcy.\n\u0026ldquo;Cały rodzaj ludzki pożąda współczucia i zrozumienia.\u0026rdquo;\nOdwołaj się do szlachetności rozmówcy.\n\u0026hellip; ludzie są uczciwi i chcą wywiązywać się ze swoich zobowiązań\u0026hellip; nawet ci, którzy chcieliby nas zrobić w konia, w większości przypadków zareagują uczciwie, jeśli damy im do zrozumienia, za uważamy ich za ludzi szczerych, porządnych i uczciwych.\nUdramatyzuj swoje poglady. Nie tylko dane, ale i sposób ich zaprezentowania ma znaczenie.\nRzuć człowiekowi wyzwanie. To właśnie kocha każdy człowiek sukcesu: hazard. Szansę na pokazanie własnego ja. Szansę na udowodnienie swojej własnej wartości, szansę na współzawodnictwo i na wygraną.\nBądź przywódcą: Jak zmieniać ludzi, nie zrażając ich i nie zniechęcając Jeśli już musisz wytknąć komuś błąd\u0026hellip; Zacznij od szczerej pochwały i uznania.\nWytykaj innym błędy tylko pośrednio.\nZasugeruj inne użycie, zaproponuj spojrzenie w lustro i samodzielne podjęcie decyzji.\nZanim skrytykujesz innych, przyznaj się do własnych błędów.\nNikt nie lubi wykonywać rozkazów. Zadawaj pytania zamiast wydawać rozkazy.\nPozwól rozmówcy zachować twarz.\nLiczy się nie to, co ja myślę o nim, lecz to, co on myśli o sobie samym.\nPochwal najmniejsze nawet osiągnięcie. Pochwal każdy postęp. Bądź \u0026ldquo;serdeczny w aprobacie i nie skąp pochwał\u0026rdquo;.\nWystaw ludziom dobrą opinię, której będą musieli sprostać.\nZachęcaj innych do poprawy. Spraw, aby uwierzyli, że mogą się zmienić.\nSpraw, aby rozmówca z przyjemnością zrobił to, czego od niego oczekujesz.\nPokaż korzyści płynące z wykonania zadania dla wykonującego, by zmniejszyć jego opór.\n","permalink":"https://gagor.pro/books/2023/jak-zjednac-sobie-ludzi/","summary":"Jak zdobyć przyjaciół i zjednać sobie ludziAuthor: Carnegie Dale\nempik.com Książka pisana w typowo amerykańskim stylu - \u0026ldquo;rób tak, a tak, a twoje życie odmieni się na zawsze\u0026hellip;\u0026rdquo; \u0026#x1f604;\nZbyt dosadne stosowanie reguł z książki może trącić manipulacją i nasz rozmówca wyczuje to. Można natomiast zastanowić się nad pewnymi błędami, które popełniając utrudniają nam kontakty z ludźmi, np. gdy to my mówimy zbyt dużo i nie dajemy innym przebić się do głosu.","title":"Jak zdobyć przyjaciół i zjednać sobie ludzi"},{"content":" Lider w trampkachCzyli jak być przywódcą w zgodzie ze sobą\nAuthor: Małgorzata Jakubicz\nempik.com ","permalink":"https://gagor.pro/books/2023/lider-w-trampkach/","summary":" Lider w trampkachCzyli jak być przywódcą w zgodzie ze sobą\nAuthor: Małgorzata Jakubicz\nempik.com ","title":"Lider w trampkach"},{"content":" Taka praca nie ma sensuCzyli jak zarządzać swoją energią\nAuthor: Tony Schwartz\namazon.plempik.com To ciekawa książka, którą o dziwo przeczytałem do końca. Miałem wrażenie, że trochę przeskakuje z tematu na temat, ale pasuje mi to. Czytałem wiele książek szczegółowo opisujących \u0026ldquo;ten jeden sposób\u0026rdquo; by rozwiązać wszystkie problemy świata. Ta kompletuje różne pomysły i idee, miksuje i całkiem przyjemnie uogólnia.\nKsiążka nie była dla mnie wielkim odkryciem, ale przjemnie było sobie przypomnieć pewne pomysły.\n","permalink":"https://gagor.pro/books/2023/taka-praca-nie-ma-sensu/","summary":"Taka praca nie ma sensuCzyli jak zarządzać swoją energią\nAuthor: Tony Schwartz\namazon.plempik.com To ciekawa książka, którą o dziwo przeczytałem do końca. Miałem wrażenie, że trochę przeskakuje z tematu na temat, ale pasuje mi to. Czytałem wiele książek szczegółowo opisujących \u0026ldquo;ten jeden sposób\u0026rdquo; by rozwiązać wszystkie problemy świata. Ta kompletuje różne pomysły i idee, miksuje i całkiem przyjemnie uogólnia.\nKsiążka nie była dla mnie wielkim odkryciem, ale przjemnie było sobie przypomnieć pewne pomysły.","title":"Taka praca nie ma sensu"},{"content":" Collaborating with the EnemyHow to Work with People You Don\u0026rsquo;t Agree with or Like or Trust\nAuthor: Adam Kahane\namazon.com In this book the author, Adam Kahane, presents several key concepts to help individuals navigate collaboration with people they may not agree with, like, or trust. Here are some of the main concepts from the book:\nEmbracing Discomfort: Kahane emphasizes the importance of stepping outside one\u0026rsquo;s comfort zone when working with people with differing perspectives. Embracing discomfort allows individuals to open up to new ideas and find common ground.\nActive Listening: The book stresses the significance of active listening as a crucial skill for effective collaboration. By truly understanding the concerns and motivations of others, individuals can build trust and find areas of agreement.\nCommon Ground: Kahane explores the idea of identifying common ground, even in the most challenging situations. Finding shared interests or goals can serve as a foundation for collaboration and problem-solving.\nInnovation Through Conflict: The book challenges the notion that conflict is always detrimental. Instead, Kahane suggests that constructive conflict can lead to innovation and creative solutions when approached with an open mind.\nAdaptive Leadership: Kahane introduces the concept of adaptive leadership, which involves adjusting one\u0026rsquo;s approach based on the evolving dynamics of a situation. This adaptability is crucial when dealing with complex and changing collaborative environments.\nReal-World Examples: Throughout the book, Kahane illustrates his concepts with real-world examples and case studies, making the principles tangible and applicable to various professional and personal contexts.\nI don\u0026rsquo;t see this book as a big discover, although it surprised me by stating that \u0026ldquo;sometimes you can\u0026rsquo;t reach agreement\u0026rdquo;. If sides do now will to cooperate, enforced agreement won\u0026rsquo;t be respected. The book serves as a valuable guide for those looking to navigate collaboration effectively in diverse and dynamic environments.\n","permalink":"https://gagor.pro/books/2023/collaborating-with-the-enemy/","summary":"Collaborating with the EnemyHow to Work with People You Don\u0026rsquo;t Agree with or Like or Trust\nAuthor: Adam Kahane\namazon.com In this book the author, Adam Kahane, presents several key concepts to help individuals navigate collaboration with people they may not agree with, like, or trust. Here are some of the main concepts from the book:\nEmbracing Discomfort: Kahane emphasizes the importance of stepping outside one\u0026rsquo;s comfort zone when working with people with differing perspectives.","title":"Collaborating with the Enemy"},{"content":" Legendarna obsługa klientaTroska jest najważniejsza\nAuthors: Blanchard Ken, Halsey Vicki, Cuff Kathy\nempik.com ","permalink":"https://gagor.pro/books/2023/legendarna-obsluga-klienta/","summary":" Legendarna obsługa klientaTroska jest najważniejsza\nAuthors: Blanchard Ken, Halsey Vicki, Cuff Kathy\nempik.com ","title":"Legendarna obsługa klienta"},{"content":" The Effective Manager: Second EditionCompletely Revised and Updated\nAuthors: Mark Horstman, Kate Braun, Sarah Sentes\namazon.pllubimyczytac.plgoodreads.com The Effective Manager by Mark Horstman is a practical guide to great management at every level. Based on the author\u0026rsquo;s experience and the world\u0026rsquo;s number-one business podcast, Manager Tools, the book teaches four critical behaviors and four major tools that every manager should use. Here are some of the main concepts presented in the book:\nOne-on-ones: A weekly 30-minute meeting with each direct report to build trust, communication, and performance. The agenda is simple: 10 minutes for the direct report, 10 minutes for the manager, and 10 minutes for coaching or feedback. Feedback: A way to communicate how the direct report\u0026rsquo;s behavior affects the team\u0026rsquo;s results. The feedback model is: ask, tell, encourage or discourage. For example: \u0026ldquo;Can I give you some feedback? When you deliver your reports on time, it makes our team look more professional. Keep it up.\u0026rdquo; Coaching: A process to help the direct report improve their skills and achieve their goals. The coaching model is: collaborate, ask, tell, commit. For example: \u0026ldquo;Let\u0026rsquo;s talk about your goal of becoming a senior analyst. What are some of the skills you need to develop? How can I help you with that? What are you going to do next? When can we follow up on your progress?\u0026rdquo; Delegation: A technique to assign tasks and responsibilities to the direct report, while maintaining accountability and quality. The delegation model is: describe, clarify, confirm, monitor. For example: \u0026ldquo;I need you to prepare a presentation for the client meeting next week. It should include the following points: \u0026hellip; Do you have any questions? What are the next steps? How will I know you\u0026rsquo;re on track?\u0026rdquo; The best about this books is, that it provides step by step explanation of what to do, how to start, etc. Especially as new people manager, you just might not know what might work and what might not - here you will find it. There are things that hit me by it\u0026rsquo;s simplicity, for example: as a leader you\u0026rsquo;re not a part of the team - you can impact how much people earn or fire them, it\u0026rsquo;s not equal partnership and they know it. It\u0026rsquo;s straight, simple and honest. That\u0026rsquo;s how you also feel about your boss. You might be in good relations, but there still will be things you won\u0026rsquo;t say your boss. Remember about it sitting on the other side and don\u0026rsquo;t be naive expecting that people will ignore your recent promotion. It might take some time, but they will figure it out.\nSecond edition have some updates, especially on the remote work. Must have if you manage people!\n","permalink":"https://gagor.pro/books/2023/the-effective-manager-second-edition/","summary":"The Effective Manager: Second EditionCompletely Revised and Updated\nAuthors: Mark Horstman, Kate Braun, Sarah Sentes\namazon.pllubimyczytac.plgoodreads.com The Effective Manager by Mark Horstman is a practical guide to great management at every level. Based on the author\u0026rsquo;s experience and the world\u0026rsquo;s number-one business podcast, Manager Tools, the book teaches four critical behaviors and four major tools that every manager should use. Here are some of the main concepts presented in the book:","title":"The Effective Manager: Second Edition"},{"content":" The Effective ManagerAuthor: Mark Horstman\namazon.pl I\u0026rsquo;ve borrowed this one and I liked it so I bought Second Edition . Look for summary there.\n","permalink":"https://gagor.pro/books/2023/effective-manager/","summary":"The Effective ManagerAuthor: Mark Horstman\namazon.pl I\u0026rsquo;ve borrowed this one and I liked it so I bought Second Edition . Look for summary there.","title":"The Effective Manager"},{"content":"TL;DR While reasoning is important, readers may not be interested in all the frustrations I experienced while figuring out how to get things done. If you\u0026rsquo;re looking for a quick solution, skip to the \u0026ldquo;What eventually worked?\u0026rdquo; section. However, if you\u0026rsquo;re interested in the thought process behind the solution, keep reading.\nWhy? Some might bother why the hell I\u0026rsquo;d like to make my life so hard? \u0026#x1f923;\nWe used to use nodeenv\u0026thinsp; external link for that purpose. It provides a simple script that allows to fetch any version of Node. You have to configure PATH variable and you\u0026rsquo;re done. It\u0026rsquo;s very simple from the perspective of Docker images operator. There\u0026rsquo;s one problem with nodeenv comparing it to the nvm - popularity.\nnodeenv nvm I was explaining to a lot of people, how to use nodeenv. No one knew it! But many of them knew nvm and were asking: Why can\u0026rsquo;t I just install version of Node that my project needs with NVM? That\u0026rsquo;s how I started thinking about better ways to deal with Node version management.\nI\u0026rsquo;m supporting organization with thousands of projects. Node projects can be counted in hundreds. It\u0026rsquo;s not possible to provide up to date base images for all the Node versions that people would request. But I can provide a base, which would allow to install any Node version you need on up to date base images. That\u0026rsquo;s what this article is about.\nNVM is nice and simple, how hard it might be to get it working with CI/CD? Over the past few days, I\u0026rsquo;ve been working on providing nvm\u0026thinsp; external link for both my company\u0026rsquo;s Docker base images and for CI/CD. However, it hasn\u0026rsquo;t been an easy task.\nIn general, nvm (know as Node Version Manager) is loved by frontend developers because it allows them to drop a .nvmrc file in their project, and each time they switch between projects, everything works seamlessly. nvm is responsible for installing (or activating) the version of Node required by a specific project. A similar functionality is provided by nodeenv\u0026thinsp; external link , which is Python-based and based on virtualenv\u0026thinsp; external link .\nWhile setting up nodeenv is straightforward because it\u0026rsquo;s a regular CLI command, nvm is not. When you follow nvm\u0026rsquo;s installation instructions1, you end up with something like the following code in one of the startup files (~/.bashrc, ~/.profile, or ~/.zshrc):\nTypical nvm installation in ~/.bashrc export NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/bash_completion\u0026#34; Installing nvm involves \u0026ldquo;sourcing\u0026rdquo; (that\u0026rsquo;s what . do) nvm.sh with some shell functions into your current shell. This is not a big deal for your local machine, but it becomes an issue if you want to use nvm in non-interactive shells for your CI/CD.\nLet\u0026rsquo;s take bash as an example. Suddenly, it becomes important to know which file you\u0026rsquo;ve put the nvm lines in because not all of them are loaded for non-interactive shells2. The startup files for bash (in order) are:\nInteractive (called with --login) /etc/profile ~/.bash_profile ~/.bash_login ~/.profile Non-interactive ~/.bashrc It\u0026rsquo;s worth noting that ~/.profile usually loads ~/.bashrc automatically (check yours). But what about sh, dash or zsh? If you\u0026rsquo;re calling nvm on Jenkins, what shell will it call? These varieties of combinations make it incredibly hard to make nvm behave consistently.\nTo better understand my use case, I use:\nJenkins\u0026thinsp; external link for CI/CD Which runs as a Docker container on a K8S cluster The container contains most of the tools needed by developers (but just single LTS version of Node). Getting it working in the Dockerfile I eventually found a nice way to make nvm work in a Dockerfile - by using the SHELL directive\u0026thinsp; external link . By default, it\u0026rsquo;s set to: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;].\nThis shell does not load the files we need and starts a non-interactive shell. We can fix it with:\nEnforcing interactive shell in the Dockerfile SHELL [\u0026#34;/usr/bin/bash\u0026#34;, \u0026#34;--login\u0026#34;, \u0026#34;-c\u0026#34;] The whole Dockerfile might look like this: Example Dockerfile FROM ubuntu:22.04 as fetcher ENV NVM_VERSION v0.39.3 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y git \u0026amp;\u0026amp; \\ git clone \\ --depth 1 \\ --branch $NVM_VERSION \\ https://github.com/nvm-sh/nvm.git FROM ubuntu:22.04 # we don\u0026#39;t want to store cached files in the image VOLUME /var/cache/apt # prerequisites RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y curl SHELL [\u0026#34;/bin/bash\u0026#34;, \u0026#34;--login\u0026#34;, \u0026#34;-c\u0026#34;] ENV NVM_DIR=/opt/nvm # copy the nvm COPY --from=fetcher nvm $NVM_DIR # change ownership if needed RUN chown -R $(id -un):$(id -gn) $NVM_DIR \u0026amp;\u0026amp; \\ echo \u0026#39;[ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; --no-use\u0026#39; \u0026gt;\u0026gt; ~/.profile What happen here?\nA shallow clone of the nvm repository is performed with git and it\u0026rsquo;s pinned to a specific version.\nThe final image requires some dependencies such as bash and curl.\nThe SHELL is set to pretend interactive.\nThe nvm files are copied to /opt/nvm.\nchown is used to set the ownership of the files.\nThe steps in the installation guide1 are followed with some exceptions such as setting the NVM_DIR environment variable directly in the Dockerfile, and skipping bash completion.\nThe sourcing of nvm.sh is followed by --no-use to lazily load nvm so that it\u0026rsquo;s not loaded until it\u0026rsquo;s used3.\nLastly, you can test if it\u0026rsquo;s working by adding some commands at the end of the Dockerfile to check the nvm version, install different versions of Node, and check their versions as well.\nGetting it working in Jenkins The pattern of running Jenkins agents as containers is great for packing development tools into Docker images and running them when needed. However, it can be challenging to get nvm.sh sourced properly when Jenkins agents are started as containers.\nNow that we have a working nvm in Docker image, we can use it in Jenkins. The easiest way is to use the Docker image we created above, as a base for a custom Jenkins agent image which includes nvm and all the necessary Node versions.\nExample Dockerfile:\nJenkins agent Dockerfile FROM our-company/our-nvm-base:latest # you might need to create jenkins user earlier USER jenkins ENV NODE_VERSIONS=\u0026#34;lts/* 16\u0026#34; # install Node versions RUN nvm install $NODE_VERSIONS \u0026amp;\u0026amp; \\ nvm use \u0026#34;lts/*\u0026#34; \u0026amp;\u0026amp; \\ npm install -g yarn And that\u0026rsquo;s it! Right? \u0026#x1f914;\nWas it enough to get it working? Not really \u0026#x1f615;. Why?\nIt all depends how do you call the commands. While it works well during the build time, it may not work when running the container or calling commands in different ways.\nFor example, it may not work if you use docker exec to run commands in containers - which we do when we test Docker base images. Similar situation happen when sh commands are used in Jenkins pipelines, ~/.profile is not loaded, and so nvm.sh is not sourced properly. While it\u0026rsquo;s possible to teach users to source nvm.sh in every sh command, this can be cumbersome and error-prone. Therefore, we need a better solution.\nWhat eventually worked? After struggling with the previous approach, I started thinking of an easier and more straightforward way to work with nvm and avoid common mistakes. I wanted something that would work the same way on both my local machine and in the CI/CD environment.\nI realized that there were two main things that nvm did: managing installations of different Node versions and loading them on demand by modifying the PATH environment variable.\nWhile thinking about how to simplify this process, it occurred to me that it would be much easier if nvm was a command rather than a bash function. And then, it hit me – why not make nvm a command?\nTo do this, I created a file with the following code: First try on nvm wrapper #!/usr/bin/env bash # load nvm [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; # call nvm with all the parameters nvm $@ Next, I made this file executable and available for execution by writing it to /usr/local/bin/nvm, ~/.bin/nvm, or ~/.local/bin/nvm - whatever works for you.\nWith this solution, there was no longer any need to source nvm.sh so we could use nvm. Instead, it could be called directly as a command, which simplified and streamlined the process considerably.\nI was able to install and manage different versions of Node. However, attempting to use node or npm commands resulted in an error message stating that the command was not recognized. This is because nvm installed like that do not mangle the PATH environment variable anymore. To fix this issue, there are two potential solutions.\nThe first solution involves manually setting the PATH variable to include the desired version of Node. If you only need one specific version in your Docker image, this solution might suffice. Additionally, exposing the bin directory will automatically expose all custom binaries installed with npm.\nIt\u0026rsquo;s as easy as adding in Dockerfile of agent:\nENV PATH=$NVM_DIR/versions/node/v18.16.0/bin:$PATH The second solution involves creating wrappers for node and npm commands in the same style as the nvm command. While this approach has the downside of not automatically exposing binaries installed via npm, it is a great idea overall. By running node wrapper, we can guess which version of Node should be used by sourcing nvm and running it.\nHere are examples of the wrapper scripts for node and npm:\nWrapper script for node: Node wrapper #!/usr/bin/env bash [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; exec node $@ Wrapper script for npm: NPM wrapper #!/usr/bin/env bash [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; exec npm $@ I use exec here for node and npm but not for nvm because nvm is a function while node and npm are executables. By using exec to run them, we eliminate one layer of bash shell, which saves a few megabytes of RAM in the container.\nHow to automatically expose binaries installed by npm? The \u0026ldquo;wrappers\u0026rdquo; approach has a downside: binaries installed via npm are not immediately available. They have to be wrapped or linked, or the PATH has to be modified. But we can automate this in our script!\nHere\u0026rsquo;s an improved version of /usr/local/bin/npm:\nImproved /usr/local/bin/npm #!/usr/bin/env bash [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; set -e npm $@ if [ \u0026#34;$1\u0026#34; == \u0026#34;install\u0026#34; ]; then find $(dirname $(nvm which node)) \\ -executable \\ \\( -type f -o -type l \\) \\ -print \\ | sed \u0026#39;/node$/d;/npm$/d\u0026#39; \\ | xargs -I{} ln -sf {} /usr/local/bin/ fi This script works by first executing npm $@ normally, without exec. After successful execution of npm install, it will find all the executables in the same directory as the current node executable, except for node and npm themselves, and symlink them to a directory already in the PATH, such as /usr/local/bin. Note that using /usr/local/bin will require root permissions. Alternatively, you can use ~/bin or ~/.local/bin, as long as one of these directories is in the PATH.\nWhat about npx? A colleague suggest me, that npx command should be wrapped too. I used same script as for npm but without linking after install. Final script will look like:\nExample /usr/local/bin/npx #!/usr/bin/env bash [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; exec npx $@ Complete example I prepared example repo with Dockerfiles\u0026thinsp; external link implementing both ways + some test proving why solution with wrappers works better4.\nThis example presents a base image, which provides just NVM a top Ubuntu, without any version of Node available out of the box. You can use this base in a Docker image:\nHow to use it in the real app FROM tgagor/base-v2/nvm # copy sources COPY ./src ./ COPY .nvmrc ./ # install Node you need, set it as default, upgrade npm to latest version RUN nvm install --default --latest-npm # install your app RUN npm install ... Use those wrapper scripts in your CI/CD agents and you will be able to use NVM in the same way.\nFinal words In conclusion, managing multiple versions of Node can be a hassle, but by creating a simple wrapper script, we can make nvm a command and automate the process of exposing the correct binaries installed via npm. With these techniques, we can easily manage multiple Node versions and ensure that our development and CI/CD environments will work in the same way.\nHopefully, this article has provided some useful insights and tips for managing Node versions with nvm. Happy coding!\nWhat\u0026rsquo;s pretty interesting, I review a huge part of Internet looking for solutions like this and no one does it like me. Am I wrong? Are there better ways to achieve it?\nEnjoyed? https://github.com/nvm-sh/nvm#git-install\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/nvm-sh/nvm#additional-notes\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tgagor/docker-nvm-example\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2023/04/the-best-way-to-get-nvm-working-in-ci-cd-systems/","summary":"TL;DR While reasoning is important, readers may not be interested in all the frustrations I experienced while figuring out how to get things done. If you\u0026rsquo;re looking for a quick solution, skip to the \u0026ldquo;What eventually worked?\u0026rdquo; section. However, if you\u0026rsquo;re interested in the thought process behind the solution, keep reading.\nWhy? Some might bother why the hell I\u0026rsquo;d like to make my life so hard? \u0026#x1f923;\nWe used to use nodeenv\u0026thinsp; external link for that purpose.","title":"The best way to get NVM working in CI/CD systems"},{"content":" Nowy jednominutowy menedżerAuthors: Blanchard Ken, Spencer Johnson\nempik.com ","permalink":"https://gagor.pro/books/2023/nowy-jednominutowy-menedzer/","summary":" Nowy jednominutowy menedżerAuthors: Blanchard Ken, Spencer Johnson\nempik.com ","title":"Nowy jednominutowy menedżer"},{"content":" Psychologia pieniędzyPonadczasowe lekcje o bogactwie, chciwości i szczęściu\nAuthor: Morgan Housel\nempik.com ","permalink":"https://gagor.pro/books/2023/psychologia-pieniedzy/","summary":" Psychologia pieniędzyPonadczasowe lekcje o bogactwie, chciwości i szczęściu\nAuthor: Morgan Housel\nempik.com ","title":"Psychologia pieniędzy"},{"content":"I switched recently from Docker Desktop\u0026thinsp; external link on my MacBook to Rancher Desktop\u0026thinsp; external link . The most important reason for me to do it, was possibility to gently switch between docker and containerd runtimes.\nThere\u0026rsquo;s still one feature that I miss on Rancher Desktop - possibility to change Docker daemon configuration. I used to enable experimental features1 and BuildKit2. Sadly, there\u0026rsquo;s no easy way to do it on Rancher\u0026hellip; But there\u0026rsquo;s a magical way3.\nYou have to enter to the running Rancher Desktop virtual machine. As machine is created during first run and usually not changed untile next upgrade, it\u0026rsquo;s good enough solution. Also if you reset the machine (to regain free space), the whole config is gone too.\nEnter the Rancher $ LIMA_HOME=\u0026#34;$HOME/Library/Application Support/rancher-desktop/lima\u0026#34; \u0026#34;/Applications/Rancher Desktop.app/Contents/Resources/resources/darwin/lima/bin/limactl\u0026#34; shell 0 Then, you have to switch to root:\nProvide the config sudo su - cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;features\u0026#34;: { \u0026#34;buildkit\u0026#34; : true }, \u0026#34;experimental\u0026#34;: true } EOF Now restart the Rancher Desktop (turn it off and on again) and voilà! Custom config loaded.\nhttps://docs.docker.com/engine/reference/commandline/dockerd/#description\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.docker.com/build/buildkit/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/rancher-sandbox/rancher-desktop/discussions/1477#discussioncomment-2110490\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2023/03/change-configuration-of-docker-daemon-in-rancher-desktop/","summary":"I switched recently from Docker Desktop\u0026thinsp; external link on my MacBook to Rancher Desktop\u0026thinsp; external link . The most important reason for me to do it, was possibility to gently switch between docker and containerd runtimes.\nThere\u0026rsquo;s still one feature that I miss on Rancher Desktop - possibility to change Docker daemon configuration. I used to enable experimental features1 and BuildKit2. Sadly, there\u0026rsquo;s no easy way to do it on Rancher\u0026hellip; But there\u0026rsquo;s a magical way3.","title":"Change configuration of Docker daemon in Rancher Desktop"},{"content":" Who Moved My Cheese?An Amazing Way to Deal With Change in Your Work and in Your Life\nAuthor: Spencer Johnson\namazon.pl ","permalink":"https://gagor.pro/books/2023/who-moved-my-cheese/","summary":" Who Moved My Cheese?An Amazing Way to Deal With Change in Your Work and in Your Life\nAuthor: Spencer Johnson\namazon.pl ","title":"Who Moved My Cheese?"},{"content":" Rycerz nieistniejącyAuthor: Calvino Italo\nempik.com ","permalink":"https://gagor.pro/books/2023/rycerz-nieistniejacy/","summary":" Rycerz nieistniejącyAuthor: Calvino Italo\nempik.com ","title":"Rycerz nieistniejący"},{"content":" Przywództwo i oszukiwanie samego siebieJak uwolnić się z pułapki uprzedzeń\nAuthor: Praca zbiorowa\nempik.com ","permalink":"https://gagor.pro/books/2023/przywodztwo-i-oszukiwanie-samego-siebie/","summary":" Przywództwo i oszukiwanie samego siebieJak uwolnić się z pułapki uprzedzeń\nAuthor: Praca zbiorowa\nempik.com ","title":"Przywództwo i oszukiwanie samego siebie"},{"content":" Praca głębokaJak odnieść sukces w świecie, w którym ciągle coś nas rozprasza\nAuthor: Cal Newport\namazon.plamazon.comempik.comhelion.pl Deep Work is a book written by Cal Newport that explores the concept of deep work and how it can be leveraged to achieve professional success and lead a more fulfilling life. Newport argues that in an increasingly connected and fast-paced world, the ability to focus and produce high-quality work is becoming increasingly valuable and rare. He provides practical strategies and tools for developing the discipline of deep work, including minimizing distractions, optimizing your work environment, and training your mind to focus.\nThe book is well-written and presents a compelling case for the importance of deep work in today\u0026rsquo;s economy. Newport draws on a variety of research, case studies, and personal experiences to illustrate his points, and provides a wealth of practical tips and advice for developing deep work habits.\nOverall, I would highly recommend Deep Work to anyone looking to improve their focus, productivity, and overall performance in their professional and personal life. Whether you\u0026rsquo;re an entrepreneur, a knowledge worker, or simply someone looking to lead a more fulfilling life, this book provides a wealth of practical and actionable advice that can help you get there.\n","permalink":"https://gagor.pro/books/2023/praca-gleboka/","summary":"Praca głębokaJak odnieść sukces w świecie, w którym ciągle coś nas rozprasza\nAuthor: Cal Newport\namazon.plamazon.comempik.comhelion.pl Deep Work is a book written by Cal Newport that explores the concept of deep work and how it can be leveraged to achieve professional success and lead a more fulfilling life. Newport argues that in an increasingly connected and fast-paced world, the ability to focus and produce high-quality work is becoming increasingly valuable and rare.","title":"Praca głęboka"},{"content":" Zaczynaj od DLACZEGOJak wielcy liderzy inspirują innych do działania\nAuthor: Simon Sinek\namazon.pl ","permalink":"https://gagor.pro/books/2020/zaczynaj-od-dlaczego/","summary":" Zaczynaj od DLACZEGOJak wielcy liderzy inspirują innych do działania\nAuthor: Simon Sinek\namazon.pl ","title":"Zaczynaj od DLACZEGO"},{"content":" Pięć dysfunkcji pracy zespołowejOpowieść o przywództwie\nAuthor: Patrick M. Lencioni\namazon.plamazon.comempik.com ","permalink":"https://gagor.pro/books/2022/5-disfunctions-of-a-team/","summary":" Pięć dysfunkcji pracy zespołowejOpowieść o przywództwie\nAuthor: Patrick M. Lencioni\namazon.plamazon.comempik.com ","title":"Pięć dysfunkcji pracy zespołowej"},{"content":" Starość aksolotlaHardware dreams\nAuthor: Jacek Dukaj\namazon.plamazon.comempik.com Starość Aksolotla is a book written by Polish author Jacek Dukaj that I recently had the pleasure of reading. The book is a science fiction novel that explores themes of longevity, immortality, and the meaning of life. The protagonist of the story is a centuries-old axolotl, a species of salamander that is capable of regenerating its limbs and organs, who embarks on a journey of self-discovery as he grapples with the consequences of his own immortality.\nI was impressed by Dukaj\u0026rsquo;s imaginative and thought-provoking writing, as well as his skillful handling of complex scientific concepts. The characters are well-drawn and the story is richly imagined, with a detailed and immersive world that kept me engaged from beginning to end.\nOverall, I would highly recommend Starość Aksolotla to anyone who enjoys science fiction novels that challenge the mind and explore deep philosophical ideas. It is a thought-provoking and well-written book that is sure to leave a lasting impression on its readers.\n","permalink":"https://gagor.pro/books/2022/starosc-aksolotla/","summary":"Starość aksolotlaHardware dreams\nAuthor: Jacek Dukaj\namazon.plamazon.comempik.com Starość Aksolotla is a book written by Polish author Jacek Dukaj that I recently had the pleasure of reading. The book is a science fiction novel that explores themes of longevity, immortality, and the meaning of life. The protagonist of the story is a centuries-old axolotl, a species of salamander that is capable of regenerating its limbs and organs, who embarks on a journey of self-discovery as he grapples with the consequences of his own immortality.","title":"Starość aksolotla"},{"content":" Błądzą wszyscy (ale nie ja)Dlaczego usprawiedliwiamy głupie poglądy, złe decyzje i szkodliwe działania?\nAuthors: Elliot Aronson, Carol Tavris\namazon.comempik.com ","permalink":"https://gagor.pro/books/2022/bladza-wszyscy-ale-nie-ja/","summary":" Błądzą wszyscy (ale nie ja)Dlaczego usprawiedliwiamy głupie poglądy, złe decyzje i szkodliwe działania?\nAuthors: Elliot Aronson, Carol Tavris\namazon.comempik.com ","title":"Błądzą wszyscy (ale nie ja)"},{"content":"Since upgrade to Ubuntu 22.04 keep seeing those warnings:\nW: http://ppa.launchpad.net/yubico/stable/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://updates.signal.org/desktop/apt/dists/xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://repo.skype.com/deb/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://packagecloud.io/AtomEditor/atom/any/dists/any/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://packagecloud.io/slacktechnologies/slack/debian/dists/jessie/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://apt.syncthing.net/dists/syncthing/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. I already cleared all of them in my Docker images, but somehow I didn\u0026rsquo;t fix my station.\nWhat\u0026rsquo;s wrong with the old /etc/apt/trusted.gpg keyring? ALL keys put there are used to verify ALL the repositories. Why it\u0026rsquo;s a problem? Because potentially someone can put in one of repositories package signed by key not supposed to be used in this specific repo.\nIt\u0026rsquo;s hard to imagine a real world attack but on the other side, possibility to install package that is not verified by repo key is not nice.\nSo, how to do it? First, location We have to figure out where to put those keys and I saw already few options.\n/etc/apt/trusted.gpg.d/ It\u0026rsquo;s not a good idea as this works exactly the same way as the old single db file - authorizes all the repos with keys there. /usr/share/keyrings/ This path is ok for me, for the keys coming from official distribution, but not to add them manually there\u0026hellip; /etc/apt/keyrings/ This is my preference as it\u0026rsquo;s left in /etc close to the other repo files. It\u0026rsquo;s even already created on recent Ubuntu\u0026rsquo;s. Second, file name According to Debian1, those files should be named like something-archive-keyring.asc or something-archive-keyring.ppg (although I see them more often as something-archive-keyring.gpg).\nWhy there\u0026rsquo;s a difference in the extension? It depends, if your key is armored[^amored] or not. Armored keys should use .asc extension, binary should use .gpg.\nIt\u0026rsquo;s easy to recognize, armored files are in ASCII format and they look like:\n-----BEGIN PGP PUBLIC KEY BLOCK----- ... bla bla bla here -----END PGP PUBLIC KEY BLOCK----- Complete example Without de-armoring:\nsudo wget -O /etc/apt/keyrings/signal-desktop-archive-keyring.asc echo \u0026#39;deb [arch=amd64 signed-by=/etc/apt/keyrings/signal-desktop-archive-keyring.asc] https://updates.signal.org/desktop/apt xenial main\u0026#39; |\\ sudo tee -a /etc/apt/sources.list.d/signal-xenial.list With de-armoring:\nwget -O- https://updates.signal.org/desktop/apt/keys.asc | gpg --dearmor | sudo tee -a /etc/apt/keyrings/signal-desktop-archive-keyring.gpg \u0026gt; /dev/null echo \u0026#39;deb [arch=amd64 signed-by=/usr/share/keyrings/signal-desktop-archive-keyring.gpg] https://updates.signal.org/desktop/apt xenial main\u0026#39; |\\ sudo tee -a /etc/apt/sources.list.d/signal-xenial.list Remember to verify it with:\nsudo apt update Let\u0026rsquo;s link 2.\nhttps://wiki.debian.org/DebianRepository/UseThirdParty#OpenPGP_certificate_distribution\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://www.askapache.com/htaccess/apache-authentication-in-htaccess.html\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2022/10/ubuntu-key-is-stored-in-legacy-trusted.gpg-keyring.../","summary":"Since upgrade to Ubuntu 22.04 keep seeing those warnings:\nW: http://ppa.launchpad.net/yubico/stable/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://updates.signal.org/desktop/apt/dists/xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://repo.skype.com/deb/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details. W: https://packagecloud.io/AtomEditor/atom/any/dists/any/InRelease: Key is stored in legacy trusted.","title":"Ubuntu - Key is stored in legacy trusted.gpg keyring..."},{"content":"IMO people don\u0026rsquo;t understand how VOLUME1 works so they don\u0026rsquo;t use it. It\u0026rsquo;s generally used far too rarely!\nIn short VOLUME means two things:\nWhatever is left in directory marked as VOLUME, stays there and can\u0026rsquo;t be changed in later layers (actually it can be changed but changes won\u0026rsquo;t be persistent). Volumes are not part of layered image FS. They\u0026rsquo;re mounted as anonymous volumes located on standard file system. This means they\u0026rsquo;re working much faster. Let me explain it a bit.\nUse VOLUME for temporary files directories That\u0026rsquo;s one of things that piss me off in the official base images . They just don\u0026rsquo;t set up volumes for temporary directories by default. That\u0026rsquo;s why in my base images I always have something like that, somewhere at beginning:\nExample Dockerfile # on Debian VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/apt\u0026#34;, \u0026#34;/var/lib/apt/lists\u0026#34;] # on CentOS VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/yum\u0026#34;, \u0026#34;/var/cache/dnf\u0026#34;] What it does? It keeps images clean. Earlier, I was installing packages like this:\nHow everybody do it - without VOLUMEs FROM debian RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y nginx \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* Debian by default saves package lists (that\u0026rsquo;s what apt-get update downloads) in /var/cache/apt/lists. Then it\u0026rsquo;s downloading deb packages to /var/cache/apt/archives and all of this rubbish will be left in image by default, unless you remove it. But by marking /var/cache/apt as a VOLUME, all content became immutable and is removed/dropped at the end of each directive (like each RUN). So I can do it right now like this:\nHow I do it - with VOLUMEs FROM debian VOLUME [\u0026#34;/tmp\u0026#34;, \u0026#34;/var/tmp\u0026#34;, \u0026#34;/var/cache/apt\u0026#34;, \u0026#34;/var/lib/apt/lists\u0026#34;] RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y nginx It just makes life easier \u0026#x1f604;\nAdd clean-up task for anonymous volumes Info\nSadly, those anonymous volumes are just left after you deploy new image version or shut it down. You have to clean them from time to time!\nWhich you probably should already know, if you manage cluster of docker hosts. It might be as simple as running in cron:\nCron cleanup task docker volume prune -f https://docs.docker.com/engine/reference/builder/#volume\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2022/09/best-practices-for-writing-dockerfiles-use-volume-for-all-mutable-temporary-file-locations/","summary":"IMO people don\u0026rsquo;t understand how VOLUME1 works so they don\u0026rsquo;t use it. It\u0026rsquo;s generally used far too rarely!\nIn short VOLUME means two things:\nWhatever is left in directory marked as VOLUME, stays there and can\u0026rsquo;t be changed in later layers (actually it can be changed but changes won\u0026rsquo;t be persistent). Volumes are not part of layered image FS. They\u0026rsquo;re mounted as anonymous volumes located on standard file system. This means they\u0026rsquo;re working much faster.","title":"Best practices for writing Dockerfiles - Use VOLUME for all mutable, temporary file locations"},{"content":"People often complain, that building Docker image takes a long time. \u0026ldquo;I just added a single jar package\u0026rdquo; they say\u0026hellip; Really?\nThey often don\u0026rsquo;t remember that whole \u0026ldquo;build context\u0026rdquo;1 is uploaded to Docker daemon during build, which often means they\u0026rsquo;re not only adding \u0026ldquo;single jar\u0026rdquo;, but also all sources, test results and whatever they have in working directory.\nSolution is simple - to use .dockerignore file2. Syntax is similar to .gitignore. It excludes what shouldn\u0026rsquo;t be uploaded to Docker daemon.\nTake a look at an example file below:\n.dockerignore src tests target/*.xml # exclude temp files */temp* */*/temp* temp? # exclude things you won\u0026#39;t need anyway *~ .DS_Store *.old .vscode https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-ontext\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#exclude-with-dockerignore\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2022/09/best-practices-for-writing-dockerfiles-use-dockerignore/","summary":"People often complain, that building Docker image takes a long time. \u0026ldquo;I just added a single jar package\u0026rdquo; they say\u0026hellip; Really?\nThey often don\u0026rsquo;t remember that whole \u0026ldquo;build context\u0026rdquo;1 is uploaded to Docker daemon during build, which often means they\u0026rsquo;re not only adding \u0026ldquo;single jar\u0026rdquo;, but also all sources, test results and whatever they have in working directory.\nSolution is simple - to use .dockerignore file2. Syntax is similar to .","title":"Best practices for writing Dockerfiles - Use .dockerignore"},{"content":"I\u0026rsquo;ve been thinking for a long time about writing set of articles on the topic of: \u0026ldquo;Dockerfile writing best practices\u0026rdquo;.\nAs it\u0026rsquo;s often my daily job to prepare best in class containers, that are later used by thousands of company\u0026rsquo;s applications, I have quite good insights on the topic. Some experience and knowledge gathered is often against intuition and building it took me a while. I want to share it, with a hope that feedback I get will allow me to excel on the topic even further.\nInitially I was thinking about writing one big article, connect all the dots there and make it great\u0026hellip; I even started it. But I failed by it\u0026rsquo;s scale, so I changed strategy and now I plan to release them as series of smaller articles, that should be easier to deliver and maintain.\nTopics I want to cover Use .dockerignore Follow \u0026ldquo;Filesystem Hierarchy Standard\u0026rdquo; Don\u0026rsquo;t leave packages you don\u0026rsquo;t need in images Use VOLUME for all mutable, temporary files locations Don\u0026rsquo;t rely on Docker official images Don\u0026rsquo;t run applications as a root Use multi-stage builds Use Label Schema/OCI Image Label Spcification Image security, how to scan them and when Build cache - to use it or not? And more\u0026hellip; I will curate list of links to dedicated articles on this page as they will be arriving.\n","permalink":"https://gagor.pro/2022/09/dockerfile-writing-best-practices/","summary":"I\u0026rsquo;ve been thinking for a long time about writing set of articles on the topic of: \u0026ldquo;Dockerfile writing best practices\u0026rdquo;.\nAs it\u0026rsquo;s often my daily job to prepare best in class containers, that are later used by thousands of company\u0026rsquo;s applications, I have quite good insights on the topic. Some experience and knowledge gathered is often against intuition and building it took me a while. I want to share it, with a hope that feedback I get will allow me to excel on the topic even further.","title":"Dockerfile writing best practices"},{"content":" Pułapki myśleniaO myśleniu szybkim i wolnym\nAuthor: Daniel Kahneman\namazon.plamazon.comempik.com ","permalink":"https://gagor.pro/books/2022/thinking-fast-and-slow/","summary":" Pułapki myśleniaO myśleniu szybkim i wolnym\nAuthor: Daniel Kahneman\namazon.plamazon.comempik.com ","title":"Pułapki myślenia"},{"content":" The three-body problemAuthor: Cixin Liu\namazon.plamazon.comempik.comhelion.pl The novel is set in the future and follows the discovery of an alien civilization, the Trisolarans, and humanity\u0026rsquo;s attempts to make contact and defend against them. The book is known for its complex scientific concepts, philosophical ideas, and intricate storyline.\nOverall, \u0026ldquo;The Three-Body Problem\u0026rdquo; has been highly praised by readers and critics for its unique and thought-provoking approach to science fiction.\nPersonally, I found this book to be annoying in a few places, especially when it referred to people\u0026rsquo;s commitment following government decisions. I understand that it has a Chinese background, but I found it irritating when political correctness was emphasized more than rationality. Nonetheless, it was a nice journey.\n","permalink":"https://gagor.pro/books/2022/three-body-problem/","summary":"The three-body problemAuthor: Cixin Liu\namazon.plamazon.comempik.comhelion.pl The novel is set in the future and follows the discovery of an alien civilization, the Trisolarans, and humanity\u0026rsquo;s attempts to make contact and defend against them. The book is known for its complex scientific concepts, philosophical ideas, and intricate storyline.\nOverall, \u0026ldquo;The Three-Body Problem\u0026rdquo; has been highly praised by readers and critics for its unique and thought-provoking approach to science fiction.\nPersonally, I found this book to be annoying in a few places, especially when it referred to people\u0026rsquo;s commitment following government decisions.","title":"The three-body problem"},{"content":"Auta się zmieniają a problemy z nimi pozostają te same :)\nKasowanie ostrzeżenia wymiany oleju 1 Przekręcić kluczyk w stacyjce do drugiej pozycji, gdy zapalają się wszystkie kontrolki (nie uruchamiamy silnika). Wciskamy równocześnie pedały hamulca i gazu do oporu, trzymamy do zakończenia procesu. Pojawi komunikat o rozpoczęciu resetowania inspekcji. Możemy zatwierdzić OK. Czekamy aż pojawi się komunikat: Zatwierdzamy OK. Dopiero teraz zwalniamy pedały. https://forum.fordclubpolska.org/showthread.php?t=108532\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2022/09/ford-s-max-kasowanie-ostrzezenia-wymiany-oleju/","summary":"Auta się zmieniają a problemy z nimi pozostają te same :)\nKasowanie ostrzeżenia wymiany oleju 1 Przekręcić kluczyk w stacyjce do drugiej pozycji, gdy zapalają się wszystkie kontrolki (nie uruchamiamy silnika). Wciskamy równocześnie pedały hamulca i gazu do oporu, trzymamy do zakończenia procesu. Pojawi komunikat o rozpoczęciu resetowania inspekcji. Możemy zatwierdzić OK. Czekamy aż pojawi się komunikat: Zatwierdzamy OK. Dopiero teraz zwalniamy pedały. https://forum.fordclubpolska.org/showthread.php?t=108532\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Ford S-MAX - kasowanie ostrzeżenia wymiany oleju"},{"content":" Czy jesteś tym, który puka?Author: Dariusz Użycki\nempik.com ","permalink":"https://gagor.pro/books/2022/czy-jestes-tym-ktory-puka/","summary":" Czy jesteś tym, który puka?Author: Dariusz Użycki\nempik.com ","title":"Czy jesteś tym, który puka?"},{"content":" Kubernetes OperatorsAutomating the Container Orchestration Platform\nAuthors: Jason Dobies, Joshua Wood\namazon.plamazon.comhelion.pl ","permalink":"https://gagor.pro/books/2022/kubernetes-operators/","summary":" Kubernetes OperatorsAutomating the Container Orchestration Platform\nAuthors: Jason Dobies, Joshua Wood\namazon.plamazon.comhelion.pl ","title":"Kubernetes Operators"},{"content":"I\u0026rsquo;m back on the big stage!\nI haven\u0026rsquo;t attend any big conferences as presenter for some time, but this year will change it. I\u0026rsquo;m starting big, with a talk: Docker base images - Ideas how to manage them on scale\u0026thinsp; external link on Devoxx\u0026thinsp; external link conference in Kraków, that will take place on 22-24th June 2022.\nWant to meet? Meet there \u0026#x1f604;\nUpdate I uploaded slides from presentation to my Github account\u0026thinsp; external link .\nThere\u0026rsquo;s also a video available: ","permalink":"https://gagor.pro/2022/06/back-on-the-big-stage/","summary":"I\u0026rsquo;m back on the big stage!\nI haven\u0026rsquo;t attend any big conferences as presenter for some time, but this year will change it. I\u0026rsquo;m starting big, with a talk: Docker base images - Ideas how to manage them on scale\u0026thinsp; external link on Devoxx\u0026thinsp; external link conference in Kraków, that will take place on 22-24th June 2022.\nWant to meet? Meet there \u0026#x1f604;\nUpdate I uploaded slides from presentation to my Github account\u0026thinsp; external link .","title":"Back on the big stage!"},{"content":" DriveThe Surprising Truth About What Motivates Us\nAuthor: Daniel H. Pink\namazon.plamazon.comempik.com Drive: The Surprising Truth About What Motivates Us is a book that I recently read and found to be highly insightful. The author, Daniel H. Pink, explores the science of motivation and argues that the traditional carrot-and-stick approach is outdated and ineffective. He claims that in today\u0026rsquo;s rapidly changing and complex world, autonomy, mastery, and purpose are the most important drivers of motivation.\nI was impressed by Pink\u0026rsquo;s research and the range of real-world examples he uses to support his arguments. He presents his ideas in a clear and engaging style, making the book accessible to a wide audience.\nOverall, I found Drive to be a thought-provoking and well-written book that challenges conventional wisdom about motivation. It provides practical insights for individuals and organizations looking to create more motivating and productive work environments. I would definitely recommend this book to anyone interested in the science of motivation and how to foster a more engaged and motivated workforce.\n","permalink":"https://gagor.pro/books/2022/drive/","summary":"DriveThe Surprising Truth About What Motivates Us\nAuthor: Daniel H. Pink\namazon.plamazon.comempik.com Drive: The Surprising Truth About What Motivates Us is a book that I recently read and found to be highly insightful. The author, Daniel H. Pink, explores the science of motivation and argues that the traditional carrot-and-stick approach is outdated and ineffective. He claims that in today\u0026rsquo;s rapidly changing and complex world, autonomy, mastery, and purpose are the most important drivers of motivation.","title":"Drive"},{"content":" Wewnętrzna graKształtowanie psychiki gracza giełdowego\nAuthors: Robert Koppel, Howard Abell\nmaklerska.pl ","permalink":"https://gagor.pro/books/2022/wewnetrzna-gra/","summary":" Wewnętrzna graKształtowanie psychiki gracza giełdowego\nAuthors: Robert Koppel, Howard Abell\nmaklerska.pl ","title":"Wewnętrzna gra"},{"content":"I was so happy when Netflix became available in my country. I was waiting for any serious streaming provider, that will give me easy and legal access to the movies I love. Ok, looks like not to all of them were there, but future was looking bright. As time was passing, offer was extending. They eventually added even some Polish movies, which my wife appreciated. Seems like my country is not in the same league like USA, Germany or UK. Screw it! It\u0026rsquo;s still better than nothing, right?\nI prefer original audio and subtitles, but rest of my family prefer dubbing. It was all slowly propagating and right now most movies have it.\nUI is simple and responsive on all devices I use. I\u0026rsquo;m quite disappointed, that in basic plan there\u0026rsquo;s no FullHD quality and if you want 4K, you have to pay even more. Pirates just provide you a variety of resolutions, audio codecs\u0026hellip; But it\u0026rsquo;s at least legal, legal is good, right?\nWhen I started my adventure with them, I was quite satisfied. I like to watch Sci-Fi movies. I had there \u0026ldquo;The Expanse\u0026rdquo;, \u0026ldquo;Star Trek Discovery\u0026rdquo; and few more. There were Marvel movies, not all of them but most.\nI also have HBO GO. I was following \u0026ldquo;Game of Thrones\u0026rdquo;, \u0026ldquo;Chernobyl\u0026rdquo; and few more. Quality is not as good as on Netflix. I think it\u0026rsquo;s max FullHD, but probably some of videos are even worse. I pay for it half the price of Netflix, but I also have access to 3 FullHD TV channels. Video quality is lower, but actually movies and TV series are in my opinion of better quality. More engaging, better rated on IMDB, etc.\nTwo years passed and landscape is kinda different.\nI have now:\nNetflix HBO Go (for one third of the price of Netflix) Apple TV (half of the price of Netflix) Amazon Prime (costs ~15 times less than Netflix) Netflix was pioneering the whole market. They were fist, that provided an easy to use and legal service allowing people to watch any movie in the front of TV/PC. They even make it simpler than downloading pirate copy. No need to dig through multiple torrents and hoping that it will download. It\u0026rsquo;s all already there, ready to use.\nOthers saw their success and decided to copy it and that IMO started decline of it\u0026rsquo;s quality/value. I can\u0026rsquo;t watch \u0026ldquo;The Expanse\u0026rdquo; anymore on Netflix, it was moved to Amazon Prime. I can\u0026rsquo;t watch new season of \u0026ldquo;Star Trek\u0026rdquo; because it was moved to some \u0026ldquo;Paramount+\u0026rdquo;. I\u0026rsquo;m not even sure, if I can register to this service from my country right now.\nMy daughter wanted to binge watch Marvel movies during Xmas holidays, so we could catch all nuances together. We can\u0026rsquo;t. There are no Marvel movies on Netflix, just one or two on HBO Go. There\u0026rsquo;s this new Disney+, but it\u0026rsquo;s not available in my country. I can\u0026rsquo;t watch Avengers, Star Wars or even some fairy tales disappear (which my younger kids enjoy).\nOn Apple TV, I watched \u0026ldquo;The Fundation\u0026rdquo;. Not bad, but actually I don\u0026rsquo;t see anything more interesting. There are more movies and even quite fresh block busters, but buying them cost around the monthly value of Netflix. There\u0026rsquo;s also option to borrow for ~20% of Netflix monthly price. Quality of video is really good, maybe even better than on Netflix. On the other side application on my TV is not of the Apple class. It\u0026rsquo;s crappy, not intuitive. I just don\u0026rsquo;t like it. When I think about the service, I have mixed feelings. I pay, to gain access to the service so I could pay even more to watch something recent. Summing it up, I have to pay more than for Netflix or even a cinema ticket to watch something on my own TV. Streaming on mass scale, don\u0026rsquo;t cost that much like whole service in cinema. I don\u0026rsquo;t have 7.1 audio and boxes of pop-corn. Why would I pay more to buy movie like that than going to cinema?\nI have 4 paid services active right now and I\u0026rsquo;m not able to watch what I want to. It feels, like history turned the full circle and again pirates can provide higher quality of the service, giving people easy access to all the materials and you can even choose the quality you want. They might again gain the market and I can already hear yelling of corporations how they\u0026rsquo;re responsible for artists loosing their money. But is it ok, if I loose my money for 4 paid services, but can\u0026rsquo;t really get access to what I want? Is it ok, to block access to some movies in other streaming platform but do not provide legal alternative?\nI wanted a fair service, for fair amount of money. Now I feel like someone cheated me. I believe for next 2-3 years they will choke with all those new Stream+ services of smaller and lower portfolios. They won\u0026rsquo;t be profitable and people will be more and more annoyed. I hope they all will fail and eventually instead of building closed platforms, they will find way to share media in more open way.\nFor now I will share my opinion and cancel most of the subscriptions. It\u0026rsquo;s time to review the list of books I wanted to read \u0026#x1f603;\n","permalink":"https://gagor.pro/2022/01/on-slow-decline-of-streaming-services/","summary":"I was so happy when Netflix became available in my country. I was waiting for any serious streaming provider, that will give me easy and legal access to the movies I love. Ok, looks like not to all of them were there, but future was looking bright. As time was passing, offer was extending. They eventually added even some Polish movies, which my wife appreciated. Seems like my country is not in the same league like USA, Germany or UK.","title":"On slow decline of streaming services"},{"content":" Projekt JednorożecPowieść o szansie w epoce przewrotów cyfrowych\nAuthor: Gene Kim\nhelion.pl ","permalink":"https://gagor.pro/books/2021/projekt-jednorozec/","summary":" Projekt JednorożecPowieść o szansie w epoce przewrotów cyfrowych\nAuthor: Gene Kim\nhelion.pl ","title":"Projekt Jednorożec"},{"content":"What I want to do? I use my pool to securely store backups, archive my old documents and keep huge family\u0026rsquo;s photo library.\nI have new disks. They were tortured with badblocks , so they\u0026rsquo;re ready to create ZFS pool.\nI\u0026rsquo;ve read few documents about different approaches 1 2 3. I wanted to be sure if anything changed during past years. One of articles recommends mirroring over RAIDZ. Resilvering is faster, at the same time putting IO less stress on whole pool. But pool as small as mine, relies on single drive which might die in between and data won\u0026rsquo;t be recoverable. Eventually, I decided to go for RAIDZ1 for now and in the future I rather move to RAIDZ2. For that, I have to buy one more disk - Black Friday is close, we will see.\nOne thing that is available now, but was not, when I created ZFS pool last time, is encryption. I want it. I won\u0026rsquo;t be running with my PC anywhere, but in case if disk will be damaged and I will have to return it. No worries, data is unavailable. So I want it \u0026#x1f604;\nLet\u0026rsquo;s do it I use Ubuntu 21.10 but despite installation of ZFS utils, rest of commands is not Ubuntu specific and should work on any distro.\nInstall ZFS sudo apt install -y zfsutils-linux zfs-zed Encryption key First, I have to generate secure key/file, that will be used to encrypt/decrypt file system. I will keep it on my root filesystem, actually in /root directory, as it won\u0026rsquo;t be easily available for other users.\nIt\u0026rsquo;s possible to use password or raw file. On laptop, I will choose password, but on my desktop PC I prefer file. First, I was thinking to place it in /etc/zfs but it\u0026rsquo;s world readable. So I decided I will drop it in /root dir.\nGenerate encryption key sudo dd if=/dev/random of=/root/.zfs-encrypt.key bs=1 count=32 I strongly advice to make backup of this key. Multiple backups actually :)\nCreate a fully encrypted pool Now I can create pool. Most examples refer to disks as sda, sdb, etc. But I prefer to use their labels as drive model plus serial number. Just check /dev/disk/by-id/. Other way is by use of wwn-* ids, they\u0026rsquo;re also stable across restarts, cable changes, etc. I strongly recommend such ids over standard letters.\nCreate encrypted ZFS pool sudo zpool create \\ -o ashift=12 \\ -o feature@encryption=enabled \\ -O encryption=on \\ -O keylocation=file:///root/.zfs-encrypt.key \\ -O keyformat=raw \\ storage raidz1 \\ ata-WDC_WD140EDGZ-11B1PA0_9MGJK4YK \\ ata-WDC_WD140EDGZ-11B1PA0_Y6GVH40C \\ ata-WDC_WD140EDGZ-11B1PA0_Y6GWHD3C What happen here?\n-o ashift=12 - treat disks sector size as 4KB, it was detected automatically but I wanted to be sure :) -o feature@encryption=enabled -O encryption=on -O keylocation=file:///root/.zfs-encrypt.key -O keyformat=raw - enable encryption on whole pool, use raw key and default encryption method: aes-256-gcm Automatically decrypt on boot So my pool is encrypted now, but I don\u0026rsquo;t want to decrypt it manually each time my PC starts. So I added such service:\n/etc/systemd/system/zfs-load-key.service sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/zfs-load-key.service [Unit] Description=Load encryption keys DefaultDependencies=no After=zfs-import.target Before=zfs-mount.service [Service] Type=oneshot RemainAfterExit=yes ExecStart=/sbin/zfs load-key -a StandardInput=tty-force [Install] WantedBy=zfs-mount.service EOF It have to enabled it now:\nEnable key load service sudo systemctl daemon-reload sudo systemctl enable zfs-load-key In ArchLinux wiki 4, you can find example for service, that can load individual key per pool, but I had trouble getting it to work. Service above loads all keys for all pools and it\u0026rsquo;s matching my case of just two pools ;)\nLet\u0026rsquo;s configure pool There are few options worth to setup just after pool creation:\nInitial pool configuration # automatically expand pool when new disk is added sudo zpool set autoexpand=on storage # automatically replace failed disk with hot spare sudo zpool set autoreplace=on storage # enable LZ4 compression sudo zfs set compression=lz4 storage # disable access time - for better performance sudo zfs set atime=off storage For better explanation of options, check best practices on Aaron\u0026rsquo;s Toponce blog 5.\nCreate datasets (actual mount points) Now I\u0026rsquo;m ready to create few datasets, that will be actually used to store files.\nCreate ZFS datasets and configure them # photos are well compressed, so I don\u0026#39;t need to compress them again sudo zfs create storage/photos sudo zfs set compression=off storage/photos # on backups I often just copy files staring, so let use stronger compression ZSTF sudo zfs create storage/backup sudo zfs set compression=zstd storage/backup # I like to keep my downloads on pool too sudo zfs create -o mountpoint=/home/timor/Downloads storage/downloads Setting TLER on boot Having disks in RAID, it\u0026rsquo;s good to have TLER enabled to rely on RAID for error recovery instead of internal hard drive recover 6. My disks support it, but it have to be enabled\nChecking if TLER is supported sudo smartctl -l scterc /dev/sdd sudo smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org SCT Error Recovery Control: Read: Disabled Write: Disabled It\u0026rsquo;s there but disabled.\nEnabling TLER sudo smartctl -l scterc,70,70 /dev/sdd sudo smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org SCT Error Recovery Control set to: Read: 70 (7.0 seconds) Write: 70 (7.0 seconds) Warning\nExample below will overwrite /etc/rc.local. If you already use this file, please edit it on your own!\nMake it persistent between restarts:\n~/2021/11/creating-fully-encrypted-zfs-pool/ sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/rc.local #!/bin/bash # I have such disks, let\u0026#39;s iterate over them # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_9MGJK4YK # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_Y6GVH40C # /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_Y6GWHD3C for i in 9MGJK4YK Y6GVH40C Y6GWHD3C; do echo smartctl -l scterc,70,70 /dev/disk/by-id/ata-WDC_WD140EDGZ-11B1PA0_\\$i \u0026gt; /dev/null; done EOF sudo chmod +x /etc/rc.local sudo systemctl enable rc-local.service That\u0026rsquo;s it. All done.\nEnjoyed? https://serverfault.com/questions/972496/can-i-encrypt-a-whole-pool-with-zfsol-0-8-1\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.delphix.com/blog/delphix-engineering/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://jrs-s.net/2015/02/06/zfs-you-should-use-mirror-vdevs-not-raidz/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/ZFS#Native_encryption\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pthree.org/2012/12/13/zfs-administration-part-viii-zpool-best-practices-and-caveats/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.timlinden.com/checking-tler-setting-for-linux-hard-drives/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2021/11/creating-fully-encrypted-zfs-pool/","summary":"What I want to do? I use my pool to securely store backups, archive my old documents and keep huge family\u0026rsquo;s photo library.\nI have new disks. They were tortured with badblocks , so they\u0026rsquo;re ready to create ZFS pool.\nI\u0026rsquo;ve read few documents about different approaches 1 2 3. I wanted to be sure if anything changed during past years. One of articles recommends mirroring over RAIDZ. Resilvering is faster, at the same time putting IO less stress on whole pool.","title":"Creating fully encrypted ZFS pool"},{"content":"I used to have RAID (or at least some variation of it) for my main storage. For redundancy, in case of disk failure. I started with some crazy LVM mirrors done on two disks of different size. Sync job was starting on every boot \u0026#x1f604;\nThen came time for RAID5 on mdadm + LVM for volume management. It was working nice until the moment when disks became bigger. Long array rebuilds or checks, required my PC to stay turned on overnight just to validate if stuff works still.\nThen in work, I had my first contact with ZFS and it was love at first sight \u0026#x2764;\u0026#xfe0f; Biggest improvement was, that after creation of pool there\u0026rsquo;s no need to re-sync whole disk. Both resilvering or scrub were much faster than standard mdadm check.\nMy old disks are 2TB big and almost each of them is different now. I started with 3 x ST2000DL003 2TB, but after failure of one I was unable to buy exact replacement. Closest possible was: ST2000DM001. Since that moment, I had disks running at variable speed around 5400RPM and one on 7200RPM. I decided to add another drive to the pool as spare, it was Samsung HD204UI. After it failed, I added WDC Black WD2003FYYS 2TB.\nToday, oldest disk is around 10 years old, youngest are around 8. Just this is pretty awful but what pushed me to replace them, was actual lack of space \u0026#x1f604;\nI was hoping to find 4 x 8TB drives which will quadruple my capacity and should provide enough space and safety for years to come, but I couldn\u0026rsquo;t find any reasonably priced offers. I even bought 4 x Toshiba X300 8TB. I started copying data to them\u0026hellip; And they started dying one by one \u0026#x1f61e; Actually, they didn\u0026rsquo;t die, but SMART was reporting \u0026ldquo;pre-fail\u0026rdquo; condition, when they were working heavily for too long. What if I have to re-sync/scrub whole pool? Will they die in the middle. I returned all of them. It was first time I gave a chance to Toshiba. It was last time \u0026#x1f609;\nThen I saw a movie on Youtube or article - someone was buying at bulk cheap external HDDs and \u0026ldquo;shucking\u0026rdquo; them 1. It was pretty close to how I understand building RAID as \u0026ldquo;Redundant Array of Inexpensive Disks\u0026rdquo; \u0026#x1f609; Those disk are usually pretty good disks - either WD RED which are actually designed for NAS/RAID workload, or so called \u0026ldquo;white labels\u0026rdquo; which are just variations of HGST drives (sometimes slower, with less cache, etc).\nI noticed a promo on Amazon on WD Elements 14TB and I bought 3 of them. Wanted 4, but they were limiting orders. They were 30% cheaper than similar internal HDDs.\nWhat\u0026rsquo;s inside? I connected them as they\u0026rsquo;re provided, in those plastic coffins and started playing. First I stress tested them with badblocks 2 3.\nRun badblocks badblocks -b 4096 -wsv /dev/sde After around 1 hour, all of them were quite warm. Having between 53~55°C. Theoretically those drives can operate in up to 60°C, but at the end of 2nd day, two disk turned off and I have to turn them off and on again to make them discoverable, which just reminds me:\nAs SMART didn\u0026rsquo;t notice anything worrying, I was quite sure it\u0026rsquo;s just because of temperature (coldest one stayed alive). I finished my torture testing here and decided to \u0026ldquo;shuck\u0026rdquo; them.\nThere\u0026rsquo;s really good instruction on how to do that on iFixit 4, so I won\u0026rsquo;t be explaining it here.\nI ended up having 3 new WDC WD140EDGZ-11B1PA0 14TB 5400RPM disks, with average transfers around 170~200MB/s. What\u0026rsquo;s interesting SMART recognise them as 7200RPM, but I heard it\u0026rsquo;s not worth to trust it in case of WDC/HGST drives.\nsmartctl output smartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.13.0-21-generic] (local build) Copyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org === START OF INFORMATION SECTION === Device Model: WDC WD140EDGZ-11B1PA0 LU WWN Device Id: 5 000cca 2adcc7fcb Firmware Version: 85.00A85 User Capacity: 14 000 519 643 136 bytes [14,0 TB] Sector Sizes: 512 bytes logical, 4096 bytes physical Rotation Rate: 7200 rpm Form Factor: 3.5 inches Device is: Not in smartctl database [for details use: -P showall] ATA Version is: ACS-2, ATA8-ACS T13/1699-D revision 4 SATA Version is: SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s) Local Time is: Fri Nov 12 15:59:23 2021 CET SMART support is: Available - device has SMART capability. SMART support is: Enabled === START OF READ SMART DATA SECTION === SMART overall-health self-assessment test result: PASSED General SMART Values: Offline data collection status: (0x80)\tOffline data collection activity was never started. Auto Offline Data Collection: Enabled. Self-test execution status: ( 0)\tThe previous self-test routine completed without error or no self-test has ever been run. Total time to complete Offline data collection: ( 101) seconds. Offline data collection capabilities: (0x5b) SMART execute Offline immediate. Auto Offline data collection on/off support. Suspend Offline collection upon new command. Offline surface scan supported. Self-test supported. No Conveyance Self-test supported. Selective Self-test supported. SMART capabilities: (0x0003)\tSaves SMART data before entering power-saving mode. Supports SMART auto save timer. Error logging capability: (0x01)\tError logging supported. General Purpose Logging supported. Short self-test routine recommended polling time: ( 2) minutes. Extended self-test routine recommended polling time: (1383) minutes. SCT capabilities: (0x003d)\tSCT Status supported. SCT Error Recovery Control supported. SCT Feature Control supported. SCT Data Table supported. SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 1 Raw_Read_Error_Rate 0x000b 100 100 001 Pre-fail Always - 0 2 Throughput_Performance 0x0004 100 100 054 Old_age Offline - 0 3 Spin_Up_Time 0x0007 092 092 001 Pre-fail Always - 0 (Average 328) 4 Start_Stop_Count 0x0012 100 100 000 Old_age Always - 5 5 Reallocated_Sector_Ct 0x0033 100 100 001 Pre-fail Always - 0 7 Seek_Error_Rate 0x000a 100 100 001 Old_age Always - 0 8 Seek_Time_Performance 0x0004 100 100 020 Old_age Offline - 0 9 Power_On_Hours 0x0012 100 100 000 Old_age Always - 0 10 Spin_Retry_Count 0x0012 100 100 001 Old_age Always - 0 12 Power_Cycle_Count 0x0032 100 100 000 Old_age Always - 5 22 Unknown_Attribute 0x0023 100 100 025 Pre-fail Always - 100 192 Power-Off_Retract_Count 0x0032 100 100 000 Old_age Always - 5 193 Load_Cycle_Count 0x0012 100 100 000 Old_age Always - 5 194 Temperature_Celsius 0x0002 062 062 000 Old_age Always - 25 (Min/Max 20/27) 196 Reallocated_Event_Count 0x0032 100 100 000 Old_age Always - 0 197 Current_Pending_Sector 0x0022 100 100 000 Old_age Always - 0 198 Offline_Uncorrectable 0x0008 100 100 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x000a 100 100 000 Old_age Always - 0 SMART Error Log Version: 1 No Errors Logged SMART Self-test log structure revision number 1 No self-tests have been logged. [To run self-tests, use: smartctl -t] SMART Selective self-test log data structure revision number 1 SPAN MIN_LBA MAX_LBA CURRENT_TEST_STATUS 1 0 0 Not_testing 2 0 0 Not_testing 3 0 0 Not_testing 4 0 0 Not_testing 5 0 0 Not_testing Selective self-test flags (0x0): After scanning selected spans, do NOT read-scan remainder of disk. If Selective self-test is pending on power-up, resume after 0 minute delay. That\u0026rsquo;s enough for today. Next time, I will build ZFS array with them .\n2024-03: Update on the usage I own those drives for 3 years now. I used the tape patch on the electric pin to install them in my PC and they work since then. They behave quite nice, no issues reported, no relocated bad sectors.\nReported temperatures inside PC are ranging between 35~45C, which is much better than in those plastic boxes. They\u0026rsquo;re not the fastest drives, but as I use them for storing growing library of my photos, they\u0026rsquo;re perfect.\nRecent smartctl output sudo smartctl -a /dev/sdc ... SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 1 Raw_Read_Error_Rate 0x000b 100 100 001 Pre-fail Always - 0 2 Throughput_Performance 0x0004 133 133 054 Old_age Offline - 108 3 Spin_Up_Time 0x0007 081 081 001 Pre-fail Always - 394 (Average 393) 4 Start_Stop_Count 0x0012 098 098 000 Old_age Always - 1048 5 Reallocated_Sector_Ct 0x0033 100 100 001 Pre-fail Always - 0 7 Seek_Error_Rate 0x000a 100 100 001 Old_age Always - 0 8 Seek_Time_Performance 0x0004 128 128 020 Old_age Offline - 18 9 Power_On_Hours 0x0012 100 100 000 Old_age Always - 2547 10 Spin_Retry_Count 0x0012 100 100 001 Old_age Always - 0 12 Power_Cycle_Count 0x0032 085 085 000 Old_age Always - 1039 22 Unknown_Attribute 0x0023 100 100 025 Pre-fail Always - 100 192 Power-Off_Retract_Count 0x0032 100 100 000 Old_age Always - 1722 193 Load_Cycle_Count 0x0012 100 100 000 Old_age Always - 1722 194 Temperature_Celsius 0x0002 048 048 000 Old_age Always - 34 (Min/Max 18/52) 196 Reallocated_Event_Count 0x0032 100 100 000 Old_age Always - 0 197 Current_Pending_Sector 0x0022 100 100 000 Old_age Always - 0 198 Offline_Uncorrectable 0x0008 100 100 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x000a 100 100 000 Old_age Always - 0 Enjoyed? https://www.reddit.com/r/DataHoarder/comments/elels8/wd_my_book_14_tb_shucked_wd140edfz_us7sap140/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://wiki.archlinux.org/title/badblocks\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://zackreed.me/new-disk-stress-test/\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.ifixit.com/Guide/How+to+Shuck+a+WD+Elements+External+Hard+Drive/137646\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2021/11/shucking-wd-elements-14tb/","summary":"I used to have RAID (or at least some variation of it) for my main storage. For redundancy, in case of disk failure. I started with some crazy LVM mirrors done on two disks of different size. Sync job was starting on every boot \u0026#x1f604;\nThen came time for RAID5 on mdadm + LVM for volume management. It was working nice until the moment when disks became bigger. Long array rebuilds or checks, required my PC to stay turned on overnight just to validate if stuff works still.","title":"Shucking WD Elements 14TB"},{"content":"I don\u0026rsquo;t know how it is in your company, but in mine it\u0026rsquo;s considered a good practice to add ticket numbers to commit messages. It allows to easily determine why something was changed, etc. Makes sense, but this also means, that I should be adding this ticket to every message\u0026hellip; And this doesn\u0026rsquo;t make sense for me. I will accidentally avoid it from time to time or make a lot of typos.\nI prepared little automation to handle that. I use two git aliases. One reads ticket Id from git branch, the other automatically pre-fixes all my commit messages with it. If there\u0026rsquo;s no ticket, commit will fail.\nBranch name might start with feature/, bugfix/ or hotfix/. Then comes ticket number, I guess your tickets also have format that can be easily matched with a regular expressions. Then, I like to describe branch purpose.\nI have such aliases in my ~/.gitconfig:\n~/.gitconfig # my custom aliases [alias] jira = !\u0026#34;f() { git rev-parse --abbrev-ref HEAD | sed -n -E \u0026#39;s#^(feature|(bug|hot)-?(fix)?)/([A-Z]+-[0-9]+)[^a-zA-Z0-9].*#\\\\4#p\u0026#39; ; }; f\u0026#34; cm = !\u0026#34;f() { if echo \\\u0026#34;$1\\\u0026#34; | egrep -q \u0026#39;^[A-Z]+-[0-9]+ \u0026#39;; then git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$1\\\u0026#34;; else JIRA=$(git jira); if [ -z \u0026#34;$JIRA\u0026#34; ]; then echo \u0026gt;\u0026amp;2 \u0026#39;#### Start message with Jira ticket number! ####\u0026#39;; exit 1; else git add -A \u0026amp;\u0026amp; git commit -m \\\u0026#34;$JIRA $1\\\u0026#34;; fi; fi; }; f\u0026#34; How to use it? Whether I use git-flow or just simple feature branching, before any commit, I always start with fresh branch: Create new branch git checkout -b feature/ABC-123-descriptive-branch-purpose My aliases read ticket from branch, so I have to provide ticket number only once - during branch creation. Then, I commit my changes like this: Commit changes git cm \u0026#34;Descriptive message\u0026#34; Result of it is: What it does git add -A git commit -m \u0026#34;ABC-123 Descriptive message\u0026#34; With this simple trick, I have to pay attention to ticket number once in a while. Rest is magic \u0026#x1f60e;\n","permalink":"https://gagor.pro/2021/11/automatically-add-ticket-id-to-every-commit-message-in-git/","summary":"I don\u0026rsquo;t know how it is in your company, but in mine it\u0026rsquo;s considered a good practice to add ticket numbers to commit messages. It allows to easily determine why something was changed, etc. Makes sense, but this also means, that I should be adding this ticket to every message\u0026hellip; And this doesn\u0026rsquo;t make sense for me. I will accidentally avoid it from time to time or make a lot of typos.","title":"Automatically add ticket ID to every commit message in Git"},{"content":"I was updating my blog and needed to generate few variants of images, in different resolution.\nOption 1 - sips There\u0026rsquo;s simple, builtin tool sips, that can be used for simple resizing 1:\nResize single image sips -Z 36 orig.png --out static/favicon36x36.png -Z - maintain image aspect ratio 36 - maximum height and width It can be also used for batch image processing:\nWarning\nBeware, without \u0026ndash;out param, it will overwrite images in place!\nBatch image resizing sips -Z 1024 *.jpg Option 2 - imagemagick For more complicated use cases, imagemagick have no competition. It\u0026rsquo;s not available out of the box, you have to install it:\nInstall imagemagick brew install imagemagick Imagemagick provides additional abilities to resize and then crop images. Let use cat image below as a demo.\nSource: www.pexels.com\u0026thinsp; external link Just scale keeping whole image Scale down convert demo.webp \\ -resize 300x100 \\ demo-just-resize.webp The resulting images is: Check the result identify demo-just-resize.webp demo-just-resize.webp WEBP 67x100 67x100+0+0 8-bit sRGB 4076B 0.000u 0:00.000 Scale down but keeping width Scale down keeping width convert demo.webp \\ -resize 300x100^ \\ demo-just-resize2.webp The resulting image is: Check the result identify demo-just-resize2.webp demo-just-resize2.webp WEBP 300x450 300x450+0+0 8-bit sRGB 8078B 0.000u 0:00.002 Resize and crop from top Resize and crop from top convert demo.webp \\ -resize 300x100^ \\ -extent 300x100 \\ demo-resize-crop.webp The resulting image is: Check the result identify demo-resize-crop.webp demo-resize-crop.webp WEBP 300x100 300x100+0+0 8-bit sRGB 3400B 0.000u 0:00.001 Resize and crop center Resize, then crop convert demo.webp \\ -resize 300x100^ \\ -gravity Center \\ -crop 300x100+0+0 +repage \\ demo-resize-crop2.webp The resulting image is: Check the result identify demo-resize-crop2.webp demo-resize-crop2.webp WEBP 300x100 300x100+0+0 8-bit sRGB 5202B 0.000u 0:00.001 As you might guess, there are many cases and far more options, than described in this post. Personally, I was pretty satisfied with the last one. I was able to cut most of images properly, being able to generate nice cover images with just one command.\nhttps://lifehacker.com/batch-resize-images-quickly-in-the-os-x-terminal-5962420\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2021/11/resize-images-from-command-line-on-macos/","summary":"I was updating my blog and needed to generate few variants of images, in different resolution.\nOption 1 - sips There\u0026rsquo;s simple, builtin tool sips, that can be used for simple resizing 1:\nResize single image sips -Z 36 orig.png --out static/favicon36x36.png -Z - maintain image aspect ratio 36 - maximum height and width It can be also used for batch image processing:\nWarning\nBeware, without \u0026ndash;out param, it will overwrite images in place!","title":"Resize images from command line on MacOS"},{"content":"I use brew extensively on MacOS. It\u0026rsquo;s just as convenient as many Linux package managers. What I don\u0026rsquo;t like, it leaves dependencies after removal of formula. There\u0026rsquo;s simple way to clean it up by running one command 1.\nUninstall with dependencies brew uninstall FORMULA brew autoremove Info\nIn my case running brew autoremove actually removed few packages I really wanted to have. Check the output carefully!\nhttps://stackoverflow.com/questions/7323261/uninstall-remove-a-homebrew-package-including-all-its-dependencies\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2021/11/homebrew-uninstall-formula-with-dependencies/","summary":"I use brew extensively on MacOS. It\u0026rsquo;s just as convenient as many Linux package managers. What I don\u0026rsquo;t like, it leaves dependencies after removal of formula. There\u0026rsquo;s simple way to clean it up by running one command 1.\nUninstall with dependencies brew uninstall FORMULA brew autoremove Info\nIn my case running brew autoremove actually removed few packages I really wanted to have. Check the output carefully!\nhttps://stackoverflow.com/questions/7323261/uninstall-remove-a-homebrew-package-including-all-its-dependencies\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Homebrew - uninstall formula with dependencies"},{"content":"I\u0026rsquo;ve build new PC - it\u0026rsquo;s based on Asus ROG STRIX Z590-E GAMING WIFI\u0026thinsp; external link motherboard. Generally, I\u0026rsquo;m quite satisfied, but it have one irritating downside - after each UEFI BIOS upgrade, it\u0026rsquo;s silently resetting some of settings.\nLet me note, what I want to have there:\nAi Tweaker (use my RAM capabilities) AI Overcloack Tuner -\u0026gt; [XMP I] DRAM Frequency -\u0026gt; [DDR4-3600MHz] DRAM CAS# Latency -\u0026gt; [16] DRAM RAS# to CAS# Delay -\u0026gt; [19] DRAM RAS# ACT Time -\u0026gt; [39] DRAM Voltage -\u0026gt; [1.35000] Advanced Platform Misc Configuration (enable some power management) PCI Express Native Power Management -\u0026gt; [Enabled] Native ASPM -\u0026gt; [Auto] APM Configuration ErP Ready -\u0026gt; [Enable(S4+S5)] CPU Configuration (virtualization optimisations) Intel (VMX) Virtualization Technology -\u0026gt; [Enabled] System Agent (SA) Configuration VT-d -\u0026gt; [Enabled] Trusted Computing (TPM v2 for Windows 11) Security Device Support -\u0026gt; [Enabled] Onboard Devices Configuration USB Audio -\u0026gt; [Disabled] (don\u0026rsquo;t work well on Linux) INTEL 2.5G LAN1 -\u0026gt; [Disabled] (I don\u0026rsquo;t use them) INTEL 2.5G LAN2 -\u0026gt; [Disabled] USB power delivery in Soft Off state (S5) -\u0026gt; [Disabled] (disable mouse lightning when shut down) LED lightning (disable MB lightning when shut down) When system is in sleep, hibernate or soft off states -\u0026gt; [Stealth Mode] M.2_4 Configuration -\u0026gt; [PCIE] PRIEX16_3 Bandwidth -\u0026gt; [X2 Mode] (Xonar is only x2) CPU PCIE Configuration Mode -\u0026gt; [PCIEX16_1 + PCIEX16_2 + M.2_2] (I need that for 1st NVMe to work) Boot Boot Configuration POST Delay Time -\u0026gt; [1 sec] ","permalink":"https://gagor.pro/2021/10/asus-rog-strix-z590-e-gaming-wifi-my-uefi-bios-settings/","summary":"I\u0026rsquo;ve build new PC - it\u0026rsquo;s based on Asus ROG STRIX Z590-E GAMING WIFI\u0026thinsp; external link motherboard. Generally, I\u0026rsquo;m quite satisfied, but it have one irritating downside - after each UEFI BIOS upgrade, it\u0026rsquo;s silently resetting some of settings.\nLet me note, what I want to have there:\nAi Tweaker (use my RAM capabilities) AI Overcloack Tuner -\u0026gt; [XMP I] DRAM Frequency -\u0026gt; [DDR4-3600MHz] DRAM CAS# Latency -\u0026gt; [16] DRAM RAS# to CAS# Delay -\u0026gt; [19] DRAM RAS# ACT Time -\u0026gt; [39] DRAM Voltage -\u0026gt; [1.","title":"Asus ROG STRIX Z590-E GAMING WIFI - my UEFI BIOS settings"},{"content":" Od dobrego do wielkiegoCzynniki trwałego rozwoju i zwycięstwa ﬁrm\nAuthor: Jim Collins\namazon.plamazon.comempik.com The book is a comprehensive analysis of what makes a good company great, and how organizations can achieve long-term success. Collins and his team conducted extensive research to identify common characteristics and practices among companies that have made the leap from good to great, and the book presents the results of their findings.\nThe book is well-written and provides a wealth of practical insights and advice for organizations looking to improve their performance and achieve lasting success. I was impressed by Collins\u0026rsquo;s ability to distill complex ideas into simple and actionable steps.\nOverall, I would recommend \u0026ldquo;Good to Great\u0026rdquo; to anyone looking to improve their organizational performance, but I feel it would provide most value to the people on higher management positions. Persoanlly, I feel I\u0026rsquo;m not yet ready too take from this book fully, but is definitely worth checking out.\n","permalink":"https://gagor.pro/books/2021/od-dobrego-do-wielkiego/","summary":"Od dobrego do wielkiegoCzynniki trwałego rozwoju i zwycięstwa ﬁrm\nAuthor: Jim Collins\namazon.plamazon.comempik.com The book is a comprehensive analysis of what makes a good company great, and how organizations can achieve long-term success. Collins and his team conducted extensive research to identify common characteristics and practices among companies that have made the leap from good to great, and the book presents the results of their findings.\nThe book is well-written and provides a wealth of practical insights and advice for organizations looking to improve their performance and achieve lasting success.","title":"Od dobrego do wielkiego"},{"content":" Miłość i matematykaIstota ukrytej rzeczywistości\nAuthor: Edward Frenkel\nempik.com ","permalink":"https://gagor.pro/books/2021/milosc-i-matematyka/","summary":" Miłość i matematykaIstota ukrytej rzeczywistości\nAuthor: Edward Frenkel\nempik.com ","title":"Miłość i matematyka"},{"content":"Finally, they\u0026rsquo;re available! Wait a moment.. Actually they\u0026rsquo;re available for few months, just nobody published information about moving them to quay.io\u0026thinsp; external link and dropped poor guys using hub.docker.com\u0026thinsp; external link without any updates! Yes, that how they did!\nI found new place accidentally, reading some news about CentOS Stream 9 on their blog\u0026thinsp; external link . There was reference to CentOS 9 Stream dev builds of Docker images and I found \u0026ldquo;missing\u0026rdquo; stream and stream8 tags too. It\u0026rsquo;s not adding more confidence on my side to the CentOS project, when they\u0026rsquo;re not even communicating such changes publicly, sick!\nSome might remember, I was so desperate because of no official Docker images of CentOS 8 Stream available in Docker Hub, I build them on my own .\nWhat I don\u0026rsquo;t like, is that they completely stopped upgrading base images for centos8 and centos7. There\u0026rsquo;s still a lot of stuff using them, but they\u0026rsquo;re not updated for months:\nSource: https://quay.io/repository/centos/centos\u0026thinsp; external link What next? I switched my image to use as base new location quay.io/centos/centos:*. I\u0026rsquo;ll probably stay with my builds as I upgrade them on weekly basis. And because I squash them to single layer, they\u0026rsquo;re close to size of original images. Official images with all updates can easily get above 300MB.\nREPOSITORY TAG CREATED SIZE centos 7 8 months ago 204MB quay.io/centos/centos 7 8 months ago 204MB tgagor/centos 7 2 hours ago 214MB quay.io/centos/centos 8 7 months ago 209MB centos 8 7 months ago 209MB tgagor/centos 8 2 hours ago 262MB quay.io/centos/centos stream8 2 days ago 404MB tgagor/centos stream8 2 hours ago 230MB tgagor/centos stream9 2 hours ago 164MB quay.io/centos/centos stream9-development 4 days ago 166MB Where can you get it? You can fetch docker image here:\ntgagor/centos\u0026thinsp; external link Update on EOL - CentOS 8 Stream is dead\u0026thinsp; external link . Citing from this official blog:\nCentOS Linux 7 End of Life\u0026thinsp; external link : June 30, 2024 After June 30, 2024, there will be no updates published for CentOS Linux 7.\nCentOS Stream 8 End of Builds\u0026thinsp; external link : May 31, 2024 After May 31, 2024, CentOS Stream 8 will be archived and no further updates will be provided.\nI will keep my up to date images as long as updates will be available, but it\u0026rsquo;s better to not waste and upgrade to CentOS 9 Stream.\nEnjoyed? ","permalink":"https://gagor.pro/2021/07/official-centos-8-stream-docker-image-finally-available/","summary":"Finally, they\u0026rsquo;re available! Wait a moment.. Actually they\u0026rsquo;re available for few months, just nobody published information about moving them to quay.io\u0026thinsp; external link and dropped poor guys using hub.docker.com\u0026thinsp; external link without any updates! Yes, that how they did!\nI found new place accidentally, reading some news about CentOS Stream 9 on their blog\u0026thinsp; external link . There was reference to CentOS 9 Stream dev builds of Docker images and I found \u0026ldquo;missing\u0026rdquo; stream and stream8 tags too.","title":"Official CentOS 8 Stream Docker image finally available!"},{"content":" Inteligentny inwestorNajlepsza książka o inwestowaniu wartościowym\nAuthor: Benjamin Graham\nempik.com ","permalink":"https://gagor.pro/books/2021/inteligentny-inwestor/","summary":" Inteligentny inwestorNajlepsza książka o inwestowaniu wartościowym\nAuthor: Benjamin Graham\nempik.com ","title":"Inteligentny inwestor"},{"content":" Bogaty albo biednyPo prostu różni mentalnie\nAuthor: T. Harv Eker\nempik.com ","permalink":"https://gagor.pro/books/2021/bogaty-albo-biedny/","summary":" Bogaty albo biednyPo prostu różni mentalnie\nAuthor: T. Harv Eker\nempik.com ","title":"Bogaty albo biedny"},{"content":" Github repositories tgagor/ansible-role-docker - Installs Docker service on Ubuntu/Debian/RHEL tgagor/ansible-role-docker-compose - Simple Ansible role that will install Docker Compose tgagor/ansible-role-rpi-unifi - Installs Ubiquiti\u0026#39;s Unifi software on Raspberry Pi tgagor/ansible-role-spotify tgagor/ansible-role-template-with-molecule-tests - Template for Ansible role with Molecule and Testinfra for testing tgagor/conferences - Presentations and materials from conferences where I attended as a speaker tgagor/docker-centos - CentOS docker images, build weekly with latest security updates tgagor/docker-nvm-example - Repo with example NVM in Docker image implementation for use in easy CI/CD tgagor/docker-unifi-controller - UniFi Controllver v7 tgagor/docker-wp-cli - Docker image with WP-CLI command line interface for Wordpress tgagor/fslint-snap - Snapcraft package of FSlint Janitor Ghists How old official Docker images are? Script that can be used to purge nexus v3 releases Docker images tgagor/wp-cli - Image providing wp-cli tool. tgagor/packtpublishingfreelearning - Ready to run container with code from: https://github.com/igbt6/Packt-Publishing-Free-Learning tgagor/centos-stream - There\u0026#39;s no official image for CentOS 8 Stream - so I prepared it, something for early adopters tgagor/centos - Pure, based on official CentOS images, upgraded every Monday tgagor/unifi-controller ","permalink":"https://gagor.pro/projects/","summary":"Github repositories tgagor/ansible-role-docker - Installs Docker service on Ubuntu/Debian/RHEL tgagor/ansible-role-docker-compose - Simple Ansible role that will install Docker Compose tgagor/ansible-role-rpi-unifi - Installs Ubiquiti\u0026#39;s Unifi software on Raspberry Pi tgagor/ansible-role-spotify tgagor/ansible-role-template-with-molecule-tests - Template for Ansible role with Molecule and Testinfra for testing tgagor/conferences - Presentations and materials from conferences where I attended as a speaker tgagor/docker-centos - CentOS docker images, build weekly with latest security updates tgagor/docker-nvm-example - Repo with example NVM in Docker image implementation for use in easy CI/CD tgagor/docker-unifi-controller - UniFi Controllver v7 tgagor/docker-wp-cli - Docker image with WP-CLI command line interface for Wordpress tgagor/fslint-snap - Snapcraft package of FSlint Janitor Ghists How old official Docker images are?","title":"My projects"},{"content":"I wanted to share publicly some photos, but I performed them with navigation enabled so they contained accurate localization of my house. I wanted to remove EXIF data GPS tags, my phone type and other irrelevant stuff.\nTip\nThere\u0026rsquo;s a way that requires less effort. Check how to automate this process with pre-commit hooks\u0026thinsp; external link .\nTL;DR You will need imagemagick installed (use apt/yum/dnf of whatever you have there):\nInstall imagemagick sudo apt install -y imagemagick To remove them just use: Strip EXIF data mogrify -strip image.jpg How to check if it\u0026rsquo;s working? First, let\u0026rsquo;s get some example images 1.\nSource: github.com/ianare/exif-samples\nTo check what tags image provides, you can use identify tool (I limited output to only GPS data because it\u0026rsquo;s just too much stuff there): Review EXIF data identify -verbose image.jpg | wc -l 156 identify -verbose image.jpg | grep GPS exif:GPSAltitudeRef: 0 exif:GPSDateStamp: 2008:10:23 exif:GPSImgDirectionRef: exif:GPSInfo: 926 exif:GPSLatitude: 43/1, 28/1, 281400000/100000000 exif:GPSLatitudeRef: N exif:GPSLongitude: 11/1, 53/1, 645599999/100000000 exif:GPSLongitudeRef: E exif:GPSMapDatum: WGS-84 exif:GPSSatellites: 06 exif:GPSTimeStamp: 14/1, 27/1, 724/100 There was 156 lines of different tags!\nAfter cleanup: Check EXIF data again identify -verbose image.jpg | wc -l 88 identify -verbose image.jpg | grep GPS you will get only generic information data, without any GPS tags.\nhttps://github.com/ianare/exif-samples/tree/master/jpg/gps\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2021/03/how-to-remove-geo-localization/exif-data-from-photos/","summary":"I wanted to share publicly some photos, but I performed them with navigation enabled so they contained accurate localization of my house. I wanted to remove EXIF data GPS tags, my phone type and other irrelevant stuff.\nTip\nThere\u0026rsquo;s a way that requires less effort. Check how to automate this process with pre-commit hooks\u0026thinsp; external link .\nTL;DR You will need imagemagick installed (use apt/yum/dnf of whatever you have there):","title":"How to remove geo-localization/EXIF data from photos"},{"content":" KsiążęAuthor: Nicolo Machiavelli\nempik.com ","permalink":"https://gagor.pro/books/2021/ksiaze/","summary":" KsiążęAuthor: Nicolo Machiavelli\nempik.com ","title":"Książę"},{"content":"It\u0026rsquo;s sometimes useful to quickly connect to JMX console, to checkout what\u0026rsquo;s going on in your application, but the whole thing get\u0026rsquo;s tricky if you\u0026rsquo;re running your app in a container. I need it from time to time and I keep myself few times searching for set of params below:\n~/2021/02/how-to-run-jmx-monitoring-in-docker-image/ java \\ ... -Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.rmi.port=${PORT1} \\ -Dcom.sun.management.jmxremote.port=${PORT1} \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=${HOST} The whole magic here is that PORT1 in container is app\u0026rsquo;s second port. So if you expose both HTTP and HTTPS on two separate ports, you will need here PORT2. I setup both RMI and JMX ports to same value. Second thing here is a hostname, it have to be hostname which you will use to connect to the container. If you use domain name, it have to be this domain name. If you want to use IP, set it to IP. It\u0026rsquo;s because it\u0026rsquo;s actually a virtual hostname for JMX HTTP server - if you mix them, you won\u0026rsquo;t get results. Container orchestrators often automatically export variable like HOST or HOSTNAME which you can use too hook.\nAfter such configuration and mapping ports to container, you should be able to connect to container with VisualVM\u0026thinsp; external link or Jconsole\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2021/02/how-to-run-jmx-monitoring-in-docker-image/","summary":"It\u0026rsquo;s sometimes useful to quickly connect to JMX console, to checkout what\u0026rsquo;s going on in your application, but the whole thing get\u0026rsquo;s tricky if you\u0026rsquo;re running your app in a container. I need it from time to time and I keep myself few times searching for set of params below:\n~/2021/02/how-to-run-jmx-monitoring-in-docker-image/ java \\ ... -Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.rmi.port=${PORT1} \\ -Dcom.sun.management.jmxremote.port=${PORT1} \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=${HOST} The whole magic here is that PORT1 in container is app\u0026rsquo;s second port.","title":"How to run JMX monitoring in Docker image?"},{"content":"We\u0026rsquo;re all divided with recent decision to focus on CentOS Stream\u0026thinsp; external link , which essentially means that stable, professional distro will turn into rolling release now. Also CentOS board members don\u0026rsquo;t gave us more confidence for the future\u0026thinsp; external link .\nI don\u0026rsquo;t want to be totally sceptic, I would like to test it on my own and only then, decide if it\u0026rsquo;s stable enough. But I work mostly with Docker containers and there are no official Docker images with Stream variant. I decided to create it on my own, based on official instruction\u0026thinsp; external link .\nImages after a switch were twice times bigger than basic centos:8, so I used old school way to squash them:\nSquash image by export/import docker run --name tgagor-centos-stream tgagor/centos-stream true docker export tgagor-centos-stream | docker import - tgagor/centos-stream:squashed docker rm tgagor-centos-stream This way I\u0026rsquo;m receiving just single layer with final packages and configuration. Result was pretty impressive:\nREPOSITORY VARIANT SIZE centos 8 209MB tgagor/centos-stream latest 455MB tgagor/centos-stream squashed 297MB New image is still bigger (because upgrade install additional dependencies), but it\u0026rsquo;s acceptable. I also used Docker Hub\u0026rsquo;s hooks\u0026thinsp; external link to customize build. Thanks to that, when you fetch image from registry, it will be already squashed ;)\nNow at least I can try it and decide on my own, if it\u0026rsquo;s stable enough for production workloads.\nWhere can you get it? You can fetch docker image here:\ntgagor/centos\u0026thinsp; external link Update Because other CentOS images do not receive updates anymore, I renamed repo from centos-stream -\u0026gt; centos and now I\u0026rsquo;m building all the variants installing all updates on them every week (automatically).\nIf you still use tgagor/centos-stream images, please switch to tgagor/centos.\n","permalink":"https://gagor.pro/2021/02/centos-8-stream-docker-image/","summary":"We\u0026rsquo;re all divided with recent decision to focus on CentOS Stream\u0026thinsp; external link , which essentially means that stable, professional distro will turn into rolling release now. Also CentOS board members don\u0026rsquo;t gave us more confidence for the future\u0026thinsp; external link .\nI don\u0026rsquo;t want to be totally sceptic, I would like to test it on my own and only then, decide if it\u0026rsquo;s stable enough. But I work mostly with Docker containers and there are no official Docker images with Stream variant.","title":"CentOS 8 Stream Docker image"},{"content":" Info\nI published recently more up to date article. Check here\u0026thinsp; external link .\nTL;DR\nCentOS base images sucks! They\u0026rsquo;re old, not updated for months!\nAs a professional DevOps I concern about a lot of things\u0026hellip; but security is always close to the top of the list. With Docker build environments and deployments became much more stable, which often is a result of just being stale ;/\nI\u0026rsquo;ve been talking about this for long time but it\u0026rsquo;s still hard for people to believe it. Let\u0026rsquo;s check then few, most popular images and when were they last time updated. I wrote a script for that, so feel free to check your list:\nResults show that not all images are frequently updated and if we get a little bit deeper and check how many packages require upgrade:\nImage Creation date Age (in days) Packages to upgrade centos:7 2020-11-14T00:20:04.644613188Z 75 16 centos:8 2020-12-08T00:22:53.076477777Z 51 12 debian:9 2021-01-12T00:35:06.08981705Z 16 0 debian:10 2021-01-12T00:32:37.071722022Z 16 0 ubuntu:18.04 2021-01-21T03:38:05.801776526Z 7 2 ubuntu:20.04 2021-01-21T03:38:23.37559427Z 7 4 alpine:3.11 2020-12-17T00:19:49.284211148Z 42 0 alpine:3.12 2020-12-17T00:19:42.11518025Z 42 0 alpine:3.13 2021-01-15T02:23:51.238454884Z 13 5 node:10 2021-01-27T20:32:54.257201224Z 0 13 node:12 2021-01-12T10:36:27.349274428Z 15 13 node:14 2021-01-12T10:33:48.195283512Z 15 13 node:15 2021-01-27T20:29:39.779176105Z 0 13 openjdk:8 2021-01-21T02:40:05.312239007Z 7 0 openjdk:11 2021-01-21T02:38:20.819671373Z 7 0 openjdk:15 2021-01-20T00:45:36.664060993Z 8 ? Personally, I consider running yum upgrade or apt upgrade/apt dist-upgrade in Dockerfile as anti-pattern - instead builds should be running so frequently to automatically pull all new upgrades from base images. That\u0026rsquo;s theory, but with images like CentOS, you have to do it or risk running your software on unpatched and potentially unsecure system. That\u0026rsquo;s why I don\u0026rsquo;t like CentOS images as a base in general, they just suck from this perspective.\nThere\u0026rsquo;s also another issue here - running those upgrades makes your image just bigger. Sometimes significantly bigger. That\u0026rsquo;s not what I expect from base images.\n","permalink":"https://gagor.pro/2021/01/how-old-are-official-docker-images/","summary":"Info\nI published recently more up to date article. Check here\u0026thinsp; external link .\nTL;DR\nCentOS base images sucks! They\u0026rsquo;re old, not updated for months!\nAs a professional DevOps I concern about a lot of things\u0026hellip; but security is always close to the top of the list. With Docker build environments and deployments became much more stable, which often is a result of just being stale ;/\nI\u0026rsquo;ve been talking about this for long time but it\u0026rsquo;s still hard for people to believe it.","title":"How old are Official Docker images?"},{"content":" ErykAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-eryk/","summary":" ErykAuthor: Terry Pratchett\nempik.com ","title":"Eryk"},{"content":" CzarodzicielstwoAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-czarodzicielstwo/","summary":" CzarodzicielstwoAuthor: Terry Pratchett\nempik.com ","title":"Czarodzicielstwo"},{"content":" MortAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-mort/","summary":" MortAuthor: Terry Pratchett\nempik.com ","title":"Mort"},{"content":" RównoumagicznienieAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-rownoumagicznienie/","summary":" RównoumagicznienieAuthor: Terry Pratchett\nempik.com ","title":"Równoumagicznienie"},{"content":" Blask fantastycznyAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-blask-fantastyczny/","summary":" Blask fantastycznyAuthor: Terry Pratchett\nempik.com ","title":"Blask fantastyczny"},{"content":" Kolor magiiAuthor: Terry Pratchett\nempik.com ","permalink":"https://gagor.pro/books/2021/pratchett-kolor-magii/","summary":" Kolor magiiAuthor: Terry Pratchett\nempik.com ","title":"Kolor magii"},{"content":"I started my blog on custom (written by my) engine, but as I didn\u0026rsquo;t had enough time to enhance it I switched to Wordpress. I\u0026rsquo;ve been using Wordpress as an engine of my blog for past 8~9 years. I have small VPS with PHP + Nginx and you can find a lot of configuration examples from my config on this site \u0026#x1f604;\nThere was a time, when I was really satisfied by what it provides. Not only because of features, but also beacause I was able to play with insane configuration options (check out my caching reverse proxy config ). For me it was opportuninty to excel with my skills.\nBut things change. I don\u0026rsquo;t have that much time to carry this server configuration, keep it properly updated and play with new features. Actually I don\u0026rsquo;t need that anymore as I\u0026rsquo;ve been there, I saw it already\u0026hellip;\nI\u0026rsquo;ve been thinking about switching to static page generator and putting my blog in eg. github pages for really long time. I even almost completely migrated it to pelican\u0026thinsp; external link (after playing for a while with Octopress\u0026thinsp; external link too). Eventually I\u0026rsquo;ve found Hugo\u0026thinsp; external link and I loved it!\nAnd here we are, I switched blog to Hugo. I will try to share why I choose this configuration soon and how I handle it.\nKeep warm, be positive, stay negative!\n","permalink":"https://gagor.pro/2020/10/bye-bye-wordpress/","summary":"I started my blog on custom (written by my) engine, but as I didn\u0026rsquo;t had enough time to enhance it I switched to Wordpress. I\u0026rsquo;ve been using Wordpress as an engine of my blog for past 8~9 years. I have small VPS with PHP + Nginx and you can find a lot of configuration examples from my config on this site \u0026#x1f604;\nThere was a time, when I was really satisfied by what it provides.","title":"Bye Bye Wordpress!"},{"content":" W transie inwestowaniaPodbij rynek pewnością siebie, żelazną dyscypliną i postawą zwycięzcy\nAuthor: Mark Douglas\nempik.com ","permalink":"https://gagor.pro/books/2020/w-transie-investowania/","summary":" W transie inwestowaniaPodbij rynek pewnością siebie, żelazną dyscypliną i postawą zwycięzcy\nAuthor: Mark Douglas\nempik.com ","title":"W transie inwestowania"},{"content":" Teoretyczne minimumCo musisz wiedzieć, żeby zacząć zajmować się fizyką\nAuthors: Leonard Susskind, George Hrabovsky\nlegimi.pl ","permalink":"https://gagor.pro/books/2020/teoretyczne-minimum/","summary":" Teoretyczne minimumCo musisz wiedzieć, żeby zacząć zajmować się fizyką\nAuthors: Leonard Susskind, George Hrabovsky\nlegimi.pl ","title":"Teoretyczne minimum"},{"content":" Przeciw bogomNiezwykłe dzieje ryzyka\nAuthor: Peter L. Bernstein\nempik.com ","permalink":"https://gagor.pro/books/2020/przeciw-bogom/","summary":" Przeciw bogomNiezwykłe dzieje ryzyka\nAuthor: Peter L. Bernstein\nempik.com ","title":"Przeciw bogom"},{"content":" Podróż ludzi KsięgiAuthor: Olga Tokarczuk\nempik.com ","permalink":"https://gagor.pro/books/2020/tokarczuk/","summary":" Podróż ludzi KsięgiAuthor: Olga Tokarczuk\nempik.com ","title":"Podróż ludzi Księgi"},{"content":" Java Performance CompanionAuthors: Charlie Hunt, Monica Beckwith, Poonam Parhar, Bengt Rutisson\namazon.pl ","permalink":"https://gagor.pro/books/2020/java-performance-companion/","summary":" Java Performance CompanionAuthors: Charlie Hunt, Monica Beckwith, Poonam Parhar, Bengt Rutisson\namazon.pl ","title":"Java Performance Companion"},{"content":" Troubleshooting Java PerformanceDetecting Anti-Patterns with Open Source Tools 1st ed. Edition\nAuthor: Erik Ostermueller\namazon.com ","permalink":"https://gagor.pro/books/2020/trubleshooting-java-performance/","summary":" Troubleshooting Java PerformanceDetecting Anti-Patterns with Open Source Tools 1st ed. Edition\nAuthor: Erik Ostermueller\namazon.com ","title":"Troubleshooting Java Performance"},{"content":"Few years ago I moved from Linux desktop to MacOS for my business, day to day work. There were 2 main reasons for that:\nCorporations don\u0026rsquo;t like Linux - they can\u0026rsquo;t manage it, they can\u0026rsquo;t support it, so they blocked it with \u0026ldquo;Security policy\u0026rdquo;, ISO20001, or other nonsense. Actually they\u0026rsquo;re partially right but in different place - many business collaboration applications don\u0026rsquo;t work well on LInux (or they don\u0026rsquo;t work at all) Skype for Business - there\u0026rsquo;s open source alternative but to get full support you have to pay for additional codecs (as far as I remember) - it\u0026rsquo;s not working stable even in paid version Outlook and calendar support - I love Thunderbird and I use it for years, but calendar invitations didn\u0026rsquo;t work nice (honestly, they didn\u0026rsquo;t work nice even between different Outlook versions\u0026hellip;) Corporate VPN apps - Christ, I always was able to get it working eventually, but\u0026hellip; why bother I\u0026rsquo;m older, maybe lazier, maybe smarter - I don\u0026rsquo;t like to spend my time resolving problems that don\u0026rsquo;t give me any value. That\u0026rsquo;s how I switched to MacOS - for business purposes only. Privately I still prefer Linux.\nAfter the switch I\u0026rsquo;ve found some differences. Annoying stuff like different behavior of home/end buttons, etc. So right now, on every Mac that I\u0026rsquo;m working with, I\u0026rsquo;m making it to work more like Linux desktop. I\u0026rsquo;ve found those information useful to few my friends too. I decided to publish this because I received too many questions about what to do, how to start?\nIf you think I\u0026rsquo;m missing something important or I did something really bad way - please comment, I will updated it.\nHow to make screnshots (full screen/partial/desktop recording)? https://support.apple.com/pl-pl/HT201361\u0026thinsp; external link How to change screenshot save localisation? By default screenshots are saved on desktop which will turn into mess quickly. It\u0026rsquo;s possible to change default save localization for created screenshots: https://discussions.apple.com/docs/DOC-9081\u0026thinsp; external link Keyboard and keyboard shortcuts\u0026hellip; Polish keyboard layout is terrific, location of tilde and backslash buttons cause both Left Shift and Enter to be really far from normal hands position - in my case it\u0026rsquo;s causing pain in hands after few hours of use Another problem is location of Right Alt, it\u0026rsquo;s hidden deeply under hand during writing so it\u0026rsquo;s not convenient to write polish letters like ąśłóćź, etc. Maybe it will be possible to remap few keys to make this layout more usable but right now experience is terrific.\nThis is really big issue. Because on Mac Win/CMD key is used a lot switch to normal keyboard doesn\u0026rsquo;t help. Use of most common shortcuts really overload my thumbs.\nBest solutions I\u0026rsquo;ve found is Karabiner-Elements\u0026thinsp; external link . It allow to remap keys (ex. switch right alt/cmd) and you can define different options per device (internal/external keyboard). It\u0026rsquo;s also very useful to make standard PC keyboards to be mapped like Apple keyboard.\nSpecial function keys do not work from external keyboard (it\u0026rsquo;s not Mac compatible ) I don\u0026rsquo;t know if it\u0026rsquo;s possible to configure them. With Karabiner-Elements\u0026thinsp; external link it\u0026rsquo;s possible to add support for some of them.\nKeyboard shortcuts are totally different than on Windows or Linux Here you could find introduction to most typical shortcuts: https://www.apple.com/support/pages/shortcuts/body.html No other way - you have to learn them.\nFew of my favorites, I use everyday:\nCmd + Space - Spotlight search - think about it like ‘Win\u0026rsquo; key in Gnome 3, you can start writing app or file name to start/open it Ctrl + Left/Right - switch Desktop on specific screen (full screen apps use \u0026ldquo;whole desktop\u0026rdquo; so it\u0026rsquo;s easy way to see what you have there or start new empty desktop) Ctrl + Up - shows all active windows, desktops, etc. Useful if you\u0026rsquo;re searching specific window Problem with bash completion on linux boxes -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) I solved it by marking option in iTerm2 to always set language system variables.\nBash completion do not work well on Mac, there are no completions for hosts configured in ssh_config or /etc/hosts I initially tried this one: http://davidalger.com/development/bash-completion-on-os-x-with-brew/ - it generally works but only for some common tools, ex, svn requires manual download of:\ncurl -L http://svn.apache.org/repos/asf/subversion/trunk/tools/client-side/bash_completion -o /usr/local/etc/bash_completion.d/svn Right now I thing that Brew makes this even easier, because it\u0026rsquo;s installing a lot of bash_completion configs.\nHow to add bash completion for docker? Those two commands will solve problem:\ncurl -L https://raw.githubusercontent.com/docker/compose/master/contrib/completion/bash/docker-compose -o /usr/local/etc/bash_completion.d/docker-compose curl -L https://raw.githubusercontent.com/docker/docker-ce/blob/master/components/cli/contrib/completion/bash/docker -o /usr/local/etc/bash_completion.d/docker SSH agent for key management does not work by default There is a Keychain application installed on MacOS by default, it\u0026rsquo;s responsible for storing keys and managing access to them. To add ssh key to Keychain you have to run:\nssh-add -K and provide password to unlock key.\nSadly this works only one time, I have to manually add key to keychain every time I login by:\nssh-add ~/.ssh/id_rsa How to automatically unlock private SSH keys on login (how to keep SSH private key password in OS X Keychain)? It\u0026rsquo;s all nice described here: https://apple.stackexchange.com/a/250572\u0026thinsp; external link How to write to multiple terminal panes in iTerm2? use cmd + shift + i to write to all panes on all tabs, or cmd + alt + i to write to all panes on current tab only\nBlurry fonts on external monitors Fonts on external monitors are really blurry - they\u0026rsquo;re badly anti-aliased or hinting is bad. I\u0026rsquo;ve found, that MacOS disable hinting on external monitors. It\u0026rsquo;s possible to enable it back. Check below and play with it to get what would work for you.\nhttps://www.howtogeek.com/358596/how-to-fix-blurry-fonts-on-macos-mojave-with-subpixel-antialiasing/\u0026thinsp; external link Jump word left/right (Ctrl+left/right) shortcut don\u0026rsquo;t work on console (iTerm2) You have to configure special escape sequences for Alt+left/right, described here: http://apple.stackexchange.com/a/136931\u0026thinsp; external link Packages on MacOS are outdated and updates arrive later than on Linux Yes, that\u0026rsquo;s sad true. When I have Ansible 2.3 on Jenkins server for MacOS only version 2.2 was available. Version 2.3 will arrive but some time later. This is causing problems in compatibility of code (newer features/options on Jenkins cause problems during deployment and I\u0026rsquo;m not able to test this all on my workstation before real release). Another problem connected to that is that some command line tool on Mac have different switches than on Linux, ex. date -rfc-3339=s is not available, causing scripts to broke on Mac when working on Linux, this also makes testing harder.\nMacOS also use quite old version of bash. As a result .bashrc won\u0026rsquo;t be parsed, you have to put everything to .bash_profile which will slow down starting of each new terminal session (ex. python virtual envs can add significant delay).\nEnd/Home keys behave differently on MacOS Generally you won\u0026rsquo;t find Home/End keys on typical MacBook keyboad - by default on Mac you have Command + Right keyboard shortcut to mimic End, and Command + Left to mimic Home.\nBut\u0026hellip; by default Home/End will move you to the end of page, not line. If you want this behavior back in most of your apps you could try to change keybinding:\nOne option is to create ~/Library/KeyBindings/ and save a property list like this as ~/Library/KeyBindings/DefaultKeyBinding.dict:\n{ \u0026#34;\\UF729\u0026#34; = moveToBeginningOfLine:; \u0026#34;\\UF72B\u0026#34; = moveToEndOfLine:; \u0026#34;$\\UF729\u0026#34; = moveToBeginningOfLineAndModifySelection:; \u0026#34;$\\UF72B\u0026#34; = moveToEndOfLineAndModifySelection:; } Quit and reopen applications to apply the changes. Note that DefaultKeyBinding.dict is not supported by some applications like Xcode or Firefox.\nhttps://apple.stackexchange.com/questions/18016/can-i-change-the-behavior-of-the-home-and-end-keys-on-an-apple-keyboard-with-num\u0026thinsp; external link I can\u0026rsquo;t use X forwarding with MacOS ssh client and Linux on second end There\u0026rsquo;s additional X11 server app that you can install on MacOS (it\u0026rsquo;s called XQuartz\u0026thinsp; external link . I tried it for short time but I don\u0026rsquo;t need it anymore.\nI have problems working with terminator on Mac For example:\nit\u0026rsquo;s running as python process but it\u0026rsquo;s not available in Lunchpad (not easy to switch with Cmd + Tab keyboard shortcuts are different than on Linux, so this is not making switch easier I\u0026rsquo;ve found iTerm2, which is \u0026ldquo;state of the art\u0026rdquo; terminal for MacOS. It\u0026rsquo;s popular, well supported and feature complete.\nUseful key shortcuts:\nCmd + T - new tab Cmd + D - spit vertically Cmd + Shift + D - split horizontally Cmd + Opt + Left/Right/Up/Down - move between shell windows (after split) Cmd + Left/Right - prev/next tab Ctrl + Cmd + Left/Right/Up/Down - change size of windows after split How to access /opt folder and make it visible in Finder I use Homebrew\u0026thinsp; external link as a package manager on MacOS. It installs packages in /opt/homebrew and if I want to configure for example my code editor to use java or some ansible-lint from this location it\u0026rsquo;s not trivial. MacOS hides \u0026ldquo;such system paths\u0026rdquo; in file selector window. There are two approaches we can do:\nTo force Finder to show us such directories, we can run:\nEnable showing system files defaults write com.apple.Finder AppleShowAllFiles YES Now we have to reinitialize the Finder, either by rebooting or right-clicking the Finder on Dock, holding Option key, then selecting Relaunch.\nIf we want to turn it off again, we use:\nDisable showing system files defaults write com.apple.Finder AppleShowAllFiles NO And as above, reluch Finder.\nThere\u0026rsquo;s also another way, that works nicely with file selection windows. Just press Command + Shift + G, which will open a dialog box in which you can input path, like: /opt/homebrew/opt/openjdk...\nhttps://macpaw.com/how-to/access-opt-folder-on-mac\u0026thinsp; external link ","permalink":"https://gagor.pro/2020/01/moving-from-linux-to-macos-first-steps/","summary":"Few years ago I moved from Linux desktop to MacOS for my business, day to day work. There were 2 main reasons for that:\nCorporations don\u0026rsquo;t like Linux - they can\u0026rsquo;t manage it, they can\u0026rsquo;t support it, so they blocked it with \u0026ldquo;Security policy\u0026rdquo;, ISO20001, or other nonsense. Actually they\u0026rsquo;re partially right but in different place - many business collaboration applications don\u0026rsquo;t work well on LInux (or they don\u0026rsquo;t work at all) Skype for Business - there\u0026rsquo;s open source alternative but to get full support you have to pay for additional codecs (as far as I remember) - it\u0026rsquo;s not working stable even in paid version Outlook and calendar support - I love Thunderbird and I use it for years, but calendar invitations didn\u0026rsquo;t work nice (honestly, they didn\u0026rsquo;t work nice even between different Outlook versions\u0026hellip;) Corporate VPN apps - Christ, I always was able to get it working eventually, but\u0026hellip; why bother I\u0026rsquo;m older, maybe lazier, maybe smarter - I don\u0026rsquo;t like to spend my time resolving problems that don\u0026rsquo;t give me any value.","title":"Moving from Linux to MacOS – first steps"},{"content":" Teoria kwantowa nie gryzieAuthor: Marcus Chown\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2019/teoria-kwanotowa-nie-gryzie/","summary":" Teoria kwantowa nie gryzieAuthor: Marcus Chown\nlubimyczytac.pl ","title":"Teoria kwantowa nie gryzie"},{"content":" Czas SpekulacjiAuthor: Gregory J. Millman\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2019/czas-spekulacji/","summary":" Czas SpekulacjiAuthor: Gregory J. Millman\nlubimyczytac.pl ","title":"Czas Spekulacji"},{"content":" Krótka historia czasuOd Wielkiego Wybuchu do czarnych dziur\nAuthor: Stephen Hawking\nempik.com ","permalink":"https://gagor.pro/books/2019/jeszcze-krotsza-historia-czasu/","summary":" Krótka historia czasuOd Wielkiego Wybuchu do czarnych dziur\nAuthor: Stephen Hawking\nempik.com ","title":"Krótka historia czasu"},{"content":" Teoria wszystkiego, czyli krótka historia wszechświataAuthor: Stephen Hawking\nempik.com ","permalink":"https://gagor.pro/books/2019/teoria-wszystkiego/","summary":" Teoria wszystkiego, czyli krótka historia wszechświataAuthor: Stephen Hawking\nempik.com ","title":"Teoria wszystkiego, czyli krótka historia wszechświata"},{"content":" Bezpieczeństwo aplikacji webowychAuthors: Michał Bentkowski, Gynvael Coldwind, Artur Czyż, Rafał Janicki, Jarosław Kamiński, Adrian Michalczyk, Mateusz Niezabitowski, Marcin Piosek, Michał Sajdak, Grzegorz Trawiński, Bohdan Widła\nksiazka.sekurak.pl ","permalink":"https://gagor.pro/books/2019/bezpieczenstwo-aplikacji-webowych/","summary":" Bezpieczeństwo aplikacji webowychAuthors: Michał Bentkowski, Gynvael Coldwind, Artur Czyż, Rafał Janicki, Jarosław Kamiński, Adrian Michalczyk, Mateusz Niezabitowski, Marcin Piosek, Michał Sajdak, Grzegorz Trawiński, Bohdan Widła\nksiazka.sekurak.pl ","title":"Bezpieczeństwo aplikacji webowych"},{"content":" DevOpsŚwiatowej klasy zwinność, niezawodność i bezpieczeństwo w Twojej organizacji\nAuthors: Gene Kim, Patrick Debois, John Willis, Jez Humble, John Allspaw\nhelion.pl ","permalink":"https://gagor.pro/books/2019/devops-gene-kim/","summary":" DevOpsŚwiatowej klasy zwinność, niezawodność i bezpieczeństwo w Twojej organizacji\nAuthors: Gene Kim, Patrick Debois, John Willis, Jez Humble, John Allspaw\nhelion.pl ","title":"DevOps"},{"content":" DevOps HiringAuthor: Dave Zwieback\noreilly.com ","permalink":"https://gagor.pro/books/2019/devops-hiring/","summary":" DevOps HiringAuthor: Dave Zwieback\noreilly.com ","title":"DevOps Hiring"},{"content":" Book of LifeAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","permalink":"https://gagor.pro/books/2018/book-of-life/","summary":" Book of LifeAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","title":"Book of Life"},{"content":" Shadow of NightAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","permalink":"https://gagor.pro/books/2018/shadow-of-night/","summary":" Shadow of NightAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","title":"Shadow of Night"},{"content":" A Discovery of WitchesAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","permalink":"https://gagor.pro/books/2018/a-discovery-of-witches/","summary":" A Discovery of WitchesAuthor: Deborah Harkness\namazon.plamazon.comempik.com ","title":"A Discovery of Witches"},{"content":" Projekt FeniksPowieść o IT, modelu DevOps i o tym, jak pomóc firmie w odniesieniu sukcesu\nAuthors: Gene Kim, Kevin Behr, George Spafford\nhelion.pl ","permalink":"https://gagor.pro/books/2018/projekt-feniks/","summary":" Projekt FeniksPowieść o IT, modelu DevOps i o tym, jak pomóc firmie w odniesieniu sukcesu\nAuthors: Gene Kim, Kevin Behr, George Spafford\nhelion.pl ","title":"Projekt Feniks"},{"content":" Broń matematycznej zagładyJak algorytmy zwiększają nierówności i zagrażają demokracji\nAuthor: Cathy O\u0026#39;Neil\nhelion.pl ","permalink":"https://gagor.pro/books/2018/bron-matematycznej-zaglady/","summary":" Broń matematycznej zagładyJak algorytmy zwiększają nierówności i zagrażają demokracji\nAuthor: Cathy O\u0026#39;Neil\nhelion.pl ","title":"Broń matematycznej zagłady"},{"content":" Paragraf 22Author: Joseph Heller\nempik.com ","permalink":"https://gagor.pro/books/2018/paragraf-22/","summary":" Paragraf 22Author: Joseph Heller\nempik.com ","title":"Paragraf 22"},{"content":" Cisza w sieciAuthor: Michał Zalewski\nhelion.pl ","permalink":"https://gagor.pro/books/2018/cisza-w-sieci/","summary":" Cisza w sieciAuthor: Michał Zalewski\nhelion.pl ","title":"Cisza w sieci"},{"content":" Nowy domTrylogia Mrocznego Elfa. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2018/drizzt-nowy-dom/","summary":" Nowy domTrylogia Mrocznego Elfa. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Nowy dom"},{"content":" WygnanieTrylogia Mrocznego Elfa. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2018/drizzt-wygnanie/","summary":" WygnanieTrylogia Mrocznego Elfa. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Wygnanie"},{"content":" Odliczając do dnia zeroStuxnet, czyli prawdziwa historia cyfrowej broni\nAuthor: Kim Zetter\nhelion.pl ","permalink":"https://gagor.pro/books/2018/odliczajac-do-dnia-zero/","summary":" Odliczając do dnia zeroStuxnet, czyli prawdziwa historia cyfrowej broni\nAuthor: Kim Zetter\nhelion.pl ","title":"Odliczając do dnia zero"},{"content":" OjczyznaTrylogia Mrocznego Elfa. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2018/drizzt-ojczyzna/","summary":" OjczyznaTrylogia Mrocznego Elfa. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Ojczyzna"},{"content":" Klejnot halflingaTrylogia Doliny Lodowego Wichru. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2017/tdlw-klejnot-halflinga/","summary":" Klejnot halflingaTrylogia Doliny Lodowego Wichru. Tom 3\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Klejnot halflinga"},{"content":" Strumienie srebraTrylogia Doliny Lodowego Wichru. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2017/tdlw-strumienie-srebra/","summary":" Strumienie srebraTrylogia Doliny Lodowego Wichru. Tom 2\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Strumienie srebra"},{"content":" Kryształowy reliktTrylogia Doliny Lodowego Wichru. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","permalink":"https://gagor.pro/books/2017/tdlw-krysztalowy-relikt/","summary":" Kryształowy reliktTrylogia Doliny Lodowego Wichru. Tom 1\nAuthor: R. A. Salvatore\nlubimyczytac.pl ","title":"Kryształowy relikt"},{"content":" Czysty kodPodręcznik dobrego programisty\nAuthor: Robert C. Martin\nhelion.pl ","permalink":"https://gagor.pro/books/2017/czysty-kod/","summary":" Czysty kodPodręcznik dobrego programisty\nAuthor: Robert C. Martin\nhelion.pl ","title":"Czysty kod"},{"content":" Java. Efektywne programowanieWydanie II\nhelion.pl ","permalink":"https://gagor.pro/books/2017/java-efektywne-programowanie/","summary":" Java. Efektywne programowanieWydanie II\nhelion.pl ","title":"Java. Efektywne programowanie"},{"content":" Pragmatyczny programistaOd czeladnika do mistrza\nAuthors: Andrew Hunt, David Thomas\nhelion.pl ","permalink":"https://gagor.pro/books/2017/pragmatyczny-programista/","summary":" Pragmatyczny programistaOd czeladnika do mistrza\nAuthors: Andrew Hunt, David Thomas\nhelion.pl ","title":"Pragmatyczny programista"},{"content":" Perfekcyjna niedoskonałośćAuthor: Jacek Dukaj\nempik.com ","permalink":"https://gagor.pro/books/2017/perfekcyjna-niedoskonalosc/","summary":" Perfekcyjna niedoskonałośćAuthor: Jacek Dukaj\nempik.com ","title":"Perfekcyjna niedoskonałość"},{"content":" Trylogia ciąguNeuromancer / Graf Zero / Mona Liza Turbo\nAuthor: William Gibson\nempik.com ","permalink":"https://gagor.pro/books/2017/trylogia-ciagu/","summary":" Trylogia ciąguNeuromancer / Graf Zero / Mona Liza Turbo\nAuthor: William Gibson\nempik.com ","title":"Trylogia ciągu"},{"content":" Inne pieśniAuthor: Jacek Dukaj\nempik.com ","permalink":"https://gagor.pro/books/2017/inne-piesni/","summary":" Inne pieśniAuthor: Jacek Dukaj\nempik.com ","title":"Inne pieśni"},{"content":" Black Hat PythonJęzyk Python dla hakerów i pentesterów\nAuthor: Justin Seitz\namazon.plempik.comhelion.pl ","permalink":"https://gagor.pro/books/2017/black-hack-python/","summary":" Black Hat PythonJęzyk Python dla hakerów i pentesterów\nAuthor: Justin Seitz\namazon.plempik.comhelion.pl ","title":"Black Hat Python"},{"content":" SzpiegCzyli podstawy szpiegowskiego fachu\nAuthor: Wiktor Suworow\nempik.com ","permalink":"https://gagor.pro/books/2017/szpieg/","summary":" SzpiegCzyli podstawy szpiegowskiego fachu\nAuthor: Wiktor Suworow\nempik.com ","title":"Szpieg"},{"content":" Sztuka wojnyWydanie 3\nAuthors: Sun Tzu, Sun Pin\nmaklerska.pl ","permalink":"https://gagor.pro/books/2017/sztuka-wojny/","summary":" Sztuka wojnyWydanie 3\nAuthors: Sun Tzu, Sun Pin\nmaklerska.pl ","title":"Sztuka wojny"},{"content":" ExtensaAuthor: Jacek Dukaj\nempik.com ","permalink":"https://gagor.pro/books/2017/extensa/","summary":" ExtensaAuthor: Jacek Dukaj\nempik.com ","title":"Extensa"},{"content":" Wydajność JavyPoznaj i wykorzystaj optymalne sposoby na regulowanie wydajności oprogramowania Java!\nAuthors: Charlie Hunt, Binu John\nhelion.pl ","permalink":"https://gagor.pro/books/2017/wydajnosc-javy/","summary":" Wydajność JavyPoznaj i wykorzystaj optymalne sposoby na regulowanie wydajności oprogramowania Java!\nAuthors: Charlie Hunt, Binu John\nhelion.pl ","title":"Wydajność Javy"},{"content":" Gra EnderaSaga Endera. Tom 1\nAuthor: Card Orson Scott\nempik.com ","permalink":"https://gagor.pro/books/2016/gra-endera/","summary":" Gra EnderaSaga Endera. Tom 1\nAuthor: Card Orson Scott\nempik.com ","title":"Gra Endera"},{"content":"I had stragne statistics on one memcached servers. I had to look what it\u0026rsquo;s doing there. I found such commands that may be used to sniff, extract and make statistics from running memcached server.\nDebug GET commands tcpflow -c dst port 11211 | cut -b46- | grep ^get cut command will remove 46 bytes at beginning of every string (src, dst, port). You may need to adjust numeric parameter for cut to leave commands only. Output should look like:\nget myapp-cache-theme_registry get myapp-cache_field get myapp-cache-schema ... Debug everything except GET commands tcpflow -c dst port 11211 | cut -b46- | grep -v ^get Check transfer Add pv -r at the end to calculate transfer:\n$ tcpflow -c dst port 11211 | cut -b46- | grep ^get | pv -r \u0026gt; /dev/null tcpflow[11140]: listening on eth0 [8.25kB/s] Check command rates To check command rates not the transfer add -l param (stands for lines) to pv command:\n$ tcpflow -c dst port 11211 | cut -b46- | grep -v ^get | pv -rl \u0026gt; /dev/null tcpflow[9271]: listening on eth0 [1.51k/s] That should be enough for you to start debugging \u0026#x1f604;\nSource: http://www.streppone.it/cosimo/blog/tag/memcached/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/07/debuging-commands-running-on-memcached/","summary":"I had stragne statistics on one memcached servers. I had to look what it\u0026rsquo;s doing there. I found such commands that may be used to sniff, extract and make statistics from running memcached server.\nDebug GET commands tcpflow -c dst port 11211 | cut -b46- | grep ^get cut command will remove 46 bytes at beginning of every string (src, dst, port). You may need to adjust numeric parameter for cut to leave commands only.","title":"Debuging commands running on memcached"},{"content":"It happen to me all the time that one of developers notifies me about some kind of problem that I can\u0026rsquo;t confirm from my account. Sometimes it was because of bad ssh keys configuration, other times file permissions, mostly such stuff. It\u0026rsquo;s sometimes convenient to \u0026ldquo;enter into someone\u0026rsquo;s shoes\u0026rdquo; to see what\u0026rsquo;s going on there.\nIf you\u0026rsquo;re root on machine you may do that like this:\nsu developer - Easy one but that\u0026rsquo;s not enough for all cases. When you use bastion host (or similar solutions) sometimes users have connection problems and it\u0026rsquo;s harder to check. When such user have ForwardAgent ssh option enabled you may stole this session to check login problems. After you switch to such user, you may wan\u0026rsquo;t to hide history (it\u0026rsquo;s optional 😉 ) - disable history like that:\nexport HISTFILESIZE=0 export HISTSIZE=0 unset HISTFILE Now you may stole ssh session, but first check if you have your dev is logged on:\n$ ls -la /tmp/ | grep ssh drwx------ 2 root root 4096 Apr 27 20:56 ssh-crYKv29798 drwx------ 2 developer developer 4096 Apr 27 18:03 ssh-cVXFo28108 Export SSH_AUTH_SOCK with path to developer\u0026rsquo;s agent socket:\nSSH_AUTH_SOCK=/tmp/ssh-cVXFo28108/agent.28108 Finally you may try to login via ssh as developer and see with his eyes what\u0026rsquo;s now working.\n","permalink":"https://gagor.pro/2016/04/how-to-stole-ssh-session-when-youre-root/","summary":"It happen to me all the time that one of developers notifies me about some kind of problem that I can\u0026rsquo;t confirm from my account. Sometimes it was because of bad ssh keys configuration, other times file permissions, mostly such stuff. It\u0026rsquo;s sometimes convenient to \u0026ldquo;enter into someone\u0026rsquo;s shoes\u0026rdquo; to see what\u0026rsquo;s going on there.\nIf you\u0026rsquo;re root on machine you may do that like this:\nsu developer - Easy one but that\u0026rsquo;s not enough for all cases.","title":"How to stole ssh session when you’re root"},{"content":"Virtualenvs in python are cheap but from time to time you will install something with pip on your system and when time comes removing all this crap could be difficult. I found this bash snippet that will uninstall package with all dependencies:\nfor dep in $(pip show python-neutronclient | grep Requires | sed \u0026#39;s/Requires: //g; s/,//g\u0026#39;) ; do sudo pip uninstall -y $dep ; done pip uninstall -y python-neutronclient Source: http://stackoverflow.com/a/32698209/4828478\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/04/pip-uninstall-package-with-dependencies/","summary":"Virtualenvs in python are cheap but from time to time you will install something with pip on your system and when time comes removing all this crap could be difficult. I found this bash snippet that will uninstall package with all dependencies:\nfor dep in $(pip show python-neutronclient | grep Requires | sed \u0026#39;s/Requires: //g; s/,//g\u0026#39;) ; do sudo pip uninstall -y $dep ; done pip uninstall -y python-neutronclient Source: http://stackoverflow.","title":"pip - uninstall package with dependencies"},{"content":"I\u0026rsquo;ve been using standard MySQL dumps as backup technique on my VPS for few years. It works fine and backups were usable few times when I needed them. But in other places I\u0026rsquo;m using xtrabackup. It\u0026rsquo;s faster when crating backups and a lot faster when restoring them - they\u0026rsquo;re binary so there is no need to reevaluate all SQL create tables/inserts/etc. Backups also include my.cnf config file so restoring on other machine should be easy.\nAfter I switched from MariaDB to Percona I have Percona repos configured, so I will use latest version of xtrabackup.\napt-get install -y percona-xtrabackup Prerequisities xtrabackup requires configured user to be able to make backups. One way is to write user and password in plaintext in ~/.my.cnf. Another is using mysql_config_editor to generate ~/.mylogin.cnf file with encrypted credentials. To be honest I didn\u0026rsquo;t check what kind of security provides this encryption but it feels better than keeping password in plaintext.\nI do not want to create new user for this task - I just used debian-sys-maint user. Check password for this user like this:\ngrep password /etc/mysql/debian.cnf Now create encrypted file:\nmysql_config_editor set --login-path=client --host=localhost --user=debian-sys-maint --password Hit enter and copy/paste password. File .mylogin.cnf should be created with binary content. We may check this with:\n# mysql_config_editor print [client] user = debian-sys-maint password = ***** host = localhost Looks OK.\nBackuping Now backup script. I placed it directly in cron.daily dir ex. /etc/cron.daily/zz-percona-backup with content:\n#!/bin/bash DATE=`date +%F-%H%M%S` DIR=/backup/xtrabackup DST=$DIR/${DATE}.tar.xz # this will produce directories with compresses files # mkdir -p $DST # xtrabackup --backup --compress --target-dir=$DST # this will produce tar.xz archives xtrabackup --backup --stream=tar | xz -9 \u0026gt; $DST # delete files older than 30 days find $DIR -type f -mtime +30 -delete I prefer to have single archive with backup because I\u0026rsquo;m transferring those files to my NAS (for security). But for local backups directories are more convenient and faster when restoring. Also tar archives have to be decompressed with -ioption\u0026thinsp; external link .\nRestoring First time I saw it it scared me a little but after all worked fine and without problems\u0026hellip;\nservice mysql stop rm -rf /var/lib/mysql mkdir /var/lib/mysql Now prepare backup, if you used directory backups it\u0026rsquo;s easy:\nxtrabackup --decompress --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --prepare --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --copy-back --target-dir=/backup/xtrabackup/2016-03-14-214233 But if you used tar archives it\u0026rsquo;s little more messy\u0026hellip; You have to create temporary dir and extract archive there:\nmkdir /tmp/restore tar -xvif /backup/xtrabackup/2016-03-14-214233.tar.xz -C /tmp/restore xtrabackup --prepare --target-dir=/tmp/restore xtrabackup --copy-back --target-dir=/tmp/restore We have to fix ownership of restored files and db may be started:\nchown -R mysql:mysql /var/lib/mysql service mysql start If your backup is huge you should reorder commands to shutdown database after backup decompression.\nSource: https://www.percona.com/doc/percona-xtrabackup/2.3/xtrabackup_bin/xtrabackup_binary.html\u0026thinsp; external link http://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html\u0026thinsp; external link https://www.percona.com/doc/percona-xtrabackup/2.1/innobackupex/streaming_backups_innobackupex.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/04/daily-mysql-backups-with-xtrabackup/","summary":"I\u0026rsquo;ve been using standard MySQL dumps as backup technique on my VPS for few years. It works fine and backups were usable few times when I needed them. But in other places I\u0026rsquo;m using xtrabackup. It\u0026rsquo;s faster when crating backups and a lot faster when restoring them - they\u0026rsquo;re binary so there is no need to reevaluate all SQL create tables/inserts/etc. Backups also include my.cnf config file so restoring on other machine should be easy.","title":"Daily MySQL backups with xtrabackup"},{"content":"When you deploy your application in cloud you don\u0026rsquo;t need and don\u0026rsquo;t want your hosts exposed via SSH to the world. Malware scans whole network for easy SSH access and when find something will try some brute force attacks, overloading such machines. It\u0026rsquo;s easier to have one exposed, but secured host, that doesn\u0026rsquo;t host anything and is used as proxy/gateway to access our infrastructure- it\u0026rsquo;s called bastion host\u0026thinsp; external link .\nAnsible is quite easy to integrate with bastion host configuration. We will need custom ansible.cfg and ssh_config file. So let\u0026rsquo;s start with ssh_config:\nHost bastion Hostname ip.xxx.xxx.xxx.xxx.or.host.name User ubuntu IdentityFile ~/.ssh/id_rsa PasswordAuthentication no ForwardAgent yes ServerAliveInterval 60 TCPKeepAlive yes ControlMaster auto ControlPath ~/.ssh/ansible-%r@%h:%p ControlPersist 15m ProxyCommand none LogLevel QUIET Host * User ubuntu IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 TCPKeepAlive yes ProxyCommand ssh -q -A ubuntu@bastion nc %h %p LogLevel QUIET StrictHostKeyChecking no Now I will describe what most important options mean. For bastion:\nUser - I\u0026rsquo;m using Ubuntu kickstarted on cloud as bastion host with it\u0026rsquo;s default user. Never use root here - you don\u0026rsquo;t need that ForwardAgent yes - we want to forward our ssh keys through bastion to destination hosts, ServerAliveInterval 60 - this is like keepalive connection, ssh will send small ping/pong packets every 60 seconds so your connection won\u0026rsquo;t hung/terminate after long time, ControlMaster auto - we will open one connection to bastion host and multiplex other ssh connections through it, connection will be opened for ControlPersist time, ControlPath - this have to be configured same way like in ansible.cfg, ProxyCommand none - we\u0026rsquo;re setting ProxyCommand for all hosts but we need it disabled for bastion,`` Default hosts configuration:\nProxyCommand ssh -q -A ubuntu@bastion nc %h %p - this is what makes all magic, it will pipe your ssh connection via bastion to destination host, StrictHostKeyChecking no - this options shouldn\u0026rsquo;t be there for production but it\u0026rsquo;s useful at beginning when you create and destroy machines few times before you test everything. Normally this will cause notifications about ssh key changes, but you\u0026rsquo;re aware of that - you just recreated those machines. I\u0026rsquo;ve found examples without netcat but was unable to get them working - this one worked for me really well.\nTo test if connections work fine use this configuration like:\nssh -F ssh_config bastion ssh -F ssh_config other.host.behind.bastion And now ansible.cfg:\n[defaults] forks=20 [ssh_connection] ssh_args = -F ./ssh_config -o ControlMaster=auto -o ControlPersist=5m -o LogLevel=QUIET control_path = ~/.ssh/ansible-%%r@%%h:%%p pipelining=True Most important section here is in ssh_args where we\u0026rsquo;re pointing to ssh_config file in current dir with -F option. I also have to reenter configuration for multiplexing here - it wasn\u0026rsquo;t working with ssh only configuration. control_path option have to use same paths like ssh_config (% signs are escaped with %%).\nYou should be able to run ansible/ansible-playbook commands normally now - all traffic will be forwarded through bastion.\nIt\u0026rsquo;s good time now to install fail2ban on bastion and maybe reconfigure it to run ssh on crazy high port 🙂\nSources http://alexbilbie.com/2014/07/using-ansible-with-a-bastion-host/\u0026thinsp; external link http://blog.scottlowe.org/2015/12/24/running-ansible-through-ssh-bastion-host/\u0026thinsp; external link https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/04/use-bastion-host-with-ansible/","summary":"When you deploy your application in cloud you don\u0026rsquo;t need and don\u0026rsquo;t want your hosts exposed via SSH to the world. Malware scans whole network for easy SSH access and when find something will try some brute force attacks, overloading such machines. It\u0026rsquo;s easier to have one exposed, but secured host, that doesn\u0026rsquo;t host anything and is used as proxy/gateway to access our infrastructure- it\u0026rsquo;s called bastion host\u0026thinsp; external link .","title":"Use bastion host with Ansible"},{"content":"Lately I was searching for mobile notebook that I could use for remote work. I checked f ThinkPad series but they were huge bricks that have nothing in common with \u0026lsquo;mobile\u0026rsquo; word. Then I saw ASUS Zenbook that I didn\u0026rsquo;t take into account before and it was exactly what I was searching for.\nConfiguration of Skylake based notebook right now is not straightforward - there are still glitches and small bugs that are waiting to be fixed. I want to sum up what I\u0026rsquo;ve done after installation. I started with fresh Ubuntu 16.04 to get Debian based distro with possibly latest kernel and patches.\nSome SSD tweaks Change mount options for filesystems on SSD from:\n/dev/mapper/ubuntu--vg-root / ext4 errors=remount-ro 0 1 to:\n/dev/mapper/ubuntu--vg-root / ext4 discard,noatime,errors=remount-ro 0 1 And move /tmp to RAM with this additional line in /etc/fstab:\ntmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0 Now add deadline scheduler for root disk - edit /etc/rc.local and add this line before exit 0:\necho deadline \u0026gt; /sys/block/sda/queue/scheduler echo 1 \u0026gt; /sys/block/sda/queue/iosched/fifo_batch I have configured swap but I don\u0026rsquo;t want to use it too much and setting low swappines sysctl option will help. Run this as root:\necho \u0026#34;vm.swappiness = 1\u0026#34; \u0026gt; /etc/sysctl.conf.d/90-swappines.conf sysctl -p /etc/sysctl.conf.d/90-swappines.conf Power usage tweaks I\u0026rsquo;ve installed laptop-mode-tools to achieve lower power usage on battery. So:\napt-get install -y laptop-mode-tools By default it\u0026rsquo;s cutting hard CPU performance on battery (half performance, no turbo) so I fixed this by changing /etc/laptop-mode/conf.d/intel_pstate.conf section On battery:\n#On battery BATT_INTEL_PSTATE_PERF_MIN_PCT=0 # Minimum performance, in percent BATT_INTEL_PSTATE_PERF_MAX_PCT=100 # Maximum performance, in percent BATT_INTEL_PSTATE_NO_TURBO=0 # Disable \u0026#34;Turbo Boost\u0026#34;? Laptop mode tools won\u0026rsquo;t start automatically so we may integrate them with pm-utils (that are already installed on Ubuntu) to get it running when needed. We have to create new config file:\nsudo touch /etc/pm/sleep.d/10-laptop-mode-tools sudo chmod a+x /etc/pm/sleep.d/10-laptop-mode-tools with content like this:\ncase $1 in hibernate) /etc/init.d/laptop-mode stop ;; suspend) /etc/init.d/laptop-mode stop ;; thaw)d /etc/init.d/laptop-mode start ;; resume) /etc/init.d/laptop-mode start ;; *) echo Something is not right. ;; esac Now I will enable ALPM for SATA in AHCI mode optimizations:\necho SATA_ALPM_ENABLE=true | sudo tee /etc/pm/config.d/sata_alpm And some kernel parameters in /etc/default/grub:\nGRUB_CMDLINE_LINUX=\u0026#34;pcie_aspm=force drm.vblankoffdelay=1 i915.semaphores=1\u0026#34; and update-grub with:\nupdate-grub You may use powertop to nail power heavy processes. There is also powerstat to benchmark power usage through time - I have:\nsudo pm-powersave true powerstat ...... Summary: System: 4.49 Watts on average with standard deviation 0.46 It\u0026rsquo;s really nice. I should be able to run about 8~9h! Sweet!\nSources https://www.reddit.com/r/linux/comments/3ia8ta/review_of_ubuntu_on_asus_ux305fa/\u0026thinsp; external link https://help.ubuntu.com/community/PowerManagement/ReducedPower#Using_less_power_with_laptop-mode-tools\u0026thinsp; external link https://help.ubuntu.com/community/AsusZenbook\u0026thinsp; external link https://wiki.ubuntu.com/Kernel/PowerManagementALPM\u0026thinsp; external link Disable touchpad when writing It\u0026rsquo;s crazy annoying when you tap touchpad during writing text and lose focus on editor window. There is solution for that, it\u0026rsquo;s even installed by default on Ubuntu and it\u0026rsquo;s called: syndaemon. It\u0026rsquo;s started by default like this:\nsyndaemon -i 1.0 -t -K -R 1 second feels too small for me. I will adjust it to 2s. There is no easy way to do this. I created script to run on login:\n#!/bin/bash killall syndaemon syndaemon -d -i 2.0 -t -K -R Now better 🙂\nVD-PAU I installed vdpauinfo tool to see if it\u0026rsquo;s working:\napt-get install -y vdpauinfo It wasn\u0026rsquo;t:\nvdpauinfo display: :0 screen: 0 Failed to open VDPAU backend libvdpau_va_gl.so: cannot open shared object file: No such file or directory Error creating VDPAU device: 1 I checked this library and couldn\u0026rsquo;t find it - it wasn\u0026rsquo;t installed. Easy fix:\napt-get install -y libvdpau-va-gl1 Check again:\nvdpauinfo display: :0 screen: 0 libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 API version: 1 Information string: OpenGL/VAAPI/libswscale backend for VDPAU Video surface: name width height types ------------------------------------------- 420 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 422 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 444 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 Decoder capabilities: name level macbs width height ---------------------------------------------------- MPEG1 --- not supported --- MPEG2_SIMPLE --- not supported --- MPEG2_MAIN --- not supported --- H264_BASELINE 51 16384 2048 2048 H264_MAIN 51 16384 2048 2048 H264_HIGH 51 16384 2048 2048 VC1_SIMPLE --- not supported --- VC1_MAIN --- not supported --- VC1_ADVANCED --- not supported --- MPEG4_PART2_SP --- not supported --- MPEG4_PART2_ASP --- not supported --- DIVX4_QMOBILE --- not supported --- DIVX4_MOBILE --- not supported --- DIVX4_HOME_THEATER --- not supported --- DIVX4_HD_1080P --- not supported --- DIVX5_QMOBILE --- not supported --- DIVX5_MOBILE --- not supported --- DIVX5_HOME_THEATER --- not supported --- DIVX5_HD_1080P --- not supported --- H264_CONSTRAINED_BASELINE 51 16384 2048 2048 H264_EXTENDED --- not supported --- H264_PROGRESSIVE_HIGH --- not supported --- H264_CONSTRAINED_HIGH --- not supported --- H264_HIGH_444_PREDICTIVE --- not supported --- HEVC_MAIN --- not supported --- HEVC_MAIN_10 --- not supported --- HEVC_MAIN_STILL --- not supported --- HEVC_MAIN_12 --- not supported --- HEVC_MAIN_444 --- not supported --- Output surface: name width height nat types ---------------------------------------------------- B8G8R8A8 8192 8192 y R8G8B8A8 8192 8192 y R10G10B10A2 8192 8192 y B10G10R10A2 8192 8192 y A8 8192 8192 y Bitmap surface: name width height ------------------------------ B8G8R8A8 8192 8192 R8G8B8A8 8192 8192 R10G10B10A2 8192 8192 B10G10R10A2 8192 8192 A8 8192 8192 Video mixer: feature name sup ------------------------------------ DEINTERLACE_TEMPORAL - DEINTERLACE_TEMPORAL_SPATIAL - INVERSE_TELECINE - NOISE_REDUCTION - SHARPNESS - LUMA_KEY - HIGH QUALITY SCALING - L1 - HIGH QUALITY SCALING - L2 - HIGH QUALITY SCALING - L3 - HIGH QUALITY SCALING - L4 - HIGH QUALITY SCALING - L5 - HIGH QUALITY SCALING - L6 - HIGH QUALITY SCALING - L7 - HIGH QUALITY SCALING - L8 - HIGH QUALITY SCALING - L9 - parameter name sup min max ----------------------------------------------------- VIDEO_SURFACE_WIDTH - VIDEO_SURFACE_HEIGHT - CHROMA_TYPE - LAYERS - attribute name sup min max ----------------------------------------------------- BACKGROUND_COLOR - CSC_MATRIX - NOISE_REDUCTION_LEVEL - SHARPNESS_LEVEL - LUMA_KEY_MIN_LUMA - LUMA_KEY_MAX_LUMA - Looks better now\u0026hellip; But not impressive, there\u0026rsquo;s only H264 support.\nI\u0026rsquo;ve tried it in VLC but it was crashing from time to time the whole VLC (leaving it running in background). Time to test VA-API 🙂\nVA-API Like earlier I have to install one tool to see what we have: vainfo\napt-get install -y vainfo Checking what we have on system:\nvainfo libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 vainfo: VA-API version: 0.39 (libva 1.7.0) vainfo: Driver version: Intel i965 driver for Intel(R) Skylake - 1.7.0 vainfo: Supported profile and entrypoints VAProfileMPEG2Simple :\tVAEntrypointVLD VAProfileMPEG2Simple :\tVAEntrypointEncSlice VAProfileMPEG2Main :\tVAEntrypointVLD VAProfileMPEG2Main :\tVAEntrypointEncSlice VAProfileH264ConstrainedBaseline:\tVAEntrypointVLD VAProfileH264ConstrainedBaseline:\tVAEntrypointEncSlice VAProfileH264Main :\tVAEntrypointVLD VAProfileH264Main :\tVAEntrypointEncSlice VAProfileH264High :\tVAEntrypointVLD VAProfileH264High :\tVAEntrypointEncSlice VAProfileH264MultiviewHigh :\tVAEntrypointVLD VAProfileH264MultiviewHigh :\tVAEntrypointEncSlice VAProfileH264StereoHigh :\tVAEntrypointVLD VAProfileH264StereoHigh :\tVAEntrypointEncSlice VAProfileVC1Simple :\tVAEntrypointVLD VAProfileVC1Main :\tVAEntrypointVLD VAProfileVC1Advanced :\tVAEntrypointVLD VAProfileNone :\tVAEntrypointVideoProc VAProfileJPEGBaseline :\tVAEntrypointVLD VAProfileJPEGBaseline :\tVAEntrypointEncPicture VAProfileVP8Version0_3 :\tVAEntrypointVLD VAProfileVP8Version0_3 :\tVAEntrypointEncSlice VAProfileHEVCMain :\tVAEntrypointVLD VAProfileHEVCMain :\tVAEntrypointEncSlice It requires package i965-va-driver to work but on my system it was installed (probably during VDPAU installation as dependency).\nIt was working almost fine\u0026hellip; In VLC on my machine VA-API on X11 was drawing through all desktops. VA-API DRM was working better\u0026hellip; But crashed my X11 server after few minutes of watching ;/\nOpenCL You may thing: for what the hell you need OpenCL on such tiny machine? I doesn\u0026rsquo;t care - I want it 🙂\nFirst install clinfo package:\napt-get install -y clinfo And run it:\nclinfo Number of platforms 0 Not too much 😀\nFor Intel GPU/CPU OpenCL support we will need beignet package:\napt-get install -y beignet clinfo Number of platforms 1 Platform Name Intel Gen OCL Driver Platform Vendor Intel Platform Version OpenCL 1.2 beignet 1.1.1 Platform Profile FULL_PROFILE Platform Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_spir cl_khr_icd Platform Extensions function suffix Intel Platform Name Intel Gen OCL Driver Number of devices 1 Device Name Intel(R) HD Graphics Skylake ULX GT2 Device Vendor Intel Device Vendor ID 0x8086 Device Version OpenCL 1.2 beignet 1.1.1 Driver Version 1.1.1 Device OpenCL C Version OpenCL C 1.2 beignet 1.1.1 Device Type GPU Device Profile FULL_PROFILE Max compute units 24 Max clock frequency 1000MHz Device Partition (core) Max number of sub-devices 1 Supported partition types None, None, None Max work item dimensions 3 Max work item sizes 512x512x512 Max work group size 512 Preferred work group size multiple 16 Preferred / native vector sizes char 16 / 8 short 8 / 8 int 4 / 4 long 2 / 2 half 0 / 8 (cl_khr_fp16) float 4 / 4 double 0 / 2 (n/a) Half-precision Floating-point support (cl_khr_fp16) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Single-precision Floating-point support (core) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Double-precision Floating-point support (n/a) Address bits 32, Little-Endian Global memory size 2147483648 (2GiB) Error Correction support No Max memory allocation 1073741824 (1024MiB) Unified memory for Host and Device Yes Minimum alignment for any data type 128 bytes Alignment of base address 1024 bits (128 bytes) Global Memory cache type Read/Write Global Memory cache size 8192 Global Memory cache line 64 bytes Image support Yes Max number of samplers per kernel 16 Max size for 1D images from buffer 65536 pixels Max 1D or 2D image array size 2048 images Max 2D image size 8192x8192 pixels Max 3D image size 8192x8192x2048 pixels Max number of read image args 128 Max number of write image args 8 Local memory type Global Local memory size 65536 (64KiB) Max constant buffer size 134217728 (128MiB) Max number of constant args 8 Max size of kernel argument 1024 Queue properties Out-of-order execution No Profiling Yes Prefer user sync for interop Yes Profiling timer resolution 80ns Execution capabilities Run OpenCL kernels Yes Run native kernels Yes SPIR versions printf() buffer size 1048576 (1024KiB) Built-in kernels __cl_copy_region_align4;__cl_copy_region_align16;__cl_cpy_region_unalign_same_offset;__cl_copy_region_unalign_dst_offset;__cl_copy_region_unalign_src_offset;__cl_copy_buffer_rect;__cl_copy_image_1d_to_1d;__cl_copy_image_2d_to_2d;__cl_copy_image_3d_to_2d;__cl_copy_image_2d_to_3d;__cl_copy_image_3d_to_3d;__cl_copy_image_2d_to_buffer;__cl_copy_image_3d_to_buffer;__cl_copy_buffer_to_image_2d;__cl_copy_buffer_to_image_3d;__cl_fill_region_unalign;__cl_fill_region_align2;__cl_fill_region_align4;__cl_fill_region_align8_2;__cl_fill_region_align8_4;__cl_fill_region_align8_8;__cl_fill_region_align8_16;__cl_fill_region_align128;__cl_fill_image_1d;__cl_fill_image_1d_array;__cl_fill_image_2d;__cl_fill_image_2d_array;__cl_fill_image_3d; Device Available Yes Compiler Available Yes Linker Available Yes Device Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_fp16 cl_khr_spir cl_khr_icd NULL platform behavior clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...) Intel Gen OCL Driver clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...) Success [Intel] clCreateContext(NULL, ...) [default] Success [Intel] clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version 2.2.8 ICD loader Profile OpenCL 1.2 NOTE:\tyour OpenCL library declares to support OpenCL 1.2, but it seems to support up to OpenCL 2.1 too. Sources https://wiki.archlinux.org/index.php/GPGPU#Intel\u0026thinsp; external link Suspend/Hibernate on lid close Default configuration of Ubuntu 16.04 was that after I close lid screen was blocked and LCD disabled. But system was still working normally - I strongly prefer to hibernate in such case and use no battery at all.\nI achieved that with systemd-logind. Edit /etc/systemd/logind.conf and uncomment line with HandleLidSwitch:\n[Login] HandleLidSwitch=suspend HandleLidSwitchDocked=ignore Now restart systemd-logind service with:\nsystemctl restart systemd-logind.service Problem with function keys Function keys were mostly working but not always like I expected. For example when I disable touchpad - it\u0026rsquo;s not disabled 🙂\nI found that module asus-nb-wmi is responssible for that and it\u0026rsquo;s still buggy. So I disabled it at all with:\necho \u0026#34;blacklist asus-nb-wmi\u0026#34; \u0026gt; /etc/modprobe.d/blacklist-ux305.conf Volume UP/DOWN/MUTE are still working fine - that\u0026rsquo;s enough for me. Rest could be configured with some keyboard shortcuts - more info here\u0026thinsp; external link .\nTODO/Issues I still face some bugs:\nI could see occasional flickering from time to time. Rather after running notebook for some time than overheating/overloading it. This may be driver issue or maybe SNA acceleration method - I have to experiment a little to get this solved.\nLooks like disabling Virtualization support and VT-d in BIOS helped. It\u0026rsquo;s not final solution but for now I don\u0026rsquo;t need it\u0026hellip; A lot 😉 Tapping sometimes behave strange, for ex. tap to click stops to work and I have to use touchpad buttons for that. I think this may be related to syndaemon configuration because it started after I tuned it.\nIt was that. My hack for syndaemon broke touchpad. I will play with this a little more later. I like to use copy by selection and paste by middle click on my desktop - I\u0026rsquo;m addicted to this option but it\u0026rsquo;s not working on my laptop. I\u0026rsquo;m not sure if this will be convenient enough on touchpad to use.\nTo right click just tap with two fingers, to middle click (third button) tap with three fingers. Copy/paste is again easy like before. If you found errors in my text of know better solutions for described problems, please let me know in comments.\n","permalink":"https://gagor.pro/2016/04/tweaking-asus-zenbook-ux305ca-on-linux/","summary":"Lately I was searching for mobile notebook that I could use for remote work. I checked f ThinkPad series but they were huge bricks that have nothing in common with \u0026lsquo;mobile\u0026rsquo; word. Then I saw ASUS Zenbook that I didn\u0026rsquo;t take into account before and it was exactly what I was searching for.\nConfiguration of Skylake based notebook right now is not straightforward - there are still glitches and small bugs that are waiting to be fixed.","title":"Tweaking ASUS Zenbook UX305CA on Linux"},{"content":"I try to use IPv6 where it\u0026rsquo;s available but it\u0026rsquo;s sometimes so hard\u0026hellip; It happen quite often that I can\u0026rsquo;t download packages from repos because they weren\u0026rsquo;t configured on IPv6 vhosts even when host is available via IPv6 address. For APT you may use this trick to force IPv4 connections only:\necho \u0026#39;Acquire::ForceIPv4 \u0026#34;true\u0026#34;;\u0026#39; \u0026gt; /etc/apt/apt.conf.d/99force-ipv4 If you need more than that, then gai.conf will allow you to filter where you will be connecting via IPv4 and where via IPv6 - in example bellow you will prefer IPv4 whenever it\u0026rsquo;s available:\necho \u0026#39;precedence ::ffff:0:0/96 100\u0026#39; \u0026gt;\u0026gt; /etc/gai.conf ","permalink":"https://gagor.pro/2016/03/prefer-ipv4-over-ipv6/","summary":"I try to use IPv6 where it\u0026rsquo;s available but it\u0026rsquo;s sometimes so hard\u0026hellip; It happen quite often that I can\u0026rsquo;t download packages from repos because they weren\u0026rsquo;t configured on IPv6 vhosts even when host is available via IPv6 address. For APT you may use this trick to force IPv4 connections only:\necho \u0026#39;Acquire::ForceIPv4 \u0026#34;true\u0026#34;;\u0026#39; \u0026gt; /etc/apt/apt.conf.d/99force-ipv4 If you need more than that, then gai.conf will allow you to filter where you will be connecting via IPv4 and where via IPv6 - in example bellow you will prefer IPv4 whenever it\u0026rsquo;s available:","title":"Prefer IPv4 over IPv6"},{"content":"Sometimes it\u0026rsquo;s easier to use octal file permissions but they\u0026rsquo;re not so easy to list. I caught myself few times that I didn\u0026rsquo;t remember how to list them - so this is a reason for that note.\nstat -c \u0026#34;%a %n\u0026#34; * 755 bin 755 games 755 include Yes, it\u0026rsquo;s that easy \u0026#x1f603;\nAnd here also with human readable attributes:\nstat -c \u0026#39;%A %a %n\u0026#39; * drwxr-xr-x 755 bin drwxr-xr-x 755 games drwxr-xr-x 755 include ","permalink":"https://gagor.pro/2016/02/list-octal-file-permissions-in-bash/","summary":"Sometimes it\u0026rsquo;s easier to use octal file permissions but they\u0026rsquo;re not so easy to list. I caught myself few times that I didn\u0026rsquo;t remember how to list them - so this is a reason for that note.\nstat -c \u0026#34;%a %n\u0026#34; * 755 bin 755 games 755 include Yes, it\u0026rsquo;s that easy \u0026#x1f603;\nAnd here also with human readable attributes:\nstat -c \u0026#39;%A %a %n\u0026#39; * drwxr-xr-x 755 bin drwxr-xr-x 755 games drwxr-xr-x 755 include ","title":"List octal file permissions in bash"},{"content":"I was configuring WordPress with HyperDB\u0026thinsp; external link plugin on PHP 7.0 but the only I get were constant 500 errors. As I found here\u0026thinsp; external link PHP 7.0 is not supported by HyperDB for now - it\u0026rsquo;s rely on mysql php extension but in PHP 7.0 there is only mysqli extension. But few folks fixed it and it\u0026rsquo;s possible to use it.\ncurl -O https://raw.githubusercontent.com/soulseekah/hyperdb-mysqli/master/db.php mv db.php /var/www/wordpress/wp-content/ And configure it ex. like this:\ncat \u0026lt;\u0026lt;DBCONFIG \u0026gt; /var/www/wordpress/db-config.php \u0026lt;?php \\$wpdb-\u0026gt;save_queries = false; \\$wpdb-\u0026gt;persistent = false; \\$wpdb-\u0026gt;max_connections = 10; \\$wpdb-\u0026gt;check_tcp_responsiveness = true; \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;master.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 1, \u0026#39;read\u0026#39; =\u0026gt; 1, )); \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;slave.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 0, \u0026#39;read\u0026#39; =\u0026gt; 1, )); DBCONFIG Now WordPress could handle crash of master database.\nSources https://www.digitalocean.com/community/tutorials/how-to-optimize-wordpress-performance-with-mysql-replication-on-ubuntu-14-04\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/wordpress-with-hyperdb-on-php-7-0/","summary":"I was configuring WordPress with HyperDB\u0026thinsp; external link plugin on PHP 7.0 but the only I get were constant 500 errors. As I found here\u0026thinsp; external link PHP 7.0 is not supported by HyperDB for now - it\u0026rsquo;s rely on mysql php extension but in PHP 7.0 there is only mysqli extension. But few folks fixed it and it\u0026rsquo;s possible to use it.\ncurl -O https://raw.githubusercontent.com/soulseekah/hyperdb-mysqli/master/db.php mv db.php /var/www/wordpress/wp-content/ And configure it ex.","title":"WordPress with HyperDB on PHP 7.0"},{"content":"I\u0026rsquo;m playing a lot with Docker lately. Building images, and then rebuilding, and then building again\u0026hellip; It\u0026rsquo;s pretty boring. To automate this task a little I used inotify to build automatically after I changed any file. This trick could be used in many different situations.\nYou will need inotify-tools package:\nsudo apt-get install -y inotify-tools Then run something like this:\nwhile inotifywait -e modify -r .; do docker-compose build; done This commands will rebuild my Docker images after any file change in current directory. Use Ctrl+c to exit from loop.\n","permalink":"https://gagor.pro/2016/02/automatically-build-after-file-change/","summary":"I\u0026rsquo;m playing a lot with Docker lately. Building images, and then rebuilding, and then building again\u0026hellip; It\u0026rsquo;s pretty boring. To automate this task a little I used inotify to build automatically after I changed any file. This trick could be used in many different situations.\nYou will need inotify-tools package:\nsudo apt-get install -y inotify-tools Then run something like this:\nwhile inotifywait -e modify -r .; do docker-compose build; done This commands will rebuild my Docker images after any file change in current directory.","title":"Automatically build after file change"},{"content":"I never tried it before but today I needed to install WordPress\u0026hellip; From command line only. And there is a way to do this with wp-cli\u0026thinsp; external link .\nWP-CLI installation First some requirements (as root):\napt-get install php5-cli php5-mysql mysql-client curl And now installation of wp-cli (as root too):\ncurl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar mv wp-cli.phar /usr/local/bin/wp Check if it\u0026rsquo;s working:\n$ wp --version WP-CLI 0.22.0 WordPress installation Now you should switch to user of your web application, ex. like this:\nsu - www-data -s /bin/bash And install WP:\nwp core download --path=/var/www/wordpress wp core config --path=/var/www/wordpress \\ --dbname=wordpress \\ --dbuser=wordpress \\ --dbpass=wordpresspass \\ --dbhost=localhost \\ --locale=pl_PL wp core install --path=/var/www/wordpress \\ --url=\u0026#34;http://example.com\u0026#34; \\ --title=\u0026#34;Example blog\u0026#34; \\ --admin_user=never_use_admin_here \\ --admin_password=admin_pass \\ --admin_email=admin@example.com \\ --skip-email Here you may find more about wp-cli configuration\u0026thinsp; external link and commands\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2016/02/install-wordpress-from-command-line/","summary":"I never tried it before but today I needed to install WordPress\u0026hellip; From command line only. And there is a way to do this with wp-cli\u0026thinsp; external link .\nWP-CLI installation First some requirements (as root):\napt-get install php5-cli php5-mysql mysql-client curl And now installation of wp-cli (as root too):\ncurl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar mv wp-cli.phar /usr/local/bin/wp Check if it\u0026rsquo;s working:\n$ wp --version WP-CLI 0.22.0 WordPress installation Now you should switch to user of your web application, ex.","title":"Install WordPress from command-line"},{"content":"When I started playing with Docker I was running a lot of commands to build image, delete containers running on old image, run containers based on new image, etc\u0026hellip; A lot of log commands with links, volumes, etc\u0026hellip;\nThen I started searching for something to automate this task and here I get to docker-compse command, this is how you may install it:\npip install docker-compose And install additional bash completions (run as root):\ncurl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/bash/docker-compose \u0026gt; /etc/bash_completion.d/docker-compose Then you may prepare docker-compose.yml file like:\nweb: build: . command: php -S 0.0.0.0:8000 -t /code ports: - \u0026#34;8000:8000\u0026#34; links: - db volumes: - .:/code db: image: orchardup/mysql environment: MYSQL_DATABASE: wordpress More informations about syntax may be found here: https://docs.docker.com/compose/compose-file/\u0026thinsp; external link And run such environment with:\ndocker-compose up Or to run this in background:\ndocker-compose up -d To stop and cleanup it use:\ndocker-compose stop \u0026amp;\u0026amp; docker-compose rm -f -v Other usable commands are:\ndocker-compose build --force-rm # to rebuild images and clean after docker-compose ps # to list containers I\u0026rsquo;m still playing with volumes in this but don\u0026rsquo;t have anything interesting enough to paste here - maybe later.\nSources https://docs.docker.com/compose/install/\u0026thinsp; external link https://docs.docker.com/compose/completion/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/install-docker-compose/","summary":"When I started playing with Docker I was running a lot of commands to build image, delete containers running on old image, run containers based on new image, etc\u0026hellip; A lot of log commands with links, volumes, etc\u0026hellip;\nThen I started searching for something to automate this task and here I get to docker-compse command, this is how you may install it:\npip install docker-compose And install additional bash completions (run as root):","title":"Install Docker Compose"},{"content":"I\u0026rsquo;ve played with Docker a little in it early days but didn\u0026rsquo;t stick for longer with it. It\u0026rsquo;s stable now so I wanted to check how it\u0026rsquo;s running now.\nI really can\u0026rsquo;t accept this method of installation:\ncurl -fsSL https://get.docker.com/ | sh I think that world is going to it\u0026rsquo;s end when I see such scritps\u0026hellip; I prefer to do this manually, knowing exactly what I have to do.\nInstall prerequisites:\napt-get update apt-get install -y apt-transport-https ca-certificates Purge old packages if you used them:\napt-get purge lxc-docker* apt-get purge docker.io* Add GPG key:\napt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add repo - use ONLY ONE repo appropriate for your system (lsb_release -a to check):\necho \u0026#34;deb https://apt.dockerproject.org/repo debian-wheezy main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-jessie main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-stretch main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-precise main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-trusty main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list Refresh repos and install Docker:\napt-get update apt-get install -y docker-engine Start service if it\u0026rsquo;s not running:\nservice docker start Grant access to docker service for non-root user I don\u0026rsquo;t like to use apps that require me to use root account. Docker even advice not to do so - service is running as root and you should add docker group and user to it to grant access to service socket:\ngroupadd docker gpasswd -a ${USER} docker service docker restart Now logout, login again and and you should be able to use docker command:\ndocker version docker info docker run hello-world Have fun \u0026#x1f603;\nSources https://docs.docker.com/linux/\u0026thinsp; external link https://docs.docker.com/engine/installation/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/manual-installation-of-docker-on-debian-ubuntu/","summary":"I\u0026rsquo;ve played with Docker a little in it early days but didn\u0026rsquo;t stick for longer with it. It\u0026rsquo;s stable now so I wanted to check how it\u0026rsquo;s running now.\nI really can\u0026rsquo;t accept this method of installation:\ncurl -fsSL https://get.docker.com/ | sh I think that world is going to it\u0026rsquo;s end when I see such scritps\u0026hellip; I prefer to do this manually, knowing exactly what I have to do.\nInstall prerequisites:","title":"Manual installation of Docker on Debian/Ubuntu"},{"content":"I started playing with Docker and here I will write some commands that where not so obvious at beginning \u0026#x1f603;\nList running containers:\ndocker ps List also not running containers:\ndocker ps -a Remove all containers (be careful with that):\ndocker rm $(docker ps -a -q) Remove all images:\ndocker rmi $(docker images -q) Docker won\u0026rsquo;t remove any old volumes used by containers, so after some time you may be interested in deleting them all:\ndocker volume rm $(docker volume ls -q) Run container and enter bash:\ndocker run --name deb -t -i debian:jessie /bin/bash Show build logs from container:\ndocker logs deb Enter bash into running container:\ndocker exec -it deb /bin/bash Build image from Dockerfile in current directory:\ndocker build -t my_web . After playing a little with this command I started searching for something to automate my tasks a little, and found docker-compose\u0026thinsp; external link - you may be interested in it too.\nSources https://techoverflow.net/blog/2013/10/22/docker-remove-all-images-and-containers/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/some-useful-commands-in-docker/","summary":"I started playing with Docker and here I will write some commands that where not so obvious at beginning \u0026#x1f603;\nList running containers:\ndocker ps List also not running containers:\ndocker ps -a Remove all containers (be careful with that):\ndocker rm $(docker ps -a -q) Remove all images:\ndocker rmi $(docker images -q) Docker won\u0026rsquo;t remove any old volumes used by containers, so after some time you may be interested in deleting them all:","title":"Some useful commands in Docker"},{"content":" RubyWzorce projektowe\nAuthor: Russ Olsen\nhelion.pl ","permalink":"https://gagor.pro/books/2016/ruby-wzorce-projektowe/","summary":" RubyWzorce projektowe\nAuthor: Russ Olsen\nhelion.pl ","title":"Ruby"},{"content":" Sztuka podstępuŁamałem ludzi, nie hasła\nAuthors: Kevin Mitnick, William L. Simon\nhelion.pl ","permalink":"https://gagor.pro/books/2016/sztuka-podstepu/","summary":" Sztuka podstępuŁamałem ludzi, nie hasła\nAuthors: Kevin Mitnick, William L. Simon\nhelion.pl ","title":"Sztuka podstępu"},{"content":"I was doing a lot of changes to my old posts, switched to HTTPS, etc. Sometimes it was useful to change some particular text in all my old posts at a time, but there is no such feature in WordPress. But WordPress runs on MySQL and I could use SQL query to update such posts.\nMake backup - it\u0026rsquo;s not required but strongly advised \u0026#x1f603;\nNow use this query as template to replace in place whatever you need:\ne\u003eYou should see something like:\nQuery OK, 157 rows affected (0.04 sec) Rows matched: 455 Changed: 157 Warnings: 0 This will remove \u0026lt;!--more--\u0026gt;from all posts (it\u0026rsquo;s used by WordPress to span article when showed on tag/category pages).\nAnother example to update all URLs to HTTPS:\nUPDATE wp_posts SET post_content = REPLACE(post_content, \u0026#34;http://gagor.pl\u0026#34;, \u0026#34;https://gagor.pl\u0026#34;); Be careful with that and make DB backup before you start.\n","permalink":"https://gagor.pro/2016/02/mass-replace-in-wordpress-posts-via-mysql-query/","summary":"\u003cp\u003eI was doing a lot of changes to my old posts, switched to HTTPS, etc. Sometimes it was useful to change some particular text in all my old posts at a time, but there is no such feature in WordPress. But WordPress runs on MySQL and I could use SQL query to update such posts.\u003c/p\u003e\n\u003cp\u003eMake backup - it\u0026rsquo;s not required but strongly advised \u0026#x1f603;\u003c/p\u003e\n\u003cp\u003eNow use this query as template to replace in place whatever you need:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e","title":"Mass replace in WordPress posts via MySQL query"},{"content":"From few days I have access to UPC\u0026rsquo;s www.horizon.tv\u0026thinsp; external link platform - until now it was useless on Linux. But there is Pipelight that will use Wine to emulate Silverlight on Linux and it\u0026rsquo;s working pretty well - you\u0026rsquo;re just few commands away from achieving that:\n# stop browser killall firefox # remove old version if you have it sudo apt-get remove pipelight Now configure repos and install packages:\nsudo apt-add-repository ppa:pipelight/stable sudo apt-get update sudo apt-get install --install-recommends pipelight-multi sudo pipelight-plugin --update Enable plugin (run it with sudo for system wide installation):\npipelight-plugin --enable silverlight Start Firefox and test if plugin is working here: http://bubblemark.com/silverlight2.html\u0026thinsp; external link Now enter www.horizon.tv\u0026thinsp; external link and try it yourself.\nP.S. It works only on Firefox because Chrome do not support NPAPI plugins anymore\u0026thinsp; external link \u0026#x1f603;\nSources http://www.webupd8.org/2013/08/pipelight-use-silverlight-in-your-linux.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/use-www-horizon-tv-with-pipelight-silverlight-on-linux-ubuntu/","summary":"From few days I have access to UPC\u0026rsquo;s www.horizon.tv\u0026thinsp; external link platform - until now it was useless on Linux. But there is Pipelight that will use Wine to emulate Silverlight on Linux and it\u0026rsquo;s working pretty well - you\u0026rsquo;re just few commands away from achieving that:\n# stop browser killall firefox # remove old version if you have it sudo apt-get remove pipelight Now configure repos and install packages:","title":"Use www.horizon.tv with Pipelight/Silverlight on Linux/Ubuntu"},{"content":"I just bought new wifi card for my desktop computer. Like in topic, it\u0026rsquo;s Intel Dual Band Wireless-AC 7260 for Desktop\u0026thinsp; external link .\nI was searching for card that:\nsupport AC standard have 5GHz network support (2,4GHz channels are cluttered heavily in my neighborhood have PCI/PCIx or USB3 connector is Linux friendly (no modules compilation by hand, support for aircrack-ng, kismet) This one is the only I found that comply my expectations.\nI found time to play with kismet and aircrack-ng and it was working fine. Card works without problems on kernel 4.2.0. Highest transfer on my net I could get from my NAS - about 23 MB/s (megabytes per second) - much better than on my old N router (approx 6,5 MB/s).\nHere\u0026rsquo;s information from lspci -vvv:\n05:00.0 Network controller: Intel Corporation Wireless 7260 (rev 73) Subsystem: Intel Corporation Dual Band Wireless-AC 7260 Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast \u0026gt;TAbort- \u0026lt;TAbort- \u0026lt;MAbort- \u0026gt;SERR- \u0026lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 39 Region 0: Memory at f7c00000 (64-bit, non-prefetchable) [size=8K] Capabilities: [c8] Power Management version 3 Flags: PMEClk- DSI+ D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold+) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [d0] MSI: Enable+ Count=1/1 Maskable- 64bit+ Address: 00000000fee0400c Data: 4123 Capabilities: [40] Express (v2) Endpoint, MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0, Latency L0s \u0026lt;512ns, L1 unlimited ExtTag- AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd- ExtTag- PhantFunc- AuxPwr+ NoSnoop+ FLReset- MaxPayload 128 bytes, MaxReadReq 128 bytes DevSta: CorrErr+ UncorrErr- FatalErr- UnsuppReq- AuxPwr+ TransPend- LnkCap: Port #0, Speed 2.5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s \u0026lt;4us, L1 \u0026lt;32us ClockPM+ Surprise- LLActRep- BwNot- ASPMOptComp- LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+ ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed 2.5GT/s, Width x1, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt- DevCap2: Completion Timeout: Range B, TimeoutDis+, LTR+, OBFF Via WAKE# DevCtl2: Completion Timeout: 16ms to 55ms, TimeoutDis-, LTR-, OBFF Disabled LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis- Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS- Compliance De-emphasis: -6dB LnkSta2: Current De-emphasis Level: -3.5dB, EqualizationComplete-, EqualizationPhase1- EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest- Capabilities: [100 v1] Advanced Error Reporting UESta: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UEMsk: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol- CESta: RxErr- BadTLP+ BadDLLP- Rollover- Timeout- NonFatalErr+ CEMsk: RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr+ AERCap: First Error Pointer: 00, GenCap- CGenEn- ChkCap- ChkEn- Capabilities: [140 v1] Device Serial Number 7c-5c-f8-ff-xx-xx-xx-xx Capabilities: [14c v1] Latency Tolerance Reporting Max snoop latency: 0ns Max no snoop latency: 0ns Capabilities: [154 v1] Vendor Specific Information: ID=cafe Rev=1 Len=014 \u0026lt;?\u0026gt; Kernel driver in use: iwlwifi And iwconfig:\nwlp5s0 IEEE 802.11abgn ESSID:\u0026#34;cis5\u0026#34; Mode:Managed Frequency:5.5 GHz Access Point: 34:7A:60:XX:XX:XX Bit Rate=780 Mb/s Tx-Power=22 dBm Retry short limit:7 RTS thr:off Fragment thr:off Power Management:off Link Quality=60/70 Signal level=-50 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:7 Invalid misc:184 Missed beacon:0 If you\u0026rsquo;re thinking about buying any Linux friendly AC wifi card - this one is worth it\u0026rsquo;s price.\n","permalink":"https://gagor.pro/2016/02/intel-dual-band-wireless-ac-7260-for-desktop-on-linux/","summary":"I just bought new wifi card for my desktop computer. Like in topic, it\u0026rsquo;s Intel Dual Band Wireless-AC 7260 for Desktop\u0026thinsp; external link .\nI was searching for card that:\nsupport AC standard have 5GHz network support (2,4GHz channels are cluttered heavily in my neighborhood have PCI/PCIx or USB3 connector is Linux friendly (no modules compilation by hand, support for aircrack-ng, kismet) This one is the only I found that comply my expectations.","title":"Intel Dual Band Wireless-AC 7260 for Desktop on Linux"},{"content":"I watched nice presentation about how Cloudflare protects itself against DoS. Most of us are not able to do that exactly like them but some of tips were general enough to be used on typical web front server.\nI took notes from this presentation and presented here. Thanks to Marek agreement I also reposted all examples (in easier to copy paste way).\nHowto prepare against ACK/FIN/RST/X-mas flood Use conntrack rule:\niptables -A INPUT --dst 1.2.3.4 -m conntrack --ctstate INVALID -j DROP which will only work with disabled tcp_loose setting (it\u0026rsquo;s by default enabled) with addition to sysctl:\nsysctl -w net.netfilter.nf_conntrack_tcp_loose=0 Howto prepare against SYN floods SYN flood is hard case - because when you use conntrack it will make your performance worst validating state for every new single packet.\nThe only way to get around this is to enable syncookies:\nsysctl -w net.ipv4.tcp_syncookies=1 sysctl -w net.ipv4.tcp_timestamps=1 Enabling syncookies will cause loose of some of connection informations, that are pretty useful like:\nwindow scaling factor ECN bit (Explicit Congestion Notification) For that we will use tcp_timestamp option, that will use few bits from timestamp field to store some of this informations.\nThis still may be not efficient enough, but in kernel 4.4 there will be some update to how syncookies are served that should make it few times faster than with older one.\nRelated docs: https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\u0026thinsp; external link Howto prepare against botnet attack Symptoms:\nconcurrent connection count going up many sockets in orphaned state many sockets in time wait state Solutions:\nEnable connlimit feature on conntrack to limit amount of concurrent connections to our service Use hashlimits to rate limit SYN packets per IP Use ipsets to efficiently block many IP/subnet addresses manual blacklisting - feed IP blacklist from HTTP server logs supports subnets, timeouts automatic blacklisting hashlimits Disable HTTP keep-alives to make this attack look more like SYN flood This may still not work against DDoS because huge amount of bots won\u0026rsquo;t allow you to block them efficiently enough.\nSome exciting system tweaks and examples from this presentation I hope to find some time to merge them into template/script that could be used much easier - but first I have to play with these rules a little and test what will be most useful.\nNIC: Discard with flow steering ethtool -N eth3 flow-type udp4 dst-ip 129.168.254.30 dst-port 53 action -1 Flow steering for priority ethtool -X eth3 weight 0 1 1 1 1 1 1 1 1 1 1 ethtool -N eth3 flow-type tcp4 dst-port 22 action 0 SYN backlog size sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.tcp_max_syn_backlog=65535 It\u0026rsquo;s rounded to next power of two (in this case to 65536).\nSYN backlog decay sysctl -w net.ipv4.tcp_synack_retries=1 L7 connection count sysctl -w net.ipv4.tcp_max_orphans=262144 sysctl -w net.ipv4.tcp_orphan_retries=1 sysctl -w net.ipv4.tcp_max_tw_buckets=360000 sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w net.ipv4.tcp_fin_timeout=5 L3: u32 iptables -A INPUT \\ --dst 1.2.3.4 \\ --p udp -m udp --dport 53 \\ -m u32 --u32 \u0026#34;6\u0026amp;0xFF=0x6 \u0026amp;\u0026amp; 4\u0026amp;0x1FFF=0 \u0026amp;\u0026amp; 0\u0026gt;\u0026gt;22\u0026amp;0x3C@4=0x29\u0026#34; \\ -j DROP L4: Conntrack iptables -t raw -A PREROUTING \\ -i eth2 \\ --dst 1.2.3.4 \\ -j ACCEPT iptables -t raw -A PREROUTING \\ -i eth2 \\ -j NOTRACK iptables -A INPUT \\ --dst 1.2.3.4 \\ -m conntrack --ctstate INVALID \\ -j DROP Tuning conntrack sysctl -w net.netfilter.nf_conntrack_tcp_loose=0 sysctl -w net.netfilter.nf_conntrack_helper=0 sysctl -w net.nf_conntrack_max=2000000 echo 2500000 \u0026gt; /sys/module/nf_conntrack/parameters/hashsize More info about conntrack sysctl options: https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt\u0026thinsp; external link L7: Connlimit iptables -t raw -A PREROUTING \\ -i eth2 \\ --dst 1.2.4.5 \\ -j ACCEPT iptables -A INPUT \\ --dst 1.2.3.4 \\ -p tcp -m tcp --dport 80 \\ -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN \\ -m connlimit \\ --connlimit-above 10 \\ --connlimit-mask 32 \\ --connlimit-saddr \\ -j DROP L7: ipset for blacklisting ipset -exist create ta_d335c5 hash:net family inet ipset add ta_d335c5 192.168.0.0/16 ipset add ta_d335c5 10.0.0/8 iptables -A INPUT \\ -m set --match-set ta_d335c5 src \\ -j DROP L7: being evil - TARPIT iptables -A INPUT \\ -m set --match-set ta_d335c5 src \\ -j TARPIT TARPIT target will imitate successful connection for the client (bot in this case) but without responding to it\u0026rsquo;s queries. It will cost that bot a lot more resources and time to timeout and drop this connection than when using DROP or REJECT here.\nL7: hashlimit for rate limiting iptables -A INPUT \\ --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\ --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\ -m hashlimit \\ --hashlimi-above 123/sec \\ --hashlimit-burst 5 \\ --hashlimit-mode srcip \\ --hashlimit-srcmask 24 \\ --hashlimit-name 341654b1d4af9bf \\ -j DROP L7: auto-blacklist ipset -exist create blacklist hash:net timeout 60 iptables -A INPUT \\ --dst 1.2.3.4 \\ -m set --match-set blaclist src \\ -j DROP iptables -A INPUT \\ --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\ --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\ -m hashlimit \\ --hashlimit-above 100/sec \\ --hashlimit-mode srcip \\ --hashlimit-srcmask 24 \\ --hashlimit-name hl_blacklist \\ -j SET --add-set blacklist src L7+: payload in TCP - string iptables -A INPUT \\ --dst 1.2.3.4 \\ -p tcp --dport 80 \\ -m string \\ --hex-string 486f737777777777... \\ --from 231 --to 300 \\ -j DROP For more informations and explanations watch this great presentation:\nAnd here is the whole presentation with additional examples:\nhttps://speakerdeck.com/majek04/lessons-from-defending-the-indefensible\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/02/prepare-for-dos-like-cloudflare-do/","summary":"I watched nice presentation about how Cloudflare protects itself against DoS. Most of us are not able to do that exactly like them but some of tips were general enough to be used on typical web front server.\nI took notes from this presentation and presented here. Thanks to Marek agreement I also reposted all examples (in easier to copy paste way).\nHowto prepare against ACK/FIN/RST/X-mas flood Use conntrack rule:","title":"Prepare for DoS like Cloudflare do"},{"content":"I was looking at backup task running on my desk and saw that it\u0026rsquo;s spending a lot of time on ~/.local/share/zeitgeist directory. I checked and it had 4.6GB:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,4G activity.sqlite-wal 311M fts.index WTF? Fortunately I found here\u0026thinsp; external link that I could easily delete some of this:\nzeitgeist-daemon --quit Now check that it\u0026rsquo;s not running:\nps axu | grep zeitgeist-daemon timor 9105 0.0 0.0 10756 2140 pts/5 S+ 19:01 0:00 grep --color=auto zeitgeist-daemon Nothing there so we may start deleting:\ncd ~/.local/share/zeitgeist rm -r fts.index rm activity.sqlite-wal rm activity.sqlite-shm Start zeitgeist-daemon again:\nzeitgeist-daemon --replace\u0026amp; Now check files sizes again:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,0K activity.sqlite-wal 640K fts.index Much better \u0026#x1f603;\n","permalink":"https://gagor.pro/2016/02/zeitgeist-activity-sqlite-wal-getting-huge/","summary":"I was looking at backup task running on my desk and saw that it\u0026rsquo;s spending a lot of time on ~/.local/share/zeitgeist directory. I checked and it had 4.6GB:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,4G activity.sqlite-wal 311M fts.index WTF? Fortunately I found here\u0026thinsp; external link that I could easily delete some of this:\nzeitgeist-daemon --quit Now check that it\u0026rsquo;s not running:\nps axu | grep zeitgeist-daemon timor 9105 0.","title":"Zeitgeist activity.sqlite-wal getting huge"},{"content":"There are many possible real life cases and not all optimization technics will be suitable for you but I hope it will be a good starting place.\nAlso you shouldn\u0026rsquo;t copy paste examples with faith that they will make your server fly \u0026#x1f603; You have to support your decisions with excessive tests and help of monitoring system (ex. Grafana ).\nCache static and dynamic content Setting caching static and dynamic content strategy may offload your server from additional load from repetitive downloads of same, rarely updated files. This will make your site to load faster for frequent visitors.\nExample configuration:\nlocation ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf|m4v)$ { access_log off; log_not_found off; tcp_nodelay off; open_file_cache max=500 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; expires max; } For additional performance gain, you may:\ndisable logging for static files, disable tcp_nodelay option - it\u0026rsquo;s useful to send a lot of small files (ideally smaller than single TCP packet - 1,5Kb), but images are rather big files and sending them all together will gain better performance, play with open_file_cache - it will take off some IO load, add long long expires. Caching dynamic content is harder case. There are articles that are rarely updated and they may lay in cache forever but other pages are pretty dynamic and shouldn\u0026rsquo;t be cached for long. Even if caching dynamic content sounds scary for you it\u0026rsquo;s not. So called micro caching (caching for short period of time, like 1s) - is great solution for digg effect\u0026thinsp; external link or slashdotting\u0026thinsp; external link .\nLet say your page gets ten views per second and you will cache ever site for 1s, then you will be able to server 90% of requests from cache. Leaving precious CPU cycles for other tasks.\nCompress data On your page you should use filetypes that are efficiently compressed like: JPEG, PNG, MP3, etc. But all HTML, CSS, JS may be compressed too on the fly by web server, just enable options like that globally:\ngzip on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml image/svg+xml; You may also precompress these files stronger during build/deploy process and use gzip_static module to serve them without additional overhead for compression. Ex.:\ngzip_static on; Then use script like this to compress files:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; Files have to had same timestamp like original (not compressed) file to be used by Nginx.\nOptimize SSL/TLS New optimized versions of HTTP protocols like HTTP/2 or SPDY require HTTPS configuration (at least in browsers implementation). Then SSL/TLS high cost of every new HTTPS connection became crucial case for further optimizations.\nThere are few steps required for improved SSL/TLS performance.\nEnable SSL session caching Use ssl_session_cache directive to cache parameters used when securing each new connection, ex.:\nssl_session_cache builtin:1000 shared:SSL:10m; Enable SSL session tickets Tickets store information about specific SSL/TLS connection so connection may be reused without new handshake, ex.:\nssl_session_tickets on; Configure OCSP stapling for SSL This will lower handshaking time by caching SSL/TLS certificate informations. This is per site/certificate configuration, ex.:\nssl_stapling on; ssl_stapling_verify on; ssl_certificate /etc/ssl/certs/my_site_cert.crt; ssl_certificate_key /etc/ssl/private/my_site_key.key; ssl_trusted_certificate /etc/ssl/certs/authority_cert.pem; A ssl_trusted_certificate file have to point to trusted certificate chain file - root + intermediate certificates (this can be downloaded from your certificate provider site (sometimes you have to merge by yourself those files).\nExcessive article in this topic could be found here: https://raymii.org/s/tutorials/OCSP_Stapling_on_nginx.html\u0026thinsp; external link Implement HTTP/2 or SPDY If you have HTTPS configured the only thing you have to do is to add two options on listen directive, ex.:\nlisten 443 ssl http2; # currently http2 is preferred against spdy; # on SSL enabled vhost ssl on; You may also advertise for HTTP connection that you have newer protocol available, for that on HTTP connections use this header:\nadd_header Alternate-Protocol 443:npn-spdy/3; SPDY and HTTP/2 protocols use:\nheaders compression, single, multiplexed connection (carrying pieces of multiple requests and responses at the same time) rather than multiple connection for every piece of web page. After SPDY or HTTP/2 implementation you no longer need typical HTTP/1.1 optimizations like:\ndomain sharding, resource (JS/CSS) merging, image sprites. Tune other nginx performance options Access logs Disable access logs were you don\u0026rsquo;t need them, ex.: for static files. You may also use buffer and flush options with access_log directive, ex.:\naccess_log /var/log/nginx/access.log buffer=1m flush=10s; With buffer Nginx will hold that much data in memory before writing it to disk. flush tells Nginx how often it should write gathered logs to disk.\nProxy buffering Turning proxy buffering\u0026thinsp; external link may impact performance of your reverse proxy.\nNormally when buffering is disabled, Nginx will pass response directly to client synchronously.\nWhen buffering is enable it will store response in memory set by proxy_buffer_size option and if response is too big it will be stored in temporary file.\nproxy_buffering on; proxy_buffer_size 16k; Keepalive for client and upstream connections] Every new connection costs some time for handshake and will add latency to requests. By using keepalive connections will be reused without this overhead.\nFor client connections\u0026thinsp; external link :\nkeepalive_timeout = 120s; For upstream connections\u0026thinsp; external link :\nupstream web_backend { server 127.0.0.1:80; server 10.0.0.2:80; keepalive 32; } Limit connections to some resources Some time users/bots overload your service by querying it to fast. You may limit allowed connections\u0026thinsp; external link to protect your service in such case, ex.:\nlimit_conn_zone $binary_remote_addr zone=owncloud:1m; server { # ... limit_conn owncloud 10; # ... } Adjust woker count Normally Nginx will start with only 1 worker process\u0026thinsp; external link , you should adjust this variable to at the number of CPU\u0026rsquo;s, in case of quad core CPU use in main section:\nworker_processes 4; Use socket sharding In latest kernel and Nginx versions (at least 1.9.1) there is new feature of sockets sharding. This will offload management of new connections to kernel. Each worker will create a socket listener and kernel will assign new connections to them as they become available.\nlisten 80 reuseport; Thread pools Thread pools\u0026thinsp; external link are solution for mostly long blocking IO operations that may block whole Nginx event queue (ex. when used with big files or slow storage).\nlocation / { root /storage; aio threads; } This will help a lot if you see many Nginx processes in D state, with high IO wait times.\nTune Linux for performance Backlog queue If you could see on your system connection that appear to be staling then you have to increase net.core.somaxconn. This system parameter describes the maximum number of backlogged sockets. Default is 128 so setting this to 1024 should be no big deal on any decent machine.\necho \u0026#34;net.core.somaxconn=1024\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf File descriptors If your system is serving a lot of connections you may get reach system wide open descriptor limit. Nginx uses up to two descriptors for each connection. Then you have to increase sys.fs.fs_max.\necho \u0026#34;sys.fs.fs_max=3191256\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf Ephemeral ports Nginx used as a proxy creates temporary (ephemeral) ports for each upstream server. On busy proxy servers this will result in many connection in TIME_WAIT state.\nSolution for that is to increase range of available ports by setting net.ipv4.ip_local_port_range. You may also benefit from lowering net.ipv4.tcp_fin_timeout setting (connection will be released faster, but be careful with that).\nUse reverse-proxy This with microcaching technic is worth separate article, I will add link here when it will be ready.\nSource: http://www.fromdual.com/huge-amount-of-time-wait-connections\u0026thinsp; external link https://www.nginx.com/blog/10-tips-for-10x-application-performance/\u0026thinsp; external link https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/\u0026thinsp; external link https://www.nginx.com/blog/thread-pools-boost-performance-9x/\u0026thinsp; external link https://tweaked.io/guide/kernel/\u0026thinsp; external link https://t37.net/nginx-optimization-understanding-sendfile-tcp_nodelay-and-tcp_nopush.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/01/optimize-nginx-for-performance/","summary":"There are many possible real life cases and not all optimization technics will be suitable for you but I hope it will be a good starting place.\nAlso you shouldn\u0026rsquo;t copy paste examples with faith that they will make your server fly \u0026#x1f603; You have to support your decisions with excessive tests and help of monitoring system (ex. Grafana ).\nCache static and dynamic content Setting caching static and dynamic content strategy may offload your server from additional load from repetitive downloads of same, rarely updated files.","title":"Optimize Nginx for performance"},{"content":"Sometime you need to make quick and dirty image backup of VM running on XenServer and this post is about such case \u0026#x1f603;\nList machines:\nxl list Name ID Mem VCPUs State Time(s) Domain-0 0 4066 8 r----- 3526567.3 webfront1.example.com 1 4096 4 r----- 3186487.2 webfront2.example.com 2 2048 2 -b---- 920408.2 Now you may export one:\nxe vm-export vm=webfront1.example.com filename=/srv/backup/webfront.xva Export succeeded You may also use uuid for that - list machines with xe vm-list (best with less) and then:\nxe vm-export uuid=1234a43d-c5af-f1ef-b3c1-12347f63d84c filename=/srv/backup/webfront.xva That\u0026rsquo;s all!\n","permalink":"https://gagor.pro/2016/01/xenserver-export-vm-to-file/","summary":"Sometime you need to make quick and dirty image backup of VM running on XenServer and this post is about such case \u0026#x1f603;\nList machines:\nxl list Name ID Mem VCPUs State Time(s) Domain-0 0 4066 8 r----- 3526567.3 webfront1.example.com 1 4096 4 r----- 3186487.2 webfront2.example.com 2 2048 2 -b---- 920408.2 Now you may export one:\nxe vm-export vm=webfront1.example.com filename=/srv/backup/webfront.xva Export succeeded You may also use uuid for that - list machines with xe vm-list (best with less) and then:","title":"XenServer - export VM to file"},{"content":"Sometimes deployment process or other havy task may cause some Nagios checks to rise below normal levels and bother admin. If this is expected and you want to add downtime on host/service during this task you may use this script:\n#!/bin/bash function die { echo $1; exit 1; } if [[ $# -eq 0 ]] ; then die \u0026#34;Give hostname and time in minutes as parameter!\u0026#34; fi if [[ $# -eq 1 ]] ; then MINUTES=15 else MINUTES=$2 fi HOST=$1 NAGURL=http://nagios.example.com/nagios/cgi-bin/cmd.cgi USER=nagiosuser PASS=nagiospassword SERVICENAME=someservice COMMENT=\u0026#34;Deploying new code\u0026#34; export MINUTES echo \u0026#34;Scheduling downtime on $HOST for $MINUTES minutes...\u0026#34; # The following is urlencoded already STARTDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34;` # This gives us the date/time X minutes from now ENDDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34; -d \u0026#34;$MINUTES min\u0026#34;` curl --silent --show-error \\ --data cmd_typ=56 \\ --data cmd_mod=2 \\ --data host=$HOST \\ --data-urlencode \u0026#34;service=$SERVICENAME\u0026#34; \\ --data-urlencode \u0026#34;com_data=$COMMENT\u0026#34; \\ --data trigger=0 \\ --data-urlencode \u0026#34;start_time=$STARTDATE\u0026#34; \\ --data-urlencode \u0026#34;end_time=$ENDDATE\u0026#34; \\ --data fixed=1 \\ --data hours=2 \\ --data minutes=0 \\ --data btnSubmit=Commit \\ --insecure \\ $NAGURL -u \u0026#34;$USER:$PASS\u0026#34;| grep -q \u0026#34;Your command request was successfully submitted to Nagios for processing.\u0026#34; || die \u0026#34;Failed to con tact nagios\u0026#34;; echo Scheduled downtime on nagios from $STARTDATE to $ENDDATE Threat this script as template with some tips:\nI you want to add downtime on service, then provide SERVICENAME and --data cmd_typ=56 \\. If you want downtime on whole host, just remove this line: --data-urlencode \u0026quot;service=$SERVICENAME\u0026quot; \\ and --data cmd_typ=86 \\ Another thing that in my example nagios page use basic auth for security, if your don\u0026rsquo;t use it, you may remove -u \u0026quot;$USER:$PASS\u0026quot; from parameters. If you get Start or end time not valid, then you have to adapt dates to your formats of dates accepted by Nagios (probably this depends on Nagios version or timezone configuration). Sources http://stackoverflow.com/questions/6842683/how-to-set-downtime-for-any-specific-nagios-host-for-certain-time-from-commandli\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/01/nagios-downtime-on-hostservice-from-command-line-with-curl/","summary":"Sometimes deployment process or other havy task may cause some Nagios checks to rise below normal levels and bother admin. If this is expected and you want to add downtime on host/service during this task you may use this script:\n#!/bin/bash function die { echo $1; exit 1; } if [[ $# -eq 0 ]] ; then die \u0026#34;Give hostname and time in minutes as parameter!\u0026#34; fi if [[ $# -eq 1 ]] ; then MINUTES=15 else MINUTES=$2 fi HOST=$1 NAGURL=http://nagios.","title":"Nagios - downtime on host/service from command line with curl"},{"content":"Now when you have CollectD and InfluxDB installed you may configure Grafana \u0026#x1f603;\nFirst configure repo with current Grafana version (select your distro):\ncurl https://packagecloud.io/gpg.key | sudo apt-key add - deb https://packagecloud.io/grafana/testing/debian/ wheezy main Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get update apt-get install -y apt-transport-https apt-get install -y grafana By default Grafana will use sqlite database to keep information about users, etc:\n[database] # Either \u0026#34;mysql\u0026#34;, \u0026#34;postgres\u0026#34; or \u0026#34;sqlite3\u0026#34;, it\u0026#39;s your choice ;type = sqlite3 ;host = 127.0.0.1:3306 ;name = grafana ;user = root ;password = If that\u0026rsquo;s ok for you, you may leave it as is. I prefer to configure MySQL database (create user, database, grant permissions to user):\n[database] type = mysql host = 127.0.0.1:3306 name = grafana user = grafana password = mydbpassword So Grafana should be running on port 3000 by default, now it\u0026rsquo;s time to connect ex.: http://localhost:3000 (use your host). Now click Data sources on left panel, then Add new on top panel and fill source data like below:\nBecause we didn\u0026rsquo;t set authorization for InfluxDB you may just type whatever login/password there. Now Test Connection and Save and you should be ready to play with Grafana.\nI also used scripted dashboard for Grafana to add easily statistics for my hosts, you may find it here: https://github.com/anryko/grafana-influx-dashboard\u0026thinsp; external link Sources http://docs.grafana.org/installation/debian/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/01/grafana-installation-and-configuraton-with-influxdb-and-collectd-on-debian-ubuntu/","summary":"Now when you have CollectD and InfluxDB installed you may configure Grafana \u0026#x1f603;\nFirst configure repo with current Grafana version (select your distro):\ncurl https://packagecloud.io/gpg.key | sudo apt-key add - deb https://packagecloud.io/grafana/testing/debian/ wheezy main Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get update apt-get install -y apt-transport-https apt-get install -y grafana By default Grafana will use sqlite database to keep information about users, etc:","title":"Grafana - installation and configuraton with InfluxDB and CollectD on Debian/Ubuntu"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nLet\u0026rsquo;s start! On project page there is no info about repo but it\u0026rsquo;s available, configure it:\ncurl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/debian wheezy stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf for Ubuntu use url like (of course selecting your version):\necho \u0026#34;deb https://repos.influxdata.com/ubuntu wily stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get install -y apt-transport-https apt-get install -y influxdb Now edit /etc/influxdb/influxdb.conf and uncoment/fill [collectd] section like this:\n[collectd] enabled = true bind-address = \u0026#34;:8096\u0026#34; database = \u0026#34;collectd_db\u0026#34; typesdb = \u0026#34;/usr/share/collectd/types.db\u0026#34; You may adjust port to whatever suits you best. database sets InfluxDB database used to store collectd data, and typesdb is file from collectd package defining collectd metrics structure (this is location for Debian) - so you have collectd service installed earlier .\nNow you may check if InfluxDB is working fine by connecting to web admin panel, by standard on port 8083.\nSources https://github.com/influxdata/influxdb/issues/585\"\u0026thinsp; external link https://anomaly.io/collectd-metrics-to-influxdb/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/01/influxdb-installation-and-configuration-on-debianubuntu/","summary":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nLet\u0026rsquo;s start! On project page there is no info about repo but it\u0026rsquo;s available, configure it:\ncurl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/debian wheezy stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf for Ubuntu use url like (of course selecting your version):","title":"InfluxDB - installation and configuration on Debian/Ubuntu"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nInstallation of collectd is easy on Debian because packages are in default repo. One problem is that packages may be old, ex. on wheezy it version 5.1. But in backports/backports-sloppy you may find current 5.5, so enable backports first:\necho \u0026#34;deb http://http.debian.net/debian wheezy-backports main contrib non-free\u0026#34; \u0026gt; /etc/apt/sources.list.d/backports.list echo \u0026#34;deb http://http.debian.net/debian wheezy-backports-sloppy main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/backports.list Install package:\napt-get update apt-get install -y -t backports-sloppy collectd collectd-utils # or on recent system just apt-get install -y collectd collectd-utils Now edit configuration /etc/collectd/collectd.conf and add network section:\nLoadPlugin network \u0026lt;Plugin \u0026#34;network\u0026#34;\u0026gt; Server \u0026#34;localhost\u0026#34; \u0026#34;8096\u0026#34; \u0026lt;/Plugin\u0026gt; Use your InfluxDB hostname:port.\nNow select and add enable some plugins - list here\u0026thinsp; external link and restart service:\nservice collectd restart That\u0026rsquo;s all - now install InfluxDB .\nSources https://anomaly.io/collectd-metrics-to-influxdb/\u0026thinsp; external link http://backports.debian.org/Instructions/\u0026thinsp; external link ","permalink":"https://gagor.pro/2016/01/collectd-installation-and-configuration-with-influxdb-on-debianubuntu/","summary":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nInstallation of collectd is easy on Debian because packages are in default repo. One problem is that packages may be old, ex. on wheezy it version 5.1. But in backports/backports-sloppy you may find current 5.","title":"CollectD - installation and configuration with InfluxDB on Debian/Ubuntu"},{"content":"From the first moment I heard about Let\u0026rsquo;s Encrypt I liked it and wanted to use it as fast as possible. But the more I read how they want to implement it, the more I dislike it.\nCurrent project with automatic configuration is not what I want to use at all. I have many very complicated configs and I do not trust such tools enough to use them. I like UNIX\u0026rsquo;s single purpose principle, tools should do one thing and do it well - nothing more.\nBut there is one neet tool that use Let\u0026rsquo;s Encrypt API only leaving all configuration for me, it\u0026rsquo;s acme-tiny\u0026thinsp; external link python based script. I won\u0026rsquo;t copy/paste examples - documentation is written pretty well.\n","permalink":"https://gagor.pro/2016/01/lets-encrypt-without-auto-configuration/","summary":"From the first moment I heard about Let\u0026rsquo;s Encrypt I liked it and wanted to use it as fast as possible. But the more I read how they want to implement it, the more I dislike it.\nCurrent project with automatic configuration is not what I want to use at all. I have many very complicated configs and I do not trust such tools enough to use them. I like UNIX\u0026rsquo;s single purpose principle, tools should do one thing and do it well - nothing more.","title":"Let’s Encrypt - without auto configuration"},{"content":"Lately I had a lot of brute force attacks on my WordPress blog. I used basic auth to /wp-admin part in nginx configuration to block this and as a better solution I wan\u0026rsquo;t to block source IPs at all on firewall.\nTo do this, place this filter code in /etc/fail2ban/filter.d/wp-login.conf:\n# WordPress brute force wp-login.php filter: # # Block IPs trying to authenticate in WordPress blog # # Matches e.g. # 178.218.54.109 - - [31/Dec/2015:10:39:34 +0100] \u0026#34;POST /wp-login.php HTTP/1.1\u0026#34; 401 188 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0\u0026#34; # [Definition] failregex = ^\u0026lt;HOST\u0026gt; .* \u0026#34;POST /wp-login.php ignoreregex = Then edit your /etc/fail2ban/jail.local and add:\n[wp-login] enabled = true port = http,https filter = wp-login logpath = /var/log/nginx/access.log maxretry = 3 Now restart fail2ban:\nservice fail2ban restart All done \u0026#x1f604;\n","permalink":"https://gagor.pro/2015/12/fail2ban-block-wp-login-php-brute-force-attacks/","summary":"Lately I had a lot of brute force attacks on my WordPress blog. I used basic auth to /wp-admin part in nginx configuration to block this and as a better solution I wan\u0026rsquo;t to block source IPs at all on firewall.\nTo do this, place this filter code in /etc/fail2ban/filter.d/wp-login.conf:\n# WordPress brute force wp-login.php filter: # # Block IPs trying to authenticate in WordPress blog # # Matches e.g. # 178.","title":"fail2ban - block wp-login.php brute force attacks"},{"content":"I have some Ansible roles to configure my vps, Raspberry Pi, etc. I like to test them before I broke something on my real, not clustered machines - I use Vagrant for that.\nBut with it I had one problem - in playbooks I define hosts as groups of severs ex. web for my vps:\nExample Ansible playbook - hosts: web gather_facts: True sudo: True ... But testing machine wasn\u0026rsquo;t in this group and when I run vagrant I could only see:\nAnsible run $ vagrant provision ==\u0026gt; default: Running provisioner: fix-no-tty (shell)... default: Running: inline script ==\u0026gt; default: Running provisioner: ansible... PLAY [web] ******************************************************************** skipping: no hosts matched PLAY RECAP ******************************************************************** To get rid of this I have to add default vagrant machine to my default group in Vagrantfile:\nVagrantfile config.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.groups = { \u0026#34;web\u0026#34; =\u0026gt; [\u0026#34;default\u0026#34;] } ansible.sudo = true ansible.limit = \u0026#34;all\u0026#34; ansible.playbook = \u0026#34;web.yml\u0026#34; end And that solved my problem \u0026#x1f604;\n","permalink":"https://gagor.pro/2015/12/ansible-on-vagrant-skipping-no-hosts-matched/","summary":"I have some Ansible roles to configure my vps, Raspberry Pi, etc. I like to test them before I broke something on my real, not clustered machines - I use Vagrant for that.\nBut with it I had one problem - in playbooks I define hosts as groups of severs ex. web for my vps:\nExample Ansible playbook - hosts: web gather_facts: True sudo: True ... But testing machine wasn\u0026rsquo;t in this group and when I run vagrant I could only see:","title":"Ansible on Vagrant - skipping: no hosts matched"},{"content":"Normally you want dynamic content to be fresh and not catchable. But sometimes it may be useful to cache it, like when you have website behind reverse proxy. To do this try something like this:\n\u0026lt;filesmatch \u0026#34;\\.(php|cgi|pl)$\u0026#34;\u0026gt; Header unset Pragma Header unset Expires Header set Cache-Control \u0026#34;max-age=3600, public\u0026#34; \u0026lt;/filesmatch\u0026gt; Sources http://www.askapache.com/htaccess/speed-up-your-site-with-caching-and-cache-control.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2015/12/apache-force-caching-dynamic-php-content-with-mod_headers/","summary":"Normally you want dynamic content to be fresh and not catchable. But sometimes it may be useful to cache it, like when you have website behind reverse proxy. To do this try something like this:\n\u0026lt;filesmatch \u0026#34;\\.(php|cgi|pl)$\u0026#34;\u0026gt; Header unset Pragma Header unset Expires Header set Cache-Control \u0026#34;max-age=3600, public\u0026#34; \u0026lt;/filesmatch\u0026gt; Sources http://www.askapache.com/htaccess/speed-up-your-site-with-caching-and-cache-control.html\u0026thinsp; external link ","title":"Apache - Force caching dynamic PHP content with mod_headers"},{"content":"It will happen from time to time, that you\u0026rsquo;re on alien machine and have to brutally update things in db without knowing credentials. Example is for root (quite secure candidate to change because it shouldn\u0026rsquo;t be used in app \u0026#x1f603; ) but will work for any user.\nshutdown db service mysql stop create text file with command like this (update user accordingly) ex. in /tmp/pwchange.txt SET PASSWORD FOR \u0026#34;root\u0026#34;@\u0026#34;localhost\u0026#34; = PASSWORD(\u0026#34;HereYourNewPassword\u0026#34;); start mysqld with --init-file param mysqld_safe --init-file=/tmp/pwchange.txt sometimes you may require to point configuration file ex. --defaults-file=/etc/mysql/my.cnf\nwait until it loads and kill mysql (ex. Ctrl+C / kill / etc) start mysql service mysql start delete file with password rm -f /tmp/pwchange.txt You should be able to login with updated password.\nSources https://dev.mysql.com/doc/refman/5.5/en/resetting-permissions.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2015/12/mysql-reset-root-password/","summary":"It will happen from time to time, that you\u0026rsquo;re on alien machine and have to brutally update things in db without knowing credentials. Example is for root (quite secure candidate to change because it shouldn\u0026rsquo;t be used in app \u0026#x1f603; ) but will work for any user.\nshutdown db service mysql stop create text file with command like this (update user accordingly) ex. in /tmp/pwchange.txt SET PASSWORD FOR \u0026#34;root\u0026#34;@\u0026#34;localhost\u0026#34; = PASSWORD(\u0026#34;HereYourNewPassword\u0026#34;); start mysqld with --init-file param mysqld_safe --init-file=/tmp/pwchange.","title":"MySQL - reset root password"},{"content":"I hate movies recorded on phone in vertical position. This just short tip how I dealt with with it last time:\nfor m in *.mp4 do avconv -i $m -vf \u0026#34;transpose=1\u0026#34; -codec:a copy -codec:v libx264 -preset slow -crf 23 rotated-$m done Other examples:\nhttp://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg\u0026thinsp; external link http://superuser.com/questions/578321/how-to-flip-a-video-180\u0026thinsp; external link °-vertical-upside-down-with-ffmpeg\n","permalink":"https://gagor.pro/2015/12/rotate-movies/","summary":"I hate movies recorded on phone in vertical position. This just short tip how I dealt with with it last time:\nfor m in *.mp4 do avconv -i $m -vf \u0026#34;transpose=1\u0026#34; -codec:a copy -codec:v libx264 -preset slow -crf 23 rotated-$m done Other examples:\nhttp://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg\u0026thinsp; external link http://superuser.com/questions/578321/how-to-flip-a-video-180\u0026thinsp; external link °-vertical-upside-down-with-ffmpeg","title":"Rotate movies"},{"content":"I had some passwords saved in remmina but like it always happen, I wasn\u0026rsquo;t been able to remember them when needed. Trying to restore them I found that they\u0026rsquo;re encrypted in .remmina directory.\nThen I used this script to decrypt them 1:\nExtract script import base64 from Crypto.Cipher import DES3 secret = base64.decodestring(\u0026#34;\u0026lt;STRING FROM remmina.prefs\u0026gt;\u0026#34;) password = base64.decodestring(\u0026#34;\u0026lt;STRING FROM XXXXXXX.remmina\u0026gt;\u0026#34;) print DES3.new(secret[:24], DES3.MODE_CBC, secret[24:]).decrypt(password) http://askubuntu.com/questions/290824/how-to-extract-saved-password-from-remmina\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2015/12/extract-password-saved-in-remmina/","summary":"I had some passwords saved in remmina but like it always happen, I wasn\u0026rsquo;t been able to remember them when needed. Trying to restore them I found that they\u0026rsquo;re encrypted in .remmina directory.\nThen I used this script to decrypt them 1:\nExtract script import base64 from Crypto.Cipher import DES3 secret = base64.decodestring(\u0026#34;\u0026lt;STRING FROM remmina.prefs\u0026gt;\u0026#34;) password = base64.decodestring(\u0026#34;\u0026lt;STRING FROM XXXXXXX.remmina\u0026gt;\u0026#34;) print DES3.new(secret[:24], DES3.MODE_CBC, secret[24:]).decrypt(password) http://askubuntu.com/questions/290824/how-to-extract-saved-password-from-remmina\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Extract password saved in Remmina"},{"content":"After long break I\u0026rsquo;m thinking about writing more on my blog. I was reviewing my favorites/bookmarks and half of them was broken, so I can\u0026rsquo;t rely on them in case of knowledge management.\nI think I will write shorter, less descriptive articles just to be pointers to useful solutions from past.\n","permalink":"https://gagor.pro/2015/12/im-back/","summary":"After long break I\u0026rsquo;m thinking about writing more on my blog. I was reviewing my favorites/bookmarks and half of them was broken, so I can\u0026rsquo;t rely on them in case of knowledge management.\nI think I will write shorter, less descriptive articles just to be pointers to useful solutions from past.","title":"I’m back"},{"content":"Allow from IP without password prompt, and also allow from any address with password prompt\nOrder deny,allow Deny from all AuthName \u0026#34;htaccess password prompt\u0026#34; AuthUserFile /web/askapache.com/.htpasswd AuthType Basic Require valid-user Allow from 172.17.10.1 Satisfy Any Sources http://www.askapache.com/htaccess/apache-authentication-in-htaccess.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2015/12/apache-authbasic-but-excluding-ip/","summary":"Allow from IP without password prompt, and also allow from any address with password prompt\nOrder deny,allow Deny from all AuthName \u0026#34;htaccess password prompt\u0026#34; AuthUserFile /web/askapache.com/.htpasswd AuthType Basic Require valid-user Allow from 172.17.10.1 Satisfy Any Sources http://www.askapache.com/htaccess/apache-authentication-in-htaccess.html\u0026thinsp; external link ","title":"Apache AuthBasic but excluding IP"},{"content":" Splątana siećPrzewodnik po bezpieczeństwie nowoczesnych aplikacji WWW\nAuthor: Michał Zalewski\nhelion.pl ","permalink":"https://gagor.pro/books/2015/splatana-siec/","summary":" Splątana siećPrzewodnik po bezpieczeństwie nowoczesnych aplikacji WWW\nAuthor: Michał Zalewski\nhelion.pl ","title":"Splątana sieć"},{"content":" RubyProgramowanie\nAuthors: David Flanagan, Yukihiro Matsumoto\nhelion.pl ","permalink":"https://gagor.pro/books/2015/ruby-programowanie/","summary":" RubyProgramowanie\nAuthors: David Flanagan, Yukihiro Matsumoto\nhelion.pl ","title":"Ruby"},{"content":" GITRozproszony system kontroli wersji\nAuthor: Włodzimierz Gajda\nhelion.pl ","permalink":"https://gagor.pro/books/2015/git/","summary":" GITRozproszony system kontroli wersji\nAuthor: Włodzimierz Gajda\nhelion.pl ","title":"GIT"},{"content":"When configuring RAID it\u0026rsquo;s quite important to have the same partition tables on every disk. I\u0026rsquo;v done this many times on msdos partition tables like this:\nsfdisk -d /dev/sda | sfdisk /dev/sdb but it\u0026rsquo;s not working any more on GPT partition tables. Hopefully it still can be done but with different toolstack \u0026#x1f604;\nInstall gdisk:\napt-get install -y gdisk Then use sgdisk like this:\nsgdisk -R /dev/sd_dest /dev/sd_src sgdisk -G /dev/sd_dest First command will copy partition from /dev/sd_src to /dev/sd_dest. Second will randomize partition UUID\u0026rsquo;s - needed only if you want to use disks in same machine (this is my case).\n","permalink":"https://gagor.pro/2014/07/copy-gtp-partiotion-table-between-disks/","summary":"When configuring RAID it\u0026rsquo;s quite important to have the same partition tables on every disk. I\u0026rsquo;v done this many times on msdos partition tables like this:\nsfdisk -d /dev/sda | sfdisk /dev/sdb but it\u0026rsquo;s not working any more on GPT partition tables. Hopefully it still can be done but with different toolstack \u0026#x1f604;\nInstall gdisk:\napt-get install -y gdisk Then use sgdisk like this:\nsgdisk -R /dev/sd_dest /dev/sd_src sgdisk -G /dev/sd_dest First command will copy partition from /dev/sd_src to /dev/sd_dest.","title":"Copy GTP partiotion table between disks"},{"content":"There is need plugin for Django, named django-debug-toolbar but it needs some time to configure. So when I need simple way to debug SQL queries I use small hack. Add to your settings.py:\nLOGGING = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;handlers\u0026#39;: { \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, } }, \u0026#39;loggers\u0026#39;: { \u0026#39;django.db.backends\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;], \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, }, } } To get this working DEBUG option have to be set to True:\nDEBUG = True After this setup, when you run you app in development mode:\n./manage.py runserver you will see SQL queries in console output.\nSources https://docs.djangoproject.com/en/1.6/topics/logging/#examples\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/05/quickly-setup-sql-query-logging-on-console-in-django/","summary":"There is need plugin for Django, named django-debug-toolbar but it needs some time to configure. So when I need simple way to debug SQL queries I use small hack. Add to your settings.py:\nLOGGING = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;handlers\u0026#39;: { \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, } }, \u0026#39;loggers\u0026#39;: { \u0026#39;django.db.backends\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;], \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, }, } } To get this working DEBUG option have to be set to True:","title":"Quickly setup SQL query logging on console in Django"},{"content":"On Debian in default installation you have different configuration files for PHP in Apache, FPM, CLI, etc. But on CentOS you have only one php.ini for all of them. In case I have, I need to have different configuration file for scripts running in CLI mode (more memory, etc). I could run it like this:\nphp -c /etc/php-cli.ini script.php But this a little burdensome. So I do it like this:\ncat \u0026gt; /etc/profile.d/php-cli-ini.sh \u0026lt;\u0026lt;SCRIPT #!/bin/bash alias php=\u0026#34;php -c /etc/php-cli.ini\u0026#34; SCRIPT cp /etc/php.ini /etc/php-cli.ini Logout, login and now every user can run PHP scripts in CLI with different configuration - exactly what I need :)\n","permalink":"https://gagor.pro/2014/05/changing-default-php-ini-file-for-php-cli-on-centos/","summary":"On Debian in default installation you have different configuration files for PHP in Apache, FPM, CLI, etc. But on CentOS you have only one php.ini for all of them. In case I have, I need to have different configuration file for scripts running in CLI mode (more memory, etc). I could run it like this:\nphp -c /etc/php-cli.ini script.php But this a little burdensome. So I do it like this:","title":"Changing default php.ini file for PHP-CLI on CentOS"},{"content":"Everybody knows passwd command but it\u0026rsquo;s useless when you need to change ex. root password from command line without waiting for input. In such case oneliner below could help:\necho \u0026#34;root:new_password\u0026#34; | chpasswd ","permalink":"https://gagor.pro/2014/05/command-to-change-root-password/","summary":"Everybody knows passwd command but it\u0026rsquo;s useless when you need to change ex. root password from command line without waiting for input. In such case oneliner below could help:\necho \u0026#34;root:new_password\u0026#34; | chpasswd ","title":"Command to change root password"},{"content":"These are few steps to get Steam running on Ubuntu:\nwget -c media.steampowered.com/client/installer/steam.deb dpkg -i steam.deb apt-get install -f apt-get update Solutions for some issues Some time ago I needed 32 bit flash even on 64 bit system - I don\u0026rsquo;t need it currently but I\u0026rsquo;m living this as a tip.\napt-get install adobe-flashplugin:i386 After Ubuntu upgrade I was unable to run Steam anymore - It shouted on me with strange \u0026ldquo;networking problem\u0026rdquo;. I have to clean Steam configuration with:\nsteam --reset Sources http://linuxg.net/how-to-install-the-latest-steam-client-available-on-ubuntu-13-10-13-04-12-10-12-04-and-linux-mint-15-14-13/\u0026thinsp; external link http://askubuntu.com/questions/353522/why-is-steam-not-able-to-connect-steam-network\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/04/install-steam-on-debian-ubuntu/","summary":"These are few steps to get Steam running on Ubuntu:\nwget -c media.steampowered.com/client/installer/steam.deb dpkg -i steam.deb apt-get install -f apt-get update Solutions for some issues Some time ago I needed 32 bit flash even on 64 bit system - I don\u0026rsquo;t need it currently but I\u0026rsquo;m living this as a tip.\napt-get install adobe-flashplugin:i386 After Ubuntu upgrade I was unable to run Steam anymore - It shouted on me with strange \u0026ldquo;networking problem\u0026rdquo;.","title":"Install Steam on Debian/Ubuntu"},{"content":"When I was trying to update packages on one host I\u0026rsquo;ve stuck with yum hung on update. I run strace and see:\nstrace -p 43734 Process 43734 attached - interrupt to quit futex(0x807c938, FUTEX_WAIT, 1, NULL \u0026lt;unfinished ...\u0026gt; Process 43734 detached It looks like yum database was corrupted, to repair this run:\nrm -f /var/lib/rpm/__db* rpm --rebuilddb yum clean all yum update Instead rm on db-files you could use gzip to have backup of these files.\n","permalink":"https://gagor.pro/2014/04/rebuild-yum-rpm-database/","summary":"When I was trying to update packages on one host I\u0026rsquo;ve stuck with yum hung on update. I run strace and see:\nstrace -p 43734 Process 43734 attached - interrupt to quit futex(0x807c938, FUTEX_WAIT, 1, NULL \u0026lt;unfinished ...\u0026gt; Process 43734 detached It looks like yum database was corrupted, to repair this run:\nrm -f /var/lib/rpm/__db* rpm --rebuilddb yum clean all yum update Instead rm on db-files you could use gzip to have backup of these files.","title":"Rebuild yum/rpm database"},{"content":"I\u0026rsquo;ve few Nagios checks that require root privileges but running nrpe as root user is not acceptable. I prefer to use sudo for only these few commands.\nRun visudo and coment out this line:\n#Defaults requiretty This change is crucial to get scripts working.\nThen add at the end of file:\n%nrpe ALL=(ALL) NOPASSWD: /usr/lib64/nagios/plugins/ I\u0026rsquo;ve used nrpe group, but you have to add exactly group that your nrpe process uses.\nNow you should be able to run checks as root - edit /etc/nagios/nrpe.cfg and add check like this:\ncommand[check_as_root]=/usr/bin/sudo /usr/lib64/nagios/plugins/check_with_root_privileges ","permalink":"https://gagor.pro/2014/03/nagios-run-checks-as-root-with-nrpe/","summary":"I\u0026rsquo;ve few Nagios checks that require root privileges but running nrpe as root user is not acceptable. I prefer to use sudo for only these few commands.\nRun visudo and coment out this line:\n#Defaults requiretty This change is crucial to get scripts working.\nThen add at the end of file:\n%nrpe ALL=(ALL) NOPASSWD: /usr/lib64/nagios/plugins/ I\u0026rsquo;ve used nrpe group, but you have to add exactly group that your nrpe process uses.","title":"Nagios - run checks as root with NRPE"},{"content":"After reading some SEO stuff I wanted to add some meta tags to my WordPress blog. I found this site: codex.wordpress.org/Meta_Tags_in_WordPress\u0026thinsp; external link .\nSo WordPress thinks that it\u0026rsquo;s not necessary to have this meta tags any more\u0026hellip; But I want it! \u0026#x1f603; Next funny thing is how they suggest to add meta tags: copy header.php - what about theme updates?\nI prefer to use functions.php file - just create it in your courrent theme directory with such content:\n\u0026lt;?php // this will remove WordPress version from header remove_action(\u0026#39;wp_head\u0026#39;, \u0026#39;wp_generator\u0026#39;); // handler function for adding custom tags function custom_header_meta() { ?\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;Tomasz Gągor\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;contact\u0026#34; content=\u0026#34;my@mail.com\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;\u0026lt;?php wp_title( \u0026#39;|\u0026#39;, true, \u0026#39;right\u0026#39; ); ?\u0026gt;\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;some keywords\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;verify-v1\u0026#34; content=\u0026#34;google webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;bing webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;fb:admins\u0026#34; content=\u0026#34;facebook identificatio\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;copyright\u0026#34; content=\u0026#34;Copyright (c)1997-2014 Tomasz Gągor. All Rights Reserved.\u0026#34; /\u0026gt; \u0026lt;?php } // running handler function add_action(\u0026#39;wp_head\u0026#39;, \u0026#39;custom_header_meta\u0026#39;); ","permalink":"https://gagor.pro/2014/03/wordpress-add-meta-tags-author-description-keywords-etc/","summary":"After reading some SEO stuff I wanted to add some meta tags to my WordPress blog. I found this site: codex.wordpress.org/Meta_Tags_in_WordPress\u0026thinsp; external link .\nSo WordPress thinks that it\u0026rsquo;s not necessary to have this meta tags any more\u0026hellip; But I want it! \u0026#x1f603; Next funny thing is how they suggest to add meta tags: copy header.php - what about theme updates?\nI prefer to use functions.php file - just create it in your courrent theme directory with such content:","title":"WordPress - add meta tags: author, description, keywords, etc"},{"content":"Let say you have MediaWiki installation but you lost admin credentials. If you have other account or if you could create one without any rights we\u0026rsquo;re in home 😉\nWe have few options to do this.\nReset admin password We have to connect to database and use this SQL:\nUPDATE `user` SET user_password = CONCAT( SUBSTRING(user_password, 1, 3), SUBSTRING(MD5(user_name), 1, 8), \u0026#39;:\u0026#39;, MD5(CONCAT(SUBSTRING(MD5(user_name), 1, 8), \u0026#39;-\u0026#39;, MD5(\u0026#39;new password\u0026#39;)))) WHERE user_name = \u0026#39;Admin\u0026#39;; Just replace Admin with your username and new password with your password.\nRaise another user rights If we don\u0026rsquo;t want to mess with admin account we could raise permissions for other user, ex. Tom:\nINSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;bureaucrat\u0026#39;); INSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;sysop\u0026#39;); Sources http://www.mediawiki.org/wiki/Manual_talk:Resetting_passwords\u0026thinsp; external link http://www.mediawiki.org/wiki/Manual_talk:AdminSettings.php\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/03/mediawiki-recover-admin-rights/","summary":"Let say you have MediaWiki installation but you lost admin credentials. If you have other account or if you could create one without any rights we\u0026rsquo;re in home 😉\nWe have few options to do this.\nReset admin password We have to connect to database and use this SQL:\nUPDATE `user` SET user_password = CONCAT( SUBSTRING(user_password, 1, 3), SUBSTRING(MD5(user_name), 1, 8), \u0026#39;:\u0026#39;, MD5(CONCAT(SUBSTRING(MD5(user_name), 1, 8), \u0026#39;-\u0026#39;, MD5(\u0026#39;new password\u0026#39;)))) WHERE user_name = \u0026#39;Admin\u0026#39;; Just replace Admin with your username and new password with your password.","title":"Mediawiki - recover admin rights"},{"content":"I need to check memory usage of memcached server so I used:\necho stats | nc 127.0.0.1 11211 STAT pid 2743 STAT uptime 263 STAT time 1395438951 STAT version 1.4.13 STAT pointer_size 64 STAT rusage_user 0.482926 STAT rusage_system 2.675593 STAT curr_items 8667 STAT total_items 10742 STAT bytes 23802513 STAT curr_connections 296 STAT total_connections 399 STAT connection_structures 297 STAT cmd_flush 0 STAT cmd_get 52578 STAT cmd_set 10792 STAT get_hits 28692 STAT get_misses 23886 STAT evictions 0 STAT bytes_read 35984361 STAT bytes_written 192647437 STAT limit_maxbytes 536870912 STAT threads 2 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT replication MASTER STAT repcached_qi_free 8189 STAT repcached_wdata 0 STAT repcached_wsize 1026048 END For me, bytes value was important but you could find more about all statistics here\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2014/03/checking-memcached-status/","summary":"I need to check memory usage of memcached server so I used:\necho stats | nc 127.0.0.1 11211 STAT pid 2743 STAT uptime 263 STAT time 1395438951 STAT version 1.4.13 STAT pointer_size 64 STAT rusage_user 0.482926 STAT rusage_system 2.675593 STAT curr_items 8667 STAT total_items 10742 STAT bytes 23802513 STAT curr_connections 296 STAT total_connections 399 STAT connection_structures 297 STAT cmd_flush 0 STAT cmd_get 52578 STAT cmd_set 10792 STAT get_hits 28692 STAT get_misses 23886 STAT evictions 0 STAT bytes_read 35984361 STAT bytes_written 192647437 STAT limit_maxbytes 536870912 STAT threads 2 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT replication MASTER STAT repcached_qi_free 8189 STAT repcached_wdata 0 STAT repcached_wsize 1026048 END For me, bytes value was important but you could find more about all statistics here\u0026thinsp; external link .","title":"Checking memcached status"},{"content":"I have development server with postfix - I wanted to allow outbound traffic to one domain but cut off all the rest. I definitely do not want that test mail or any debug info goes to service users.\nI have to add something like that to /etc/postfix/transport:\nallowed.domain.com : * discard: Then run:\npostmap /etc/postfix/transport At end, add these to /etc/postfix/main.cf:\ntransport_maps = hash:/etc/postfix/transport Reload postfix:\npostfix reload Test if it works:\necho test | mail -s test whatever@whatever.com You should see in logs that message was dropped:\nMar 18 21:14:28 devmx1 postfix/cleanup[29968]: 1E77654391: message-id=20140318211428.1E77280521@domain.com\u0026gt; Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: from=\u0026lt;root@domain.com\u0026gt;, size=431, nrcpt=1 (queue active) Mar 18 21:14:28 devmx1 postfix/discard[29970]: 1E77654391: to=\u0026lt;whatever@whatever.com\u0026gt;, relay=none, delay=0.1, delays=0.09/0.01/0/0, dsn=2.0.0, status=sent (whatever.com) Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: removed ","permalink":"https://gagor.pro/2014/03/postfix-automatically-drop-outbound-mail/","summary":"I have development server with postfix - I wanted to allow outbound traffic to one domain but cut off all the rest. I definitely do not want that test mail or any debug info goes to service users.\nI have to add something like that to /etc/postfix/transport:\nallowed.domain.com : * discard: Then run:\npostmap /etc/postfix/transport At end, add these to /etc/postfix/main.cf:\ntransport_maps = hash:/etc/postfix/transport Reload postfix:\npostfix reload Test if it works:","title":"Postfix - automatically drop outbound mail"},{"content":"In recent Ansible update to 1.5 version there is really nice feature ssh pipelining. This option is serious alternative to accelerated mode.\nJust add to you config file (ex. ~/.ansible.cfg):\n[ssh_connection] pipelining=True Now run any playbook - you will see the difference \u0026#x1f604;\nSource (and extended info about):\nhttp://blog.ansibleworks.com/2014/01/15/ssh-connection-upgrades-coming-in-ansible-1-5/\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/03/ansible-ssh-pipelining/","summary":"In recent Ansible update to 1.5 version there is really nice feature ssh pipelining. This option is serious alternative to accelerated mode.\nJust add to you config file (ex. ~/.ansible.cfg):\n[ssh_connection] pipelining=True Now run any playbook - you will see the difference \u0026#x1f604;\nSource (and extended info about):\nhttp://blog.ansibleworks.com/2014/01/15/ssh-connection-upgrades-coming-in-ansible-1-5/\u0026thinsp; external link ","title":"Ansible - ssh pipelining"},{"content":"To najlepszy przepis na chrusty jaki znam - wychodzą bardzo kruche i delikatne.\nSkładniki 4 zółtka, 4 łyżki wina białego lub czerwonego, 4 łyżki mąki. Sposób przygotowania Wszystkie składniki wymieszać i wyrobić. Ciasto powinno być mniej wiecej takie jak na pierogi. Bić pałką/wałkiem, składać na pół i tak kilka razy przez ok 5 minut. Potem ciasto rozwałkować bardzo cieniutko i wykrawać chrusty, małe bo mocno rosną. Następnie wrzucać na rozgrzany olej/smalec.\n","permalink":"https://gagor.pro/2014/02/chrusty-faworki/","summary":"To najlepszy przepis na chrusty jaki znam - wychodzą bardzo kruche i delikatne.\nSkładniki 4 zółtka, 4 łyżki wina białego lub czerwonego, 4 łyżki mąki. Sposób przygotowania Wszystkie składniki wymieszać i wyrobić. Ciasto powinno być mniej wiecej takie jak na pierogi. Bić pałką/wałkiem, składać na pół i tak kilka razy przez ok 5 minut. Potem ciasto rozwałkować bardzo cieniutko i wykrawać chrusty, małe bo mocno rosną. Następnie wrzucać na rozgrzany olej/smalec.","title":"Chrusty, faworki"},{"content":"I had quite simple task - compare two lists of hosts and check if hosts from first one are also on the second one. I started with diff:\ndiff -u biglist.txt hosts_to_check.txt | grep -E \u0026#34;^\\+\u0026#34; It was fine but output needs some filtering to get what I want.\nI\u0026rsquo;ve found another example with grep:\ngrep -Fxv -f biglist.txt hosts_to_check.txt | sort -n This will search for all lines in hosts_to_check.txt which don\u0026rsquo;t match any line in biglist.txt. So after this I\u0026rsquo;ve got list of hosts that I have to check. That\u0026rsquo;s exactly what I need \u0026#x1f604;\n","permalink":"https://gagor.pro/2014/02/comparing-two-lists-in-bash/","summary":"I had quite simple task - compare two lists of hosts and check if hosts from first one are also on the second one. I started with diff:\ndiff -u biglist.txt hosts_to_check.txt | grep -E \u0026#34;^\\+\u0026#34; It was fine but output needs some filtering to get what I want.\nI\u0026rsquo;ve found another example with grep:\ngrep -Fxv -f biglist.txt hosts_to_check.txt | sort -n This will search for all lines in hosts_to_check.","title":"Comparing two lists in bash"},{"content":"After WSUS installing on Windows Server 2012 I discovered that it\u0026rsquo;s running on port 8530, different than on older version of Windows (it was using port 80 from beginning). But what\u0026rsquo;s more interesting it was running ONLY on IPv6 interface! Switching binding configuration in IIS doesn\u0026rsquo;t help.\nI could stand switching port - it\u0026rsquo;s nothing hard with GPO, but IPv6 only configuration was not acceptable.\nAfter googling for some time 12 I found one command that solved my problems by switching WSUS to older behavior and run it on port 80 (on default website).\nJust run on elevated command line:\nC:\\Program Files\\Update Services\\Tools\\WSUSutil usecustomwebsite false After half a minute WSUS was working like a charm \u0026#x1f603;\nhttp://social.technet.microsoft.com/Forums/windowsserver/en-US/88514e56-1179-4af7-9f5e-5339d3e750a5/how-to-change-wsus-2012-port-to-80?forum=winserverwsus\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://community.spiceworks.com/topic/160971-how-do-you-change-the-port-number-for-your-wsus\u0026thinsp; external link \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://gagor.pro/2014/01/change-default-wsus-port-from-8530-to-80-on-windows-server-2012/","summary":"After WSUS installing on Windows Server 2012 I discovered that it\u0026rsquo;s running on port 8530, different than on older version of Windows (it was using port 80 from beginning). But what\u0026rsquo;s more interesting it was running ONLY on IPv6 interface! Switching binding configuration in IIS doesn\u0026rsquo;t help.\nI could stand switching port - it\u0026rsquo;s nothing hard with GPO, but IPv6 only configuration was not acceptable.\nAfter googling for some time 12 I found one command that solved my problems by switching WSUS to older behavior and run it on port 80 (on default website).","title":"Change default WSUS port from 8530 to 80 on Windows Server 2012"},{"content":"After reading some good opinions about MariaDB I wanted to give it a try. Upgrade looks quite straight forward but I found some issues a little tricky.\nInstallation Add repo and key:\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main deb-src http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main SRC (find more repositories here\u0026thinsp; external link )\nNow install MariaDB:\nsudo apt-get update sudo apt-get install mariadb-server It could be better to install mariadb-server-5.5 and mariadb-client-5.5 package instead, because of this error\u0026thinsp; external link .\nMariaDB repo pinning Some time after installation I have problem with newer packages from Debian repositories that upgraded my MariaDB installation back to MySQL - it\u0026rsquo;s described here\u0026thinsp; external link , so I used pinning to resolve that.\ncat \u0026gt; /etc/apt/preferences.d/mariadb.pref \u0026lt;\u0026lt;PIN Package: * Pin: origin mirrors.supportex.net Pin-Priority: 1000 PIN Results Before migration to MariaDB, front page of my blog needs about 650 ms to generate. After switch, it was only about 550ms. So it\u0026rsquo;s about 15% - absolutely for free \u0026#x1f604;\n","permalink":"https://gagor.pro/2014/01/debian-upgrade-mysql-to-mariadb/","summary":"After reading some good opinions about MariaDB I wanted to give it a try. Upgrade looks quite straight forward but I found some issues a little tricky.\nInstallation Add repo and key:\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main deb-src http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main SRC (find more repositories here\u0026thinsp; external link )\nNow install MariaDB:\nsudo apt-get update sudo apt-get install mariadb-server It could be better to install mariadb-server-5.5 and mariadb-client-5.","title":"Debian - Upgrade MySQL to MariaDB"},{"content":"I was thinking about allowing access to my website using SPDY protocol for better performance and security (and for fun of course \u0026#x1f603; ). But SPDY have one disadvantage - you need SSL certificate signed by known authority that will verfiy in common browsers. So you can\u0026rsquo;t use self signed certificates because everyone will see a warning entering your site. Certs are quite expensive so I started searching for free one and to my surprise I found such!\nI found these two sites where you can generate freeware certificates for your website:\nhttps://www.startssl.com/\u0026thinsp; external link (I prefer this one because it better recognized) https://www.cacert.org\u0026thinsp; external link I wouldn\u0026rsquo;t trust these certification authorities enough to use it for: access my mail or other private data. But I\u0026rsquo;m fine with using it for my public websites (like my blog) to gain speed from SPDY.\nConfiguring cert Fetch the Root CA and Class 1 Intermediate Server CA certificates:\nwget http://www.startssl.com/certs/ca.pem wget http://www.startssl.com/certs/sub.class1.server.ca.pem Create a unified certificate from your certificate and the CA certificates:\ncat ssl.crt sub.class1.server.ca.pem ca.pem \u0026gt; /etc/nginx/conf/ssl-unified.crt Enable SPDY Configure your nginx server to use the new key and certificate (in the global settings or a server section):\nssl on; ssl_certificate /etc/nginx/conf/ssl-unified.crt; ssl_certificate_key /etc/nginx/conf/ssl.key; Then enable SPDY like that:\nserver { listen your_ip:80; listen your_id:443 default_server ssl spdy; # other stuff } Advertise SPDY protocol Now advertise SPDY with Alternate-Protocol header\u0026thinsp; external link - add this clause in main location:\nadd_header Alternate-Protocol \u0026#34;443:npn-spdy/2\u0026#34;; Have fun with SPDY on your site \u0026#x1f604;\n","permalink":"https://gagor.pro/2014/01/nginx-enabling-spdy-with-freeware-certificate/","summary":"I was thinking about allowing access to my website using SPDY protocol for better performance and security (and for fun of course \u0026#x1f603; ). But SPDY have one disadvantage - you need SSL certificate signed by known authority that will verfiy in common browsers. So you can\u0026rsquo;t use self signed certificates because everyone will see a warning entering your site. Certs are quite expensive so I started searching for free one and to my surprise I found such!","title":"Nginx - enabling SPDY with freeware certificate"},{"content":"I\u0026rsquo;ve been using different code editors for different purposes. Gedit was fine for small scripts but not for bigger projects. It lacks intelligent code completion (function/class names, etc.). I was searching for convenient editor for Python, Perl, Ruby with support for frameworks like Django, Rails, etc. I know Sublime Text - but it\u0026rsquo;s paid\u0026thinsp; external link . There is LimeText\u0026thinsp; external link - open source clone, but it\u0026rsquo;s not ready to be used on daily basics.\nI found Brackets\u0026thinsp; external link - open source editor designed by Adobe. I\u0026rsquo;m testing it right now.\nBrackets installation on Ubuntu sudo add-apt-repository ppa:webupd8team/brackets sudo apt-get update sudo apt-get install brackets Source http://www.webupd8.org/2013/11/install-brackets-in-ubuntu-via-ppa-open.html\u0026thinsp; external link I was using Brackets for some time and while it\u0026rsquo;s really nice editor - it\u0026rsquo;s mostly designed for webmasters (writing web apps). But I also need to write in Ruby, Python, Perl and many other - then Github announced Atom editor\u0026thinsp; external link . I switched to Atom - it\u0026rsquo;s similar to Brackets but covers my interests better 🙂\nAtom installation on Ubuntu Similarly to Brackets installation. Add ppa and install:\nsudo add-apt-repository ppa:webupd8team/atom sudo apt-get update sudo apt-get install -y atom Atom plugins I use Atom is great because of all plugins available - my favorite are:\nlanguage-docker (Dockerfile syntax highlighting) language-terraform (Terraform files syntax highlighting) language-python autocomplete-python (with Kite) linter-foodcritic linter-python-pep8 linter-rubocop minimap tabs-to-spaces Atom themes I use Default dark theme in Atom is really nice - but there are better themes. My favorite are:\nUI Theme: seti-ui Syntax theme: Monokai ","permalink":"https://gagor.pro/2014/01/searching-for-better-code-editor/","summary":"I\u0026rsquo;ve been using different code editors for different purposes. Gedit was fine for small scripts but not for bigger projects. It lacks intelligent code completion (function/class names, etc.). I was searching for convenient editor for Python, Perl, Ruby with support for frameworks like Django, Rails, etc. I know Sublime Text - but it\u0026rsquo;s paid\u0026thinsp; external link . There is LimeText\u0026thinsp; external link - open source clone, but it\u0026rsquo;s not ready to be used on daily basics.","title":"Searching for better code editor"},{"content":"After connecting few computers with Windows 8.1 to domain we found that these computers are not recognized or recognized as Windows 6.3 (which is true) on WSUS 3.0 running on Windows Server 2008. The bad thing was that they can\u0026rsquo;t properly report to WSUS and get updates from it.\nI found that there are two updates that have to be installed (but they\u0026rsquo;re not working without additional steps):\nhttp://support.microsoft.com/kb/2720211\u0026thinsp; external link http://support.microsoft.com/kb/2734608\u0026thinsp; external link After installation of second update there are two additional steps that have to be performed to get WSUS working:\nReindex the WSUS Database\u0026thinsp; external link Use the Server Cleanup Wizard\u0026thinsp; external link - this one is trivial, so I hope you get this right Reindex the WSUS Database To do this perform these steps:\nCopy sript from this site\u0026thinsp; external link to file named WsusDBMaintenance.sql Install sqlcmd from this site - search for file named like \u0026ldquo;SQLServer2005_SQLCMD\u0026rdquo; with proper architecture (x86/amd64/ia64) run: sqlcmd -S np:\\\\.\\pipe\\MSSQL$MICROSOFT##SSEE\\sql\\query -i C:\\path to script saved in first point\\WsusDBMaintenance.sql Use WSUS Server Cleanup Wizard Done. Your WSUS will not recognize 8.1 clients but will work with them and serve updates.\nSources http://social.technet.microsoft.com/Forums/en-US/559fe878-e2a2-4ec6-9d91-55ea1b67caef/manage-windows-81-windows-server-2012-r2-on-wsus-30?forum=winserverwsus\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/01/manage-windows-8-1-and-windows-server-2012-r2-in-wsus-3-0/","summary":"After connecting few computers with Windows 8.1 to domain we found that these computers are not recognized or recognized as Windows 6.3 (which is true) on WSUS 3.0 running on Windows Server 2008. The bad thing was that they can\u0026rsquo;t properly report to WSUS and get updates from it.\nI found that there are two updates that have to be installed (but they\u0026rsquo;re not working without additional steps):\nhttp://support.microsoft.com/kb/2720211\u0026thinsp; external link http://support.","title":"Manage Windows 8.1 and Windows Server 2012 R2 in WSUS 3.0"},{"content":"I love Shotwell\u0026thinsp; external link for it\u0026rsquo;s simplicity and easy export to Piwigo\u0026thinsp; external link . After Christmas I added new photos to my library but after that I made some modifications to them (red eye reduction, etc\u0026hellip;). Because Shotwell generate thumbnails only on import, all my modifications were not visible on preview.\nI\u0026rsquo;ve started searching how to regenerate thumbs and found this info\u0026thinsp; external link . There were two issues with this method:\nthis howto was for old version (with old paths) and only for 128px thumbs I definitely don\u0026rsquo;t want to regenerate thumbnails for 40k photos! After some tweaking this will do work for thumbnails from last month (enough for me):\nsqlite3 ~/.local/share/shotwell/data/photo.db \\ \u0026#34;select id||\u0026#39; \u0026#39;||filename from PhotoTable where date(timestamp,\u0026#39;unixepoch\u0026#39;,\u0026#39;localtime\u0026#39;) \u0026gt; date(\u0026#39;now\u0026#39;,\u0026#39;start of month\u0026#39;,\u0026#39;-1 month\u0026#39;) order by timestamp desc\u0026#34; | while read id filename; do tf1=$(printf ~/.local/share/shotwell/thumbs/thumbs128/thumb%016x.jpg $id); tf2=$(printf ~/.local/share/shotwell/thumbs/thumbs360/thumb%016x.jpg $id); test -e \u0026#34;$tf\u0026#34; || { echo -n \u0026#34;Generating thumb for $filename\u0026#34;; convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 128x128 $tf1 convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 360x360 $tf2 echo } done Remember to install imagemagick:\napt-get install imagemagick ","permalink":"https://gagor.pro/2014/01/regenerate-thumbnails-in-shotwell-for-last-month/","summary":"I love Shotwell\u0026thinsp; external link for it\u0026rsquo;s simplicity and easy export to Piwigo\u0026thinsp; external link . After Christmas I added new photos to my library but after that I made some modifications to them (red eye reduction, etc\u0026hellip;). Because Shotwell generate thumbnails only on import, all my modifications were not visible on preview.\nI\u0026rsquo;ve started searching how to regenerate thumbs and found this info\u0026thinsp; external link . There were two issues with this method:","title":"Regenerate thumbnails in Shotwell 0.15 (for last month)"},{"content":"Few days ago I\u0026rsquo;ve read a book ‘Even Faster Web Sites‘ about websites optimisation and I found one thing usefuluseful, not only on websites. There was a small tip about looploop unlooping. I want to quote them for later use.\nFirst - with switch statement var iterations = Math.ceil(values.length / 8); var startAt = values.length % 8; var i = 0; do { switch(startAt) { case 0: process(values[i++]); case 7: process(values[i++]); case 6: process(values[i++]); case 5: process(values[i++]); case 4: process(values[i++]); case 3: process(values[i++]); case 2: process(values[i++]); case 1: process(values[i++]); } startAt = 0; } while(--iterations \u0026gt; 0); Second - without switch var iterations = Math.floor(values.length / 8); var leftover = values.length % 8; var i = 0; if(leftover \u0026gt; 0) { do { process(values[i++]); } while(--leftover \u0026gt; 0); } do { process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); } while (--iterations \u0026gt; 0); I found second example more readable and I prefer it.\nThese examples after translation could be easily used in other scripting languages.\n","permalink":"https://gagor.pro/2014/01/loop-unlooping-in-javascript/","summary":"Few days ago I\u0026rsquo;ve read a book ‘Even Faster Web Sites‘ about websites optimisation and I found one thing usefuluseful, not only on websites. There was a small tip about looploop unlooping. I want to quote them for later use.\nFirst - with switch statement var iterations = Math.ceil(values.length / 8); var startAt = values.length % 8; var i = 0; do { switch(startAt) { case 0: process(values[i++]); case 7: process(values[i++]); case 6: process(values[i++]); case 5: process(values[i++]); case 4: process(values[i++]); case 3: process(values[i++]); case 2: process(values[i++]); case 1: process(values[i++]); } startAt = 0; } while(--iterations \u0026gt; 0); Second - without switch var iterations = Math.","title":"Loop unlooping in Javascript"},{"content":"Some time ago I write article about tracking nicknames of users (from comments) on a WordPress blog with Piwik . This time I\u0026rsquo;m doing same but for Google Analytics.\nI\u0026rsquo;m using Google Analytics\u0026thinsp; external link plugin for WordPress so I\u0026rsquo;ve edited googleanalytics.php file to add some additional code for user tracking:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _gaq = _gaq || []; _gaq.push([\u0026#39;_setAccount\u0026#39;, \u0026#39;UA-YOUR-UNIQ-NUMBER\u0026#39;]); _gaq.push([\u0026#39;_setCustomVar\u0026#39;, 1, \u0026#39;Nickname\u0026#39;, comment_author, 1]); _gaq.push([\u0026#39;_trackPageview\u0026#39;]); (function() { var ga = document.createElement(\u0026#39;script\u0026#39;); ga.type = \u0026#39;text/javascript\u0026#39;; ga.async = true; ga.src = (\u0026#39;https:\u0026#39; == document.location.protocol ? \u0026#39;https://ssl\u0026#39; : \u0026#39;http://www\u0026#39;) + \u0026#39;.google-analytics.com/ga.js\u0026#39;; var s = document.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; s.parentNode.insertBefore(ga, s); })(); \u0026lt;/script\u0026gt; Source https://developers.google.com/analytics/devguides/collection/gajs/gaTrackingCustomVariables?hl=pl\u0026thinsp; external link ","permalink":"https://gagor.pro/2014/01/tracking-users-by-nickname-on-wordpress-using-google-analytics/","summary":"Some time ago I write article about tracking nicknames of users (from comments) on a WordPress blog with Piwik . This time I\u0026rsquo;m doing same but for Google Analytics.\nI\u0026rsquo;m using Google Analytics\u0026thinsp; external link plugin for WordPress so I\u0026rsquo;ve edited googleanalytics.php file to add some additional code for user tracking:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.","title":"Tracking users by nickname on WordPress using Google Analytics"},{"content":"Some time ago I\u0026rsquo;ve show how to precompress js and css file with gzip to be available for Nginx\u0026rsquo;s mod_gzip. In default configuration Apache don\u0026rsquo;t have such module but similar functionality could be achieved with few custom rewirtes.\nBasically we will start with these rewrites to serve gzipped CSS/JS files if they exist and the client accepts gzip compression:\nRewriteEngine on RewriteCond %{HTTP:Accept-encoding} gzip RewriteCond %{REQUEST_FILENAME}\\.gz -s RewriteRule ^(.*)\\.(js|css)$ $1\\.$2\\.gz [QSA] Then we need to setup proper content types for such compressed files - I know how to do this in two ways:\npure rewrites with mod_header - witch should serve correct content type and prevent mod_deflate to gzip files that are already gzipped RewriteRule \\.css\\.gz$ - [T=text/css,E=no-gzip:1,E=manualgzip:1] RewriteRule \\.js\\.gz$ - [T=text/javascript,E=no-gzip:1,E=manualgzip:1] \u0026lt;ifmodule mod_headers.c\u0026gt; # setup this header only if rewrites above were used Header set Content-Encoding \u0026#34;gzip\u0026#34; env=manualgzip \u0026lt;/ifmodule\u0026gt; by using Files clause (we could add this globally in httpd.conf) \u0026lt;files *.css.gz\u0026gt; ForceType text/css Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt; \u0026lt;files *.js.gz\u0026gt; #ForceType text/javascript # lately this one is more popular ForceType application/javascript Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt; Both ways work fine. First one sets no-gzip variable to bypass second time compression. Second one rely on such option in my mod_deflate\u0026rsquo;s config:\nSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|bz2|sit|rar|gz)$ no-gzip dont-vary which won\u0026rsquo;t compress any gz file, and this is why I have to setup Content-Encoding to gzip manually.\nIn both cases you will end with javacript and CSS files served from earlier prepared precomressed versions, with proper content type without engaging mod_deflate regardless you use js/css or js.gz/css.gz extension. But I strongly suggest to use extensions without gz - you will be able to disable this mechanism without any change in website code.\nIf you don\u0026rsquo;t know how to prepare files just look here.\nP.S.\nI found another similar but BAD example - it\u0026rsquo;s using AddEncoding clause to add gzip content type to ALL gzip files - this will cause problems with other compressed files with gz extension ex. tar.gz. Don\u0026rsquo;t do this. My rules above are more selective.\nSources http://stackoverflow.com/questions/7947906/add-expiry-headers-using-apache-for-paths-which-dont-exist-in-the-filesystem\u0026thinsp; external link http://stackoverflow.com/questions/9076752/how-to-force-apache-to-use-manually-pre-compressed-gz-file-of-css-and-js-files\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/12/apache-precompressing-static-files-with-gzip/","summary":"Some time ago I\u0026rsquo;ve show how to precompress js and css file with gzip to be available for Nginx\u0026rsquo;s mod_gzip. In default configuration Apache don\u0026rsquo;t have such module but similar functionality could be achieved with few custom rewirtes.\nBasically we will start with these rewrites to serve gzipped CSS/JS files if they exist and the client accepts gzip compression:\nRewriteEngine on RewriteCond %{HTTP:Accept-encoding} gzip RewriteCond %{REQUEST_FILENAME}\\.gz -s RewriteRule ^(.*)\\.(js|css)$ $1\\.$2\\.gz [QSA] Then we need to setup proper content types for such compressed files - I know how to do this in two ways:","title":"Apache - precompressing static files with gzip"},{"content":"I\u0026rsquo;m happy owner of Galaxy Nexus 7 and lately I updated my tablet to Android 4.4 Kitkat. One of features I most expected was ability to block some permissions of some applications. Such setting was available in 4.4 version but was removed in latest 4.4.2 - Google didn\u0026rsquo;t explain it exactly why. I don\u0026rsquo;t like when for ex. game need: camera or GPS access - for what I asked?\nBut there is new app so called App Ops that unhides build-in interface allowing edit of application permissions. I strongly suggest to install it.\nRequirements You will need rooted device with Android 4.3 or 4.4 version. This instruction could brick your device - use it on your own responsibility.\nInstall Xposed Read instructions here: http://forum.xda-developers.com/showthread.php?t=1574401\u0026thinsp; external link (because installation of this package have in history some bricked devices).\ndownload the Xposed Installer APK\u0026thinsp; external link and install it launch the Xposed Installer and go to the \u0026ldquo;Framework\u0026rdquo; section, then click on \u0026ldquo;Install/Update\u0026rdquo; reboot your device Install App Ops Read instructions here: http://forum.xda-developers.com/showthread.php?t=2564865\u0026thinsp; external link download newest version from here\u0026thinsp; external link , for now it will be AppOpsXposed-1.5\u0026thinsp; external link install it search for new App ops option in Settings under PERSONAL section ","permalink":"https://gagor.pro/2013/12/android-xposed-appops-reclaim-control-over-installed-applications-permissions/","summary":"I\u0026rsquo;m happy owner of Galaxy Nexus 7 and lately I updated my tablet to Android 4.4 Kitkat. One of features I most expected was ability to block some permissions of some applications. Such setting was available in 4.4 version but was removed in latest 4.4.2 - Google didn\u0026rsquo;t explain it exactly why. I don\u0026rsquo;t like when for ex. game need: camera or GPS access - for what I asked?\nBut there is new app so called App Ops that unhides build-in interface allowing edit of application permissions.","title":"Android: Xposed + AppOps - reclaim control over installed applications permissions"},{"content":"After the last NSA scandal I\u0026rsquo;ve found some time to read some texts about PFS and ECDSA keys lately. I always used RSA keys but wanted to give a try to ECDSA so I wanted to give it a try (test performance, etc). Here is how I\u0026rsquo;ve done it.\nFirstly find your favorite curve. A short tip about bit length and complexity could be found here. From it you will now that using 256 bit ECDSA key should be enough for next 10-20 years.\n$ openssl ecparam -list_curves secp112r1 : SECG/WTLS curve over a 112 bit prime field secp112r2 : SECG curve over a 112 bit prime field secp128r1 : SECG curve over a 128 bit prime field secp128r2 : SECG curve over a 128 bit prime field secp160k1 : SECG curve over a 160 bit prime field secp160r1 : SECG curve over a 160 bit prime field secp160r2 : SECG/WTLS curve over a 160 bit prime field secp192k1 : SECG curve over a 192 bit prime field secp224k1 : SECG curve over a 224 bit prime field secp224r1 : NIST/SECG curve over a 224 bit prime field secp256k1 : SECG curve over a 256 bit prime field secp384r1 : NIST/SECG curve over a 384 bit prime field secp521r1 : NIST/SECG curve over a 521 bit prime field prime192v1: NIST/X9.62/SECG curve over a 192 bit prime field prime192v2: X9.62 curve over a 192 bit prime field prime192v3: X9.62 curve over a 192 bit prime field prime239v1: X9.62 curve over a 239 bit prime field prime239v2: X9.62 curve over a 239 bit prime field prime239v3: X9.62 curve over a 239 bit prime field prime256v1: X9.62/SECG curve over a 256 bit prime field sect113r1 : SECG curve over a 113 bit binary field sect113r2 : SECG curve over a 113 bit binary field sect131r1 : SECG/WTLS curve over a 131 bit binary field sect131r2 : SECG curve over a 131 bit binary field sect163k1 : NIST/SECG/WTLS curve over a 163 bit binary field sect163r1 : SECG curve over a 163 bit binary field sect163r2 : NIST/SECG curve over a 163 bit binary field sect193r1 : SECG curve over a 193 bit binary field sect193r2 : SECG curve over a 193 bit binary field sect233k1 : NIST/SECG/WTLS curve over a 233 bit binary field sect233r1 : NIST/SECG/WTLS curve over a 233 bit binary field sect239k1 : SECG curve over a 239 bit binary field sect283k1 : NIST/SECG curve over a 283 bit binary field sect283r1 : NIST/SECG curve over a 283 bit binary field sect409k1 : NIST/SECG curve over a 409 bit binary field sect409r1 : NIST/SECG curve over a 409 bit binary field sect571k1 : NIST/SECG curve over a 571 bit binary field sect571r1 : NIST/SECG curve over a 571 bit binary field c2pnb163v1: X9.62 curve over a 163 bit binary field c2pnb163v2: X9.62 curve over a 163 bit binary field c2pnb163v3: X9.62 curve over a 163 bit binary field c2pnb176v1: X9.62 curve over a 176 bit binary field c2tnb191v1: X9.62 curve over a 191 bit binary field c2tnb191v2: X9.62 curve over a 191 bit binary field c2tnb191v3: X9.62 curve over a 191 bit binary field c2pnb208w1: X9.62 curve over a 208 bit binary field c2tnb239v1: X9.62 curve over a 239 bit binary field c2tnb239v2: X9.62 curve over a 239 bit binary field c2tnb239v3: X9.62 curve over a 239 bit binary field c2pnb272w1: X9.62 curve over a 272 bit binary field c2pnb304w1: X9.62 curve over a 304 bit binary field c2tnb359v1: X9.62 curve over a 359 bit binary field c2pnb368w1: X9.62 curve over a 368 bit binary field c2tnb431r1: X9.62 curve over a 431 bit binary field wap-wsg-idm-ecid-wtls1: WTLS curve over a 113 bit binary field wap-wsg-idm-ecid-wtls3: NIST/SECG/WTLS curve over a 163 bit binary field wap-wsg-idm-ecid-wtls4: SECG curve over a 113 bit binary field wap-wsg-idm-ecid-wtls5: X9.62 curve over a 163 bit binary field wap-wsg-idm-ecid-wtls6: SECG/WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls7: SECG/WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls8: WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls9: WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls10: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls11: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls12: WTLS curvs over a 224 bit prime field Oakley-EC2N-3: IPSec/IKE/Oakley curve #3 over a 155 bit binary field. Not suitable for ECDSA. Questionable extension field! Oakley-EC2N-4: IPSec/IKE/Oakley curve #4 over a 185 bit binary field. Not suitable for ECDSA. Questionable extension field! Now generate new private key with chosen curve (prime256v1 looks fine, like: c2pnb272w1, sect283k1, sect283r1 or secp256k1, etc)\n$ openssl ecparam -out ec_key.pem -name prime256v1 -genkey And generate self-signed certificate that could be directly used:\n$ openssl req -new -key ec_key.pem -x509 -nodes -days 365 -out cert.pem You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [AU]:PL State or Province Name (full name) [Some-State]:example.pl Locality Name (eg, city) []:example.pl Organization Name (eg, company) [Internet Widgits Pty Ltd]:example.pl Organizational Unit Name (eg, section) []:example.pl Common Name (e.g. server FQDN or YOUR name) []:example.pl Email Address []:hostmaster@example.pl ","permalink":"https://gagor.pro/2013/12/generate-ecdsa-key-with-openssl/","summary":"After the last NSA scandal I\u0026rsquo;ve found some time to read some texts about PFS and ECDSA keys lately. I always used RSA keys but wanted to give a try to ECDSA so I wanted to give it a try (test performance, etc). Here is how I\u0026rsquo;ve done it.\nFirstly find your favorite curve. A short tip about bit length and complexity could be found here. From it you will now that using 256 bit ECDSA key should be enough for next 10-20 years.","title":"Generate ECDSA key with OpenSSL"},{"content":"Lately I tried to remove some streams from MKV file - I wanted: video, audio in my language and no subtitles. I achieved it with mkvtoolnix utils.\nFirstly I have to identify streams in file:\n$ mkvmerge -i input_file.mkv File \u0026#39;test.mkv\u0026#39;: container: Matroska Track ID 0: video (V_MPEG4/ISO/AVC) Track ID 1: audio (A_DTS) Track ID 2: audio (A_AC3) Track ID 3: audio (A_DTS) Track ID 4: audio (A_AC3) Track ID 5: subtitles (S_TEXT/UTF8) Track ID 6: subtitles (S_TEXT/UTF8) Chapters: 16 entries You could use more verbose tool mkvinfo for that purpose too.\nNow we know what to do next:\n$ mkvmerge -o out.mkv -d 0 --audio-tracks 2 --no-subtitles input_file.mkv mkvmerge v6.3.0 (\u0026#39;You can\u0026#39;t stop me!\u0026#39;) built on Jun 29 2013 11:48:33 \u0026#39;test.mkv\u0026#39;: Using the demultiplexer for the format \u0026#39;Matroska\u0026#39;. \u0026#39;test.mkv\u0026#39; track 0: Using the output module for the format \u0026#39;AVC/h.264\u0026#39;. \u0026#39;test.mkv\u0026#39; track 2: Using the output module for the format \u0026#39;AC3\u0026#39;. The file \u0026#39;out.mkv\u0026#39; has been opened for writing. Progress: 100% The cue entries (the index) are being written... Muxing took 3 minutes 10 seconds. This is the fastest way - no need for conversion of any stream.\nSource: http://bunin.livejournal.com/357913.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/12/delete-audio-track-from-mkv-file/","summary":"Lately I tried to remove some streams from MKV file - I wanted: video, audio in my language and no subtitles. I achieved it with mkvtoolnix utils.\nFirstly I have to identify streams in file:\n$ mkvmerge -i input_file.mkv File \u0026#39;test.mkv\u0026#39;: container: Matroska Track ID 0: video (V_MPEG4/ISO/AVC) Track ID 1: audio (A_DTS) Track ID 2: audio (A_AC3) Track ID 3: audio (A_DTS) Track ID 4: audio (A_AC3) Track ID 5: subtitles (S_TEXT/UTF8) Track ID 6: subtitles (S_TEXT/UTF8) Chapters: 16 entries You could use more verbose tool mkvinfo for that purpose too.","title":"Delete audio track from mkv file"},{"content":"Some time ago I prepared a PC that was responsible for batch encoding of movies to formats suitable for web players (such as. Video.js\u0026thinsp; external link , JW Player\u0026thinsp; external link , Flowplayer\u0026thinsp; external link , etc.)\nI used HandBrake for conversion to MP4 format (becase this soft was the fastest one) and ffmpeg (aka avconv in new version) for two pass encoding to WEBM.\nBelow are commands used by me for that conversion:\nMP4 HandBrakeCLI -e x264 -q 20.0 -a 1 -E faac -B 64 -6 mono -R 44.1 -D 0.0 -f mp4 --strict-anamorphic -m -x ref=1:weightp=1:subq=2:rc-lookahead=10:trellis=0:8x8dct=0 -O -i \u0026#34;input_file.avi\u0026#34; -o \u0026#34;output_file.mp4\u0026#34; WEBM avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -an -pass 1 -f webm /dev/null avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -codec:a libvorbis -b:a 96k -pass 2 -f webm \u0026#34;output_file.webm\u0026#34; Nginx configuration for MP4 I used configuration similar to that below for MP4 pseudostreaming and to protect direct urls to videos from linking on other sites (links will expire after sometime). There is also example usage of limit_rate clause that will slow down downloading of a file (it\u0026rsquo;s still two times bigger than video streaming speed so should be enough).\nlocation ~ \\.m(p4|4v)$ { ## This must match the URI part related to the MD5 hash and expiration time. secure_link $arg_ticket,$arg_e; ## The MD5 hash is built from our secret token, the URI($path in PHP) and our expiration time. secure_link_md5 somerandomtext$uri$arg_e; ## If the hash is incorrect then $secure_link is a null string. if ($secure_link = \u0026#34;\u0026#34;) { return 403; } ## The current local time is greater than the specified expiration time. if ($secure_link = \u0026#34;0\u0026#34;) { return 403; } ## If everything is ok $secure_link is 1. mp4; mp4_buffer_size 10m; mp4_max_buffer_size 1024m; limit_rate 1024k; limit_rate_after 5m; } Sources http://nginx.org/en/docs/http/ngx_http_mp4_module.html\u0026thinsp; external link http://wiki.nginx.org/HttpSecureLinkModule\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/12/preparing-video-files-for-streaming-on-website-in-mp4-and-webm-format/","summary":"Some time ago I prepared a PC that was responsible for batch encoding of movies to formats suitable for web players (such as. Video.js\u0026thinsp; external link , JW Player\u0026thinsp; external link , Flowplayer\u0026thinsp; external link , etc.)\nI used HandBrake for conversion to MP4 format (becase this soft was the fastest one) and ffmpeg (aka avconv in new version) for two pass encoding to WEBM.\nBelow are commands used by me for that conversion:","title":"Preparing video files for streaming on website in MP4 and WEBM format"},{"content":"SPDY is new protocol proposed by Google as an alternative for HTTP(S). Currently Chrome and Firefox browsers are using it as default if available on server. It is faster in most cases by few to several percent. The side effect of using mod_spdy\u0026thinsp; external link is that it\u0026rsquo;s working well only with thread safe Apache\u0026rsquo;s modules. PHP module for Apache is not thread safe so we need to use PHP as CGI or FastCGI service. CGI is slow - so running mod_spdy for performance gain with CGI is simply pointless. FastCGI is better but it\u0026rsquo;s not possible to share APC\u0026thinsp; external link cache in FastCGI mode (ex. using spawn-fcgi), so it\u0026rsquo;s poor too. Best for PHP is PHP-FPM\u0026thinsp; external link which is FastCGI service with dynamic process manager and could use full advantages of APC. In such configuration I could switch from apache prefork to worker which should use less resources and be more predictable.\nInstallation On Squeeze we need to install dot.deb repository - instructions are here: http://www.dotdeb.org/instructions/\nThen we could install:\napt-get install apache2-mpm-worker php5-fpm libapache2-mod-fastcgi Now, mod_spdy - packages are available here: https://developers.google.com/speed/spdy/mod_spdy/ Choose your architecture.\nwget https://dl-ssl.google.com/dl/linux/direct/mod-spdy-beta_current_i386.deb dpkg -i mod-spdy-beta_current_i386.deb Installation of this package will add automatically a new apt repository for mod_spdy.\nIf you have Apache\u0026rsquo;s module for PHP still installed you should remove it (you won\u0026rsquo;t need in anymore):\napt-get purge libapache2-mod-php5 Configuring PHP-FPM First I\u0026rsquo;m changing php-fpm default pool configuration file - edit /etc/php5/fpm/pool.d/www.conf\n; I want it to listen on socket, not on port listen = /var/run/php5-fpm/site1.socket ;uncomment to set proper permission for socket listen.owner = www-data listen.group = www-data listen.mode = 0660 ;uncomment and change to - PHP leaks, so kill child after 100 requests pm.max_requests = 100 ; for proper chroot handling we will need also php_admin_value[doc_root] = /var/www/site1 php_admin_value[cgi.fix_pathinfo] = 0 Now restart php-fpm:\nservice php5-fpm restart Connecting Apache with PHP-FPM In VirtualHost paste this:\n\u0026lt;IfModule mod_fastcgi.c\u0026gt; Alias /php5.fcgi /var/www/site1/php5.fcgi FastCGIExternalServer /var/www/site1/php5.fcgi -socket /var/lib/apache2/fastcgi/site1.socket AddType application/x-httpd-fastphp5 .php Action application/x-httpd-fastphp5 /php5.fcgi \u0026lt;Directory \u0026#34;/var/www/site1/\u0026#34;\u0026gt; Order deny,allow Deny from all \u0026lt;Files \u0026#34;php5.fcgi\u0026#34;\u0026gt; Order allow,deny Allow from all \u0026lt;/Files\u0026gt; \u0026lt;/Directory\u0026gt; \u0026lt;/IfModule\u0026gt; Enable needed modules and restart Apache:\na2enmod actions a2enmod fastcgi service apache2 restart SSL SPDY requires encrypted connection so you need configured SSL (virtualhost running on port 443). Typical configuration for SSL looks similar to this:\n\u0026lt;virtualhost *:443\u0026gt; # some random stuff - exactly like in you NON SSL configuration :smile: SSLEngine on SSLCertificateFile /etc/ssl/certs/example.com.crt SSLCertificateKeyFile /etc/ssl/private/example.com.priv.key SSLCACertificateFile /etc/ssl/private/ca.crt\u0026lt;/virtualhost\u0026gt; Testing Should work now \u0026#x1f603;\nSo, use Chromium, enter the site you just configured and then on second tab go to: chrome://net-internals/#spdy . You should see your site there if it\u0026rsquo;s running on SPDY.\nYou could also use plugins for Firefox\u0026thinsp; external link or Chromium\u0026thinsp; external link to test if site is running on SPDY.\nAdvertise SPDY on HTTP When you test if SPDY is working fine (and is faster in your configuration) you could advertise availability of SPDY protocol on your HTTP VirtualHost. Thanks to that when browser supports SPDY it will use it for faster access. To do this just add header in configuration:\nHeader set Alternate-Protocol \u0026#34;443:spdy/2\u0026#34; There are more options that could be used, if you need just check docs here\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2013/12/running-apache-with-mod_spdy-and-php-fpm/","summary":"SPDY is new protocol proposed by Google as an alternative for HTTP(S). Currently Chrome and Firefox browsers are using it as default if available on server. It is faster in most cases by few to several percent. The side effect of using mod_spdy\u0026thinsp; external link is that it\u0026rsquo;s working well only with thread safe Apache\u0026rsquo;s modules. PHP module for Apache is not thread safe so we need to use PHP as CGI or FastCGI service.","title":"Running Apache with mod_spdy and PHP-FPM"},{"content":"Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as failed in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I\u0026rsquo;ve got\u0026hellip;\nAfter rebuild I\u0026rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.\nWhat I do:\nI\u0026rsquo;ve added checked disk to array as spare mdadm /dev/md0 -a /dev/sdb1 I\u0026rsquo;ve set this \u0026ldquo;slow\u0026rdquo; drive as failed (and rebuild started) mdadm /dev/md0 -f /dev/sdf1 I removed this drive echo 1 \u0026gt; /sys/block/sdf/device/delete I run dmesg to see what was scsi host of this drive dmesg [ 1302.433419] sd 8:0:0:0: [sdf] Synchronizing SCSI cache [ 1302.433468] sd 8:0:0:0: [sdf] Stopping disk Now I could readd this drive echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host8/scan I run dmesg again to see if disk was detected dmesg [ 1489.013270] scsi 8:0:0:0: Direct-Access ATA SEAGATE ST2000DM001 1AQ1 PQ: 0 ANSI: 5 [ 1489.013375] sd 8:0:0:0: [sdf] 3907029168 512-byte logical blocks: (2.00 TB/1.81 TiB) [ 1489.013397] sd 8:0:0:0: Attached scsi generic sg6 type 0 [ 1489.013445] sd 8:0:0:0: [sdf] Write Protect is off [ 1489.013447] sd 8:0:0:0: [sdf] Mode Sense: 00 3a 00 00 [ 1489.013466] sd 8:0:0:0: [sdf] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [ 1498.318159] sdf: sdf1 [ 1498.318391] sd 8:0:0:0: [sdf] Attached SCSI disk Now I could re-add this drive to the array as spare mdadm /dev/md0 -a /dev/sdf1 ","permalink":"https://gagor.pro/2013/12/re-adding-failed-drive-in-mdadm/","summary":"Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as failed in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I\u0026rsquo;ve got\u0026hellip;\nAfter rebuild I\u0026rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.\nWhat I do:","title":"Re-adding failed drive in mdadm"},{"content":"I was configuring GlusterFS on few servers using Ansible\u0026thinsp; external link and have a need to update /etc/hosts with hostnames for easier configuration. I found this one working:\n- name: Update /etc/hosts lineinfile: dest=/etc/hosts regexp=\u0026#39;.*{{item}}$\u0026#39; line=\u0026#39;{{hostvars.{{item}}.ansible_default_ipv4.address}} {{item}}\u0026#39; state=present with_items: \u0026#39;{{groups.somegroup}}\u0026#39; Source: http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/12/ansible-dynamicaly-update-etc-hosts-files-on-target-servers/","summary":"I was configuring GlusterFS on few servers using Ansible\u0026thinsp; external link and have a need to update /etc/hosts with hostnames for easier configuration. I found this one working:\n- name: Update /etc/hosts lineinfile: dest=/etc/hosts regexp=\u0026#39;.*{{item}}$\u0026#39; line=\u0026#39;{{hostvars.{{item}}.ansible_default_ipv4.address}} {{item}}\u0026#39; state=present with_items: \u0026#39;{{groups.somegroup}}\u0026#39; Source: http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html\u0026thinsp; external link ","title":"Ansible - Dynamicaly update /etc/hosts files on target servers"},{"content":"I\u0026rsquo;ve started testing new Ghost blogging platform for a while on a virtual machine before I take decision about switching to it (or maybe won\u0026rsquo;t)\u0026hellip; After few days, I wanted to go forward with more testing and stuck on \u0026ldquo;e-mail and password\u0026rdquo; login prompt \u0026#x1f603;\nI\u0026rsquo;ve started looking into files and found ghost_dir/content/data/ghost-dev.db SQLite database. It can be opened like that:\nsqlite3 content/data/ghost-dev.db Then you could see whats your mail (and other info):\nsqlite\u0026gt; select * from users Now password - after searching a little I found: ghost_dir/core/server/models/user.js file. There is a tip in it:\n// Hash the provided password with bcrypt return nodefn.call(bcrypt.hash, _user.password, null, null); So I used this site: http://bcrypthashgenerator.apphb.com/\u0026thinsp; external link to generate bcrypt hash and updated it in DB:\nsqlite\u0026gt; update users set password=\u0026#34;$2a$10$f29LDrB8S1JMfdF40Vmf1.h2OyhtlcefaMrFQVpHeX9XQ7Xiq17KC\u0026#34; where id = 1; sqlite\u0026gt; .quit Additionally as suggested by henshao: if the account has been locked, you can set status to active to unlock the account, like that:\nsqlite\u0026gt; update users set status = \u0026#34;active\u0026#34;; Now try to log with updated credentials.\nP.S. I don\u0026rsquo;t trust online hash generators, so I strongly suggest to change password after successful login.\nEnjoyed? ","permalink":"https://gagor.pro/2013/11/reset-user-password-in-your-own-ghost-blog/","summary":"I\u0026rsquo;ve started testing new Ghost blogging platform for a while on a virtual machine before I take decision about switching to it (or maybe won\u0026rsquo;t)\u0026hellip; After few days, I wanted to go forward with more testing and stuck on \u0026ldquo;e-mail and password\u0026rdquo; login prompt \u0026#x1f603;\nI\u0026rsquo;ve started looking into files and found ghost_dir/content/data/ghost-dev.db SQLite database. It can be opened like that:\nsqlite3 content/data/ghost-dev.db Then you could see whats your mail (and other info):","title":"Reset user password in your own Ghost Blog"},{"content":"It\u0026rsquo;s quite rare to have problems with XFS and inodes exhaustion. Mostly because XFS doesn\u0026rsquo;t have inode limit in a manner known from other filesystems - it\u0026rsquo;s using some percentage of whole filesystem as a limit and in most distributions it\u0026rsquo;s 25%. So it\u0026rsquo;s really huge amount of inodes. But some tools and distributions lowered limit ex. 5% or 10% and there you could have problems more often.\nYou could check what is you limit by issuing xfs_info with drive and searching for imaxpct value:\nxfs_info root@zombi:~# xfs_info /srv/backup/ metadane=/dev/mapper/slow-backup isize=256 agcount=17, agsize=2621440 blks = sectsz=512 attr=2 data = bsize=4096 blocks=44564480, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal bsize=4096 blocks=20480, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime=brak extsz=4096 blocks=0, rtextents=0 In this case I have 25% and it could be changed dynamically with xfs_growfs -m XX where XX is new percentage of volume capacity.\nIt\u0026rsquo;s also possible to change imaxpct on creation time by adding option -i maxpct=XX.\nEnjoyed? ","permalink":"https://gagor.pro/2013/11/inodes-on-xfs/","summary":"It\u0026rsquo;s quite rare to have problems with XFS and inodes exhaustion. Mostly because XFS doesn\u0026rsquo;t have inode limit in a manner known from other filesystems - it\u0026rsquo;s using some percentage of whole filesystem as a limit and in most distributions it\u0026rsquo;s 25%. So it\u0026rsquo;s really huge amount of inodes. But some tools and distributions lowered limit ex. 5% or 10% and there you could have problems more often.\nYou could check what is you limit by issuing xfs_info with drive and searching for imaxpct value:","title":"Inodes exhaustion on XFS"},{"content":"I\u0026rsquo;ve bought a NAS and customized it a little. But there was one thing which make my nights sleepless. NAS was seeking disks every 5~10 seconds - these was really irritating - especially when it was silent in room. I found that part of firmware was indexing or logging something so I wanted it dead! kill -9 was unsuccessful - process restarted after a while\u0026hellip;. wrrr\u0026hellip;\nI googled a little and found another signal I could use SIGSTOP, which will freeze process until I send SIGCONT to it - that was exactly what I need (because I normally use NFS/Samba and don\u0026rsquo;t need nothing more running on this device).\nkill -SIGSTOP `pgrep svcd` Because with this process paused Web GUI is not working, I need from time to time run it again:\nkill -SIGCONT `pgrep svcd` Sources http://major.io/2009/06/15/two-great-signals-sigstop-and-sigcont/\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/11/kill-with-sigstop-and-sigcont/","summary":"I\u0026rsquo;ve bought a NAS and customized it a little. But there was one thing which make my nights sleepless. NAS was seeking disks every 5~10 seconds - these was really irritating - especially when it was silent in room. I found that part of firmware was indexing or logging something so I wanted it dead! kill -9 was unsuccessful - process restarted after a while\u0026hellip;. wrrr\u0026hellip;\nI googled a little and found another signal I could use SIGSTOP, which will freeze process until I send SIGCONT to it - that was exactly what I need (because I normally use NFS/Samba and don\u0026rsquo;t need nothing more running on this device).","title":"Kill with SIGSTOP and SIGCONT"},{"content":"I\u0026rsquo;ve just bought new toy - Iomega StorCenter ix2-200 Cloud Edition. I have to play with few options before I could start using it. First thing - Firmware upgrade.\nFirmware upgrade I\u0026rsquo;ve started searching for latest firmware for ix2-200 Cloud and found that I have to register on Lenovo site to get firmware\u0026hellip; I don\u0026rsquo;t like such sites where they force me to give all private data, but after few clicks on \u0026ldquo;Recommended articles\u0026rdquo; on that site I landed here:\nhttps://lenovo-eu-en.custhelp.com/app/answers/detail/a_id/26790\u0026thinsp; external link So it looks that I don\u0026rsquo;t need to register - point for me.\nSSH access I like to be root on my devices so I want SSH access with root privileges - nothing easier. Login to admin panel and then go to URL:\nhttp://[your-nas-ip]/diagnostics.html On this site you have to enable \u0026ldquo;remote access for technical support\u0026rdquo;, then you could login to your device via SSH using credentials:\nlogin: root password: soho+your_admin_password That\u0026rsquo;s all I need to start \u0026#x1f603;\nOther customization This device is running Debian Lenny for ARM but changed a little by producer. This HOWTO shows few tricks about installing custom software and changing default behavior. Based on: http://techmonks.net/installing-transmission-and-dnsmasq-on-a-nas/\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/11/my-new-toy-iomega-storcenter-ix2-200-cloud-edition/","summary":"I\u0026rsquo;ve just bought new toy - Iomega StorCenter ix2-200 Cloud Edition. I have to play with few options before I could start using it. First thing - Firmware upgrade.\nFirmware upgrade I\u0026rsquo;ve started searching for latest firmware for ix2-200 Cloud and found that I have to register on Lenovo site to get firmware\u0026hellip; I don\u0026rsquo;t like such sites where they force me to give all private data, but after few clicks on \u0026ldquo;Recommended articles\u0026rdquo; on that site I landed here:","title":"My new toy - Iomega StorCenter ix2-200 Cloud Edition"},{"content":"After some configuration changes I\u0026rsquo;ve stuck with VBP not listening nor on HTTP, nor on SSH port. Last resort was to use CLI to reenable HTTP access. Connect with parameters:\nBaud rate: 9600 Parity: none Bits: 8 Stopbits: 1 Flow control: none Then in login prompt you have to use login credentials (yes - they\u0026rsquo;re the same on every box (WTF?)):\nUser: - root Pass: - @#$%^\u0026amp;*!() Password is shift + 2345678190 - there is 1 before 9!\nAfter logging on, there are three commands that should reenable HTTP access:\n/etc/conf/bin/ep_mfg cfg_commit /etc/conf/bin/config_network Works for me!\nSources http://community.polycom.com/t5/Management-Security-and-Rich/VBP-ST-Factory-Reset-Problems/td-p/8974\u0026thinsp; external link http://community.polycom.com/t5/Management-Security-and-Rich/I-can-t-access-to-a-Polycom-VBP-5300-ST-by-HTTP/td-p/18698\u0026thinsp; external link http://blogs.scansource.com/polycom-vbp-e-series-reset-procedures/\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/11/reenable-web-interface-on-polycom-vbp-5300-st-from-cli/","summary":"After some configuration changes I\u0026rsquo;ve stuck with VBP not listening nor on HTTP, nor on SSH port. Last resort was to use CLI to reenable HTTP access. Connect with parameters:\nBaud rate: 9600 Parity: none Bits: 8 Stopbits: 1 Flow control: none Then in login prompt you have to use login credentials (yes - they\u0026rsquo;re the same on every box (WTF?)):\nUser: - root Pass: - @#$%^\u0026amp;*!() Password is shift + 2345678190 - there is 1 before 9!","title":"Reenable web interface on Polycom VBP 5300 ST from CLI"},{"content":"I\u0026rsquo;ve crated this blog to get feedback from other IT guys about what I\u0026rsquo;m doing wrong (or not good enough). But this idea failed\u0026hellip;\nI have only few comments on my blog (and about thousand and a half spams per month) - so, no feedback in my national language at all.\nSwitching to English should make my audience bigger and I hope to have more attention thanks to that. This will be also a good practice of my English skill. I hope to find enough free time to translate some of older articles, but for now I\u0026rsquo;m thinking rather about changing engine of blog - to something more convenient. Maybe Octopress\u0026thinsp; external link or Ghost\u0026thinsp; external link .\nI hope you enjoy a little more.\n","permalink":"https://gagor.pro/2013/11/changing-language-of-articles-on-my-blog-to-english/","summary":"I\u0026rsquo;ve crated this blog to get feedback from other IT guys about what I\u0026rsquo;m doing wrong (or not good enough). But this idea failed\u0026hellip;\nI have only few comments on my blog (and about thousand and a half spams per month) - so, no feedback in my national language at all.\nSwitching to English should make my audience bigger and I hope to have more attention thanks to that. This will be also a good practice of my English skill.","title":"Changing language of articles on my blog to English"},{"content":"Niedawno zainteresowałem się usługą Gearman i jedynej rzeczy której mi brakowało to jakiegoś łatwego mechanizmu zarządzającego workerami. Ale jak zwykle okazało się że inni mieli już ten problem i odpowiednie narzędzie istnieje - mowa o GearmanManagerze.\nInstalacja GearmanManagera Aby zainstalować GeramanManagera na serwerze gdzie już mamy Gearmana trzeba wykonać kilka kroków (wcześniej powinniśmy też zainstalować moduł gearmana do php\u0026rsquo;a):\napt-get install git -y git clone https://github.com/brianlmoon/GearmanManager.git cd GearmanManager/install chmod +x install.sh ./install.sh Detecting linux distro as redhat- or debian-compatible Where is your php executable? (usually /usr/bin) /usr/bin Which PHP library to use, pecl/gearman or PEAR::Net_Gearman? 1) pecl 2) pear #? 1 Installing to /usr/local/share/gearman-manager Installing executable to /usr/local/bin/gearman-manager Installing configs to /etc/gearman-manager Installing init script to /etc/init.d/gearman-manager Install ok! Run /etc/init.d/gearman-manager to start and stop Worker scripts can be installed in /etc/gearman-manager/workers, configuration can be edited in /etc/gearman-manager/config.ini Mamy działającego GearmanManagera. Zalecam przyglądnięcie się plikowi config-advanced.ini bo jest tam kilka opcji, które warto dodatkowo ustawić.\nSprawdzenie działania Geramana i GearmanManagera Przykład workera można znaleźć tutaj http://brian.moonspot.net/GearmanManager\u0026thinsp; external link . Po pobraniu go i zapisaniu w pliku /etc/gearman-manager/workers/fetch_url.php możemy ręcznie zakolejkować zadanie dla Geramana:\ngearman -f fetch_url -- http://google.pl/robots.txt Źródło:\nhttps://github.com/brianlmoon/GearmanManager\u0026thinsp; external link http://brian.moonspot.net/GearmanManager\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/11/gearmanmanager-wygodne-zarzadzanie-workerami/","summary":"Niedawno zainteresowałem się usługą Gearman i jedynej rzeczy której mi brakowało to jakiegoś łatwego mechanizmu zarządzającego workerami. Ale jak zwykle okazało się że inni mieli już ten problem i odpowiednie narzędzie istnieje - mowa o GearmanManagerze.\nInstalacja GearmanManagera Aby zainstalować GeramanManagera na serwerze gdzie już mamy Gearmana trzeba wykonać kilka kroków (wcześniej powinniśmy też zainstalować moduł gearmana do php\u0026rsquo;a):\napt-get install git -y git clone https://github.com/brianlmoon/GearmanManager.git cd GearmanManager/install chmod +x install.","title":"GearmanManager: wygodne zarządzanie workerami"},{"content":"W teorii nie powinno się blokować aktualizacji pakietów bo łatają dziury itd\u0026hellip;. Ale! Zdarzyły mi się ostatnio dwie sytuacje, które do tego mnie zmusiły:\naktualizacja hudsona kończyła się błędem przy starcie usługi, aktualizacja domU Xen skończyła się problemem z kompatybilnością mechanizmu udev w systemie i jądrze (hypervisor miał starsze jądro niż spodziewało się DomU). W takich sytuacjach bardzo przydaje się możliwość zablokowania aktualizacji jednej \u0026ldquo;psującej\u0026rdquo; paczki na pewien okres czasu by nie opóźniać innych aktualizacji a sobie dać czas na rozpracowanie problemu.\nWstrzymywanie aktualizacji pakietu Aktualizację wstrzymujemy o tak:\necho \u0026#34;paczka hold\u0026#34; | dpkg --set-selections Odblokowanie aktualizacji pakietu By ponownie zezwolić na aktualizację wystarczy:\necho \u0026#34;paczka install\u0026#34; | dpkg --set-selections Sprawdzenie listy wstrzymanych paczek dpkg --get-selections | grep hold Źródło:\nhttp://www.debianadmin.com/how-to-prevent-a-package-from-being-updated-in-debian.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/11/debian-zablokowanie-aktualizacji-pakietu/","summary":"W teorii nie powinno się blokować aktualizacji pakietów bo łatają dziury itd\u0026hellip;. Ale! Zdarzyły mi się ostatnio dwie sytuacje, które do tego mnie zmusiły:\naktualizacja hudsona kończyła się błędem przy starcie usługi, aktualizacja domU Xen skończyła się problemem z kompatybilnością mechanizmu udev w systemie i jądrze (hypervisor miał starsze jądro niż spodziewało się DomU). W takich sytuacjach bardzo przydaje się możliwość zablokowania aktualizacji jednej \u0026ldquo;psującej\u0026rdquo; paczki na pewien okres czasu by nie opóźniać innych aktualizacji a sobie dać czas na rozpracowanie problemu.","title":"Debian - zablokowanie aktualizacji pakietu"},{"content":"Ostatnio trafiłem na ciekawą usługę, która pozwala oddelegować długo trwające zadania z usługi webowej. Mowa o Gearman\u0026rsquo;ie. Usługa jest o tyle ciekawa że nie narzuca ani języka dla klienta (większość popularnych ma gotowe biblioteki), ani język dla skryptów w tej usłudze nie jest narzucany. Można tę usługę wykorzystać jako most pomiędzy PHP a np. Javą/Pythonem lub do zlecenia zadań z serwera na Linux\u0026rsquo;ie do wykonania na serwerze Windowsowym (bo np. narzędzia dostępne są tylko dla Windowsa). O innych zaletach można poczytać na stronce więc nie będę przynudzać.\nStandardowo zainstalowałem paczkę z repo Debiania i rozbiłem się przy kompilacji modułu z PECL\u0026rsquo;a - w repo była jakaś prehistoryczna wersja. Postanowiłem uruchomić aktualną wersje 1.0.6 z gałęzi testowej przekompilowując ją na Wheezym (by uniknąć zależności z wersji testowej).\nInstalacja gearman\u0026rsquo;a Dorzucamy źródła z testing - dzięki temu nie aktualizujemy systemu ale będziemy mogli pobrać świeże paczki źródłowe:\necho \u0026#34;deb-src http://ftp.pl.debian.org/debian jessie main non-free contrib\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list apt-get update Przygotowujemy katalog roboczy, pobieramy paczki i kompilujemy:\nmkdir gearman cd gearman apt-get build-dep gearman-job-server -y apt-get install bzr apt-get source gearman-job-server cd gearmand-1.0.6 ./debian/rules clean ./debian/rules binary cd .. dpkg -i gearman-job-server_1.0.6-2_i386.deb gearman-tools_1.0.6-2_i386.deb libgearman-dev_1.0.6-2_i386.deb libgearman7_1.0.6-2_i386.deb apt-get install -f -y Musiałem ręcznie doinstalować bazar (paczka bzr), bo w czasie kompilacji pojawiały się błędy z tym poleceniem - nie jestem pewien na ile jest potrzebne ale oczywiście możemy to posprzątać po skompilowaniu paczek.\nP.S. Jestem przekonany że zamiast \u0026ldquo;rules clean/binary\u0026rdquo; jest jakieś polecenie, którego powinno się użyć ale nie mogłem sobie go przypomnieć\u0026hellip;\nInstalacja modułu dla PHP'a Ponieważ wcześniej zainstalowaliśmy aktualne biblioteki libgearman-dev to instalacja modułu dla PHP powinna być bardzo prosta:\npecl install gearman echo \u0026#34;extension=gearman.so\u0026#34; \u0026gt; /etc/php5/conf.d/gearman.ini P.S. W paczkach PHP 5.3 z dotdeb\u0026rsquo;a można znaleźć już skompilowany moduł dla gearman\u0026rsquo;a.\nNa razie tyle - muszę teraz poszukać jak w wygodny, zautomatyzowany sposób zarządzać skryptami zleconymi do gearman\u0026rsquo;a.\n","permalink":"https://gagor.pro/2013/10/instalacja-gearman-job-server-1-0-6-na-debianie-wheezy/","summary":"Ostatnio trafiłem na ciekawą usługę, która pozwala oddelegować długo trwające zadania z usługi webowej. Mowa o Gearman\u0026rsquo;ie. Usługa jest o tyle ciekawa że nie narzuca ani języka dla klienta (większość popularnych ma gotowe biblioteki), ani język dla skryptów w tej usłudze nie jest narzucany. Można tę usługę wykorzystać jako most pomiędzy PHP a np. Javą/Pythonem lub do zlecenia zadań z serwera na Linux\u0026rsquo;ie do wykonania na serwerze Windowsowym (bo np. narzędzia dostępne są tylko dla Windowsa).","title":"Instalacja gearman-job-server 1.0.6 na Debianie Wheezy"},{"content":"Kolejna zabawna sytuacja - pewna aplikacja dotNET\u0026rsquo;owa działała dziwnie na 64-bitowym systemie, a tymczasem na 32-bitowej maszynie ta sama aplikacja działała bez problemów. Jedyna różnica to inne wersje klientów ODBC na tych systemach, które po kilku testach okazały się być przyczyną całego zła.\nPojawił się pomysł by odpalić aplikacje na 64 bitowym systemie ale w trybie 32 bit - poniżej krótkie HOWTO jak to osiągnąć:\npotrzebujemy narzędzia corflags.exe które pozwoli oznaczyć nam binarkę jako 32-bitową, do pobrania tutaj\u0026thinsp; external link a instrukcja jej użycia tutaj\u0026thinsp; external link . Instalujemy Windows SDK i zaznaczamy wyłącznie .NET Development Tools w kategorii Developer Tools / Windows Development Tools Odpalamy CMD i w nim CorFlags z lokalizacji: C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin (przynajmniej u mnie): cd C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\ CorFlags.exe c:\\sciezka\\do\\pliku.exe /32BIT+ Tyle - aplikacja uruchomiła się bez problemu jako 32 bitowa i korzystała z 32 bitowego ODBC.\nŹródła http://stackoverflow.com/questions/10945664/run-anycpu-as-32-bit-on-64-bit-systems\u0026thinsp; external link http://stackoverflow.com/questions/242304/where-should-i-download-corflags-exe-from\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/10/uruchamiania-aplikacji-net-jako-32-bitowej-w-64-bitowym-systemie/","summary":"Kolejna zabawna sytuacja - pewna aplikacja dotNET\u0026rsquo;owa działała dziwnie na 64-bitowym systemie, a tymczasem na 32-bitowej maszynie ta sama aplikacja działała bez problemów. Jedyna różnica to inne wersje klientów ODBC na tych systemach, które po kilku testach okazały się być przyczyną całego zła.\nPojawił się pomysł by odpalić aplikacje na 64 bitowym systemie ale w trybie 32 bit - poniżej krótkie HOWTO jak to osiągnąć:\npotrzebujemy narzędzia corflags.exe które pozwoli oznaczyć nam binarkę jako 32-bitową, do pobrania tutaj\u0026thinsp; external link a instrukcja jej użycia tutaj\u0026thinsp; external link .","title":"Uruchamianie aplikacji .NET jako 32-bitowej w 64-bitowym systemie"},{"content":"Od jakiegoś czasu można kupić w NetArcie certyfikaty SSL, a niedawno zrobili na nie promocję - 15zł za pierwszy rok (za certyfikat na jedną stronkę). Tzw. tanie i dobre. Po wyrobieniu certyfikatu i zapisaniu z panelu klienta mam pliczki: stonka.crt i netart_rootca.crt, które wrzucamy do Apachego, powiedzmy tak:\nSSLCertificateFile /etc/ssl/certs/stonka.crt SSLCertificateKeyFile /etc/ssl/private/priv.key SSLCACertificateFile /etc/ssl/certs/netart_rootca.crt Certyfikat działa w Chromie ale nie weryfikuje się w Firefoxie i Internet Explorerze. FF wyświetla błąd: sec_error_unknown_issuer - co oznacza brak certyfikatu wystawcy gdzieś w łańcuchu certyfikatów. W FAQ zero jak chodzi o konfigurację certyfikatów na serwerze poza NetArt\u0026rsquo;em\u0026hellip;\nPrzeglądnąłem informacje certyfikatu rootca:\nopenssl x509 -in netart_rootca.crt -text -noout\u0026lt;/pre\u0026gt; Certificate: Data: Version: 3 (0x2) Serial Number: 46:53:b1:a6:1e:ba:2d:c7:a3:2e:f9:39:5a:4e:f8:8c Signature Algorithm: sha1WithRSAEncryption Issuer: C=PL, O=Unizeto Technologies S.A., OU=Certum Certification Authority, CN=Certum Global Services CA Validity Not Before: Jul 6 10:31:40 2012 GMT Not After : Jul 4 10:31:40 2022 GMT Subject: C=PL, O=NetArt Sp\\xC3\\xB3\\xC5\\x82ka Akcyjna S.K.A., OU=http://nazwa.pl, CN=nazwaSSL Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public Key: (2048 bit) Modulus (2048 bit): 00:cc:91:f5:f7:01:09:4f:75:c8:09:c7:14:8f:e4: 1a:99:78:20:99:40:59:6f:10:2f:ff:fe:d0:10:ff: 06:a3:39:3d:c4:f1:4b:07:cf:22:39:20:80:43:50: c1:af:b4:01:71:a0:a3:30:11:52:d3:d2:98:d9:c2: 69:f7:e3:00:d9:19:3f:3d:b3:3b:52:75:e3:d3:0c: ab:ff:57:01:3a:83:5c:f5:02:bb:28:fe:90:38:8e: a2:84:cf:61:48:e7:99:e0:72:24:b6:11:58:4a:18: 57:0d:34:18:5e:35:c8:b3:ac:04:5f:8d:38:2f:a2: cf:d2:dc:74:d8:41:02:ec:e0:db:0c:54:81:a4:7a: c5:34:d5:19:86:b6:1e:65:f7:3c:f6:b2:dd:3a:b5: b7:91:61:18:fd:81:2c:8a:68:d7:d6:a8:33:b7:47: b8:f9:48:ad:35:ee:11:93:f9:c2:a9:fa:94:8e:4f: bb:d1:1e:a7:64:74:b4:f9:0f:88:a7:11:a7:33:1a: c2:b1:14:0c:12:a8:6b:82:44:78:4e:d5:79:8f:5c: 60:29:47:4c:36:35:52:c7:ad:6c:c0:20:39:93:f1: c8:b3:3b:d9:c6:ec:dd:22:45:27:a2:50:12:07:f8: fe:38:79:24:89:b9:f7:de:e0:c6:e9:64:e3:f4:0b: fa:c7 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 CRL Distribution Points: URI:http://crl.certum.pl/gsca.crl Authority Information Access: CA Issuers - URI:http://repository.certum.pl/gsca.cer X509v3 Authority Key Identifier: keyid:45:C5:B2:86:4E:CC:DD:29:97:E4:DD:14:C4:6E:AE:4D:B8:C1:77:F8 X509v3 Subject Key Identifier: 9D:CE:F0:5A:B4:CB:25:CF:36:A5:82:5D:8F:F7:7F:98:46:19:37:2E X509v3 Key Usage: critical Certificate Sign, CRL Sign X509v3 Certificate Policies: Policy: X509v3 Any Policy CPS: https://www.certum.pl/CPS Signature Algorithm: sha1WithRSAEncryption 53:01:c7:87:ad:ac:d7:52:32:1f:79:5d:87:f0:01:88:8e:99: 3f:07:d8:e4:bc:84:0a:8d:5f:d5:d5:62:c2:9b:79:33:46:f9: 8a:d9:b2:96:ed:35:8a:29:3b:5f:38:7a:6a:70:1d:8b:84:1a: a3:90:81:f7:2e:60:77:78:f0:d0:84:a3:e9:8a:3c:ef:8a:34: 6b:b1:9c:e8:e1:76:f4:87:1e:7b:3c:18:6f:98:70:2c:2a:8a: 22:f5:ba:96:52:7e:26:62:8b:96:03:32:22:f9:80:d7:f1:dd: 9e:c2:79:b4:17:0d:40:ff:50:6a:28:6f:e8:6f:11:8a:f9:b4: 65:2b:52:86:31:50:c7:4d:e6:f3:be:de:6a:d1:89:90:27:61: 6c:1c:7d:90:1f:9a:ed:02:d4:01:22:5e:8b:0b:c9:99:34:f1: 1d:04:f4:d6:d0:71:7c:8f:0c:31:a3:2f:20:ad:35:c8:d3:b4: 0b:38:74:89:a5:d3:55:72:e9:af:b0:b8:9f:02:c9:85:69:01: d8:7e:00:44:25:91:2c:5e:5b:9f:ed:52:a8:bb:5d:94:20:f4: c4:82:35:de:e5:d3:05:3c:14:d5:08:80:e4:74:47:e3:fa:f7: 8c:73:40:a8:2d:ea:1f:96:c8:e3:03:2c:62:08:cc:44:02:46: a5:81:c2:0a CA NetArtu nie jest domyślnie zainstalowane w żadnej przeglądarce więc nic dziwnego - ale są tam klucze Unizeto/Certum - dorzucę więc klucz CA (Chrome najwidoczniej sam potrafi to zrobić):\nwget http://repository.certum.pl/gsca.cer openssl x509 -inform der -in gsca.cer -out gsca.pem cat gsca.pem \u0026gt;\u0026gt; netart_rootca.crt Restart Apachego i przeglądarki już nie krzyczą. Mogliby się tylko wysilić na jakąś instrukcję albo udostępnienie od razu cabudle.crt z wszystkimi potrzebnymi certami.\n","permalink":"https://gagor.pro/2013/10/certyfikaty-nazwassl-na-wlasnym-serwerze/","summary":"Od jakiegoś czasu można kupić w NetArcie certyfikaty SSL, a niedawno zrobili na nie promocję - 15zł za pierwszy rok (za certyfikat na jedną stronkę). Tzw. tanie i dobre. Po wyrobieniu certyfikatu i zapisaniu z panelu klienta mam pliczki: stonka.crt i netart_rootca.crt, które wrzucamy do Apachego, powiedzmy tak:\nSSLCertificateFile /etc/ssl/certs/stonka.crt SSLCertificateKeyFile /etc/ssl/private/priv.key SSLCACertificateFile /etc/ssl/certs/netart_rootca.crt Certyfikat działa w Chromie ale nie weryfikuje się w Firefoxie i Internet Explorerze. FF wyświetla błąd: sec_error_unknown_issuer - co oznacza brak certyfikatu wystawcy gdzieś w łańcuchu certyfikatów.","title":"Certyfikaty nazwaSSL na własnym serwerze"},{"content":"Trafił mi się ostatnio ciekawy problem - otóż standardowo przed końcem roku poprawiałem filtry antyspamowe i optymalizowałem konfigurację Postfix\u0026rsquo;a. Chciałem zmienić domyślną wartość smtpd_delay_reject=yes na smtpd_delay_reject=no by odrzucać spamerów najwcześniej jak to możliwe. I ciekawe kuku, które sobie zrobiłem polegało na tym że sam nie mogłem wysyłać poczty po logowaniu SSL\u0026rsquo;em\u0026hellip;\nDostawałem przy tym bardzo wymowną odpowiedź:\nOct 8 16:30:39 tyr postfix/smtpd[21039]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP\u0026lt;/unknown\u0026gt; Więc wrzuciłem debug_peer_list = 67.x.x.x do main.cf by zobaczyć dokładniej o co biega:\nOct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt;\u0026gt;\u0026gt; START Client host RESTRICTIONS \u0026lt; \u0026lt;\u0026lt; Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated status=0 Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject Oct 8 16:47:49 tyr postfix/smtpd[23899]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown[67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject status=2 Oct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied Oct 8 16:47:49 tyr postfix/smtpd[23899]: watchdog_pat: 0xb82c67f8 Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026lt; unknown[67.x.x.x]: QUIT Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 221 2.0.0 Bye\u0026lt;/unknown\u0026gt; generic_checks: name=permit_sasl_authenticated status=0 sugeruje że autoryzacja jest ok, a chwile później reject. Sprawdziłem konfigurację SASL\u0026rsquo;a (z dovecot\u0026rsquo;a) i zaczynałem bezskutecznie komentować kolejne linie w main.cf. Jaja polegały na tym że po ustawieniu smtpd_delay_reject=yes wszystko wracało do normy\u0026hellip; Ale nie chciałem tego tak zostawić.\nOlśniło mnie dopiero po chwili - przecież połączenia SSL SMTP odbywają się na inny port - zdefiniowany w master.cf - może coś tam bruździ. A tutaj od razu rzuciła mi się w oczy różnica w konfiguracji dla usługi submission i ssmtp:\nsubmission inet n - - - - smtpd ... -o smtpd_sender_restrictions=reject_sender_login_mismatch,permit_sasl_authenticated,reject ... smtps inet n - - - - smtpd ... -o smtpd_client_restrictions=permit_sasl_authenticated,reject ... Submission ustawiałem niedawno i widocznie trafiłem na lepszego FAQ\u0026rsquo;a \u0026#x1f603; Wygląda na to że sprawdzanie autoryzacji SASL w smtpd_client_restrictions odbywało się w tym przypadku zanim klient się autoryzował (albo było jakieś lekkie opóźnienie). Zamiana smtpd_client_restrictions na smtpd_sender_restrictions załatwiło sprawę. Przy okazji zauważyłem że po SSL\u0026rsquo;u można było spooflować innych użytkowników co również postanowiłem szybko naprawić. A wszystko dlatego że zachciało mi się \u0026ldquo;wczesnych optymalizacji\u0026rdquo; i chciałem w tych usługach pominąć część checków, które mam w main.cf.\nW minimalnej wersji konfiguracja w master.cf powinna wyglądać tak:\nsubmission inet n - - - - smtpd -o smtpd_tls_security_level=encrypt -o smtpd_tls_auth_only = yes -o smtpd_sasl_auth_enable=yes smtps inet n - - - - smtpd -o smtpd_tls_wrappermode=yes -o smtpd_sasl_auth_enable=yes P.S. Znalazłem potencjalnego winnego - w dokumentacji dovecot\u0026rsquo;a wykorzystano smtpd_client_restrictions: http://wiki2.dovecot.org/HowTo/PostfixAndDovecotSASL\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/10/postfix-ciekawy-problem-z-smtpd_delay_reject-i-permit_sasl_authenticated/","summary":"Trafił mi się ostatnio ciekawy problem - otóż standardowo przed końcem roku poprawiałem filtry antyspamowe i optymalizowałem konfigurację Postfix\u0026rsquo;a. Chciałem zmienić domyślną wartość smtpd_delay_reject=yes na smtpd_delay_reject=no by odrzucać spamerów najwcześniej jak to możliwe. I ciekawe kuku, które sobie zrobiłem polegało na tym że sam nie mogłem wysyłać poczty po logowaniu SSL\u0026rsquo;em\u0026hellip;\nDostawałem przy tym bardzo wymowną odpowiedź:\nOct 8 16:30:39 tyr postfix/smtpd[21039]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.","title":"Postfix: ciekawy problem z smtpd_delay_reject i permit_sasl_authenticated"},{"content":"Pomimo że Python dużo częściej wykorzystywany jest w środowiskach UNIX\u0026rsquo;owcy/Linux\u0026rsquo;owych to znajdzie się kilka fajnych zastosowań dla tego języka na Windowsie. Możliwości na instalację jest kilka, a najprostsza to wykorzystanie instalatora ActiveState. Wersja ta ma w sobie wszystko co potrzebne:\nrozszerzenia dla API Windows menadżera pakietów PyPM dokumentację Niestety jakiś czas temu zmieniły się zasady licencjonowania w ActiveState i aktualne wersje dla zastosowań produkcyjnych wymagają zakupu licencji (1000$/rok - aż chce się zacytować z Dnia Świra: czizys k\u0026hellip;wa\u0026hellip;). Wiem jak lepiej wydać taką kasę więc spróbuję uzyskać podobną funkcjonalność na tym co można pobrać za darmo z sieci.\nInstalatory Python\u0026rsquo;a dla Windows można znaleźć tutaj: http://www.python.org/download/releases/\u0026thinsp; external link Teraz pakiety z obsługą API Windows (wybieramy stosownie do wcześniej pobranej wersji Pythona): http://sourceforge.net/projects/pywin32/files/pywin32/\u0026thinsp; external link I na koniec setuptools by móc doinstalować dodatkowe moduły. Wybieramy interesującą nas wersję tutaj: https://pypi.python.org/pypi/setuptools/\u0026thinsp; external link Obecnie jest to 1.1.6 - zgodnie z opisem z tej strony: https://pypi.python.org/pypi/setuptools/1.1.6#windows\u0026thinsp; external link pobieramy ez_setup.py i uruchamiamy.\nNa koniec odpalamy CLI i instalujemy inne przydatne nam paczki, np.:\neasy_install couchdb easy_install cx-oracle P.S. I tutaj mały kruczek - instalacja cx-Oracle z pomocą easy_install uda się tylko na 32-bitowych Windowsach, na 64-bitowych konieczne jest zainstalowanie Visual Studio Express by możliwe było skompilowanie paczek\u0026hellip; (tak mnie też się w tej chwili już odechciewało\u0026hellip;)\nAle na szczęście w przypadku tej paczki da się inaczej, wystarczy pobrać już skompilowaną paczkę ze strony: http://cx-oracle.sourceforge.net\u0026thinsp; external link dopasowaną do wybranej wcześniej wersji Pythona. P.S. 2. Można się obyć bez tej paczki i wykorzystać pyodbc razem z kontrolerem ODBC z klienta Oracle, ale pyodbc nie obsługuje wywołań procedur ze zmiennymi wiązanymi in/out lub out - a ja akurat tego potrzebowałem, jeśli to nie twój problem to pyodbc będzie prostsze \u0026#x1f604; ","permalink":"https://gagor.pro/2013/09/instalacja-pythona-na-windowsie/","summary":"Pomimo że Python dużo częściej wykorzystywany jest w środowiskach UNIX\u0026rsquo;owcy/Linux\u0026rsquo;owych to znajdzie się kilka fajnych zastosowań dla tego języka na Windowsie. Możliwości na instalację jest kilka, a najprostsza to wykorzystanie instalatora ActiveState. Wersja ta ma w sobie wszystko co potrzebne:\nrozszerzenia dla API Windows menadżera pakietów PyPM dokumentację Niestety jakiś czas temu zmieniły się zasady licencjonowania w ActiveState i aktualne wersje dla zastosowań produkcyjnych wymagają zakupu licencji (1000$/rok - aż chce się zacytować z Dnia Świra: czizys k\u0026hellip;wa\u0026hellip;).","title":"Instalacja Python’a na Windowsie"},{"content":"Ten one liner załatwia sprawę:\npython -c \u0026#39;import django; print \u0026#34;.\u0026#34;.join([str(s) for s in django.VERSION]);\u0026#39; ","permalink":"https://gagor.pro/2013/09/sprawdzanie-zainstalowanej-wersji-django/","summary":"Ten one liner załatwia sprawę:\npython -c \u0026#39;import django; print \u0026#34;.\u0026#34;.join([str(s) for s in django.VERSION]);\u0026#39; ","title":"Sprawdzanie zainstalowanej wersji Django"},{"content":"I\u0026rsquo;ve just received a comment on my blog with text attached below. It looks like spam message template. I think it could be easily used for creation of banning rules, etc. Use it in a way that will make this dumbass spammer look even more stupid ;-)\n{ {I have|I’ve} been {surfing|browsing} online more than {three|3|2|4} hours today, yet I never found any interesting article like yours. {It’s|It is} pretty worth enough for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made good content as you did, the {internet|net|web} will be {much more|a lot more} useful than ever before.| I {couldn’t|could not} {resist|refrain from} commenting. {Very well|Perfectly|Well|Exceptionally well} written!| {I will|I’ll} {right away|immediately} {take hold of|grab|clutch|grasp|seize|snatch} your {rss|rss feed} as I {can not|can’t} {in finding|find|to find} your {email|e-mail} subscription {link|hyperlink} or {newsletter|e-newsletter} service. Do {you have|you’ve} any? {Please|Kindly} {allow|permit|let} me {realize|recognize|understand|recognise|know} {so that|in order that} I {may just|may|could} subscribe. Thanks.| {It is|It’s} {appropriate|perfect|the best} time to make some plans for the future and {it is|it’s} time to be happy. {I have|I’ve} read this post and if I could I {want to|wish to|desire to} suggest you {few|some} interesting things or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write next articles referring to this article. I {want to|wish to|desire to} read {more|even more} things about it!| {It is|It’s} {appropriate|perfect|the best} time to make {a few|some} plans for {the future|the longer term|the long run} and {it is|it’s} time to be happy. {I have|I’ve} {read|learn} this {post|submit|publish|put up} and if I {may just|may|could} I {want to|wish to|desire to} {suggest|recommend|counsel} you {few|some} {interesting|fascinating|attention-grabbing} {things|issues} or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write {next|subsequent} articles {relating to|referring to|regarding} this article. I {want to|wish to|desire to} {read|learn} {more|even more} {things|issues} {approximately|about} it!| {I have|I’ve} been {surfing|browsing} {online|on-line} {more than|greater than} {three|3} hours {these days|nowadays|today|lately|as of late}, {yet|but} I {never|by no means} {found|discovered} any {interesting|fascinating|attention-grabbing} article like yours. {It’s|It is} {lovely|pretty|beautiful} {worth|value|price} {enough|sufficient} for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made {just right|good|excellent} {content|content material} as {you did|you probably did}, the {internet|net|web} {will be|shall be|might be|will probably be|can be|will likely be} {much more|a lot more} {useful|helpful} than ever before.| Ahaa, its {nice|pleasant|good|fastidious} {discussion|conversation|dialogue} {regarding|concerning|about|on the topic of} this {article|post|piece of writing|paragraph} {here|at this place} at this {blog|weblog|webpage|website|web site}, I have read all that, so {now|at this time} me also commenting {here|at this place}.| I am sure this {article|post|piece of writing|paragraph} has touched all the internet {users|people|viewers|visitors}, its really really {nice|pleasant|good|fastidious} {article|post|piece of writing|paragraph} on building up new {blog|weblog|webpage|website|web site}.| Wow, this {article|post|piece of writing|paragraph} is {nice|pleasant|good|fastidious}, my {sister|younger sister} is analyzing {such|these|these kinds of} things, {so|thus|therefore} I am going to {tell|inform|let know|convey} her.| {Saved as a favorite|bookmarked!!}, {I really like|I like|I love} {your blog|your site|your web site|your website}!| Way cool! Some {very|extremely} valid points! I appreciate you {writing this|penning this} {article|post|write-up} {and the|and also the|plus the} rest of the {site is|website is} {also very|extremely|very|also really|really} good.| Hi, {I do believe|I do think} {this is an excellent|this is a great} {blog|website|web site|site}. I stumbledupon it ;) {I will|I am going to|I’m going to|I may} {come back|return|revisit} {once again|yet again} {since I|since i have} {bookmarked|book marked|book-marked|saved as a favorite} it. Money and freedom {is the best|is the greatest} way to change, may you be rich and continue to {help|guide} {other people|others}.| Woah! I’m really {loving|enjoying|digging} the template/theme of this {site|website|blog}. It’s simple, yet effective. A lot of times it’s {very hard|very difficult|challenging|tough|difficult|hard} to get that „perfect balance” between {superb usability|user friendliness|usability} and {visual appearance|visual appeal|appearance}. I must say {that you’ve|you have|you’ve} done a {awesome|amazing|very good|superb|fantastic|excellent|great} job with this. {In addition|Additionally|Also}, the blog loads {very|extremely|super} {fast|quick} for me on {Safari|Internet explorer|Chrome|Opera|Firefox}. {Superb|Exceptional|Outstanding|Excellent} Blog!| These are {really|actually|in fact|truly|genuinely} {great|enormous|impressive|wonderful|fantastic} ideas in {regarding|concerning|about|on the topic of} blogging. You have touched some {nice|pleasant|good|fastidious} {points|factors|things} here. Any way keep up wrinting.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I’ve {incorporated||added|included} you guys to {|my|our||my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey}! Someone in my {Myspace|Facebook} group shared this {site|website} with us so I came to {give it a look|look it over|take a look|check it out}. I’m definitely {enjoying|loving} the information. I’m {book-marking|bookmarking} and will be tweeting this to my followers! {Terrific|Wonderful|Great|Fantastic|Outstanding|Exceptional|Superb|Excellent} blog and {wonderful|terrific|brilliant|amazing|great|excellent|fantastic|outstanding|superb} {style and design|design and style|design}.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I’ve {incorporated|added|included} you guys to {|my|our|my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey} would you mind {stating|sharing} which blog platform you’re {working with|using}? I’m {looking|planning|going} to start my own blog {in the near future|soon} but I’m having a {tough|difficult|hard} time {making a decision|selecting|choosing|deciding} between BlogEngine/Wordpress/B2evolution and Drupal. The reason I ask is because your {design and style|design|layout} seems different then most blogs and I’m looking for something {completely unique|unique}. P.S {My apologies|Apologies|Sorry} for {getting|being} off-topic but I had to ask!| {Howdy|Hi there|Hi|Hey there|Hello|Hey} would you mind letting me know which {webhost|hosting company|web host} you’re {utilizing|working with|using}? I’ve loaded your blog in 3 {completely different|different} {internet browsers|web browsers|browsers} and I must say this blog loads a lot {quicker|faster} then most. Can you {suggest|recommend} a good {internet hosting|web hosting|hosting} provider at a {honest|reasonable|fair} price? {Thanks a lot|Kudos|Cheers|Thank you|Many thanks|Thanks}, I appreciate it!| {I love|I really like|I like|Everyone loves} it {when people|when individuals|when folks|whenever people} {come together|get together} and share {opinions|thoughts|views|ideas}. Great {blog|website|site}, {keep it up|continue the good work|stick with it}!| Thank you for the {auspicious|good} writeup. It in fact was a amusement account it. Look advanced to {far|more} added agreeable from you! {By the way|However}, how {can|could} we communicate?| {Howdy|Hi there|Hey there|Hello|Hey} just wanted to give you a quick heads up. The {text|words} in your {content|post|article} seem to be running off the screen in {Ie|Internet explorer|Chrome|Firefox|Safari|Opera}. I’m not sure if this is a {format|formatting} issue or something to do with {web browser|internet browser|browser} compatibility but I {thought|figured} I’d post to let you know. The {style and design|design and style|layout|design} look great though! Hope you get the {problem|issue} {solved|resolved|fixed} soon. {Kudos|Cheers|Many thanks|Thanks}| This is a topic {that is|that’s|which is} {close to|near to} my heart… {Cheers|Many thanks|Best wishes|Take care|Thank you}! {Where|Exactly where} are your contact details though?| It’s very {easy|simple|trouble-free|straightforward|effortless} to find out any {topic|matter} on {net|web} as compared to {books|textbooks}, as I found this {article|post|piece of writing|paragraph} at this {website|web site|site|web page}.| Does your {site|website|blog} have a contact page? I’m having {a tough time|problems|trouble} locating it but, I’d like to {send|shoot} you an {e-mail|email}. I’ve got some {creative ideas|recommendations|suggestions|ideas} for your blog you might be interested in hearing. Either way, great {site|website|blog} and I look forward to seeing it {develop|improve|expand|grow} over time.| {Hola|Hey there|Hi|Hello|Greetings}! I’ve been {following|reading} your {site|web site|website|weblog|blog} for {a long time|a while|some time} now and finally got the {bravery|courage} to go ahead and give you a shout out from {New Caney|Kingwood|Huffman|Porter|Houston|Dallas|Austin|Lubbock|Humble|Atascocita} {Tx|Texas}! Just wanted to {tell you|mention|say} keep up the {fantastic|excellent|great|good} {job|work}!| Greetings from {Idaho|Carolina|Ohio|Colorado|Florida|Los angeles|California}! I’m {bored to tears|bored to death|bored} at work so I decided to {check out|browse} your {site|website|blog} on my iphone during lunch break. I {enjoy|really like|love} the {knowledge|info|information} you {present|provide} here and can’t wait to take a look when I get home. I’m {shocked|amazed|surprised} at how {quick|fast} your blog loaded on my {mobile|cell phone|phone} .. I’m not even using WIFI, just 3G .. {Anyhow|Anyways}, {awesome|amazing|very good|superb|good|wonderful|fantastic|excellent|great} {site|blog}!| Its {like you|such as you} {read|learn} my {mind|thoughts}! You {seem|appear} {to understand|to know|to grasp} {so much|a lot} {approximately|about} this, {like you|such as you} wrote the {book|e-book|guide|ebook|e book} in it or something. {I think|I feel|I believe} {that you|that you simply|that you just} {could|can} do with {some|a few} {%|p.c.|percent} to {force|pressure|drive|power} the message {house|home} {a bit|a little bit}, {however|but} {other than|instead of} that, {this is|that is} {great|wonderful|fantastic|magnificent|excellent} blog. {A great|An excellent|A fantastic} read. {I’ll|I will} {definitely|certainly} be back.| I visited {multiple|many|several|various} {websites|sites|web sites|web pages|blogs} {but|except|however} the audio {quality|feature} for audio songs {current|present|existing} at this {website|web site|site|web page} is {really|actually|in fact|truly|genuinely} {marvelous|wonderful|excellent|fabulous|superb}.| {Howdy|Hi there|Hi|Hello}, i read your blog {occasionally|from time to time} and i own a similar one and i was just {wondering|curious} if you get a lot of spam {comments|responses|feedback|remarks}? If so how do you {prevent|reduce|stop|protect against} it, any plugin or anything you can {advise|suggest|recommend}? I get so much lately it’s driving me {mad|insane|crazy} so any {assistance|help|support} is very much appreciated.| Greetings! {Very helpful|Very useful} advice {within this|in this particular} {article|post}! {It is the|It’s the} little changes {that make|which will make|that produce|that will make} {the biggest|the largest|the greatest|the most important|the most significant} changes. {Thanks a lot|Thanks|Many thanks} for sharing!| {I really|I truly|I seriously|I absolutely} love {your blog|your site|your website}.. {Very nice|Excellent|Pleasant|Great} colors \u0026amp; theme. Did you {create|develop|make|build} {this website|this site|this web site|this amazing site} yourself? Please reply back as I’m {looking to|trying to|planning to|wanting to|hoping to|attempting to} create {my own|my very own|my own personal} {blog|website|site} and {would like to|want to|would love to} {know|learn|find out} where you got this from or {what the|exactly what the|just what the} theme {is called|is named}. {Thanks|Many thanks|Thank you|Cheers|Appreciate it|Kudos}!| {Hi there|Hello there|Howdy}! This {post|article|blog post} {couldn’t|could not} be written {any better|much better}! {Reading through|Looking at|Going through|Looking through} this {post|article} reminds me of my previous roommate! He {always|constantly|continually} kept {talking about|preaching about} this. {I will|I’ll|I am going to|I most certainly will} {forward|send} {this article|this information|this post} to him. {Pretty sure|Fairly certain} {he will|he’ll|he’s going to} {have a good|have a very good|have a great} read. {Thank you for|Thanks for|Many thanks for|I appreciate you for} sharing!| {Wow|Whoa|Incredible|Amazing}! This blog looks {exactly|just} like my old one! It’s on a {completely|entirely|totally} different {topic|subject} but it has pretty much the same {layout|page layout} and design. {Excellent|Wonderful|Great|Outstanding|Superb} choice of colors!| {There is|There’s} {definately|certainly} {a lot to|a great deal to} {know about|learn about|find out about} this {subject|topic|issue}. {I like|I love|I really like} {all the|all of the} points {you made|you’ve made|you have made}.| {You made|You’ve made|You have made} some {decent|good|really good} points there. I {looked|checked} {on the internet|on the web|on the net} {for more info|for more information|to find out more|to learn more|for additional information} about the issue and found {most individuals|most people} will go along with your views on {this website|this site|this web site}.| {Hi|Hello|Hi there|What’s up}, I {log on to|check|read} your {new stuff|blogs|blog} {regularly|like every week|daily|on a regular basis}. Your {story-telling|writing|humoristic} style is {awesome|witty}, keep {doing what you’re doing|up the good work|it up}!| I {simply|just} {could not|couldn’t} {leave|depart|go away} your {site|web site|website} {prior to|before} suggesting that I {really|extremely|actually} {enjoyed|loved} {the standard|the usual} {information|info} {a person|an individual} {supply|provide} {for your|on your|in your|to your} {visitors|guests}? Is {going to|gonna} be {back|again} {frequently|regularly|incessantly|steadily|ceaselessly|often|continuously} {in order to|to} {check up on|check out|inspect|investigate cross-check} new posts| {I wanted|I needed|I want to|I need to} to thank you for this {great|excellent|fantastic|wonderful|good|very good} read!! I {definitely|certainly|absolutely} {enjoyed|loved} every {little bit of|bit of} it. {I have|I’ve got|I have got} you {bookmarked|book marked|book-marked|saved as a favorite} {to check out|to look at} new {stuff you|things you} post…| {Hi|Hello|Hi there|What’s up}, just wanted to {mention|say|tell you}, I {enjoyed|liked|loved} this {article|post|blog post}. It was {inspiring|funny|practical|helpful}. Keep on posting!| I {{leave|drop|{write|create}} a {comment|leave a response}|drop a {comment|leave a response}|{comment|leave a response}} {each time|when|whenever} I {appreciate|like|especially enjoy} a {post|article} on a {site|{blog|website}|site|website} or {I have|if I have} something to {add|contribute|valuable to contribute} {to the discussion|to the conversation}. {It is|Usually it is|Usually it’s|It’s} {a result of|triggered by|caused by} the {passion|fire|sincerness} {communicated|displayed} in the {post|article} I {read|looked at|browsed}. And {on|after} this {post|article} Kopiowanie wolumenów LVM z dd i netcat | timor’s site. I {{was|was actually} moved|{was|was actually} excited} enough to {drop|{leave|drop|{write|create}}|post} a {thought|{comment|{comment|leave a response}a response}} {:-P|:)|;)|;-)|: -)} I {do have|actually do have} {{some|a few} questions|a couple of questions|2 questions} for you {if you {don’t|do not|usually do not|tend not to} mind|if it’s {allright|okay}}. {Is it|Could it be} {just|only|simply} me or {do|does it {seem|appear|give the impression|look|look as if|look like} like} {some|a few} of {the|these} {comments|responses|remarks} {look|appear|come across} {like they are|as if they are|like} {coming from|written by|left by} brain dead {people|visitors|folks|individuals}? :-P And, if you are {posting|writing} {on|at} {other|additional} {sites|social sites|online sites|online social sites|places}, {I’d|I would} like to {follow|keep up with} {you|{anything|everything} {new|fresh} you have to post}. {Could|Would} you {list|make a list} {all|every one|the complete urls} of {your|all your} {social|communal|community|public|shared} {pages|sites} like your {twitter feed, Facebook page or linkedin profile|linkedin profile, Facebook page or twitter feed|Facebook page, twitter feed, or linkedin profile}?| {Hi there|Hello}, I enjoy reading {all of|through} your {article|post|article post}. I {like|wanted} to write a little comment to support you.| I {always|constantly|every time} spent my half an hour to read this {blog|weblog|webpage|website|web site}’s {articles|posts|articles or reviews|content} {everyday|daily|every day|all the time} along with a {cup|mug} of coffee.| I {always|for all time|all the time|constantly|every time} emailed this {blog|weblog|webpage|website|web site} post page to all my {friends|associates|contacts}, {because|since|as|for the reason that} if like to read it {then|after that|next|afterward} my {friends|links|contacts} will too.| My {coder|programmer|developer} is trying to {persuade|convince} me to move to .net from PHP. I have always disliked the idea because of the {expenses|costs}. But he’s tryiong none the less. I’ve been using {Movable-type|WordPress} on {a number of|a variety of|numerous|several|various} websites for about a year and am {nervous|anxious|worried|concerned} about switching to another platform. I have heard {fantastic|very good|excellent|great|good} things about blogengine.net. Is there a way I can {transfer|import} all my wordpress {content|posts} into it? {Any kind of|Any} help would be {really|greatly} appreciated!| {Hello|Hi|Hello there|Hi there|Howdy|Good day}! I could have sworn I’ve {been to|visited} {this blog|this web site|this website|this site|your blog} before but after {browsing through|going through|looking at} {some of the|a few of the|many of the} {posts|articles} I realized it’s new to me. {Anyways|Anyhow|Nonetheless|Regardless}, I’m {definitely|certainly} {happy|pleased|delighted} {I found|I discovered|I came across|I stumbled upon} it and I’ll be {bookmarking|book-marking} it and checking back {frequently|regularly|often}!| {Terrific|Great|Wonderful} {article|work}! {This is|That is} {the type of|the kind of} {information|info} {that are meant to|that are supposed to|that should} be shared {around the|across the} {web|internet|net}. {Disgrace|Shame} on {the {seek|search} engines|Google} for {now not|not|no longer} positioning this {post|submit|publish|put up} {upper|higher}! Come on over and {talk over with|discuss with|seek advice from|visit|consult with} my {site|web site|website} . {Thank you|Thanks} =)| Heya {i’m|i am} for the first time here. I {came across|found} this board and I find It {truly|really} useful \u0026amp; it helped me out {a lot|much}. I hope to give something back and {help|aid} others like you {helped|aided} me.| {Hi|Hello|Hi there|Hello there|Howdy|Greetings}, {I think|I believe|I do believe|I do think|There’s no doubt that} {your site|your website|your web site|your blog} {might be|may be|could be|could possibly be} having {browser|internet browser|web browser} compatibility {issues|problems}. {When I|Whenever I} {look at your|take a look at your} {website|web site|site|blog} in Safari, it looks fine {but when|however when|however, if|however, when} opening in {Internet Explorer|IE|I.E.}, {it has|it’s got} some overlapping issues. {I just|I simply|I merely} wanted to {give you a|provide you with a} quick heads up! {Other than that|Apart from that|Besides that|Aside from that}, {fantastic|wonderful|great|excellent} {blog|website|site}!| {A person|Someone|Somebody} {necessarily|essentially} {lend a hand|help|assist} to make {seriously|critically|significantly|severely} {articles|posts} {I would|I might|I’d} state. {This is|That is} the {first|very first} time I frequented your {web page|website page} and {to this point|so far|thus far|up to now}? I {amazed|surprised} with the {research|analysis} you made to {create|make} {this actual|this particular} {post|submit|publish|put up} {incredible|amazing|extraordinary}. {Great|Wonderful|Fantastic|Magnificent|Excellent} {task|process|activity|job}!| Heya {i’m|i am} for {the primary|the first} time here. I {came across|found} this board and I {in finding|find|to find} It {truly|really} {useful|helpful} \u0026amp; it helped me out {a lot|much}. {I am hoping|I hope|I’m hoping} {to give|to offer|to provide|to present} {something|one thing} {back|again} and {help|aid} others {like you|such as you} {helped|aided} me.| {Hello|Hi|Hello there|Hi there|Howdy|Good day|Hey there}! {I just|I simply} {would like to|want to|wish to} {give you a|offer you a} {huge|big} thumbs up {for the|for your} {great|excellent} {info|information} {you have|you’ve got|you have got} {here|right here} on this post. {I will be|I’ll be|I am} {coming back to|returning to} {your blog|your site|your website|your web site} for more soon.| I {always|all the time|every time} used to {read|study} {article|post|piece of writing|paragraph} in news papers but now as I am a user of {internet|web|net} {so|thus|therefore} from now I am using net for {articles|posts|articles or reviews|content}, thanks to web.| Your {way|method|means|mode} of {describing|explaining|telling} {everything|all|the whole thing} in this {article|post|piece of writing|paragraph} is {really|actually|in fact|truly|genuinely} {nice|pleasant|good|fastidious}, {all|every one} {can|be able to|be capable of} {easily|without difficulty|effortlessly|simply} {understand|know|be aware of} it, Thanks a lot.| {Hi|Hello} there, {I found|I discovered} your {blog|website|web site|site} {by means of|via|by the use of|by way of} Google {at the same time as|whilst|even as|while} {searching for|looking for} a {similar|comparable|related} {topic|matter|subject}, your {site|web site|website} {got here|came} up, it {looks|appears|seems|seems to be|appears to be like} {good|great}. {I have|I’ve} bookmarked it in my google bookmarks. {Hello|Hi} there, {simply|just} {turned into|became|was|become|changed into} {aware of|alert to} your {blog|weblog} {thru|through|via} Google, {and found|and located} that {it is|it’s} {really|truly} informative. {I’m|I am} {gonna|going to} {watch out|be careful} for brussels. {I will|I’ll} {appreciate|be grateful} {if you|should you|when you|in the event you|in case you|for those who|if you happen to} {continue|proceed} this {in future}. {A lot of|Lots of|Many|Numerous} {other folks|folks|other people|people} {will be|shall be|might be|will probably be|can be|will likely be} benefited {from your|out of your} writing. Cheers!| {I am|I’m} curious to find out what blog {system|platform} {you have been|you happen to be|you are|you’re} {working with|utilizing|using}? I’m {experiencing|having} some {minor|small} security {problems|issues} with my latest {site|website|blog} and {I would|I’d} like to find something more {safe|risk-free|safeguarded|secure}. Do you have any {solutions|suggestions|recommendations}?| {I am|I’m} {extremely|really} impressed with your writing skills {and also|as well as} with the layout on your {blog|weblog}. Is this a paid theme or did you {customize|modify} it yourself? {Either way|Anyway} keep up the {nice|excellent} quality writing, {it’s|it is} rare to see a {nice|great} blog like this one {these days|nowadays|today}.| {I am|I’m} {extremely|really} {inspired|impressed} {with your|together with your|along with your} writing {talents|skills|abilities} {and also|as {smartly|well|neatly} as} with the {layout|format|structure} {for your|on your|in your|to your} {blog|weblog}. {Is this|Is that this} a paid {subject|topic|subject matter|theme} or did you {customize|modify} it {yourself|your self}? {Either way|Anyway} {stay|keep} up the {nice|excellent} {quality|high quality} writing, {it’s|it is} {rare|uncommon} {to peer|to see|to look} a {nice|great} {blog|weblog} like this one {these days|nowadays|today}..| {Hi|Hello}, Neat post. {There is|There’s} {a problem|an issue} {with your|together with your|along with your} {site|web site|website} in {internet|web} explorer, {may|might|could|would} {check|test} this? IE {still|nonetheless} is the {marketplace|market} {leader|chief} and {a large|a good|a big|a huge} {part of|section of|component to|portion of|component of|element of} {other folks|folks|other people|people} will {leave out|omit|miss|pass over} your {great|wonderful|fantastic|magnificent|excellent} writing {due to|because of} this problem.| {I’m|I am} not sure where {you are|you’re} getting your {info|information}, but {good|great} topic. I needs to spend some time learning {more|much more} or understanding more. Thanks for {great|wonderful|fantastic|magnificent|excellent} {information|info} I was looking for this {information|info} for my mission.| {Hi|Hello}, i think that i saw you visited my {blog|weblog|website|web site|site} {so|thus} i came to “return the favor”.{I am|I’m} {trying to|attempting to} find things to {improve|enhance} my {website|site|web site}!I suppose its ok to use {some of|a few of} your ideas ","permalink":"https://gagor.pro/2013/09/spammer-screwed-up/","summary":"I\u0026rsquo;ve just received a comment on my blog with text attached below. It looks like spam message template. I think it could be easily used for creation of banning rules, etc. Use it in a way that will make this dumbass spammer look even more stupid ;-)\n{ {I have|I’ve} been {surfing|browsing} online more than {three|3|2|4} hours today, yet I never found any interesting article like yours. {It’s|It is} pretty worth enough for me.","title":"Spammer screwed up"},{"content":"Wyprzeć się nie mogę że gadżety działające na Linuksie po prostu mnie kręcą, więc tylko kwestią czasu było aż Pi zawita na moim biurku. Zakupiłem więc model B w drugiej wersji\u0026thinsp; external link , obudowę z możliwością mocowania VESA\u0026thinsp; external link , kabelek HDMI\u0026thinsp; external link , ładowarka z mojej myszy pasowała idealnie (5.05V i 1A) i na początek karta SD klasa 10 4GB.\nSzukając różnych systemów (a może ROM\u0026rsquo;ów) natrafiłem na oficjalną stronę: http://www.raspberrypi.org/downloads\u0026thinsp; external link Na początek wystarczy a sprawdzając na stronach projektów okazało się że wersje na tej stronie są całkiem aktualne.\nChociaż paczka NOOB wyglądała kusząco to byłą największa i ściągnęła mi się jak już działałem na OpenELEC\u0026rsquo;u. Na dobrą sprawę nie rozumiem czemu tę drogę opisano jako trudniejszą\u0026hellip;\nNagrywanie obrazu na kartę SD Wpinamy kartę SD w czytnik Odpalamy: dmesg i szukamy jaką literkę jej przyporządkowano w systemie\nOdpalamy na tym dysku: fdisk -l /dev/sdh (w moim przypadku) i upewniamy się że rozmiar i tablica partycji odpowiadają naszej karcie (pomyłka literki to bankowa utrata danych i co najmniej jedna nieprzespana noc\u0026hellip;)\nRozpakowujemy obraz: unzip 2013-07-26-wheezy-raspbian.zip Możemy wgrywać: sudo dd bs=1M if=2013-07-26-wheezy-raspbian.img of=/dev/sdh status kopiowania możemy podglądać tak:\nkill -USR1 `pidof dd` (o ile mamy uruchomiony jeden proces dd w systemi)\nW przypadku niektórych obrazów może być potrzebna zmiana rozmiaru partycji (bo domyślnie przygotowane są dla mniejszych kart - polecam parted lub gparted do tego zadania Można startować!\n","permalink":"https://gagor.pro/2013/09/raspberry-pi-pierwsze-kroki/","summary":"Wyprzeć się nie mogę że gadżety działające na Linuksie po prostu mnie kręcą, więc tylko kwestią czasu było aż Pi zawita na moim biurku. Zakupiłem więc model B w drugiej wersji\u0026thinsp; external link , obudowę z możliwością mocowania VESA\u0026thinsp; external link , kabelek HDMI\u0026thinsp; external link , ładowarka z mojej myszy pasowała idealnie (5.05V i 1A) i na początek karta SD klasa 10 4GB.\nSzukając różnych systemów (a może ROM\u0026rsquo;ów) natrafiłem na oficjalną stronę: http://www.","title":"Raspberry Pi: pierwsze kroki"},{"content":"Polubiłem Nginx\u0026rsquo;a i wykorzystuję go na coraz więcej sposobów. Kilka rzeczy udało mi się całkiem fajnie w nim skonfigurować i postanowiłem zebrać te przykłady by następnym razem gdy postanowię do nich sięgnąć nie musieć wertować konfigów po serwerach \u0026#x1f603;\nSłowo wstępu Niektóre rewrite\u0026rsquo;y kończą się znakiem ? - czemu?\nOtóż Nginx próbuje automatycznie dodawać parametry na końcu przepisanego adresu. Jeśli jednak wykorzystamy zmienną $request_uri to ona sama w sobie zawiera już parametry zapytania (czyli to co w URI znajduje się po znaku ?) i właśnie dodanie pytajnika tuż za tą zmienną powoduje że argumenty nie są dublowane.\nMa to też zastosowanie gdy chcemy by rewrite kierował np. na główną stronę beż żadnych dodatkowych argumentów (zostaną one obcięte).\nWięcej na ten temat można znaleźć w dokumentacji Nginx\u0026thinsp; external link .\nInna warta wspomnienia uwaga dotyczy drobnej optymalizacji, o której warto pamiętać na etapie tworzenia rewrite\u0026rsquo;ów (można znaleźć masę kiepskich przykładów w sieci): na początku najlepiej jest stworzyć coś co działa (i przy małym ruchu może to być wystarczające) a później optymalizować - moje przykłady starałem się zoptymalizować według zalecanych praktyk.\nDlatego zamiast pisać:\nrewrite ^(.*)$ $scheme://www.domain.com$1 permanent; lepiej napisać:\nrewrite ^ $scheme://www.domain.com$request_uri? permanent; (nie wykorzystujemy przechowywania wartości dopasowania - mniejsze zużycie pamięci i lżejsza interpretacja REGEXP\u0026rsquo;a).\nA jeszcze lepiej napisać:\nreturn 301 $scheme://www.domain.com$request_uri; (w ogóle nie wykorzystujemy REGEXP\u0026rsquo;ów praktycznie zerowy narzut na przetwarzanie) - dzięki za uwagę: lukasamd.\nPrzekierowanie starej domeny na nową server { listen 80; server_name old-domain.com www.old-domain.com; return 301 $scheme://www.new-domain.com$request_uri; # rewrite ^ $scheme://www.new-domain.com$request_uri? permanent; # or # rewrite ^ $scheme://www.new-domain.com? permanent; } Wykorzystanie return w tej sytuacji jest nieco bardziej optymalne gdyż nie angażuje w ogóle silnika REGEXP a w tej sytuacji jest wystarczające.\nPierwsza linia z rewrite i $request_uri spowoduje przepisywanie też parametrów wywołań do nowej lokalizacji co jest jak najbardziej sensowne gdy pomimo domeny nie zmieniła się zbytnio struktura strony.\nJeśli strona jednak się zmieniła to możemy zdecydować o przekierowaniu bez parametrów - po prostu na główną stronę - i to robi druga linia.\nW obu przypadkach parametr permanent nakazuje użycie kodu przekierowania HTTP 301 (Moved Permanently), co ułatwi zorientowanie się crawlerom że ta zmiana jest już na stałe.\nDodanie WWW na początku domeny server { listen 80; server_name domaim.com; return 301 $scheme://www.domain.com$request_uri; #rewrite ^ $scheme://www.domain.com$request_uri? permanent; # or #rewrite ^(.*)$ $scheme://www.domain.com$1 permanent; } Przykład zakomentowany jest według dokumentacji mniej optymalny ale również zadziała. Reszta jest prosta i samoopisująca się \u0026#x1f603;\nA to jeszcze bardziej ogólna wersja dla wielu domen:\nserver { listen 195.117.254.80:80; server_name domain.pl domain.eu domain.com; return 301 $scheme://www.$http_host$request_uri; #rewrite ^ $scheme://www.$http_host$request_uri? permanent; } Ta wersja wykorzystuje zmienną $http_host do przekierowania na domenę z zapytania (zmienna ta zawiera też numer portu jeśli jest niestandardowy np. 8080, w przeciwieństwie do zmiennej $host, która zawiera tylko domenę).\nUsunięcie WWW z początku domeny server { listen 80; server_name www.domain.com; return 301 $scheme://domain.com$request_uri; #rewrite ^ $scheme://domain.com$request_uri? permanent; } Czasami może się przydać jeszcze inny kawałek, gdy strona działa na wielu domenach i chcemy przekierować wszystkie:\nserver { server_name www.domain.com _ ; # server_name www.domain1.com www.domain2.com www.domain3.eu www.domain.etc.com; if ($host ~* www\\.(.*)) { set $pure_host $1; return 301 $scheme://$pure_host$request_uri; #rewrite ^ $scheme://$pure_host$request_uri? permanent; #rewrite ^(.*)$ $scheme://$pure_host$1 permanent; } } Choć to podejście nie jest zalecane (pomimo zwięzłości). Lepiej zdefiniować dwa bloki server z domenami www.* i domenami bez www na początku. Ale z drugiej strony to cholernie wygodne\u0026hellip; \u0026#x1f603;\nPrzekierowanie \u0026ldquo;pozostałych\u0026rdquo; zapytań na domyślną domenę server { listen 80 default; server_name _; rewrite ^ $scheme://www.domena.com; #rewrite ^ $scheme://www.domena.com/search/$host; } To bardzo przydatny przykład - czyli domyślny vhost, który \u0026ldquo;przyjmie\u0026rdquo; wszystkie zapytania do domen nie zdefiniowanych w konfiguracji i przekieruje na naszą \u0026ldquo;główną stronę\u0026rdquo;.\nZakomentowany przykład jest nieco bardziej przekombinowany bo próbuje wykorzystać wyszukiwarkę na naszej stronie do wyszukania \u0026ldquo;czegoś\u0026rdquo; pomocnego - z tym przykładem należy uważać bo jeśli do serwera trafi dużo błędnych zapytań to może zostać przeciążony \u0026ldquo;bzdetnymi\u0026rdquo; wyszukiwaniami.\nPrzekierowanie pewnych podstron po zmianie struktury strony server { listen 80; server_name www.domain.com; location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /tag/something $scheme://new.domain.com permanent; rewrite /category/hobby /category/painting permanent; # etc ... rewrite ^ /index.php last; } } Im starsza strona tym więcej zbiera się linków, których po prostu nie można usunąć, a które z racji wprowadzonych zmian nie mają prawa bytu w nowym układzie. Warto je przekierować w nowe miejsca, lub najbardziej odpowiadające/bliskie tym starym. Problemem może się wkrótce stać duża lista przekierowań, która zaciemni konfigurację.\nPowyższy sposób w dość optymalny sposób porządkuje takie przekierowania - najpierw sprawdza czy przypadkiem nie próbujemy pobrać istniejących plików, jeśli nie to wrzuca nas nas na listę przekierowań, a jeśli i tu nic nie znajdzie to zapytanie przekazywane jest do głównego skryptu strony.\nPrzekierowanie w zależności od wartości parametru w URI if ($args ~ producent=toyota){ rewrite ^ $scheme://toyota.domena.com$request_uri? permanent; } To rzadko stosowane przekierowanie a w dodatku mało czytelnie i ponoć mało wydajne\u0026hellip; Ale potrafi być bardzo przydatne gdy chcemy przepisać adres w zależności od wartości parametru np. gdy pewna podstrona doczeka się rozbudowy w zupełnie nowym serwisie lub gdy chcemy ładnie przekierować adresy ze starej strony na nową.\nBlokowanie dostępu do ukrytych plików location ~ /\\. { access_log off; log_not_found off; deny all; } Przyznaję - to nie rewrite\u0026hellip; Ale ta linijka jest równie przydatna - pozwala zablokować możliwość pobierania ukrytych plików (np. .htaccess\u0026rsquo;ów po konfiguracji z Apachego).\nWyłączenie logowania dla robots.txt i favicon.ico location = /favicon.ico { try_files /favicon.ico =204; access_log off; log_not_found off; } location = /robots.txt { try_files /robots.txt =204; access_log off; log_not_found off; } To też nie rewrite - ale bardzo fajnie obsługuje sytuację gdy mamy i gdy nie mamy powyższych dwóch pliczków. Po pierwsze wyłącza logowanie i serwuje je gdy są dostępne. Gdy nie istnieją to serwuje puste pliki (kod 204) dzięki czemu nie przeszkadzają nam 404-ki \u0026#x1f603;\nBlokowanie dostępu do obrazków dla nieznanych referererów location ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf)$ { # definiujemy poprawnych refererow valid_referers none blocked *.domain.com domain.com; if ($invalid_referer) { return 444; } expires max; break; } Zabezpieczenie warte tyle co nic bo banalne do ominięcia - ale jeśli zdarzy się że ktoś postanowi wykorzystać grafikę z naszej stronki np. w aukcji na allegro czy własnym sklepie to tą prostą sztuczką możemy go przyciąć i przeważnie jest to wystarczające.\nMuszę też zaznaczyć szczególne znaczenie wartości kodu błędu 444 w Nginx\u0026rsquo;ie - powoduje on zerwanie połączenia bez wysyłania jakiejkolwiek odpowiedzi. Jeśli nie chcemy być tak okrutni to możemy użyć innego kodu, np.: 403 albo 402\u0026thinsp; external link \u0026#x1f603;\nPrzekierowanie ciekawskich w \u0026ldquo;ciemną dupę\u0026rdquo; location ~* ^/(wp-)?admin(istrator)?/? { rewrite ^ http://whatismyipaddress.com/ip/$remote_addr redirect; } Ten prosty redirect odwodzi wielu amatorów zbyt głębokiego penetrowania naszej strony\u0026hellip; A pozostałych na pewno rozbawi \u0026#x1f603;\nInne przykłady konfiguracji na mojej stronie:\nNginx - hide server version and name in Server header and error pages Nginx - kompresowanie plików dla gzip_static Nginx - konfiguracja pod WordPress’a Nginx - ustawienie domyślnego vhosta Nginx - mój domyślny config Źródła http://www.engineyard.com/blog/2011/useful-rewrites-for-nginx/\u0026thinsp; external link http://wiki.nginx.org/HttpRewriteModule\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/09/nginx-przydatne-rewritey-i-rozne-sztuczki/","summary":"Polubiłem Nginx\u0026rsquo;a i wykorzystuję go na coraz więcej sposobów. Kilka rzeczy udało mi się całkiem fajnie w nim skonfigurować i postanowiłem zebrać te przykłady by następnym razem gdy postanowię do nich sięgnąć nie musieć wertować konfigów po serwerach \u0026#x1f603;\nSłowo wstępu Niektóre rewrite\u0026rsquo;y kończą się znakiem ? - czemu?\nOtóż Nginx próbuje automatycznie dodawać parametry na końcu przepisanego adresu. Jeśli jednak wykorzystamy zmienną $request_uri to ona sama w sobie zawiera już parametry zapytania (czyli to co w URI znajduje się po znaku ?","title":"Nginx - przydatne rewrite’y i różne sztuczki"},{"content":"Chcąc pobawić się tor\u0026rsquo;em postanowiłem udostępnić jedna z moich stron z wykorzystaniem mechanizmu hidden service. Nie spodobał mi się jedynie sposób generowania nazw, które były mało opisowe - ale najwidoczniej nie mnie jednemu, bo szybko namierzyłem Shallot\u0026thinsp; external link , który generuje kolejne nazwy aż trafi na pasującą do zadanego regexp\u0026rsquo;a.\n","permalink":"https://gagor.pro/2013/09/tor-generowanie-milszej-nazwy-dla-hidden-service/","summary":"Chcąc pobawić się tor\u0026rsquo;em postanowiłem udostępnić jedna z moich stron z wykorzystaniem mechanizmu hidden service. Nie spodobał mi się jedynie sposób generowania nazw, które były mało opisowe - ale najwidoczniej nie mnie jednemu, bo szybko namierzyłem Shallot\u0026thinsp; external link , który generuje kolejne nazwy aż trafi na pasującą do zadanego regexp\u0026rsquo;a.","title":"tor: generowanie milszej nazwy dla hidden service"},{"content":"Ostatnio zbyt dużo grzebię przy \u0026ldquo;windach\u0026rdquo; - ale cóż, czasem trzeba. Ostatnio ustawiałem DFS\u0026rsquo;a z replikacją dla dwóch sporych zasobów i jedna z rzeczy, o którą się rozbiłem to brak jakiegokolwiek podglądu tej synchronizacji z GUI. Ale znalazłem jedno polecenie, które działa w shellu (choć to się chyba batch tutaj nazywa) od Windows Server 2008 R2:\ndfsrdiag ReplicationState /member:nazwaservera Polecenie co prawda nie podaje postępu procentowego ale można zobaczyć \u0026ldquo;czy coś jeszcze się synchronizuje\u0026rdquo; i czy nie ma żadnych błędów. Jeżeli to polecenie to za mało to można spróbować bardziej gadatliwej wersji:\ndfsrdiag ReplicationState /member:nazwaservera /all ","permalink":"https://gagor.pro/2013/09/dfs-sprawdzanie-statusu-replikacji/","summary":"Ostatnio zbyt dużo grzebię przy \u0026ldquo;windach\u0026rdquo; - ale cóż, czasem trzeba. Ostatnio ustawiałem DFS\u0026rsquo;a z replikacją dla dwóch sporych zasobów i jedna z rzeczy, o którą się rozbiłem to brak jakiegokolwiek podglądu tej synchronizacji z GUI. Ale znalazłem jedno polecenie, które działa w shellu (choć to się chyba batch tutaj nazywa) od Windows Server 2008 R2:\ndfsrdiag ReplicationState /member:nazwaservera Polecenie co prawda nie podaje postępu procentowego ale można zobaczyć \u0026ldquo;czy coś jeszcze się synchronizuje\u0026rdquo; i czy nie ma żadnych błędów.","title":"DFS - sprawdzanie statusu replikacji"},{"content":"Ostatnio aktualizowałem swojego Nexusa do 4.3 i było miło tylko mi roota i recovery wystrzeliło\u0026hellip; No ale żaden problem - chwila googlania, kilka poleceń i mam recovery i roota. Dzisiaj wrzuciłem niedużą aktualizację, która łata kilka bugów i znów po root\u0026rsquo;cie ;/\nZapisze sobie więc instrukcję by kolejnym razem już nie googlać \u0026#x1f603;\nP.S. Wiem co robię ryzykując uceglenie swojego urządzenia - u mnie ta instrukcja działa ale nie mogę tego zagwarantować każdemu - dlatego jeśli już się zdecydujesz to ROBISZ TO NA WŁASNĄ ODPOWIEDZIALNOŚĆ!\nBędzie potrzebne Android SDK\u0026thinsp; external link lub przynajmniej mała paczka z fastboot i adb\u0026thinsp; external link (więcej info o instalacji w linkach na końcu).\nNa początek sprawdzamy czy nie ma nowego recovery, preferuję CWM\u0026rsquo;a więc zerkamy tutaj\u0026thinsp; external link Pobieram wersję touch (bo na tak dużym ekranie całkiem komfortowo się ją obsługuje): recovery-clockwork-touch-6.0.3.6-grouper.img\u0026thinsp; external link Następna ważna paczka to SuperSU - sprawdzamy jaka jest najnowsza wersja: www.chainfire.eu\u0026thinsp; external link , a gdy już to wiemy to googlamy za nią na xda developers lub download.chainfire.eu Aktualna paczka to: UPDATE-SuperSU-v1.55.zip\u0026thinsp; external link Zaktualizowaną paczkę SuperSU pobieramy/wrzucamy na urządzenie. Wyłączamy tablet Włączamy tablet przytrzymując równocześnie przyciski Volume down + Power do czasu aż urządzenie zacznie startować - czekamy aż zobaczymy Bootmanagera (rozgrzebany robot \u0026#x1f603; ) Przechodzimy do katalogu z fastboot i otwieramy tam Command Line (np. przytrzymując Shift na folderze i wybierając opcję Otwórz tutaj okno wiersza poleceń - u mnie: cd D:\\Nexus 7\\4.3\\adt-bundle-windows-x86_64-20130729\\sdk\\platform-tools Wpisujemy polecenie (uwzględniają wersję recovery, którą wgrywamy): fastboot flash recovery recovery-clockwork-touch-6.0.3.6-grouper.img Jeżeli wszystko pójdzie pomyślnie to powinno wypisać coś w rodzaju:\nsending \u0026#39;recovery\u0026#39; (6740 KB)... OKAY [ 0.826s] writing \u0026#39;recovery\u0026#39;... OKAY [ 0.486s] finished. total time: 1.312s Jeżeli zobaczymy: \u0026lt; waiting for device \u0026gt; tzn. że urządzenie nie jest gotowe i można wcisnąć Ctrl+C, może nawet trzeba będzie poszukać na to rozwiązania\u0026hellip; Korzystając z klawiszy Volume down/up wybieramy Recovery Mode i wybór potwierdzamy przyciskiem Power Po załadowaniu CWM\u0026rsquo;a wybieramy kolejno: install zip choose zip from sdcard (lądujemy po tym w /sdcard) i w moim przypadku idę dalej do 0/Download/ wybieramy - UPDATE-SuperSU-v1.55.zip potwierdzamy wybór Yes - Install UPDATE-SuperSU-v1.55.zip po instalacji wybieramy +++++Go Back+++++ następnie reboot system now gdy pojawi się ROM may flash stock recovery on boot. Fix? THIS CAN NOT BE UNDONE. - wybranie YES nadpisze stock\u0026rsquo;owe recovery (właściwie po to instaluję CWM\u0026rsquo;a), ale można też wybrać NO a CWM przepadnie No i brawo - mamy 4.3 build JWR66Y i root\u0026rsquo;a z CWM recovery \u0026#x1f603;\nPo restarcie sprawdzamy czy mamy roota odpalając choćby SuperSU.\nŹródła http://www.info-pc.info/2013/07/how-to-root-nexus-7-android-43-jwr66v.html\u0026thinsp; external link http://forum.xda-developers.com/showthread.php?t=2377511\u0026thinsp; external link http://forum.xda-developers.com/showthread.php?t=1538053\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/09/rootowanie-androida-4-3-na-google-nexus-7-po-aktualizacji-do-jwr66y/","summary":"Ostatnio aktualizowałem swojego Nexusa do 4.3 i było miło tylko mi roota i recovery wystrzeliło\u0026hellip; No ale żaden problem - chwila googlania, kilka poleceń i mam recovery i roota. Dzisiaj wrzuciłem niedużą aktualizację, która łata kilka bugów i znów po root\u0026rsquo;cie ;/\nZapisze sobie więc instrukcję by kolejnym razem już nie googlać \u0026#x1f603;\nP.S. Wiem co robię ryzykując uceglenie swojego urządzenia - u mnie ta instrukcja działa ale nie mogę tego zagwarantować każdemu - dlatego jeśli już się zdecydujesz to ROBISZ TO NA WŁASNĄ ODPOWIEDZIALNOŚĆ!","title":"Root’owanie Androida 4.3 na Google Nexus 7 po aktualizacji do JWR66Y"},{"content":"Niedawno chciałem skopiować maszynę wirtualną z jednego hypervisora na innego. Były to 3 wolumeny LVM o rozmiarach od 50 do 100GB. Dawno temu zrobiłem sobie skrypty do backupu - jeden kompresuje wolumeny LVM - a drugi pozwala odtworzyć z dekompresja na drugim serwerze. Tyle że przy tak dużej maszynce będzie to trwało masakrycznie długo - fajnie byłoby móc równocześnie kopiować i odtwarzać (live)\u0026hellip;\nI wtedy przypomniało mi się narzędzie netcat - zrobiłem snapshoty wolumenów i mogłem zaczynać. W najbardziej podstawowej wersji potrzebowałem tylko tyle:\nna źródle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | nc -l -p 8888 na docelowym: nc 192.168.1.10 8888 | dd of=/dev/vgsas/vm1-sys Lub wariacje z kompresją:\nna źródle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | pigz -2 | nc -l -p 8888 na docelowym: nc 192.168.1.10 8888 | pigz -d | dd of=/dev/vgsas/vm1-sys No dobra - pv nie jest najbardziej podstawowe\u0026hellip; Ale umożliwia podgląd postępu wysyłania/obierania (zależy, z której strony go wrzucić) co przy tak długim procesie jest niezmiernie przydatne.\nDo kompresji zalecałbym pigz (czyli Parallel GZIP) z ratio dostosowanym do przepustowości sieci - po gigabicie się nie opłacało nawet na ośmiordzeniowcu.\n","permalink":"https://gagor.pro/2013/09/kopiowanie-wolumenow-lvm-z-dd-i-netcat/","summary":"Niedawno chciałem skopiować maszynę wirtualną z jednego hypervisora na innego. Były to 3 wolumeny LVM o rozmiarach od 50 do 100GB. Dawno temu zrobiłem sobie skrypty do backupu - jeden kompresuje wolumeny LVM - a drugi pozwala odtworzyć z dekompresja na drugim serwerze. Tyle że przy tak dużej maszynce będzie to trwało masakrycznie długo - fajnie byłoby móc równocześnie kopiować i odtwarzać (live)\u0026hellip;\nI wtedy przypomniało mi się narzędzie netcat - zrobiłem snapshoty wolumenów i mogłem zaczynać.","title":"Kopiowanie wolumenów LVM z dd i netcat"},{"content":"Raz na jakiś czas trzeba coś niestandardowego wrzucić do instalacji w Active Directory a że nie wszystkie aplikacje mają dostępne paczki MSI to trzeba się nieco natrudzić.\nPoniżej wrzucam skrypt, który instaluje GIMP\u0026rsquo;a 2.8 z domyślnego instalatora (wersja InnoSetup) przy okazji odinstalowując wcześniejsze wersje zainstalowane ręcznie.\nZapisujemy poniższy kod jako np. gimp-install.cmd\n@echo off REM Installs GIMP cls echo ---------------------------------------------------- echo . echo . echo . Installing/Updating GIMP - Please Wait echo . echo . echo ---------------------------------------------------- REM Test if actual IF exist \u0026#34;%ProgramFiles%\\GIMP\\bin\\gimp-2.8.exe\u0026#34; GOTO SkipInstall REM Exit the application taskkill.exe /F /FI \u0026#34;IMAGENAME eq gimp-2.8.exe\u0026#34; \u0026gt;nul REM Uninstall existing GIMP version, delete folder if exist \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; /VERYSILENT :: Wait for 20 seconds ping -n 40 127.0.0.1 \u0026gt; NUL if exist \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; rd \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; /Q /S REM Install new version \u0026#34;\\\\serwerplikow.local\\Instalki\\GIMP\\gimp-2.8.4-setup.exe\u0026#34; /VERYSILENT /NORESTART /DIR=\u0026#34;%PROGRAMFILES%\\GIMP 2.8\u0026#34; REM Skip installation if acctuall :SkipInstall REM Return exit code to SCCM exit /B %EXIT_CODE% Tworzymy nową regułkę GPO i zmierzamy do: Computer Configuration\\Policies\\Windows Settings\\Scripts\\Startup\nW nowym okienku wybieramy Show Files\u0026hellip;\nWklejamy plik skryptu do tego folderu i teraz możemy dodać go w tym samym oknie (Add\u0026hellip;) - dzięki wrzuceniu skryptu w tym miejscu będzie się on automatycznie replikować na inne kontrolery. Skrypt będzie co prawda uruchamiany przy każdym starcie komputera ale pierwszy warunek będzie sprawdzać czy aplikacja jest zainstalowana więc nie spowolni to znacznie startu.\nSkrypt znalazłem gdzieś na sieci ale nie mogę namierzyć źródła.\n","permalink":"https://gagor.pro/2013/08/gpo-instalacja-gimpa-2-8/","summary":"Raz na jakiś czas trzeba coś niestandardowego wrzucić do instalacji w Active Directory a że nie wszystkie aplikacje mają dostępne paczki MSI to trzeba się nieco natrudzić.\nPoniżej wrzucam skrypt, który instaluje GIMP\u0026rsquo;a 2.8 z domyślnego instalatora (wersja InnoSetup) przy okazji odinstalowując wcześniejsze wersje zainstalowane ręcznie.\nZapisujemy poniższy kod jako np. gimp-install.cmd\n@echo off REM Installs GIMP cls echo ---------------------------------------------------- echo . echo . echo . Installing/Updating GIMP - Please Wait echo .","title":"GPO: Instalacja GIMP’a 2.8"},{"content":"Narzędzia xz-utils dostępne w nowszych systemach korzystają z mocniejszych algorytmów kompresji (jakaś odmiana LZMA, coś w stylu 7zip\u0026rsquo;a) przy zachowaniu kompatybilności składni poleceń z gzip\u0026rsquo;em/bzip\u0026rsquo;em - da się je zatem łatwo zintegrować w obecnych systemach. Ja chciałem wykorzystać xz do kompresji logów, które bywają przydatne ale przez większość czasy tylko zajmują miejsce :simple_smile:\nW /etc/logrotate.conf dopisujemy:\ncompresscmd /usr/bin/xz uncompresscmd /usr/bin/unxz compressext .xz compressoptions -9T2 compressoptions można nie ustawiać bo domyślnie ma wartość -9 (czyli kompresuj na maxa), mój dodatek (czyli -T2) użyje dwóch wątków procesora gdy już ten mechanizm zostanie zimplementowany (bo na razie nie jest) :simple_smile:\n","permalink":"https://gagor.pro/2013/07/logrotate-kompresja-logow-xz/","summary":"Narzędzia xz-utils dostępne w nowszych systemach korzystają z mocniejszych algorytmów kompresji (jakaś odmiana LZMA, coś w stylu 7zip\u0026rsquo;a) przy zachowaniu kompatybilności składni poleceń z gzip\u0026rsquo;em/bzip\u0026rsquo;em - da się je zatem łatwo zintegrować w obecnych systemach. Ja chciałem wykorzystać xz do kompresji logów, które bywają przydatne ale przez większość czasy tylko zajmują miejsce :simple_smile:\nW /etc/logrotate.conf dopisujemy:\ncompresscmd /usr/bin/xz uncompresscmd /usr/bin/unxz compressext .xz compressoptions -9T2 compressoptions można nie ustawiać bo domyślnie ma wartość -9 (czyli kompresuj na maxa), mój dodatek (czyli -T2) użyje dwóch wątków procesora gdy już ten mechanizm zostanie zimplementowany (bo na razie nie jest) :simple_smile:","title":"logrotate: kompresja logów xz"},{"content":"Na jednym z urządzeń miałem problem z odtworzeniem plików (głównie MKV) z dźwiękiem zakodowanym w DTS. Pomijając że np. na tablecie 6-cio kanały DTS jest mi \u0026ldquo;niezbędny inaczej\u0026rdquo; to konwertując go do AAC stereo plik jest po prostu sporo mniejszy. Oczywiście nie zamierzam transkodować ścieżki video i na moje potrzeby mogłem sobie odpuścić zmianę częstotliwości próbkowania.\nNajprościej wykorzystać pakiet ffmpeg (po nowemu avconv) lub mencoder (choć ten miewał niegdyś problem z poprawnym zapisywaniem wynikowych plików mkv, więc potrzebny jest dodatkowo mkvmerge z pakietu mkvtoolnix). mencoder transkoduje szybciej wykorzystując więcej rdzeniu CPU, ale później potrzebny był drugi przebieg z mkvmerge. ffmpeg jechał na jednym procku nawet z opcją threads ale za to wszystko mogę zrobić jednym poleceniem.\nPaczki instalujemy tak:\napt-get install libav-tools libavcodec-extra MKV DTS do MKV AAC stereo avconv -i input.mkv -c copy -c:a libvo_aacenc -b:a 128k -ac 2 -threads auto output-stereo-aac.mkv Gdybyśmy chcieli zakodować wszystkie kanały z DTS do AAC to wystarczy pominąć parametr -ac 2.\nMKV DTS do MKV AC3 (wszystkie kanały) avconv -i input.mkv -c copy -c:a libvo_aacenc -b:a 128k -threads auto output-ac3.mkv MKV DTS do MKV AC3 z mencoder\u0026rsquo;em AVI=`mktemp video.XXXXXX.avi` mencoder \u0026#34;input.mkv\u0026#34; -o $AVI -oac lavc -lavcopts acodec=ac3:abitrate=448 -ovc copy mkvmerge $AVI -o output.AC3.mkv rm $AVI ","permalink":"https://gagor.pro/2013/07/bezstratna-konwersja-mkv-z-dts-do-ac3-lub-aac/","summary":"Na jednym z urządzeń miałem problem z odtworzeniem plików (głównie MKV) z dźwiękiem zakodowanym w DTS. Pomijając że np. na tablecie 6-cio kanały DTS jest mi \u0026ldquo;niezbędny inaczej\u0026rdquo; to konwertując go do AAC stereo plik jest po prostu sporo mniejszy. Oczywiście nie zamierzam transkodować ścieżki video i na moje potrzeby mogłem sobie odpuścić zmianę częstotliwości próbkowania.\nNajprościej wykorzystać pakiet ffmpeg (po nowemu avconv) lub mencoder (choć ten miewał niegdyś problem z poprawnym zapisywaniem wynikowych plików mkv, więc potrzebny jest dodatkowo mkvmerge z pakietu mkvtoolnix).","title":"Bezstratna konwersja MKV z DTS do AC3 lub AAC"},{"content":"Raz na jakiś czas gdy grzebię przy maciorach muszę \u0026ldquo;odkryć\u0026rdquo; nowy volumen FC (lub rzadziej SCSI), który właśnie utworzyłem a restart serwera nie wchodzi w rachubę (zresztą na części systemów nic on nie da).\nBy to zrobić są dwie możliwości:\nRęczne wydanie poleceń odkrywających volumeny (na jajkach od 2.6.x) Sprawdzamy jakie mamy karty:\nls /sys/class/fc_host/ (wypisze się coś w stylu: host1, host2)\nWydajemy do wybranej przez nas karty żądanie wykonania LIP (to się chyba tłumaczy jako loopback initialization) co skutkuje przeskanowaniem szyny FC:\necho 1 \u0026gt;/sys/class/fc_host/host1/issue_lip Czekamy 15~30 sekund aby zadziałało polecenie.\nRządamy przeskanowania dostępnych volumenów SCSI/FC:\necho - - - \u0026gt;/sys/class/scsi_host/host1/scan (myślniki w echo oznaczają sprawdzenie wszystkich kanałów, targetów i lun\u0026rsquo;ów - jeżeli mamy bardzo dużo volumenów to można tutaj nieco optymalizować, ale to nie był mój problem)\nOdpalamy np. dmesg aby zobaczyć jakie nowe volumeny się pojawiły.\nŹródło: http://misterd77.blogspot.com/2007/12/how-to-scan-scsi-bus-with-26-kernel.html\u0026thinsp; external link Korzystamy ze skryptu rescan-scsi-bus.sh Skrypt ten automatycznie robi to co potrzebujemy, skanując wszystkie karty FC pod kątem nowych volumenów.\nwget http://rescan-scsi-bus.sh/ -O rescan-scsi-bus.sh chmod +x rescan-scsi-bus.sh ./rescan-scsi-bus.sh I tyle!\nŹródło: http://rescan-scsi-bus.sh/\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/07/dodawanie-urzadzen-scsifc-bez-restartu-serwera/","summary":"Raz na jakiś czas gdy grzebię przy maciorach muszę \u0026ldquo;odkryć\u0026rdquo; nowy volumen FC (lub rzadziej SCSI), który właśnie utworzyłem a restart serwera nie wchodzi w rachubę (zresztą na części systemów nic on nie da).\nBy to zrobić są dwie możliwości:\nRęczne wydanie poleceń odkrywających volumeny (na jajkach od 2.6.x) Sprawdzamy jakie mamy karty:\nls /sys/class/fc_host/ (wypisze się coś w stylu: host1, host2)\nWydajemy do wybranej przez nas karty żądanie wykonania LIP (to się chyba tłumaczy jako loopback initialization) co skutkuje przeskanowaniem szyny FC:","title":"Dodawanie urządzeń SCSI/FC bez restartu serwera"},{"content":"Bardzo spodobała mi się nowa aplikacja do zdalnej synchronizacji folderów z wykorzystaniem P2P. Ja wykorzystałem ją do automatycznych backupów archiwum zdjęć - zebrało mi się tego już prawie 130GB! Każde narzędzie, które chce np. je kompresować i raz na czas robić FULL backup jest skazane na porażkę - a fotek przybywa.\nNa początek pobieramy interesującą nas wersję:\nwget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_i386.tar.gz wget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_x64.tar.gz Oraz paczkę ze skryptami dla Debiana:\nwget http://www.yeasoft.com/downloads/various/btsync-linux-deploy.tar.gz Ja wyciągam z niej skrypt /etc/init.d/btsync i konfigurację /etc/*\nTeraz user z ograniczonymi uprawnieniami, na którym będzie działać nasza usługa (można wykorzystać swojego usera aby móc synchronizować swoje foldery):\nuseradd -d /var/lib/btsync -m -s /bin/sh btsync Odpalamy z domyślną konfiguracją żeby tylko działało - dokonfiguruje później:\ncd /etc/btsync mv default.conf default.btsync.btsync.conf chown root:btsync default.btsync.btsync.conf chmod 640 default.btsync.btsync.conf service btsync start Powinno działać więc sprawdźmy:\nps axu | grep bts netstat -nlp Tadam!\n","permalink":"https://gagor.pro/2013/07/debian-instalacja-bittorrent-sync-btsync/","summary":"Bardzo spodobała mi się nowa aplikacja do zdalnej synchronizacji folderów z wykorzystaniem P2P. Ja wykorzystałem ją do automatycznych backupów archiwum zdjęć - zebrało mi się tego już prawie 130GB! Każde narzędzie, które chce np. je kompresować i raz na czas robić FULL backup jest skazane na porażkę - a fotek przybywa.\nNa początek pobieramy interesującą nas wersję:\nwget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_i386.tar.gz wget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_x64.tar.gz Oraz paczkę ze skryptami dla Debiana:\nwget http://www.yeasoft.com/downloads/various/btsync-linux-deploy.tar.gz Ja wyciągam z niej skrypt /etc/init.","title":"Debian - Instalacja Bittorrent Sync (btsync)"},{"content":"Jest kilka powodów dla których tworzenie patchy jest przydatne - jeśli tu jesteś to pewnie masz jakiś własny\u0026hellip;\nTworzenie patch\u0026rsquo;a diff -crB old new \u0026gt; from-old-to-new.patch W powyższym poleceniu założyłem że old i new to katalogi z wieloma podkatalogami i plikami - stąd opcja -r. -c dodaje kilka linijek \u0026ldquo;kontekstu\u0026rdquo; przez co łatwiej rozeznać się w patch\u0026rsquo;u. Opcja -B ignoruje puste linie, których patchowanie mnie nie interesuje.\nPatchowanie Na początek zawsze warto wywołać polecenie z opcją -dry-run by zobaczyć czy patch wykona się poprawnie:\npatch --dry-run -p1 -i from-old-to-new.patch Opcja -p1 zakłada że uruchamiamy patcha z folderu \u0026ldquo;projektu\u0026rdquo;, który chcemy patchować. Opcję -i można z powodzeniem zastąpić przekierowaniem \u0026lt; .\nJeżeli nie widzimy żadnych komunikatów \u0026ldquo;FAILED\u0026rdquo; to możemy uruchomić patcha:\npatch -p1 -i from-old-to-new.patch ","permalink":"https://gagor.pro/2013/04/tworzenie-patchy-z-poleceniami-diff-i-patch/","summary":"Jest kilka powodów dla których tworzenie patchy jest przydatne - jeśli tu jesteś to pewnie masz jakiś własny\u0026hellip;\nTworzenie patch\u0026rsquo;a diff -crB old new \u0026gt; from-old-to-new.patch W powyższym poleceniu założyłem że old i new to katalogi z wieloma podkatalogami i plikami - stąd opcja -r. -c dodaje kilka linijek \u0026ldquo;kontekstu\u0026rdquo; przez co łatwiej rozeznać się w patch\u0026rsquo;u. Opcja -B ignoruje puste linie, których patchowanie mnie nie interesuje.\nPatchowanie Na początek zawsze warto wywołać polecenie z opcją -dry-run by zobaczyć czy patch wykona się poprawnie:","title":"Tworzenie patch’y z poleceniami diff i patch"},{"content":"Miałem ostatnio zabawną sytuację gdy kilka serwerów z zainstalowanym NTPD miało rozjazdy rzędu kilkunastu sekund. Wyszło na to że moje serwery synchronizowały się z różnymi zewnętrznymi serwerami NTP pomiędzy, którymi były rozjazdy i te rozjazdy synchronizowały się na moich serwerach. Jeden \u0026ldquo;z moich\u0026rdquo; ustanowiłem głównym a wszystkie inne przekierowałem na niego (komentując wszystkie inne serwery NTP w konfiguracji). Wymusiłem synchronizację:\nntp -q Sprawdziłem jak duży jest offset i jitter (powinny być bardzo małe):\nntpq -p i tak rzeczywiście było - problem z głowy.\nP.S. Jeżeli nie potrzebujemy/chcemy/etc instalować serwera NTP to możemy zamiast tego użyć polecenia ntpdate w cronie (np. co godzinę):\nntpdate moj.serwer.ntp ","permalink":"https://gagor.pro/2013/03/rozsynchronizowane-serwery-ntp/","summary":"Miałem ostatnio zabawną sytuację gdy kilka serwerów z zainstalowanym NTPD miało rozjazdy rzędu kilkunastu sekund. Wyszło na to że moje serwery synchronizowały się z różnymi zewnętrznymi serwerami NTP pomiędzy, którymi były rozjazdy i te rozjazdy synchronizowały się na moich serwerach. Jeden \u0026ldquo;z moich\u0026rdquo; ustanowiłem głównym a wszystkie inne przekierowałem na niego (komentując wszystkie inne serwery NTP w konfiguracji). Wymusiłem synchronizację:\nntp -q Sprawdziłem jak duży jest offset i jitter (powinny być bardzo małe):","title":"Rozsynchronizowane serwery NTP"},{"content":"Do niedawna na moim telefonie VPN\u0026rsquo;ami były: PPTP lub L2TP - oba niespecjalnie mi się podobały. Ale od wersji 4-tej pojawiły się dwa nowe tryby: IPSec Xauth PSK i IPSec Xauth RSA. W pierwszym autoryzacja wykorzystuje login i hasło, w drugim certyfikaty.\nTryb IPSec Xauth PSK jest bardzo wygodny bo łatwo można połączyć go z zewnętrznymi mechanizmami uwierzytelniającymi np. LDAP, Active Directory, itp.\nPokażę jak skonfigurować swojego Fortigate\u0026rsquo;a by umożliwić połączenie z telefonów i tabletów na Androidzie 4.x do \u0026ldquo;Intranetu\u0026rdquo;. Większość konfiguracji można przeprowadzić tylko w trybie CLI - zakładam że wiesz jak to zrobić. To co wygodniej można zrobić w trybie WWW to głównie tworzenie reguł dostępu na zaporze.\nP.S. Teoretycznie powinno to też zadziałać na IPhone\u0026rsquo;ach/IPad\u0026rsquo;ach itp., przy czym udało mi się to zestawić na 4GS ale na 5-ce już nie - nie miałem tych aparatów na tyle długo by dokładniej to zbadać.\nNa początek konfigurujemy fazę pierwszą - ja lubię interface mode ale oczywiście będzie to działać również w trybie policy (trzeba będzie nieco polecenia zmienić).\nconfig vpn ipsec phase1-interface edit \u0026#34;vpn-android\u0026#34; set type dynamic set interface \u0026#34;port1\u0026#34; set dhgrp 2 set peertype one set xauthtype auto set mode aggressive set mode-cfg enable set proposal aes128-sha1 aes128-md5 3des-sha1 set negotiate-timeout 15 set peerid \u0026#34;jakis-mobilny-identyfikator\u0026#34; set authusrgrp \u0026#34;grupa-ludzikow\u0026#34; set psksecret haslo set unity-support enable set ipv4-start-ip 172.16.0.5 set ipv4-end-ip 172.16.0.100 set ipv4-netmastk 255.255.255.0 set ipv4-dns-server1 172.16.0.1 set ipv4-dns-server2 8.8.8.8 next end Jedna ciekawostka - opcja unity-support domyślnie jest ustawiana na enable i proponuję ją tak zostawić - dzięki temu ta sama konfiguracja będzie działać na urządzeniach z iOS\u0026rsquo;e i OS X, wystarczy skonfigurować VPN jako Cisco \u0026#x1f603;\nNo to faza druga:\nconfig vpn ipsec phase2-interface edit \u0026#34;mobile-vpn-p2\u0026#34; set keepalive enable set pfs disable set phase1name \u0026#34;vpn-android\u0026#34; set proposal aes128-sha1 aes128-md5 3des-sha1 set keylifeseconds 3600 next end Kilka ustawień na interfejsie:\nconfig system interface edit \u0026#34;vpn-android\u0026#34; set ip 172.16.0.1 255.255.255.255 set allowaccess ping set type tunnel set remote-ip 139.x.x.10 set interface \u0026#34;port1\u0026#34; next end Włączymy sobie serwer DNS na interfejsie VPN:\nconfig system dns-server edit \u0026#34;vpn-android\u0026#34; next end No to teraz trzeba utworzyć co najmniej jedną regułę na zaporze zezwalającą na dostęp z interfejsu vpn-android (z adresów tej sieci) do jakiejś sieci intranetowej (np. wewnętrznego serwera WWW), na odpowiednich portach.\nPonadto jeśli chcemy móc korzystać z cache DNS\u0026rsquo;a na interfejsie vpn-android to musimy umożliwić do niego dostęp tworząc regułę z interfejsu vpn-android (z adresów przydzielanych przez mode-cfg) do vpn-android (przynajmniej do DNS\u0026rsquo;a - 172.16.0.1).\nJeżeli chcemy by urządzenia mobilne mogły łączyć się do zewnętrznych usług przez tunel VPN (co w niektórych przypadkach może być bardzo cenne) to musimy też utworzyć regułę z interfejsu vpn-android do któregoś z interfejsów WAN\u0026rsquo;owskich.\nA tak należy to wyklikać na telefonie\nhttp://kb.fortinet.com/kb/viewContent.do?externalId=FD31619\u0026sliceId=1\u0026thinsp; external link ","permalink":"https://gagor.pro/2013/03/fortigate-vpn-ipsec-psk-xauth-z-androida-4-x/","summary":"Do niedawna na moim telefonie VPN\u0026rsquo;ami były: PPTP lub L2TP - oba niespecjalnie mi się podobały. Ale od wersji 4-tej pojawiły się dwa nowe tryby: IPSec Xauth PSK i IPSec Xauth RSA. W pierwszym autoryzacja wykorzystuje login i hasło, w drugim certyfikaty.\nTryb IPSec Xauth PSK jest bardzo wygodny bo łatwo można połączyć go z zewnętrznymi mechanizmami uwierzytelniającymi np. LDAP, Active Directory, itp.\nPokażę jak skonfigurować swojego Fortigate\u0026rsquo;a by umożliwić połączenie z telefonów i tabletów na Androidzie 4.","title":"Fortigate - VPN IPSec PSK XAuth z Android’a 4.x"},{"content":"On Debian you have to install nginx-extras package (because it have built in headers_more\u0026thinsp; external link module). Then you need two options (best in global configuration /etc/nginx/nginx.conf file, http part):\nserver_tokens off; more_set_headers \u0026#39;Server: BadAss\u0026#39;; And it\u0026rsquo;s good to setup non standard error pages on every site (500 and 404 at minimum):\nerror_page 403 404 http://mysite.com/areyoulost; error_page 502 503 504 /500.html; ","permalink":"https://gagor.pro/2013/01/nginx-hide-server-version-and-name-in-server-header-and-error-pages/","summary":"On Debian you have to install nginx-extras package (because it have built in headers_more\u0026thinsp; external link module). Then you need two options (best in global configuration /etc/nginx/nginx.conf file, http part):\nserver_tokens off; more_set_headers \u0026#39;Server: BadAss\u0026#39;; And it\u0026rsquo;s good to setup non standard error pages on every site (500 and 404 at minimum):\nerror_page 403 404 http://mysite.com/areyoulost; error_page 502 503 504 /500.html; ","title":"Nginx - hide server version and name in Server header and error pages"},{"content":"W PHP 5.3 pojawiła się nowa zmienna: max_input_vars, która limituje ilość pól możliwych do przesłania przez formularz, obcinając nadmiarowe. Pozwala to zapobiec atakom DoS na tablice hashujące (przynajmniej w tym jednym miejscu). Domyślna wartość tej zmiennej to 1000 i kreatywnym programistom udaje się tą wartość bez problemu osiągnąć \u0026#x1f603;\nWarte odnotowania jest to że mając suhosin\u0026rsquo;a trzeba pamiętać o jeszcze dwóch innych zmiennych:\nmax_input_vars = 3000 suhosin.post.max_vars = 3000 suhosin.request.max_vars = 3000 Zmienne można zmienić od razu w /etc/php5/apache2/php.ini (choć te dla suhosin\u0026rsquo;a lepiej wrzucić do /etc/php5/conf.d/suhosin.ini). A jeszcze lepszym pomysłem jest ustawienie tych zmiennych bezpośrednio dla danego vhost\u0026rsquo;a, w Apache\u0026rsquo;m np. tak:\nphp_value max_input_vars 3000 php_value suhosin.post.max_vars 3000 php_value suhosin.request.max_vars 3000 ","permalink":"https://gagor.pro/2013/01/php-max_input_vars/","summary":"W PHP 5.3 pojawiła się nowa zmienna: max_input_vars, która limituje ilość pól możliwych do przesłania przez formularz, obcinając nadmiarowe. Pozwala to zapobiec atakom DoS na tablice hashujące (przynajmniej w tym jednym miejscu). Domyślna wartość tej zmiennej to 1000 i kreatywnym programistom udaje się tą wartość bez problemu osiągnąć \u0026#x1f603;\nWarte odnotowania jest to że mając suhosin\u0026rsquo;a trzeba pamiętać o jeszcze dwóch innych zmiennych:\nmax_input_vars = 3000 suhosin.post.max_vars = 3000 suhosin.request.max_vars = 3000 Zmienne można zmienić od razu w /etc/php5/apache2/php.","title":"PHP - max_input_vars"},{"content":"Dyski się zużywają i w końcu wcześniej czy później pojawiają się na nich bad sectory. Jeden z moich dysków ciut się posypał a że służy wyłącznie do backupów to mogę z tym żyć. Ale z drugiej strony jeżeli już będę musiał sięgnąć do backupów to chcę mieć pewność że coś odzyskam, dlatego postanowiłem zrobić kilka testów. Nawet jeśli nie naprawi to sektorów to przynajmniej zostaną zaznaczone jako uszkodzone i realokowane.\nNa początek zacząłem od próby puszczenia badblocks w trybie nie destruktywnym na całym dysku:\nbadblocks -b 4096 -nsv /dev/sdf Poszukiwanie wadliwych bloków w trybie z niedestruktywnym zapisem Od bloku 0 do 488378645 Poszukiwanie wadliwych bloków (odczyt i niedestruktywny zapis) Testowanie wzorcem losowym: ^Ctowe w 1.28%, minęło 10:47 (błędów: 0/0/0) Interrupted at block 6262912 Otrzymano przerwanie, sprzątam Zaczekałem 10 minut i na podstawie bieżących statystyk oszacowałem że test zajmie ponad 13 godzin\u0026hellip; Jak widać przerwałem - sprawdźmy więc co powiedzą testy smart\u0026rsquo;a. Co prawda niczego one nie naprawią ale zidentyfikują, na którym sektorze zaczynają się problemy. Będę mógł wtedy puścić badblocks już od tego miejsca oszczędzając nieco czasu\u0026hellip; Więc:\nsmartctl -t long /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34;. Drive command \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34; successful. Testing has begun. Please wait 255 minutes for test to complete. Test will complete after Sat Nov 24 03:59:21 2012 Use smartctl -X to abort test. 255 minut to trochę ponad 4 godziny - przynajmniej skończy się do rana. No to jeszcze:\nshutdown -h 4:15 I zerkniemy jutro co znajdzie test \u0026#x1f603;\nDzień drugi Zerkamy\u0026hellip; i:\nsmartctl -l selftest /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Extended offline Completed: read failure 90% 15196 164529056 # 2 Short offline Aborted by host 10% 15192 - Widać tutaj wyniki dwóch testów - przy czym interesuje nas ten dłuższy z numerkiem 1. Niestety mam jakiś bad sektor i mam pierwszy adres jego wystąpienia - szkoda że to gdzieś w pierwszych 10% dysku - miałem nadzieję że gdzieś dalej (byłoby mniej do sprawdzenia). Ale jeśli badsectory zaczynają się tak wcześnie to puszczę badblocks od początku - niech przejedzie cały dysk - albo się coś naprawi, albo padnie. Ale przynajmniej sprawa się wyjaśni 😉\nŹródła http://smartmontools.sourceforge.net/badblockhowto.html\u0026thinsp; external link http://sourceforge.net/apps/trac/smartmontools/wiki/SamsungF4EGBadBlocks\u0026thinsp; external link man badblocks\n","permalink":"https://gagor.pro/2012/12/linux-naprawianie-bad-sectorow/","summary":"Dyski się zużywają i w końcu wcześniej czy później pojawiają się na nich bad sectory. Jeden z moich dysków ciut się posypał a że służy wyłącznie do backupów to mogę z tym żyć. Ale z drugiej strony jeżeli już będę musiał sięgnąć do backupów to chcę mieć pewność że coś odzyskam, dlatego postanowiłem zrobić kilka testów. Nawet jeśli nie naprawi to sektorów to przynajmniej zostaną zaznaczone jako uszkodzone i realokowane.","title":"Linux - naprawianie bad sectorów"},{"content":" Getting Things DoneCzyli sztuka bezstresowej efektywności\nAuthor: David Allen\namazon.pl ","permalink":"https://gagor.pro/books/2012/getting-things-done/","summary":" Getting Things DoneCzyli sztuka bezstresowej efektywności\nAuthor: David Allen\namazon.pl ","title":"Getting Things Done"},{"content":"Korzystam z instancji Piwik\u0026rsquo;a do monitorowania odwiedzin na stronie i postanowiłem pokombinować czy da się w ten sposób monitorować wejścia konkretnych osób na bazie wpisanego w polu komentarza loginu/ksywki. Jak zacząłem grzebać to przy okazji zmieniłem też sposób ładowania skryptów Piwika na asynchroniczny.\nA leci to mniej więcej tak:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _paq = _paq || []; (function(){ var u=((\u0026#34;https:\u0026#34; == document.location.protocol) ? \u0026#34;https://url.instancji.piwika.pl/\u0026#34; : \u0026#34;http://url.instancji.piwika.pl/\u0026#34;); _paq.push([\u0026#39;setSiteId\u0026#39;, 1]); _paq.push([\u0026#39;setTrackerUrl\u0026#39;, u+\u0026#39;piwik.php\u0026#39;]); _paq.push([\u0026#39;setCustomVariable\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;Author\u0026#39;, comment_author]); _paq.push([\u0026#39;trackPageView\u0026#39;]); _paq.push([\u0026#39;enableLinkTracking\u0026#39;]); var d=document, g=d.createElement(\u0026#39;script\u0026#39;), s=d.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; g.type=\u0026#39;text/javascript\u0026#39;; g.defer=true; g.async=true; g.src=u+\u0026#39;piwik.js\u0026#39;; s.parentNode.insertBefore(g,s); })(); \u0026lt;/script\u0026gt; Źródło http://piwik.org/docs/javascript-tracking/#toc-asynchronous-tracking\u0026thinsp; external link http://codex.wordpress.org/WordPress_Cookies#Commenters\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/12/piwik-sledzenie-asynchroniczne-logowanie-ksywy-komentujacego-w-wordpressie/","summary":"Korzystam z instancji Piwik\u0026rsquo;a do monitorowania odwiedzin na stronie i postanowiłem pokombinować czy da się w ten sposób monitorować wejścia konkretnych osób na bazie wpisanego w polu komentarza loginu/ksywki. Jak zacząłem grzebać to przy okazji zmieniłem też sposób ładowania skryptów Piwika na asynchroniczny.\nA leci to mniej więcej tak:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.","title":"Piwik: śledzenie asynchroniczne + logowanie ksywy komentującego w WordPress’ie"},{"content":"Dawno, dawno temu\u0026hellip; Za górami, za lasami\u0026hellip; czytałem sobie tekst Lemat\u0026rsquo;a o dokuczaniu spamerom i pomyślałem że sam też tak mogę i nawet chcę więc popełniłem skrypcik, który dla losowych słów generował maile. Skrypcik działał z dwa lata na mojej poprzedniej stronie i nie raz zdarzyło się tam jakiejś mendzie zapętlić. Jakoś nie miałem czasu od razu, a później zapomniałem wrzucić go na nową stronie i tak zostało - na pewien czas.\nNiedawno przeglądając logi zauważyłem że jakieś spam-boty jednak tęsknią za tą podstroną i postanowiłem ją wskrzesić.\nPomysł jest prosty i polega na generowaniu na pewnej podstronie dużej ilości \u0026ldquo;z dupy\u0026rdquo; maili. Oczywiście w robots.txt dajemy Disallow dla tej ścieżki i jeśli bot jest kulturalny to nie będzie tam zaglądał. Crawlery spamerów przeważnie kulturalne nie są więc tam wpadną i się zapętlą zapychając sobie bazę śmieciami - taka baza traci wartość dla klientów spamera, więc przeważnie skutkuje to dodaniem naszej strony na spamerską \u0026ldquo;black listę - nie crawlować\u0026rdquo;. Dla mnie cool \u0026#x1f603; (w rzadszych przypadkach spamer może chcieć się zemścić\u0026hellip;).\nJeżeli ktoś będzie zainteresowany to mogę udostępnić skrypcik (ma postać szablonu strony dla WordPress\u0026rsquo;a) - choć zachęcam do samodzielnego napisania własnego - zawsze trudniej będzie spamerom z automatu go przeskoczyć (jak np. antyspam.pl). Należy przy tym zwrócić uwagę na kilka rzeczy:\nnie zrób sobie kuku - nie generuj fake emaili dla własnych, istniejących domen (bo może się okazać że któryś spamer jednak spróbuje wysłać te setki tysięcy błędnych maili i zrobić Ci DDOS\u0026rsquo;a zabijając serwer błędnie nadanymi mailami), nie rób innym kuku - nie generuj fake maili w cudzych domenach (no chyba że w domenach Canadian Pharmacy), jeżeli jednak nadal upierasz się przy istniejącej domenie (np. by śledzić statystyki zapytań\u0026hellip;) to ustaw rekord MX na hosta z IP 127.0.0.1 - jak dobrze pójdzie spamer sam sobie zrobi DOS\u0026rsquo;a (spodziewałbym się jednak w takiej sytuacji odwetu) :simple_smile: Źródło szablonu Paczkę z szablonem można pobrać tutaj.\nPaczka zawiera plik havefun_template.php, który należy umieścić w katalogu szablonu WordPress\u0026rsquo;a. Ja wykorzystałem w generatorze wp_post z WordPress\u0026rsquo;a - zmodyfikowania zapytania dla innej bazy nie powinno być zbyt trudne. Samo zapytanie nieco zoptymalizowałem przez co zwraca mniej losowe wyniki (zwraca losowy blok n wierszy z tabeli a nie całkiem losowe elementy) co dla potrzeb tego skryptu jest całkiem OK. Jeżeli pomysł z bazą się nie podoba (bo np. może generować zbyt duże obciążenie) to proponuję wyszukać sobie jakiś generator Lorem ipsum\u0026hellip;\nMiłej zabawy.\nBawiąc się ostatnio nowym szablonem Twenty Thirteen przygotowałem nową wersję pliku havefun_template.php dla tego szablonu.\n","permalink":"https://gagor.pro/2012/12/jak-dokuczac-spamerom/","summary":"Dawno, dawno temu\u0026hellip; Za górami, za lasami\u0026hellip; czytałem sobie tekst Lemat\u0026rsquo;a o dokuczaniu spamerom i pomyślałem że sam też tak mogę i nawet chcę więc popełniłem skrypcik, który dla losowych słów generował maile. Skrypcik działał z dwa lata na mojej poprzedniej stronie i nie raz zdarzyło się tam jakiejś mendzie zapętlić. Jakoś nie miałem czasu od razu, a później zapomniałem wrzucić go na nową stronie i tak zostało - na pewien czas.","title":"Jak dokuczać spamerom"},{"content":"Ruski serwer WWW\u0026thinsp; external link ma przydatną funkcję serwowania wersji plików skompresowanych gzip\u0026rsquo;em - przez co możemy plik skompresować raz i będzie on serwowany klientom obsługującym kompresję HTTP ale już bez każdorazowego kompresowania go. Jest to bardzo przydatne na stronach z dużym ruchem gdzie można w ten sposób zaoszczędzić takty CPU na właściwą obsługę połączeń a nie kompresję. Drugie miejsce gdzie może to być przydatne to VPS\u0026rsquo;y i \u0026ldquo;cienkie\u0026rdquo; serwery, które na kompresji przy większym obciążeniu spędzają zbyt dużo czasu i daje się to odczuć w działaniu strony. A że obecnie standardem jest ładowanie przez stronki np. jQuery, paru pluginów do niego, jQueryUI, masy CSS\u0026rsquo;ów, itd - to na prawdę jest co kompresować \u0026#x1f603;\nDodatkowa zaleta tego rozwiązania jest taka, że o ile w przypadku kompresji online raczej kompresujemy na niskich poziomach by nieco zmniejszyć dane a równocześnie nie zabijać CPU - to w przypadku kompresji pod ten moduł możemy raz na jakiś czas \u0026ldquo;przejechać\u0026rdquo; pliki maksymalną kompresją by były jeszcze mniejsze (zaoszczędzi to nieco pasma i równocześnie przyspieszy ładowanie strony bo jest mniej do pobrania).\nPrekompresję wybranych typów plików można przeprowadzić np. tak:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; gzip skompresuje a touch zadba by timestampy obu plików były takie same (co jest zalecaną praktyką przy wykorzystaniu tego modułu). Dwa powyższe polecenia można odpalić praktycznie w ciemno na obecnie serwowanej stronie i nie powinny spowodować zbyt dużego chaosu\u0026hellip; Trzeba pamiętać jedynie by zostały uruchomione z użytkownika, który ma dostęp do zapisu w danym vhoście i by serwer WWW miał dostęp do tak utworzonego pliku.\nZ moich obserwacji wynika że gdy timestamp pliku bez gz jest nowszy niż wersji skompresowanej to serwowana jest ta świeższa wersja ale wtedy już z ewentualną kompresją online.\nMocniejsze kompresowanie plików PNG Może nie do końca w temacie tego posta ale bardzo często gdy stosuję gzip_static_module to optymalizuję też PNG\u0026rsquo;i. Pliki PNG są kompresowane w sposób bezstratny ale przeważnie programy graficzne nie zapisują ich w najmniejszej możliwej postaci bo trwałoby to zbyt długo. Można użyć do tego celu narzędzi takich jak: optipng lub pngcrush. A poniżej odpowiedni one-liner by skompresować wszystkie PNG\u0026rsquo;i w serwisach WWW:\nfind /var/www -iname *.png -print0 |xargs -0 optipng -quiet -preserve -o7 Uwaga! Wywołanie tego skryptu może potrwać bardzo długo i z tego powodu raczej nie nadaje się do umieszczenia w cronie. Jednak ręczne puszczenie po dużych zmianach w serwisie lub z filtrem w find by kompresowane były tylko nowsze pliki będzie OK.\nOptymalizowanie plików JPG/JPEG Co prawda pliki JPG/JPEG kompresują się stratnie ale możne je nieco zoptymalizować np. usuwając niepotrzebne dane z nagłówków i zamieniając na progresywne. Możemy do tego wykorzystać takie polecenie:\nfind /var/www -iname \u0026#39;*.jpg\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;jpegtran -optimize -progressive -copy none -outfile \u0026#34;{}\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; ","permalink":"https://gagor.pro/2012/12/nginx-kompresowanie-plikow-dla-gzip_static/","summary":"Ruski serwer WWW\u0026thinsp; external link ma przydatną funkcję serwowania wersji plików skompresowanych gzip\u0026rsquo;em - przez co możemy plik skompresować raz i będzie on serwowany klientom obsługującym kompresję HTTP ale już bez każdorazowego kompresowania go. Jest to bardzo przydatne na stronach z dużym ruchem gdzie można w ten sposób zaoszczędzić takty CPU na właściwą obsługę połączeń a nie kompresję. Drugie miejsce gdzie może to być przydatne to VPS\u0026rsquo;y i \u0026ldquo;cienkie\u0026rdquo; serwery, które na kompresji przy większym obciążeniu spędzają zbyt dużo czasu i daje się to odczuć w działaniu strony.","title":"Nginx - kompresowanie plików dla gzip_static"},{"content":"Gdy już się dorobi systemu Active Directory wygodnie jest wykorzystać jego bazę użytkowników do autoryzacji w różnych miejscach, np. do pewnych \u0026ldquo;tajnych i tajniejszych\u0026rdquo; stron w Apache. Najprościej można to zrobić z wykorzystaniem LDAP.\nWarto sprawdzić czy i jak możemy dostać się do kontrolerów. Gdy już mamy wszystkie potrzebne parametry konfigurujemy Apachego - na początek aktywujemy moduły:\na2enmod ldap a2enmod authnz_ldap Teraz możemy edytujemy globalny plik konfiguracyjny mod_ldap\u0026rsquo;a by ustawić nieco cache\u0026rsquo;y (bardzo przydatne). Wartości można dostosować do potrzeb ale przykładowe powinny wystarczyć na początku:\nLDAPSharedCacheSize 500000 LDAPCacheEntries 1024 LDAPCacheTTL 600 LDAPOpCacheEntries 1024 LDAPOpCacheTTL 600 \u0026lt;Location /ldap-status\u0026gt; SetHandler ldap-status Order deny,allow Deny from all # Allow from 127.0.0.1 ::1 Allow from 192.168.1.15/32 Satisfy all \u0026lt;/Location\u0026gt; Warto zwrócić uwagę na drugą część - można aktywować podgląd statusu mod_ldap\u0026rsquo;a co bywa przydatne na etapie testów (np. by zobaczyć kto aktualnie jest zalogowany) - ja umożliwiłem dostęp ze swojego zdalnego komputera. Teraz restart Apachego aby wszystko się załadowało:\ninvoke-rc.d apache2 restart Środowisko mamy gotowe, możemy skonfigurować vhosta. Poniższe opcje wrzucamy np. do klauzuli \u0026lt;Directory\u0026gt; lub \u0026lt;Location\u0026gt;:\nAuthtype basic Authname \u0026#34;Zaloguj sie kontem domenowym\u0026#34; AuthBasicProvider ldap AuthzLDAPAuthoritative Off AuthUserFile /dev/null AuthLDAPUrl \u0026#34;ldap://server1.nazwadomeny.local:389 10.0.0.32:389/DC=nazwadomeny,DC=local?sAMAccountName?sub?(objectClass=person)\u0026#34; NONE AuthLDAPBindDN \u0026#34;CN=kontodomenowe,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; AuthLDAPBindPassword \u0026#34;haslo konta domenowego\u0026#34; Require valid-user # Require ldap-dn ou=Jakies przykladowe OU Najważniejsze parametry to:\nAuthLDAPUrl\u0026thinsp; external link - tutaj podajemy parametry do połączenia LDAP, w pierwszej części możemy podać kilka serwerów by mieć backup, później korzeń od którego będzie rozpoczynane przeszukiwanie drzewa (tutaj cała domena), na końcu filtry, AuthLDAPBindDN - konto którym autoryzujemy się do kontrolera by móc przeszukiwać na nim obiekty (podane jako ścieżka LDAP), AuthLDAPBindPassword - hasło do powyższego konta, Require ldap-dn\u0026thinsp; external link - można nie tylko sprawdzać czy użytkownik jest prawidłowy ale również czy jest w pewnym OU, grupie itd\u0026hellip; Teoretycznie wszystko jest poprawnie ale za cholerę nie chciało działać dopiero dodanie do /etc/ldap/ldap.conf opcji: REFERRALS off sprawiło że autoryzacja działa poprawnie - oneliner by to uzyskać:\necho \u0026#34;REFERRALS off\u0026#34; \u0026gt;\u0026gt; /etc/ldap/ldap.conf Ja mam Apachego 2.2 i jest to jedyna opcja by uzyskać ten efekt. W wersji od 2.4 jest dodatkowa opcja LDAPReferrals\u0026thinsp; external link , która pozwala na zmianę tego zachowania wprost z konfiguracji Apachego.\n","permalink":"https://gagor.pro/2012/12/apache-mod_authnz_ldap-z-active-directory/","summary":"Gdy już się dorobi systemu Active Directory wygodnie jest wykorzystać jego bazę użytkowników do autoryzacji w różnych miejscach, np. do pewnych \u0026ldquo;tajnych i tajniejszych\u0026rdquo; stron w Apache. Najprościej można to zrobić z wykorzystaniem LDAP.\nWarto sprawdzić czy i jak możemy dostać się do kontrolerów. Gdy już mamy wszystkie potrzebne parametry konfigurujemy Apachego - na początek aktywujemy moduły:\na2enmod ldap a2enmod authnz_ldap Teraz możemy edytujemy globalny plik konfiguracyjny mod_ldap\u0026rsquo;a by ustawić nieco cache\u0026rsquo;y (bardzo przydatne).","title":"Apache: mod_authnz_ldap z Active Directory"},{"content":"Chciałem wysłać z Python\u0026rsquo;a maila z krzakami tab by ładnie się wyświetlały i okazało się to całkiem nietrywialne.\nNa szczęście googiel podpowiedział mi doskonałego gotowca, którego zamierzam zapisać by mi nie zginął:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import smtplib from email.mime.text import MIMEText from email.Header import Header from email.Utils import parseaddr, formataddr def send_email(sender, recipient, subject, body): \u0026#34;\u0026#34;\u0026#34;Send an email. All arguments should be Unicode strings (plain ASCII works as well). Only the real name part of sender and recipient addresses may contain non-ASCII characters. The email will be properly MIME encoded and delivered though SMTP to localhost port 25. This is easy to change if you want something different. The charset of the email will be the first one out of US-ASCII, ISO-8859-1 and UTF-8 that can represent all the characters occurring in the email. \u0026#34;\u0026#34;\u0026#34; # Header class is smart enough to try US-ASCII, then the charset we # provide, then fall back to UTF-8. header_charset = \u0026#39;ISO-8859-2\u0026#39; # We must choose the body charset manually for body_charset in \u0026#39;US-ASCII\u0026#39;, \u0026#39;UTF-8\u0026#39;, \u0026#39;ISO-8859-2\u0026#39;: try: body.encode(body_charset) except UnicodeError: pass else: break # Split real name (which is optional) and email address parts sender_name, sender_addr = parseaddr(sender) recipient_name, recipient_addr = parseaddr(recipient) # We must always pass Unicode strings to Header, otherwise it will # use RFC 2047 encoding even on plain ASCII strings. sender_name = str(Header(unicode(sender_name), header_charset)) recipient_name = str(Header(unicode(recipient_name), header_charset)) # Make sure email addresses do not contain non-ASCII characters sender_addr = sender_addr.encode(\u0026#39;ascii\u0026#39;) recipient_addr = recipient_addr.encode(\u0026#39;ascii\u0026#39;) # Create the message (\u0026#39;plain\u0026#39; stands for Content-Type: text/plain) msg = MIMEText(body.encode(body_charset), \u0026#39;plain\u0026#39;, body_charset) msg[\u0026#39;From\u0026#39;] = formataddr((sender_name, sender_addr)) msg[\u0026#39;To\u0026#39;] = formataddr((recipient_name, recipient_addr)) msg[\u0026#39;Subject\u0026#39;] = Header(unicode(subject), header_charset) # Send the message via SMTP to localhost:25 smtp = smtplib.SMTP(\u0026#34;localhost\u0026#34;) smtp.sendmail(sender, recipient, msg.as_string()) smtp.quit() Wykorzystanie:\nsend_email( u\u0026#34;Gąska \u0026lt;gaska@test.pl\u0026gt;\u0026#34;, u\u0026#34;Tchórz \u0026lt;tchorz@test2.pl\u0026gt;\u0026#34;, u\u0026#34;Grzegrzółkę testując..\u0026#34;, u\u0026#34;ąśłóŧ itd...\u0026#34; ) Źródło http://mg.pov.lt/blog/unicode-emails-in-python.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/12/python-wysylanie-maili-w-unicode/","summary":"Chciałem wysłać z Python\u0026rsquo;a maila z krzakami tab by ładnie się wyświetlały i okazało się to całkiem nietrywialne.\nNa szczęście googiel podpowiedział mi doskonałego gotowca, którego zamierzam zapisać by mi nie zginął:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import smtplib from email.mime.text import MIMEText from email.Header import Header from email.Utils import parseaddr, formataddr def send_email(sender, recipient, subject, body): \u0026#34;\u0026#34;\u0026#34;Send an email. All arguments should be Unicode strings (plain ASCII works as well).","title":"Python - wysyłanie maili w unicode"},{"content":"Można lubieć AD, można go nie lubieć\u0026hellip; Ale jak już się ma to warto czasem zintegrować go z tym\u0026hellip; i tamtym\u0026hellip; Od strony Linuksa najwygodniej można to osiągnąć przez LDAP. A żeby to dobrze zrobić trzeba najpierw przetestować czy aby wszystko działa jak byśmy sobie tego życzyli. I tutaj bardzo przydatne jest narzędzie ldapsearch.\nDo odpytywania LDAP\u0026rsquo;a potrzebujemy jeden pakiecik, który zawiera kilka narzędzi do jego obsługi:\napt-get install ldap-utils Teraz możemy próbować przeszukiwać katalog np. tak:\nldapsearch -L -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 389 -W Polecenie to odpyta kontroler o adresie kontroler.nazwadomeny.local (oczywiście możemy użyć też adresu IP) o wszystkie elementy w domenie. Port 389 na kontrolerze domeny musi być otwarty na zaporze - można też wykorzystać 3268 (o ile jest otwarty).\nParametr -b określa początkową gałąź wyszukiwania - możemy tu dodać konkretne OU itd.. by zmniejszyć liczbę elementów.\n-D to użytkownik na którego się logujemy by uzyskać dostęp do katalogu.\n-W zapyta nas o hasło dla tego użytkownika.\nOczywiście możemy nie potrzebować wszystkich obiektów z katalogu a tylko np. loginy kont użytkowników (ale już nie kont maszyn) - do tego celu możemy użyć filtrów, np. tak:\nldapsearch -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 3268 -w \u0026#39;haslo czystym tekstem\u0026#39; \u0026#39;(\u0026amp;(objectClass=person)(!(objectClass=computer)))\u0026#39; sAMAccountName No i dostajemy listę loginów żywych użytkowników. Użycie opcji -w może wydawać się nieco kontrowersyjne ale z drugiej strony jest bardzo wygodne gdy chcemy dane wyjściowe wykorzystać w skrypcie. Przykładowo możemy wynik tego polecenie puścić przez awk by otrzymać same loginy i dodatkowo wszystkie małymi/dużymi literami:\npoprzednie_polecenie | awk \u0026#39;/sAMAccountName/ {print tolower($2);}\u0026#39; W podobny sposób możemy wyciągnąć wszystkie emaile osób z pewnej grupy itd, itp\u0026hellip;.\nTutaj można znaleźć podstawowe info o regułach tworzenia filtrów: http://www.ldapexplorer.com/en/manual/109010000-ldap-filter-syntax.htm\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/12/ldapsearch-w-active-directory/","summary":"Można lubieć AD, można go nie lubieć\u0026hellip; Ale jak już się ma to warto czasem zintegrować go z tym\u0026hellip; i tamtym\u0026hellip; Od strony Linuksa najwygodniej można to osiągnąć przez LDAP. A żeby to dobrze zrobić trzeba najpierw przetestować czy aby wszystko działa jak byśmy sobie tego życzyli. I tutaj bardzo przydatne jest narzędzie ldapsearch.\nDo odpytywania LDAP\u0026rsquo;a potrzebujemy jeden pakiecik, który zawiera kilka narzędzi do jego obsługi:\napt-get install ldap-utils Teraz możemy próbować przeszukiwać katalog np.","title":"ldapsearch w Active Directory"},{"content":"Składniki 1 paczka bezsmakowego kleiku ryżowego (190~170 g), 2 jajka, 1 kostka miękkiego masła (250 g), 1 płaska łyżeczka proszku do pieczenia, pół szklanki cukru, cukier waniliowy (16 g) (niekoniecznie), 8 czubatych łyżek wiórków kokosowych, dżem do wypełnienia ciastek (opcjonalnie - ja lubię je również bez dżemu). Sposób przygotowania Wszystkie składniki połączyć razem i wyrobić (najlepiej się ugniata ręką). Formować kulki wielkości małego orzecha włoskiego, układać na blaszce wyłożonej papierem do pieczenia w niewielkich odległościach (trochę urosną). Jeżeli planujemy opcję z dżemem to w każdej kulce należy zrobić wgłębienie - ja lekko przyciskałem palcem.\nPiec w temperaturze 180ºC przez 15~20 minut, aż zrobią się złociste. Po ostudzeniu wgłębienia napełnić dżemem.\nŹródło (moja wersja jest nieco przerobiona): http://www.mojewypieki.com/przepis/ciasteczka-z-kleiku-ryzowego\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/11/ciastka-z-kleiku-ryzowego/","summary":"Składniki 1 paczka bezsmakowego kleiku ryżowego (190~170 g), 2 jajka, 1 kostka miękkiego masła (250 g), 1 płaska łyżeczka proszku do pieczenia, pół szklanki cukru, cukier waniliowy (16 g) (niekoniecznie), 8 czubatych łyżek wiórków kokosowych, dżem do wypełnienia ciastek (opcjonalnie - ja lubię je również bez dżemu). Sposób przygotowania Wszystkie składniki połączyć razem i wyrobić (najlepiej się ugniata ręką). Formować kulki wielkości małego orzecha włoskiego, układać na blaszce wyłożonej papierem do pieczenia w niewielkich odległościach (trochę urosną).","title":"Ciastka z kleiku ryżowego"},{"content":"Składniki 2 szklanki miąższu dyni (startego lub zmiksowanego), 2 szklanki mąki, 0,75 szklanki cukru, 3 jajka, 5 łyżeczek cynamonu, 16 g cukru wanilinowego, 0,75 szklanki oleju, łyżeczka sody oczyszczonej, garść orzechów włoskich. Sposób przygotowania Ubijamy białka z cukrem. Do ubitej piany dodajemy żółtka i kolejne składniki: mąkę, olej, cukier wanilinowy, sodę i cynamon. Całość miksujemy jeszcze przez chwilę, a następnie dodajemy posiekany lub starty i odsączony (można np. mocno ścisnąć w dłoniach) miąższ dyni oraz drobno pokrojone orzechy włoskie i jeszcze chwilę mieszamy.\nMasę przekładamy do wysmarowanej masłem i oprószonej mąką formy - na dużej blaszce ciasto wyjdzie dość cienkie, więc można piec np. w foremce keksowej. Blaszkę z ciastem wkładamy do nagrzanego do 180°C piekarnika i w tej temperaturze pieczemy przez ok. 40-50 minut. Można podawać posypane cukrem pudrem lub cynamonem.\n","permalink":"https://gagor.pro/2012/11/ciasto-z-dynia/","summary":"Składniki 2 szklanki miąższu dyni (startego lub zmiksowanego), 2 szklanki mąki, 0,75 szklanki cukru, 3 jajka, 5 łyżeczek cynamonu, 16 g cukru wanilinowego, 0,75 szklanki oleju, łyżeczka sody oczyszczonej, garść orzechów włoskich. Sposób przygotowania Ubijamy białka z cukrem. Do ubitej piany dodajemy żółtka i kolejne składniki: mąkę, olej, cukier wanilinowy, sodę i cynamon. Całość miksujemy jeszcze przez chwilę, a następnie dodajemy posiekany lub starty i odsączony (można np. mocno ścisnąć w dłoniach) miąższ dyni oraz drobno pokrojone orzechy włoskie i jeszcze chwilę mieszamy.","title":"Ciasto z dynią"},{"content":"CouchDB databases on version 0.11.x swell very fast. They should be compacted daily for best performance and space usage. Here is my script that could be run in cron and will compact all databases:\n#!/bin/bash IP=\u0026#34;10.0.0.121\u0026#34; DBS=`curl -sS -X GET http://$IP:5984/_all_dbs | sed -r \u0026#34;s/([,\\\u0026#34;[])|(\\])+/ /g\u0026#34;` for d in $DBS; do curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://$IP:5984/$d/_compact done More informations about compacting could be found here\u0026thinsp; external link (also for version 1.2.x).\n","permalink":"https://gagor.pro/2012/11/automatically-compact-couchdb-databases-in-0-11-x/","summary":"CouchDB databases on version 0.11.x swell very fast. They should be compacted daily for best performance and space usage. Here is my script that could be run in cron and will compact all databases:\n#!/bin/bash IP=\u0026#34;10.0.0.121\u0026#34; DBS=`curl -sS -X GET http://$IP:5984/_all_dbs | sed -r \u0026#34;s/([,\\\u0026#34;[])|(\\])+/ /g\u0026#34;` for d in $DBS; do curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://$IP:5984/$d/_compact done More informations about compacting could be found here\u0026thinsp; external link (also for version 1.","title":"Automatically compact CouchDB databases in version 0.11.x"},{"content":"Jakiś czas temu napisałem swoją własną tęczową tablicę i odkąd zainteresowałem się tym tematem zastanawiałem się jak jeszcze bardziej usprawnić tę aplikację. Testowałem różne długości łańcuchów i zauważyłem że zwiększanie długości łańcucha bardzo szybko powoduje znaczne zwiększenie ilości kolizji, co drastycznie obniżało wydajność (nie wspominając o czasie potrzebnym na wygenerowanie całej tablicy). Z tego powodu ostateczna implementacja choć była rzeczywiście tęczową tablicą działała bardziej jak tablica hashy - bo łańcuchy były bardzo krótkie. Ponadto ze statystyk wynikało że bardzo rzadko hasła były odzyskiwane ze środka łańcucha - zdecydowana większość trafień pochodziła z pierwszego hasła w łańcuchu. Pierwsze hasło było przeważnie słownikowe, plus czasem jakiś dodatek w postaci cyfry/zmienionej wielkości liter, itp. Pozostałe hasła w łańcuchu były praktycznie losowe.\nNa dobrą sprawę takie wyniki można wytłumaczyć od dawna znaną prawdą: \u0026ldquo;ludzie nie używają skomplikowanych haseł\u0026rdquo;. Pomyślałem więc że dużo lepiej byłoby generować hashe ze słów, które są potencjalnie prawdopodobnymi hasłami a nie dla różnych losowych śmieci. Szczególnie można by wykorzystać pewne złe nawyki powielane przez wiele osób, np. typowe \u0026ldquo;skomplikowane\u0026rdquo; hasło zaczyna się od dużej litery, później jest kilka małych, a na końcu cyfra lub dwie - przy czym ciąg liter jest wyrazem słownikowym. Tego typu wzorców jest wiele i pomyślałem że można by to wykorzystać. Na dobrą sprawę jeżeli zamierzamy sprawdzić jakość haseł z jakiejś aplikacji to nie potrzebujemy 100% trafień, wystarczy że 20% to hasła kiepskiej jakości i te będą wymagały poprawienia.\nCzym są hybrydowe tęczowe tablice? Gdy zacząłem drążyć temat trafiłem na artykuł opisujący podejście, o którym myślałem. Klasyczne tęczowe tablice \u0026ldquo;wychodzą\u0026rdquo; od pewnego hasła, z którego generujemy hash, redukujemy go do nowego losowego hasła itd\u0026hellip; Zapisujemy pierwsze hasło i ostatni hash a resztę da się odtworzyć gdy będzie potrzebna. Hybrydowe tęczowe tablice mają bardziej rozbudowaną funkcję redukującą - nie tworzy ona całkiem losowych haseł, ale hasła wygenerowane przez sklejenie słów które potencjalnie częściej powinny w hasłach występować. Czyli zamiast generować hasła typu: L;adfa=2ja, tworzone są hasła: abc123, Qwert, Bezpieczne5, itd.\nOba podejścia mają swoje zalety i wady: odpowiednio duże tęczowe tablice mogą przechować większość haseł z danego zakresu - ale będą też przechowywać masę pseudolosowych śmieciowych haseł, których prawdopodobieństwo wykorzystanie jest małe. Hybrydy przechowują dużo mniejszy zakres haseł ale za to takich \u0026ldquo;stosunkowo prawdopodobnych\u0026rdquo;, więc mniejsza tablica powinna dawać równie dobre wyniki (dla prostych zestawów haseł).\nKoncepcja okazała się znaną ale nie wykorzystywaną równie często jak tęczowe tablice. Cóż\u0026hellip; nic nowego nie wymyśliłem ale realizacja pomysłu wydawała mi się ciekawą wprawka do kilku nowych technologii, którym miałem zamiar się przyglądnąć.\nRealizacja O ile RainbowDB działa na bazie: PHP + PostgreSQL + moduł w C do Postgresa, to w Hybrid Rainbow DB postanowiłem wykorzystać zupełnie nowe narzędzia i frameworki. Baza to MySQL, aplikację pisałem w Pythonie na Django.\nRealizacja zaczynała się od wygenerowania kilku słowników \u0026ldquo;składowych\u0026rdquo; dla złych haseł np.: liczby (od 0 do 100, powtarzające się cyfry do 10 znaków), daty (od 1950 do 2015, we wszystkich kombinacjach np.: YYMMDD, itd.), znaki (pojedyncze, jak również w przeróżnych kombinacjach z klawiatury, alfabetycznie, itd.). Mając takie słowniki zamierzałem wygenerować zbiory hashy ze sklejonych z nich słów na zasadzie: każde słowo z jednego słownika z każdym z drugiego. Jeśli pojedynczy słownik potraktujemy jako zbiór to taka operacja na dwóch lub więcej zbiorach nazywana jest iloczynem kartezjańskim. Wykonanie tego typu operacji jest stosunkowo proste ale nie zamierzałem zapisywać wyniku hashowania każdego tak wygenerowanego hasła. Idealnie byłoby móc zapisać wyniki w sposób podobny jak w tęczowych tablicach, czyli w jednym polu zakodować wynik kilku czy nawet kilkuset hashowań. Do tego musiałem opracować algorytm pozwalający przypisać danemu hasłu z iloczynu kartezjańskiego unikatowy jednoznacznie reprezentujący go numer. Odnosząc to do implementacji tęczowej tablicy, chodziło o funkcje redukującą, która zamieniała hash na numer a następnie generowała n-ty wynik iloczynu kartezjańskiego bez generowania wcześniejszych elementów. To podejście pozwoliło mi przechowywać nawet bardzo długie hasła w postaci kilku bajtów (jako numer) a przez to bardzo efektywnie wykorzystać miejsce.\nBy dodatkowo poprawić jakość generowanej \u0026ldquo;hybrydowej tęczowej tablicy\u0026rdquo; w czasie generowania wykorzystałem wektor bitowy by \u0026ldquo;odznaczyć\u0026rdquo; już wygenerowane hasła. Więc gdy dochodziło do ponownego zredukowania hasha na hasło, które już zostało wcześniej wykorzystane i byłby to powodem wystąpienia kolizji to zapisywany zostawał poprzedzający go hash, a proces generowania łańcucha był przerywany. Ten mechanizm znacznie zmniejszył częstotliwość występowania kolizji (więc późniejsze wykorzystanie tablic będzie szybsze), a jako efekt uboczny skrócił czas generowania tablic.\nZastosowałem jeszcze jedną sztuczkę by dodatkowo zmniejszyć rozmiar tablic. W większości tablic które utworzyłem nie było potrzeby przechowywać pełnych hashy aby móc odtworzyć hasło. W małych tablicach wystarczające były pierwsze 2~3 bajty, w większych 4~8. Zacząłem więc skracać długość przechowywanych hashy tak by utrzymać stosunkowo małą ilość kolizji. Dzięki tej optymalizacji mogłem na tej samej powierzchni dyskowej zapisać nawet 10-krotnie więcej danych.\nWnioski Pobawiłem się moimi nowymi tablicami i mam kilka spostrzeżeń. Ogólnie koncepcja się sprawdziła i hybrydy \u0026ldquo;trzaskają\u0026rdquo; nawet hashe, których moje wcześniejsze narzędzie nie ruszało (np. kilka hashy z pierwszej strony Top uncracked poprzedniego narzędzia). Użycie tego narzędzia przy audytach jest o tyle bardziej zasadne że złamie ono sporą część \u0026ldquo;kiepskich\u0026rdquo; haseł, które powinny zostać zmienione - a odpuści pseudolosowe, których prawdopodobieństwo wykorzystania a realnym ataku jest dużo mniejsze.\nHybrydowe Tablice mają też wady, których nie miało wcześniejsze rozwiązanie:\njest dużo wolniejsze - ponieważ funkcja redukująca generuje nowe hasło w dość zawiły sposób (w przypadku dużych słowników jest to zapytanie do bazy danych) to sprawdzenie całego łańcucha trwa znacznie dłużej, niż w zwykłej tęczowej tablicy, by przyspieszyć redukcję część danych słowników jest cache\u0026rsquo;owana - przyspieszenie jest duże, podobnie jak zużycie pamięci ;-/ Cóż\u0026hellip; Coś za coś \u0026#x1f603;\nP.S. Ostatnio pobawiłem się chwilę Web Workerami - dzięki czemu wrzucenie dużej ilości danych spowoduje ich przetworzenie w osobnym wątku bez zamrożenia okna przeglądarki.\nNarzędziem można pobawić się tutaj: Hybrid Rainbow DB\u0026thinsp; external link Nie można - projekt zarchiwizowałem.\n","permalink":"https://gagor.pro/2012/11/hybrid-rainbow-db/","summary":"Jakiś czas temu napisałem swoją własną tęczową tablicę i odkąd zainteresowałem się tym tematem zastanawiałem się jak jeszcze bardziej usprawnić tę aplikację. Testowałem różne długości łańcuchów i zauważyłem że zwiększanie długości łańcucha bardzo szybko powoduje znaczne zwiększenie ilości kolizji, co drastycznie obniżało wydajność (nie wspominając o czasie potrzebnym na wygenerowanie całej tablicy). Z tego powodu ostateczna implementacja choć była rzeczywiście tęczową tablicą działała bardziej jak tablica hashy - bo łańcuchy były bardzo krótkie.","title":"Hybrid Rainbow DB"},{"content":"Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyrównanie do chunk\u0026rsquo;a z macierzy.\nPrawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 4~8KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128~512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART\u0026rsquo;a) że mają bloki 512B - SIC!\nJeśli mamy fart i nasz dysk raportuje rozmiar fizycznego sektora to możemy to sprawdzić tak:\ncat /sys/block/sda/queue/physical_block_size U mnie to polecenie zwróciło 4096 czyli 4KB co jest zgodne z deklaracją producenta.\nDobre podejście do tematu wyrównania partycji zaproponował Teo Tso czyli wybranie takich parametrów głowic/sektorów na ścieżce by fdisk automatycznie wyrównywał partycje do oczekiwanej przez nas wielkości bloku. Teo proponował użycie 224 głowic i 56 sektorów - co da wyrównanie do 128KB dla wszystkich partycji z wyjątkiem pierwszej (pierwsza będzie wyrównana do 4KB o ile nie wymusimy startu z 256 sektora). Jeżeli mamy dysk z blokami 4KB lub pierwszą partycję zamierzamy wykorzystać jako np. /boot (gdzie wydajność nie ma aż takiego dużego znaczenia) to jest to ok. Ale jeśli kompatybilność z DOS\u0026rsquo;em mamy w poważaniu to możemy w fdisku utworzyć pierwszą partycję wyrównaną do 128KB lub 1MB.\nPrzeważnie wolałem cfdiska od fdiska (bo po co się męczyć z topornym interfejsem) ale nie udało mi się skubańca zmusić by tworzył pierwszą partycję w sposób nie kompatybilny z DOS\u0026rsquo;em. fdisk pomimo toporności po podaniu liczby głowic i sektorów w ścieżce podpowiedział mi poprawne wyrównanie partycji (a gdybyśmy posiadali starszą wersję, która nie jest tak sprytna to przynajmniej możemy podać ręcznie od którego sektora ma zaczynać się partycja).\nWyrównanie do 128KB fdisk -u -H 224 -S 56 /dev/sdX Opcja -u zmienia domyślną jednostkę na sektory (mamy wtedy mniejsze liczby, które łatwiej się przelicza). Dla wyrównania do 128KB pierwsza partycja powinna się zaczynać na 256 sektorze. Do wyrównania do 4KB wystarczy zacząć na 56 sektorze (wystarczające przy części nowszych dysków twardych).\nWyrównanie do 1MB Jeśli jednak dysponujemy dyskiem SSD z ciężko powiedzieć jak dużym blokiem to lepiej wykorzystać wyrównanie do 1MB - zmarnujemy trochę więcej miejsca (tych parę MB jakoś przeżyjemy) ale w tym rozmiarze na pewno zmieści się każdy sektor (a może nawet Erase Block, który obecnie przeważnie ma 512KB choć zdarzają się sztuki z 4MB). Można to osiągnąć z ustawieniem 64 głowic i 32 sektorów na ścieżkę, robimy to tak:\nfdisk -u -H 64 -S 32 /dev/sdX Pierwsza partycja powinna się zaczynać na 2048 sektorze dla wyrównania do 1MB lub 8192 sektorze jeśli chcemy zacząć od 4MB.\nPo odpaleniu fdiska z tymi parametrami wybieramy opcję n i lecimy dalej zgodnie z podpowiedziami, np.:\nfdisk -u -H 224 -S 56 /dev/sdc Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;m\u0026lt;/strong\u0026gt; Polecenie a zmiana flagi rozruchu b modyfikacja etykiety dysku BSD c zmiana flagi kompatybilności z DOS-em d usunięcie partycji l wypisanie znanych typów partycji m wyświetlenie tego menu \u0026lt;strong\u0026gt;n dodanie nowej partycji\u0026lt;/strong\u0026gt; o utworzenie nowej, pustej DOS-owej tablicy partycji p wypisanie tablicy partycji q zakończenie bez zapisywania zmian s utworzenie nowej, pustej etykiety dysku Suna t zmiana ID systemu partycji u zmiana jednostek wyświetlania/wprowadzania v weryfikacja tablicy partycji w zapis tablicy partycji na dysk i zakończenie x dodatkowe funkcje (tylko dla ekspertów) Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;n\u0026lt;/strong\u0026gt; Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt;): Using default response p Numer partycji (1-4, domyślnie \u0026lt;strong\u0026gt;1\u0026lt;/strong\u0026gt;): Przyjęto wartość domyślną 1 Pierwszy sektor (2048-15240575, domyślnie \u0026lt;strong\u0026gt;2048\u0026lt;/strong\u0026gt;): Przyjęto wartość domyślną 2048 Ostatni sektor, +sektorów lub +rozmiar{K,M,G} (2048-15240575, domyślnie 15240575): \u0026lt;strong\u0026gt;+100M\u0026lt;/strong\u0026gt; Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt; Dysk /dev/sdc: 7803 MB, bajtów: 7803174912 głowic: 224, sektorów/ścieżkę: 56, cylindrów: 1214, w sumie sektorów: 15240576 Jednostka = sektorów, czyli 1 * 512 = 512 bajtów Rozmiar sektora (logiczny/fizyczny) w bajtach: 512 / 512 Rozmiar we/wy (minimalny/optymalny) w bajtach: 512 / 512 Identyfikator dysku: 0xc3072e18 Urządzenie Rozruch Początek Koniec Bloków ID System /dev/sdc1 2048 206847 102400 83 Linux Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;w\u0026lt;/strong\u0026gt; Tablica partycji została zmodyfikowana! Wywoływanie ioctl() w celu ponownego odczytu tablicy partycji. UWAGA: ponowny odczyt tablicy partycji zakończył się błędem 16: Urządzenie lub zasoby zajęte. Jądro nadal używa starej tablicy. Nowa tablica będzie używana po następnym restarcie systemu albo po uruchomieniu partprobe(8) lub kpartx(8) Synchronizacja dysków. P.S. Po co w ogóle partycjonować - można użyć całych urządzeń Co prawda w przypadku gdy zamierzam całe dyski poświęcić na RAID\u0026rsquo;a mógłbym ich nie partycjonować tylko użyć całych urządzeń i zadbać tylko by mdadm poprawnie się wyrównał (co zresztą robi automatycznie do 4KB), ale obawiałem się jak względem takich dysków zachowają się inne systemy które mam zainstalowane na komputerze - nie chciałbym aby przypadkiem uznały że to jakiś uszkodzony dysk po czym zainicjowały tablicę partycji\u0026hellip; Może to nieuzasadniona fobia ale wiem że względem partycji RAID Autodetect nic takiego mi się nie przytrafi \u0026#x1f603;\nTworzenie macierzy RAID Jak już utworzymy pożądane partycje na pierwszym dysku to musimy je powielić na wszystkich pozostałych dyskach, które zamierzamy włączyć do macierzy - najprościej zrobić to sfdisk\u0026rsquo;iem:\nsfdisk -d /dev/sdX | sfdisk /dev/sdY Przy czym dysk sdX to źródłowy dysk z gotowymi partycjami, a dysk sdY to każdy na którym chcemy odtworzyć partycje. Można by zrobić na to ładną pętelkę jeśli mamy więcej tych dysków ale celowo tego nie zrobię bo jak znam życie to kiedyś ktoś to skopiuje ze znakiem nowego wiersza i wrzuci do konsoli\u0026hellip; 😀\nTeraz tworzę zdegradowaną (z dwóch dysków) macierz RAID5\u0026hellip; Ale na dobrą sprawę bezpieczniej byłoby utworzyć macierz RAID1 z dwóch dysków (o ile miejsca byłoby wystarczająco na przeniesienie danych) a po przeniesieniu danych na macierz dodanie trzeciego dysku i powiększenie macierzy z reshapingiem do RAID5 - postanowiłem pominąć takie rozwiązanie bo nie bałem się utraty danych, miałem dokładną kopię na innej macierzy \u0026#x1f603;\nWięc tworzymy macierz:\nmdadm --create /dev/md0 --level=5 --raid-devices=2 --chunk=512k /dev/sdX1 /dev/sdY1 Na dobrą sprawę z powyższych opcji tylko chunk nadaje się do \u0026ldquo;dopasowania\u0026rdquo; - ja mam zamiar utworzyć macierz z dość dużych zasobów (3x2TB) i przechowywać na niej raczej duże pliki więc 512KB wydaje mi się dobrą wartością. Gdybyśmy jednak potrzebowali większej liczby I/O dla małych pliczków to mniejsza wartość może być lepsza. Warto zrobić kilka benchmarków dla różnych wartości chunk\u0026rsquo;a i dopasować do przewidywanego przez nas obciążenia.\nAby macierz była widoczne już w czasie startu systemu należy dodatkowo wykonać polecenie:\nmdadm --detail --scan \u0026gt;\u0026gt; /etc/mdadm/mdadm.conf Warto też w pliku /etc/mdadm/mdadm.conf odkomentować i wpisać jakieś sensowne wartości dla HOMEHOST, MAILADDR.\nI teraz możemy odbudować obraz initrd:\nupdate-initramfs -u Optymalizacja macierzy RAID5 Sam proces tworzenia macierzy może zająć kilka/kilkanaście godzin - dlatego warto mu pomóc kilkoma zmianami.\nspeed_limit_xxx Na pierwszy rzut - domyślne wartości dla minimalnej i maksymalnej prędkości budowania/regenerowania/odtwarzania macierzy RAID5, można je sprawdzić:\ncat /proc/sys/dev/raid/speed_limit_max 200000 cat /proc/sys/dev/raid/speed_limit_min 1000 Domyślnie jest to minimum 1MB/s i maksimum 200MB/s. Podbijając minimalną prędkość do 10~50MB/s można kosztem większego obciążenia systemu zmusić macierz by odbudowywała się szybciej. Osobiście uważam tą optymalizację za mało znaczącą bo cały mechanizm dość elastycznie reaguje na obciążenie systemu i jeśli nic nie robimy to prędkości odbudowy są dość wysokie. Ale można to zrobić tak:\necho 50000 \u0026gt; /proc/sys/dev/raid/speed_limit_min lub tak:\nsysctl -w dev.raid.speed_limit_min=50000 stripe_cache_size Ta optymalizacja pomogła mi dużo - ok. 30% wzrost wydajności macierzy (również w czasie odbudowy parzystości). Możemy sprawdzić wartość tego parametru, np. tak:\ncat /sys/block/md0/md/stripe_cache_size Domyślnie jest to wartość 128 - bida z nędzą, u mnie ustawienie na 32768 (maksymalna wartość tego parametru) dało największy wzrost wydajności - ale już 8192 znacznie poprawiło wydajność.\nfor i in 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;Testowanie $i\u0026#34; echo $i \u0026gt; /sys/block/md0/md/stripe_cache_size sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=/dev/zero of=file bs=1M count=10000 done Wyłączenie NCQ dla wszystkich dysków w macierzy Wydało mi się to nieco kontrowersyjne bo NCQ powinno pomagać przy losowych odczytach/zapisach - ale zapuściłem test bonnie++ i okazało się że z włączonym NCQ czasy dostępu dla niektórych obciążeń rosną nawet dziesięciokrotnie! Warto więc sprawdzić tą opcję pod przewidywanym przez nas scenariuszem obciążenia.\nNCQ dla poszczególnych dysków można wyłączyć np. tak:\nfor d in sdb sdc sdd do echo 1 \u0026gt; /sys/block/$d/device/queue_depth done Wyłączenie cache dyskowych Wyłączenie wbudowanej w dyski pamięci cache akurat nie zwiększa wydajności ale w przypadku awarii zasilania (lub innej gwałtownej awarii systemu) zwiększa szanse macierzy na przeżycie takiego incydentu. Obecnie większość systemów plików korzysta z opóźnienia zapisu by bardziej optymalnie zapisać dane na dysku - dlatego gdy zapisujemy dany to najpierw trafiają one do cache systemowego. Dopiero po sync\u0026rsquo;u są przesyłane do cache dyskowego skąd dopiero po pewnym czasie trafiają na dysk. Wyłączenie pamięci cache na dyskach \u0026ldquo;usuwa\u0026rdquo; nam to drugie opóźnienie.\nhdparm -W0 /dev/sd* Zmiana parametrów odczytu z wyprzedzeniem Bardzo ważnym parametrem mającym wpływ na wydajność macierzy jest odpowiednie ustawienie odczytu z wyprzedzeniem. Obecnie ustawioną wartość możemy sprawdzić tak (wartość wyrażona jest w 512 bajtowych sektorach):\nblockdev --getra /dev/md0 U mnie domyślnie było 4096 (a na innym starszym systemie tylko 1536) to może być zbyt mało dla konfiguracji RAID. Większe wartości można ustawić np. tak:\nblockdev --setra 65536 /dev/md0 A tak można wykonać sprawdzanie, która wartość będzie dla nas najbardziej optymalna:\ndd if=/dev/zero of=file bs=1M count=10000 for i in 1536 4096 8192 16384 32768 65536 131072 262144 524288; do echo \u0026#34;Testowanie $i\u0026#34; blockdev --setra $i /dev/md0 sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=file of=/dev/null bs=1M done Tworzenie zasobu LVM Mając już macierze tworzę na niej volumen LVM - najpierw przygotowanie zasobu:\npvcreate /dev/md0 Jeżeli w /etc/lvm/lvm.conf mamy ustawione opcje:\nmd_component_detection = 1 md_chunk_alignment = 1 data_alignment_detection = 1 To LVM powinien automatycznie wykryć rozmiar chunk\u0026rsquo;a z RAID\u0026rsquo;a i dostosować swoje metadane, oraz początek danych tak by wszystko było prawidłowo wyrównane względem macierzy.\nJeśli nie mamy szczęścia (bardzo stare jajko/LVM) to będziemy musieli użyć opcji -metadatasize i/lub -dataalignment:\npvcreate --metadatasize 500k /dev/md0 Teraz ciekawostka - LVM potrzebuje na 192KB na dane nagłówkowe każdego wolumenu i każdy utworzony później zasób byłby o te 192KB przesunięty, więc \u0026hellip; trafia nasze wyrównanie do 128KB. Dlatego zmuszamy LVM\u0026rsquo;a by zaalokował nieco więcej - tutaj 256KB. OK - ale w poleceniu jest 250 - WTF? I to aby było zabawnie jest to jak najbardziej prawidłowa wartość - nie wiem jaka w tym logika, ale by metadane zajmowały 256KB podajemy 250k, by zajmowały 512KB podajemy 500k itd\u0026hellip;\nInaczej jest z opcją -dataalignment, tutaj najbardziej optymalnie należy podać rozmiar chunk\u0026rsquo;a*ilość aktywnych dysków (dla RAID5 odejmujemy jeden) - albo minimalnie rozmiar chunka, np.:\npvcreate --dataalignment 512K /dev/md0 Poprawność możemy sprawdzić poleceniem:\npvs /dev/md0 -o+pe_start PV VG Fmt Attr PSize PFree 1st PE /dev/md0 vgraid lvm2 a- 1,82t 488,89g 512,00k U mnie 1st PE zaczyna się na 512KB, więc jest OK.\nTeraz tworzymy grupę:\nvgcreate vgraid /dev/md0 Polecenie vgcreate posiada parametr -s który pozwala określić do wielokrotności jakiej wartości będzie zaokrąglana wielkość wolumenu - wartość ta powinna być wielokrotnością chunk\u0026rsquo;a z macierzy. Domyślnie ma ona wartość 4MB więc wszystko będzie ładnie wyrównane.\nI możemy zacząć tworzyć volumeny logiczne:\nlvcreate -L1T -nsrv vgraid Formatowanie To teraz formatowanie - tutaj też czasem trzeba się wysilić by utworzony przez nas filesystem działał możliwie optymalnie na macierzy i bloku o odpowiednim rozmiarze. W przypadku filesystemu na macierzy są dwa ważne parametry: stride i stripe-width. Stride powinien odpowiadać rozmiarowi podanemu jako chunk podczas tworzenia macierzy ale wyrażonego w blokach systemu plików (domyślnie 4KB). Stripe-width powinno być ustawione na: stride * N, gdzie N to ilość aktywnych dysków w macierzy (dla RAID5 jest to ilość dysków minus 1) - przykładowo dla bloku 4KB i chunk\u0026rsquo;a 512KB, stride powinien wynosić 128 (512/4). Z kolei stripe-width dla 3 dysków to 128*(3-1)=256. Prawidłowe dobranie tych parametrów może dać wzrost wydajności rzędu 40% (według moich testów). Teoretycznie na nowszych systemach tworzone systemy plików automatycznie powinny wykryć najbardziej optymalne wyrównanie - możemy więc spróbować puścić format bez tych parametrów i później skontrolować ich wartości poleceniem:\ntune2fs -l /dev/vgraid/srv Na początek przykład dla systemu plików dla małych i średnich plików:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -m0 /dev/vgraid/srv Jeden dodatkowy parametr to resize - pozwala on zmienić domyślne ustawienie maksymalnego rozmiaru do którego możemy powiększyć dany filesystem - domyślnie jest to wartość 1000 razy większa od początkowej wielkości - ciut przekozaczone. Może nie oszczędzi to dużo miejsca na dysku (kilkadziesiąt/kilkaset MB) ale na pewno skróci czas fsck\u0026rsquo;a.\nKolejny dodatek to -m0 które wyłącza alokację 5% przestrzeni dyskowej dla root\u0026rsquo;a i usług systemowych - po prostu tutaj tego nie potrzebuję a 5% z 1TB to 50GB marnującego się miejsca!\nJeśli wiemy że będziemy przechowywać na danym zasobie tylko stosunkowo duże pliki to można użyć takich opcji:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -T largefile -m0 /dev/vgraid/srv Opcja -T to wykorzystanie szablonów opcji dla tworzenia systemów plików, które można edytować i dodawać w pliku: /etc/mke2fs.conf. Szablon largefile wykorzystuje mniejszą liczbę inodów dla nowego filesystemu, jeśli zamierzamy przechowywać głównie duże pliki to zmniejszy to czas formatowania i późniejszych fsck\u0026rsquo;ów na tym systemie plików.\nTestowanie wydajności Zalecałbym testowanie wydajności na kolejnych etapach przygotowania dysków i powtarzać te same benchmarki po każdej zmianie, tak więc testujemy:\nNa początek wszystkie dyski, z których zamierzamy zbudować RAID\u0026rsquo;a by wykluczyć ewentualne \u0026ldquo;padaki\u0026rdquo;/uszkodzone kable, itp. Np. hdparm/dd na czystym dysku i dodatkowo iozone/bonnie++ na filesystemie by sprawdzić czy nie skopaliśmy wyrównania partycji. Po zbudowaniu RAID\u0026rsquo;a powtarzamy testy - na całym urządzeniu (dd) i po sformatowaniu (iozone/bonnie++) by upewnić się że RAID jest prawidłowo wyrównany (przy czym przy RAID5 możemy się spodziewać wyników przy zapisie niższych niż przy odczycie - jest to OK). Jest to też dobry moment na sprawdzenie kilku optymalizacji dla macierzy: stripe_cache_size, wyłączenie NCQ, ustawienia odczytu z wyprzedzeniem, wyłączenie cache dysków - po każdej z tych zmian ponawiamy benchmarki by upewnić się że uzyskaliśmy poprawę/lub nie. Po przygotowaniu LVM\u0026rsquo;a - ponawiamy benchmarki na volumenie by upewnić się że nie skopaliśmy wyrównania partycji/chunk\u0026rsquo;a/itd\u0026hellip; Dopieszczamy opcje formatowania filesystemu i jego montowania (np. noatime, commit, data, itd) - i znów posiłkujemy się benchmarkami by potwierdzić że pniemy się z wydajnością w górę. Jeśli myślicie że to dużo to zalecałbym powtórzenie części benchmarków 2~3 krotnie by upewnić się że wyniki nie odbiegają znacznie od siebie. Dodatkowo powinniśmy czyścić przed każdym banchmarkiem cache dyskowy aby mieć pewność że lepsze wyniki nie są skutkiem wczytania danych do pamięci. Z tego samego powodu jeśli uruchamiamy benchmarki to ilość zapisywanych/odczytywanych danych powinna być minimum 1,5~2 razy większa niż pamieć RAM zainstalowana w systemie by na pewnie wszystkie dane nie zmieściły się w cache\u0026rsquo;u. Oczywiście można to zlać ale później nie ma się co dziwić że system działa wolno - a na produkcyjnej maszynie dużo trudniej zaorać całą konfiguracją i utworzyć od początku z prawidłowym wyrównaniem.\nZalecane jest wykorzystanie IOZone lub Bonnie++ ponieważ testują one nie tylko prosty sekwencyjny odczyt, ale również tworzenie/kasowanie plików o różnych rozmiarach i w różnej ilości - to pozwala lepiej sprawdzić opóźnienia występujące przy przewidywanych przez nas obciążeniach oraz upewnić się że cała zabawa z wyrównywaniem zasobów miała sens. Oczywiście to tylko zalecenia 😉\nhdparm W przypadku macierzy wykorzystanie prostego hdparm\u0026rsquo;a do testów:\nhdparm -tT /dev/md0 nie zwróci realnych i sensownych wyników. Pomiar jest zbyt krótki by uzyskać sensowne wyniki - lepiej wykorzystać dd z dużą ilością danych do odczytu.\ndd Narzędzie jakże prymitywne a tak przydatne. Możemy nim zmierzyć sekwencyjny odczyt/zapis z/do macierzy i uzyskać bardziej realne wyniki niż hdparm\u0026rsquo;em. Wystarczy wymusić operację na ok. dwukrotnie większej ilości danych niż ilość pamięci RAM. Dodatkowo czyścimy cache\u0026rsquo;e przed i po mierząc całościowy czas, np. tak:\nsync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) Jest to szczególnie przydatne np. przy dopasowywaniu optymalnej dla nas wartości parametru stripe_cache_size, wystarczy przygotować odpowiednią pętlę:\nfor i in 128 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;stripe_cache_size $i\u0026#34; echo $i \u0026gt; /sys/block/md0/md/stripe_cache_size sync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) done Parametr bs określa bufor wykorzystywany przy operacjach odczytu/zapisu - można go dostosować do rozmiaru chunk\u0026rsquo;a/stripe-width/itp.. count określa jak dużo takich buforów odczytać/zapisać - w powyższym przypadku jest to 10 tys. jednomegabajtowych buforów więc łącznie 10GB.\nbonnie++ Bonnie++ wymaga nieco przygotowania ale wyniki w moim przypadku bardzo odpowiadały rzeczywistym. Co pokrótce trzeba zrobić:\nmkdir /srv/test chown -R guest:guest /srv/test bonnie++ -d /srv/test -s 16g -m nazwa_maszyny -f -u guest Możemy dodać opcję -b by po każdej operacji wykonywany był sync - to byłoby coś podobnego do systemów bazodanowych lub pocztowych. Jeżeli chcemy zasymulować standardowe operacje na plikach to nie potrzebujemy tej opcji.\niozone W najprostszym wykonaniu:\niozone -a Wykona to serię pomiarów na różnych rozmiarach plików, ilości powtórzeń itd. Najbardziej interesująca opcją w konfiguracji RAID jest \u0026ldquo;Stride read\u0026rdquo;.\niozone -S 8192 -t 1 Parametr -S przekazuje rozmiar pamięci cache procesora - wykorzystywany do alokacji pamięci blokami itp (sprawdzałem czy to cokolwiek pomoże.. ale nie widziałem dużej różnicy).\nParametr -t 1 to benchmark przepustowości dysku a parametr cyfrowy określa liczbę równoczesnych wątków, które będą odczytywać/zapisywać - można w ten sposób zasymulować np. równoczesny streaming dla wielu źródeł, itp.\niozone -S 8192 -a -s 40960 Tutaj parametr -s wskazuje na jakim rozmiarze pliku w KB ma być prowadzony benchmark - tutaj 40MB.\nPodsumowanie Ogólnie zadowolony jestem że udało mi się to wszystko zebrać w jednym poście bo dotychczas miałem to zapisane w wielu różnych zakładkach i spory problem gdy potrzebowałem \u0026ldquo;właśnie tego jednego polecenia\u0026rdquo;. Ale niezadowolony jestem z tego że nie udało mi się ustalić całości tego postępowania dla dysków o sektorach/erase block\u0026rsquo;ach większych niż 4 KB. Chodzi mi szczególnie o brak jasnego wyjaśnienia czy RAID5 jest prawidłowo wyrównywany bo według jednych tak właśnie jest, a według innych tak nie jest. Stąd niektórzy zalecają by stosować format metadanych dla macierzy w wersji 0.9 lub 1.0 zamiast 1.2 ale nie ma jasnych źródeł tego rozumowania. Mam nadzieję że kiedyś uda mi się to jednoznacznie rozsądzić - na pewno zaktualizuję wtedy tego posta.\nŹródełka na których oparłem to HOWTO http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing\u0026thinsp; external link http://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously\u0026thinsp; external link http://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding\u0026thinsp; external link http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html\u0026thinsp; external link http://askubuntu.com/questions/20852/making-stripe-cache-size-permanent\u0026thinsp; external link http://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56\u0026thinsp; external link https://raid.wiki.kernel.org/index.php/Performance\u0026thinsp; external link http://www.mjmwired.net/kernel/Documentation/md.txt\u0026thinsp; external link http://wiki.hetzner.de/index.php/Partition_Alignment/en\u0026thinsp; external link http://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment\u0026thinsp; external link O tym że LVM na RAID5 sam się wyrównuje:\nhttp://marc.info/?l=linux-raid\u0026m=126267824425009\u0026w=2\u0026thinsp; external link http://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\u0026thinsp; external link MDADM metadata format 1.2 wyrownuje sie do 4KiB:\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\u0026thinsp; external link Kalkulator stride\u0026rsquo;a\nhttp://busybox.net/~aldot/mkfs_stride.html\u0026thinsp; external link Format raid\u0026rsquo;a:\nhttps://raid.wiki.kernel.org/index.php/RAID_superblock_formats\u0026thinsp; external link Wyrównanie do 4kb - choć wydaje mi się że gościu robi to na czuja i ledwie mu się udało:\nhttp://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/","summary":"Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyrównanie do chunk\u0026rsquo;a z macierzy.\nPrawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych.","title":"LVM na RAID5 i dysku z sektorami 4KB"},{"content":"Po każdej aktualizacji Ubuntu mam trochę zabawy by pozbierać do kupy skaner i drukarkę z mojego urządzenia wielofunkcyjnego Brother DCP-130C. Wybrałem je bo był to jedyny producent, który deklarował wsparcie dla Linux\u0026rsquo;a\u0026hellip; choć z perspektywy czasu nie jestem pewien czy otrzymałem to czego się spodziewałem\u0026hellip; Co prawda zamieszczają instrukcje i aktualizują drivery ale jeszcze ani raz nie zdarzyło mi się by po aktualizacji systemu postępowanie według tych instrukcji zadziałało bez dodatkowej pomocy.. Olać!\nInstrukcja jest dla mojej drukarki ale powinna zadziałać również dla innych modeli Brother\u0026rsquo;a - w razie potrzeby można się posiłkować instrukcjami producenta.\nUruchomienie drukarki Pobieramy drivery dla drukarki z poniższej strony:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_prn.html\u0026thinsp; external link Instalujemy zależności:\napt-get install ia32-libs lib32stdc++ Instalujemy drivery (w moim przypadku dla DCP-130C):\ndpkg -i --force-all dcp130clpr-1.0.1-1.i386.deb dpkg -i --force-all dcp135ccupswrapper-1.0.1-1.i386.deb Teraz w przeglądarce wchodzimy na stronę konfiguracji CUPS\u0026rsquo;a: http://localhost:631/printers - powinna tam widnieć nasza drukarka z URI usb://Brother/ (np. usb://Brother/DCP-130C?serial=BRO00000000). Jeśli URI jest inne to modyfikujemy drukarkę by wybrać odpowiednie urządzenie i sterownik. Gdyby jednak drukarka nie została automatycznie dodana to wchodzimy na http://localhost:631/admin i klikamy Dodawanie drukarki - podajemy dane autoryzacyjne dla root\u0026rsquo;a, wybieramy urządzenie i sterownik.\nPo dodaniu drukarki można wejść w jej ustawienia (u mnie http://localhost:631/printers/DCP-130C) i w menu Administracja wybrać opcję: Ustaw domyślne opcje by określić domyślne parametry wydruków.\nZ dodawaniem drukarki nie miałem dużych problemów, ciekawiej jest ze skanerem\u0026hellip;\nUruchomienie skanera Z poniższej strony pobieramy drivery dla naszego modelu drukarki:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_scn.html\u0026thinsp; external link Ja pobrałem deb\u0026rsquo;a dla wersji 64-bitowej: brscan2-0.2.5-1.amd64.deb.\nInstalujemy (instrukcja producenta tutaj):\napt-get install sane-utils dpkg -i --force-all brscan2-0.2.5-1.amd64.deb Teoretycznie powinno to wystarczyć by root mógł korzystać ze skanera, więc popracujmy by użytkownicy też mogli. Edytujemy jako root plik /lib/udev/rules.d/40-libsane.rules i odszukujemy linię \u0026ldquo;# The following rule will disable\u0026rdquo; (u mnie gdzieś w okolicy 1180 linii), przed nią wklejamy tekst:\n# Brother scanners ATTRS{idVendor}==\u0026#34;04f9\u0026#34;, ENV{libsane_matched}=\u0026#34;yes\u0026#34; Zapisujemy plik. Teraz musimy dodać zainteresowanych użytkowników do grupy scanner, robimy to np. tak:\ngpasswd -a roman scanner I powtarzamy dla innych userów.\nI na koniec wisienka na torcie - na 64 bitowym systemie postępowanie według instrukcji zwyczajnie nie działa bo potrzebne biblioteki instalują się w złym miejscu\u0026hellip; sic!\nPliki trafiają do /usr/lib64 zamiast do /usr/lib - aby wszystko działało jak trzeba musimy przekopiować (albo chociaż podlinkować) z /usr/lib64 do /usr/lib - co dokładnie należy skopiować można znaleźć tutaj.\n","permalink":"https://gagor.pro/2012/11/instalacja-drukarki-i-skanera-brother-dcp-130c-na-ubuntu-12-04/","summary":"Po każdej aktualizacji Ubuntu mam trochę zabawy by pozbierać do kupy skaner i drukarkę z mojego urządzenia wielofunkcyjnego Brother DCP-130C. Wybrałem je bo był to jedyny producent, który deklarował wsparcie dla Linux\u0026rsquo;a\u0026hellip; choć z perspektywy czasu nie jestem pewien czy otrzymałem to czego się spodziewałem\u0026hellip; Co prawda zamieszczają instrukcje i aktualizują drivery ale jeszcze ani raz nie zdarzyło mi się by po aktualizacji systemu postępowanie według tych instrukcji zadziałało bez dodatkowej pomocy.","title":"Instalacja drukarki i skanera Brother DCP-130C na Ubuntu 12.04"},{"content":"Czasami potrzebny jest nam serwer pocztowy, który przyśle informacje dla root\u0026rsquo;a (np. monity smartd, mdadm, sypnięte crony itp) ale równocześnie nie chcemy stawiać pełnego serwera typu postfix/exim. Warto w tym celu wykorzystać zestaw heirloom-mailx + ssmtp. hairloom-mailx jest prostym shellowym klientem SMTP - przy okazji linkuje polecenie mail (przydatne w skryptach). ssmtp pełni funkcję serwera SMTP ale nie działa jako demon - proces uruchamia się gdy jest potrzebny i znika po wysłaniu maili. Dodatkowo ssmtp może zostać skonfigurowany by wysyłać maile nie tylko przez relay\u0026rsquo;a ale również autoryzując się na zewnętrznym serwerze pocztowym, np. gmail\u0026rsquo;u czy gdziekolwiek indziej.\nInstalacja apt-get install ssmtp heirloom-mailx Konfiguracja z relay\u0026rsquo;em Edytujemy plik /etc/ssmtp/ssmtp.conf - w konfiguracji z relay\u0026rsquo;em wystarczą te linijki:\nroot=postmaster@domena.pl mailhub=domena.pl hostname=serwerek.domena.pl Warto zwrócić uwagę że dobrze by było aby domena serwerek.domena.pl istniała i wskazywała na nasz serwer - dzięki temu poczta nie odpadnie na prostych filtrach antyspamowych.\nKonfiguracja pod gmail\u0026rsquo;a W ssmtp.conf potrzebujemy:\nroot=postmaster@domena.pl mailhub=smtp.gmail.com:587 hostname=serwerek.domena.pl AuthUser=twojekonto@gmail.com AuthPass=twoje_haslo_do_gmaila UseTLS=YES UseSTARTTLS=YES AuthMethod=LOGIN Test Warto teraz sprawdzić czy poczta wychodzi jak powinna:\necho test | mail -s \u0026#34;testowa wiadomosc\u0026#34; postmaster@domena.pl ","permalink":"https://gagor.pro/2012/11/prosty-mta-z-heirloom-mailx-i-ssmtp/","summary":"Czasami potrzebny jest nam serwer pocztowy, który przyśle informacje dla root\u0026rsquo;a (np. monity smartd, mdadm, sypnięte crony itp) ale równocześnie nie chcemy stawiać pełnego serwera typu postfix/exim. Warto w tym celu wykorzystać zestaw heirloom-mailx + ssmtp. hairloom-mailx jest prostym shellowym klientem SMTP - przy okazji linkuje polecenie mail (przydatne w skryptach). ssmtp pełni funkcję serwera SMTP ale nie działa jako demon - proces uruchamia się gdy jest potrzebny i znika po wysłaniu maili.","title":"Prosty MTA z heirloom-mailx i ssmtp"},{"content":"Lubię mieć porządek w folderach i jedna rzecz, która nie daje mi spokoju w systemach plików ext to widoczność folderu lost+found - niby można go skasować i powinien się odtworzyć (choć podobno odtworzenie w czasie fsck\u0026rsquo;a może spowodować utratę danych - trochę to dziwne i nie znalazłem źródła no ale powiedzmy że nie chcę go usuwać). Chciałem go ukryć (choćby w Nautilusie) by mnie nie drażnił. Oczywiście opcja z \u0026ldquo;.\u0026rdquo; na początku odpada, ale na szczęście Nautilus wykorzystuje pewien hack, który umożliwia ukrycie dowolnego pliku/folderu.\nTworzymy w głównym katalogu danego punktu montowania, plik .hidden i wpisujemy do niego nazwy plików, które chcemy ukryć - w moim przypadku lost+found:\ncd /root/katalogu echo \u0026#34;lost+found\u0026#34; \u0026gt;\u0026gt; .hidden Po odświeżeniu Nautilus nie pokazuje już katalogu lost+found - dla mnie bomba.\n","permalink":"https://gagor.pro/2012/10/nautilus-ukrywanie-lostfound/","summary":"Lubię mieć porządek w folderach i jedna rzecz, która nie daje mi spokoju w systemach plików ext to widoczność folderu lost+found - niby można go skasować i powinien się odtworzyć (choć podobno odtworzenie w czasie fsck\u0026rsquo;a może spowodować utratę danych - trochę to dziwne i nie znalazłem źródła no ale powiedzmy że nie chcę go usuwać). Chciałem go ukryć (choćby w Nautilusie) by mnie nie drażnił. Oczywiście opcja z \u0026ldquo;.\u0026rdquo; na początku odpada, ale na szczęście Nautilus wykorzystuje pewien hack, który umożliwia ukrycie dowolnego pliku/folderu.","title":"Nautilus - ukrywanie lost+found"},{"content":"Składniki 1 kostka masła, 1 żółtko, 6 łyżek cukru pudru, 1 szklanka mleka, 10 dkg wiórek kokosowych, 1 kieliszek wódki (ale z dwoma jest ostrzejsze), 3 łyżki kakao, 37 dkg herbatników kakaowych (doskonale spisują się dwie paczki maślanych herbatników kakaowych). Masa kakaowa Herbatniki dokładnie rozwałkować, dodać kakao i wymieszać. Zagotować pół szklanki mleka i gorącym zalać herbatniki. Całość wymieszać na jednolitą masę. W drugim pojemniku rozmiksować pół kostki masła i 3 łyżki cukru pudru - po chwili dodać żółtko i wódkę. Do tej masy stopniowo dodajemy herbatniki.\nMasa kokosowa Zagotować pół szklanki mleka i gorącym zalać wiórki kokosowe, wymieszać by równomiernie namokły. Rozmiksować pół kostki masła z 3 łyżkami cukru pudru. Stopniowo dodawać wiórki i nadal miksować.\nSkładamy całość Wałkujemy masę kakaową na papierze do pieczenie lub folii (papier jest lepszy) - staramy się uzyskać kształt zbliżony do prostokąta (ułatwi to zwijanie rolady). Tak przygotowaną ciemną masę pokrywamy równomierną warstwą masy kokosowej. Później delikatnie zaczynamy zwijać z jednej strony jak makowiec - na koniec staramy się dość dokładnie docisnąć papier by rolada nie \u0026ldquo;rozdęła\u0026rdquo; się przed stwardnieniem. Taki pakunek wrzucamy do lodówki na minimum 2~3 godziny.\nP.S. Chciałem wrzucić fotkę ale zbyt szybko zniknęła - może innym razem \u0026#x1f604;\n","permalink":"https://gagor.pro/2012/10/rolada-kokosowo-czekoladowa-na-zimno/","summary":"Składniki 1 kostka masła, 1 żółtko, 6 łyżek cukru pudru, 1 szklanka mleka, 10 dkg wiórek kokosowych, 1 kieliszek wódki (ale z dwoma jest ostrzejsze), 3 łyżki kakao, 37 dkg herbatników kakaowych (doskonale spisują się dwie paczki maślanych herbatników kakaowych). Masa kakaowa Herbatniki dokładnie rozwałkować, dodać kakao i wymieszać. Zagotować pół szklanki mleka i gorącym zalać herbatniki. Całość wymieszać na jednolitą masę. W drugim pojemniku rozmiksować pół kostki masła i 3 łyżki cukru pudru - po chwili dodać żółtko i wódkę.","title":"Rolada kokosowo-czekoladowa na zimno"},{"content":"Zdarzyło mi się kilka usług działających na serwerach Windows, które zwyczajnie sypały się i to tak brzydko że systemowe ustawienia by restartowały się po padzie nie wystarczały. Usługa działała np. 3 godziny by potem się położyć, albo nigdy nie wstawała po starcie systemu - po prostu jakaś masakra. Zgłoszenia do producenta często wyglądały tak że w jego testowym środowisku błędu nie udaje się powtórzyć (i nic dziwnego bo nie sądzę by z ich konfiguracji korzystali rzeczywiści użytkownicy). Oczywiście można zaczekać aż ukaże się magiczna łata usuwająca błąd ale do tego czasu to na nas wszyscy będą \u0026ldquo;wieszać psy\u0026rdquo; za niedostępność usługi.\nW najprostszym podejściu można napisać bat\u0026rsquo;cha, który będzie restartował usługę (kładł i startował, sprawdzał czy działa i tylko przy braku działania uruchamiał, itp) - taki skrypcik odpalamy co 5 minut i powinno to załatwić sprawę. Załatwi to najprostsze przypadki - ale nie będzie całkiem elastyczne.\nSzukałem programu, który działałby jako usługa i \u0026ldquo;pilnował\u0026rdquo; innych usług systemowych i\u0026hellip; można coś takiego kupić za 200$ (sic!). Postanowiłem poszukać na forum AutoIt\u0026rsquo;a - basciopodobnego języka skryptowego, który wielokrotnie pomagał mi zautomatyzować pewne zadania administracyjne - trafiłem na tego posta: http://www.autoitscript.com/forum/topic/80201-service-udf-v2-run-your-exe-as-service/\u0026thinsp; external link Z tego adresu pobieramy źródła - zmieniamy nazwę pliczku ServiceExample.au3 na np. ServiceKeeper.au3 i edytujemy (szukamy linijki region -\u0026gt; insert your running code here:\n#region --\u0026gt; insert your running code here Local $status = _Service_QueryStatus(\u0026#34;shittyservice\u0026#34;) If $status[1] \u0026lt;\u0026gt; $SERVICE_RUNNING Then If $bDebug Then logprint(\u0026#34;shittyservice service not running!\u0026#34;) If $status[1] == $SERVICE_PAUSED Then If _Service_Resume(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service resumed successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service resume failded!\u0026#34;) If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) EndIf EndIf EndIf If $status[1] == $SERVICE_STOPPED Then If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service start failded!\u0026#34;) EndIf EndIf EndIf Sleep(60000) # jak czesto sprawdzac san uslug - czas w milisekundach #endregion --\u0026gt; insert your running code here Powyższy przykład sprawdza czy pewna \u0026ldquo;shittyusługa\u0026rdquo; działa, a jeśli nie to podejmuje różne próby jej uruchomienia. Przykład najprostszy z możliwych ale bardzo łatwo można zamiast sprawdzenia jedne usługi powielić ten kod dla kilku usług i sprawdzać je w pętli (w wolnej chwili może wrzucę taki kod).\nSkrypcik dostosowujemy do swoich potrzeb, kompilujemy i wrzucamy na serwer potrzebujący \u0026ldquo;rozrusznika\u0026rdquo;. Usługę trzeba zainstalować przez odpalenie skryptu z parametrem -i, w naszym przypadku wyglądało by to tak:\nServiceKeeper.exe -i Usługę można odinstalować przez uruchomienie z parametrem -u.\nDobrze ustawić tą usługę by startowała z opóźnieniem. Nie wydaje mi się sensowe sprawdzanie stanu usług częściej niż raz na minutę.\nZdarzało mi się że ta usługa sypała się przy jej zatrzymywaniu (jest o tym wzmianka na forum), ale to mnie akurat mało boli bo innych problemów nie miałem.\n","permalink":"https://gagor.pro/2012/10/utrzymanie-przy-zyciu-sypiacych-sie-uslug-na-serwerach-windows/","summary":"Zdarzyło mi się kilka usług działających na serwerach Windows, które zwyczajnie sypały się i to tak brzydko że systemowe ustawienia by restartowały się po padzie nie wystarczały. Usługa działała np. 3 godziny by potem się położyć, albo nigdy nie wstawała po starcie systemu - po prostu jakaś masakra. Zgłoszenia do producenta często wyglądały tak że w jego testowym środowisku błędu nie udaje się powtórzyć (i nic dziwnego bo nie sądzę by z ich konfiguracji korzystali rzeczywiści użytkownicy).","title":"Utrzymanie przy życiu sypiących się usług na serwerach Windows"},{"content":"Jeśli szukamy statystyk dla strony internetowej i ze względu na jej zawartość (np. sklep, coś ze zwiększonym naciskiem na poufność etc..) nie potrafimy zaufaj wujkowi Googlowi to warto przyglądnąć się Piwikowi.\nJest to system statystyk aspirujący do bycia Open Source\u0026rsquo;ową alternatywą dla Google Analytics. Aspirujący (a nie będący) z tego względu że Google przechodząc na domyślny HTTPS (SPDY) dla zalogowanych użytkowników uniemożliwił śledzenie stron z których pochodzą odwiedziny (tzw. refferals) - tym prostym sposobem tylko GA jest w stanie dostarczyć pełnych informacji o wszystkich użytkownikach.\nNiemniej Piwik nadal może być bardzo atrakcyjnym narzędziem choćby dlatego że:\nstatystyki zbieramy \u0026ldquo;lokalnie\u0026rdquo;, czyli gdzieś na naszym serwerze - mamy nad nimi władzę (z wszystkimi konsekwencjami), do Piwik\u0026rsquo;a możemy łatwo stworzyć własne rozszerzenia, które przedstawią to co chcemy w sposób jakiego oczekujemy, statystyki możemy zbierać przez dodanie odwołania do skryptu w kodzie strony lub okresowo importując logi z serwera (jak wspomniany przeze mnie kiedyś awstats) lub na oba wspomniane sposoby - stąd Piwik świetnie sprawdza się w hostingach (możemy pokazać klientowi ile miał wejść, zrobić statystyki wykorzystania pasma itp), oprawa graficzna jest świeższa niż w AWStats \u0026#x1f603; ma nieduże wymagania, głównie: PHP + MySQL. Instrukcję instalacji i więcej informacji można znaleźć tutaj: http://pl.piwik.org/dokumentacja/instalacja-piwika/\n","permalink":"https://gagor.pro/2012/09/piwik-alternatywa-dla-google-analytics/","summary":"Jeśli szukamy statystyk dla strony internetowej i ze względu na jej zawartość (np. sklep, coś ze zwiększonym naciskiem na poufność etc..) nie potrafimy zaufaj wujkowi Googlowi to warto przyglądnąć się Piwikowi.\nJest to system statystyk aspirujący do bycia Open Source\u0026rsquo;ową alternatywą dla Google Analytics. Aspirujący (a nie będący) z tego względu że Google przechodząc na domyślny HTTPS (SPDY) dla zalogowanych użytkowników uniemożliwił śledzenie stron z których pochodzą odwiedziny (tzw. refferals) - tym prostym sposobem tylko GA jest w stanie dostarczyć pełnych informacji o wszystkich użytkownikach.","title":"Piwik - alternatywa dla Google Analytics"},{"content":"Co prawda adresy URL pozwalają na stosowanie zarówno dużych jak i małych liter ale różne systemy mogą je różnie obsługiwać i może się trafić sytuacja, w której nie zechcemy by np. duże litery w ogóle pojawiały się w adresach URL. Doskonały przykład to mój niedawny wpis: Apache: ograniczenie dostępu dla zalogowanych użytkowników z mod_rewrite i mod_auth_basic .\nZachodzi tam sytuacja, w której katalog użytkownika jest jego loginem małymi literami (bądź dużymi - jak kto woli), a użytkownik wpisując login może użyć zarówno małych jak i dużych liter i tutaj zaczyna się jazda. Można użyć modyfikatora NC (no case), ale to wpłynie tylko na porównania - przepisanie ścieżki na nazwę podaną przez usera (z dużymi i małymi literami) przekieruje do katalogu, którego nie ma (bo jest katalog tylko małymi/dużymi).\nI wtedy przyda się taka sztuczka:\nRewriteEngine On RewriteMap lc int:tolower RewriteCond %{REQUEST_URI} [A-Z] RewriteRule (.*) ${lc:$1} [R=301,L] Definiujemy mapę korzystając z wbudowanego w moduł słownika tolower, a następnie jeśli w URL\u0026rsquo;u występują (w tym przypadku) duże litery to przekierowujemy na URL\u0026rsquo;a z małymi.\n","permalink":"https://gagor.pro/2012/09/mod_rewrite-wymuszenie-malych-liter-w-adresie-url/","summary":"Co prawda adresy URL pozwalają na stosowanie zarówno dużych jak i małych liter ale różne systemy mogą je różnie obsługiwać i może się trafić sytuacja, w której nie zechcemy by np. duże litery w ogóle pojawiały się w adresach URL. Doskonały przykład to mój niedawny wpis: Apache: ograniczenie dostępu dla zalogowanych użytkowników z mod_rewrite i mod_auth_basic .\nZachodzi tam sytuacja, w której katalog użytkownika jest jego loginem małymi literami (bądź dużymi - jak kto woli), a użytkownik wpisując login może użyć zarówno małych jak i dużych liter i tutaj zaczyna się jazda.","title":"mod_rewrite - wymuszenie małych liter w adresie URL"},{"content":"Obecnie dostępna jest już beta 2 Debiana Wheezy i utrzymywane są aktualizacje bezpieczeństwa więc powolutku można na testowych maszynach sprawdzać co i jak się zmieniło.\nPonieważ do finalnej wersji pewnie sporo się jeszcze zmieni to postaram się z czasem aktualizować ten post by zawierał bieżące informacje.\nW razie wątpliwości patrz tutaj: http://wiki.debian.org/DebianTesting\nRobimy backup Aktualizujemy źródła wskazywały na paczki gałęzi testing (poniższe polecenie nadpisze Twoje obecne repozytoria): cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb http://security.debian.org/ wheezy/updates main contrib non-free deb-src http://security.debian.org/ wheezy/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib SRC Teraz odświeżamy repozytoria:\nsudo apt-get update Proponuję pobrać też pliki by podczas aktualizacji wszystkie leżały w cache\u0026rsquo;u - na wypadek gdyby nagle padło łącze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz zaktualizujemy kluczowe paczki:\nsudo apt-get install apt dpkg I resztę systemu:\nsudo apt-get dist-upgrade To teraz pozostało sprawdzić co poszło nie tak\u0026hellip; Powodzenia 😉\n","permalink":"https://gagor.pro/2012/09/aktualizacja-debian-squeeze-do-wheezy/","summary":"Obecnie dostępna jest już beta 2 Debiana Wheezy i utrzymywane są aktualizacje bezpieczeństwa więc powolutku można na testowych maszynach sprawdzać co i jak się zmieniło.\nPonieważ do finalnej wersji pewnie sporo się jeszcze zmieni to postaram się z czasem aktualizować ten post by zawierał bieżące informacje.\nW razie wątpliwości patrz tutaj: http://wiki.debian.org/DebianTesting\nRobimy backup Aktualizujemy źródła wskazywały na paczki gałęzi testing (poniższe polecenie nadpisze Twoje obecne repozytoria): cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.","title":"Aktualizacja Debian Squeeze do Wheezy"},{"content":"Dziś TIP z przeciwnego obozu - oprócz linuksowych systemów administruję również paroma serwerami windowsowymi i tutaj również (a czasem nawet bardziej) uda mi się znaleźć coś wartego zapamiętania.\nJedna z najbardziej charakterystycznych rzeczy na komputerach przyłączonych do domeny Windows to wyświetlanie \u0026ldquo;różnych dziwnych rzeczy\u0026rdquo; przy starcie systemu. Zarówno na Windowsie 2000 jak i na XP\u0026rsquo;ku na małym okienku przewijają informacje o aktualizacji polityk, instalacji oprogramowania itp\u0026hellip;\nZachowanie to zmieniło się na Vistach i 7-kach, które są nieco mniej rozmowne i wyświetlają jedynie komunikat typu \u0026ldquo;Trwa uruchamianie systemu\u0026hellip;\u0026rdquo; i tyla\u0026hellip; Załóżmy że wrzucimy do instalacji kilka paczek i jeszcze zmienimy kilka polityk i przez to komputer na tym napisie zatrzyma się na 5~10 minut - co zrobi użyszkodnik po 3 minutach? Dojdzie do wniosku że \u0026ldquo;coś się zwiesiło\u0026rdquo; i zresetuje komputer, a że akurat było nieco operacji dyskowych to mamy praktycznie pewną rozwałkę tego systemu.\nJednym z rozwiązań jest zwiększenie \u0026ldquo;gadatliwości\u0026rdquo; tego etapu uruchamiania systemu - jest to możliwe o ile posiadamy kontroler domeny na minimum Windows Server 2008. Tworzymy GPO w którym ustawiamy: Computer Config -\u0026gt; Admin Templates -\u0026gt; System -\u0026gt; Verbose vs normal status messages na Enabled.\nWłączenie tej opcji spowoduje zwiększenie liczby wypisywanych komunikatów, nazw instalowanych programów i operacji wykonywanych przez system w fazie uruchamiania - większość użyszkodników widząc że zmieniają się statusy (coś instaluje itp) zwiększa \u0026ldquo;okienko czasowe\u0026rdquo; do naciśnięcia resetu do ok 15 minut 😉\n","permalink":"https://gagor.pro/2012/09/gpo-windows-7-postep-przetwarzania-polityk-przy-starcie-systemu/","summary":"Dziś TIP z przeciwnego obozu - oprócz linuksowych systemów administruję również paroma serwerami windowsowymi i tutaj również (a czasem nawet bardziej) uda mi się znaleźć coś wartego zapamiętania.\nJedna z najbardziej charakterystycznych rzeczy na komputerach przyłączonych do domeny Windows to wyświetlanie \u0026ldquo;różnych dziwnych rzeczy\u0026rdquo; przy starcie systemu. Zarówno na Windowsie 2000 jak i na XP\u0026rsquo;ku na małym okienku przewijają informacje o aktualizacji polityk, instalacji oprogramowania itp\u0026hellip;\nZachowanie to zmieniło się na Vistach i 7-kach, które są nieco mniej rozmowne i wyświetlają jedynie komunikat typu \u0026ldquo;Trwa uruchamianie systemu\u0026hellip;\u0026rdquo; i tyla\u0026hellip; Załóżmy że wrzucimy do instalacji kilka paczek i jeszcze zmienimy kilka polityk i przez to komputer na tym napisie zatrzyma się na 5~10 minut - co zrobi użyszkodnik po 3 minutach?","title":"GPO: Windows 7 - postęp przetwarzania polityk przy starcie systemu"},{"content":"Gdy administruje się dużymi stronami internetowymi raz na czas np. po większych zmianach w konfiguracji zachodzi potrzeba sprawdzenia czy na stronie nie ma stron prowadzących donikąd. O ile w małych serwisach można samemu szybko przeklikać się przez stronkę to dla starych rozrośniętych serwisów nie jest to takie proste.\nJest kilka narzędzi których można użyć do testowania linków na stronach - każde z nich ma swoje zalety i wady, postaram się je przybliżyć.\nwget Wget\u0026rsquo;a najprawdopodobniej już masz i możesz zaczynać:\nwget -o /tmp/wget.log -nv -r -p http://example.com W pliku /tmp/wget.log możemy znaleźć komunikaty błędów i na dobrą sprawę tyle. Ciężko to przetworzyć ale jeśli nasz serwis ma mechanizm do np. mailowego powiadomienia w momencie wystąpienia krytycznego błędu to wget\u0026rsquo;em najszybciej można takie strony wyłapać.\nlinkchecker Uruchamia się go tak:\nlinkchecker -t3 --no-warnings http://example.com Program jakoś nie przypadł mi do gustu - działał cholernie wolno i to na całkiem małej stronie.\nlinklint Z trzech programów ten ma najdziwniejszą składnię - ale da się tego nauczyć, a możliwości ma chyba najwięcej.\nlinklint -error -warn -xref -forward -out report.txt -net -http -host example.com /@ Wystarczająco szybki i generuje przejrzyste raporty.przejrzyste raporty.\n","permalink":"https://gagor.pro/2012/09/sprawdzanie-nieaktywnych-linkow-na-stronie/","summary":"Gdy administruje się dużymi stronami internetowymi raz na czas np. po większych zmianach w konfiguracji zachodzi potrzeba sprawdzenia czy na stronie nie ma stron prowadzących donikąd. O ile w małych serwisach można samemu szybko przeklikać się przez stronkę to dla starych rozrośniętych serwisów nie jest to takie proste.\nJest kilka narzędzi których można użyć do testowania linków na stronach - każde z nich ma swoje zalety i wady, postaram się je przybliżyć.","title":"Sprawdzanie nieaktywnych linków na stronie"},{"content":"Jeśli posiadasz napęd taśmowy LTO do archiwizacji/backupu danych to wiesz że bardzo ważne jest opisywanie taśm szczególnie gdy trzeba coś odzyskać. Jeżeli masz szczęście to posiadasz nie pojedynczy napęd a autoloader obsługujący wiele taśm i wiesz że najlepiej gdy taśmy są opisane kodami paskowymi które autoloader potrafi rozpoznać. Dzięki temu nie ma potrzeby odczytywania nr. seryjnego z taśmy tylko szybciutko skanerem kodów. Tasiemki można kupować już z kodami ale tańsze są te bez kodów\u0026hellip; i tu pytanie - czy da się tanio zdobyć etykiety?\nKtoś poświęcił trochę czasu i przygotował webowe narzędzie do generowania kodów paskowych dla napędów LTO - plik generowany jest do postaci pdf\u0026rsquo;a który można później wydrukować na papierze samoprzylepnym, pomachać chwilę nożyczkami i TA DAM! Mamy etykiety 😉\nOczywiście zabawa ma sens jeśli potrzebujemy tylko kilku szt. taśm tygodniowo - przy większej ilości lepiej wydać kasę i niech ktoś inny powycina etykiety za nas.\n","permalink":"https://gagor.pro/2012/09/generator-kodow-paskowych-dla-napedow-tasmowych-lto/","summary":"Jeśli posiadasz napęd taśmowy LTO do archiwizacji/backupu danych to wiesz że bardzo ważne jest opisywanie taśm szczególnie gdy trzeba coś odzyskać. Jeżeli masz szczęście to posiadasz nie pojedynczy napęd a autoloader obsługujący wiele taśm i wiesz że najlepiej gdy taśmy są opisane kodami paskowymi które autoloader potrafi rozpoznać. Dzięki temu nie ma potrzeby odczytywania nr. seryjnego z taśmy tylko szybciutko skanerem kodów. Tasiemki można kupować już z kodami ale tańsze są te bez kodów\u0026hellip; i tu pytanie - czy da się tanio zdobyć etykiety?","title":"Generator kodów paskowych dla napędów taśmowych LTO"},{"content":"Szkoda że polecenia do obsługi NFS\u0026rsquo;a nie zaczynają się od nfs* - łatwiej byłoby mi je zapamiętać. A jednym z takich, zapominanych najczęściej jest listowanie zasobów, szczególnie przydatne gdy korzysta się z NFS\u0026rsquo;a na jakimś NAS\u0026rsquo;ie (którego magiczny soft nie pokazuje gdzie i co eksportuje):\nshowmount -e 192.168.1.10 Export list for 192.168.1.10: /mnt/pools/A/A0/Music * /mnt/pools/A/A0/Movies * /mnt/pools/A/A0/Backups * /mnt/pools/A/A0/Pictures * /mnt/pools/A/A0/Documents * Teraz możemy zamontować zasób:\nmount 192.168.1.10:/mnt/pools/A/A0/Music /mnt/music ","permalink":"https://gagor.pro/2012/09/listowanie-zasobow-nfs/","summary":"Szkoda że polecenia do obsługi NFS\u0026rsquo;a nie zaczynają się od nfs* - łatwiej byłoby mi je zapamiętać. A jednym z takich, zapominanych najczęściej jest listowanie zasobów, szczególnie przydatne gdy korzysta się z NFS\u0026rsquo;a na jakimś NAS\u0026rsquo;ie (którego magiczny soft nie pokazuje gdzie i co eksportuje):\nshowmount -e 192.168.1.10 Export list for 192.168.1.10: /mnt/pools/A/A0/Music * /mnt/pools/A/A0/Movies * /mnt/pools/A/A0/Backups * /mnt/pools/A/A0/Pictures * /mnt/pools/A/A0/Documents * Teraz możemy zamontować zasób:\nmount 192.168.1.10:/mnt/pools/A/A0/Music /mnt/music ","title":"Listowanie zasobów NFS"},{"content":"Niedawno trafiłem na ciekawy problem w mod_rewrite - by przekierowywać użytkowników logujących się jednym z modułów mod_auth_basic do dedykowanych im katalogów, równocześnie blokując dostęp do katalogów innych użytkowników. Nie brzmi to jakoś strasznie ale problem okazał się być całkiem nietrywialnym. Teoretyczne rozwiązanie sprowadzało się do wyszukania loginu użytkownika ze ścieżki URI i porównania z nazwą użytkownika ze zmiennej %{REMOTE_USER} - jeśli wartości się różnią to Forbidden. Ale szybko okazało się że w RewriteCond zmienne z dopasowań można podstawiać tylko w pierwszym parametrze i że o ile można RewriteCond\u0026rsquo;y połączyć wyrażeniami logicznymi typu AND/OR to nie ma możliwości porównania czy dopasowania z kolejnych RewriteCond\u0026rsquo;ów są identyczne. Po kilku dniach szperania w dokumentacji i różnych tutorialach udało mi się trafić na jedną wartościową wskazówkę ale tej stronki już nie ma, więc opiszę problem dla potomnych.\nZałożenia są takie:\nmamy vhost\u0026rsquo;a który udostępnia wszystkie foldery użytkowników, każdy użytkownik posiada folder o nazwie dokładnie takiej samej jak jego login, użytkownik po zalogowaniu ma być przekierowany do swojego folderu i przy próbie przejścia do folderów innych użytkowników albo nawracamy go do jego folderu/albo dajemy forbidden. Konfiguracja vhost\u0026rsquo;a Poniżej podstawowa konfiguracja vhost\u0026rsquo;a:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Rewrite\u0026rsquo;y I najciekawsza część czyli rewrite\u0026rsquo;y. Zaczynamy od przekierowania użytkownika do jego folderu:\nRewriteRule ^$ /%{REMOTE_USER} [R,L] Powyższy rewrite sprawdza czy próbujemy wejść do głównego katalogu, jeśli tak to przekierowujemy do katalogu użytkownika.\nTo teraz magia, której długo szukałem \u0026#x1f603;\nRewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] Już wyjaśniam co to robi - zacznę od przypomnienia że pomimo takiego zapisu w konfiguracji reguły są przetwarzane nieco inaczej: Apache najpierw sprawdza czy dany URI pasuje do wyrażenia w RewriteRule, a dopiero gdy tak jest sprawdzane są warunki w RewriteCond. Czyli RewriteRule dopasowuje pierwszą część URI aż do znaku ukośnika / i zapamiętuje w zmiennej $1. Dopiero teraz RewriteCond dopasowuje i zapamiętuje login użytkownika w zmiennej %1 (tak rule zapamiętuje w zmiennych z $, cond w zmiennych z %). Teraz gdy mamy już zapamiętane loginy z URI i zmiennej to możemy je zapisać obok siebie w kolejnym RewriteCond oddzielając znakiem który w loginie wystąpić nie powinien (np. dwukropkiem) - $1:%1. Teraz dopasowujemy pierwszą część \u0026ldquo;sklejki\u0026rdquo;, czyli ^([^:]+): i zaraz potem wymagamy by pojawiła się ta sama wartość przez wsteczną referencję \\1$ - to porównywane jest z pierwszym parametrem cond\u0026rsquo;a. To dopasowanie jest prawdziwe gdy użytkownik loguje się prawidłowo, więc negujemy je stawiając ! na początku regexp\u0026rsquo;a, by każde błędne logowanie powodowało wywołanie RewriteRule, czyli Forbidden.\nPogmatwane? Więc teraz na przykładzie:\nbłędne logowanie (bo prostsze):\nlogin: roman\nuri: zbyszek/\npo dopasowaniu w $1 mamy zbyszek, a w %1 mamy roman, więc $1:%1 to zbyszek:roman, ostatni cond sprawdza czy zbyszek:roman różni się od zbyszek:zbyszek - a skoro tak to blokujemy dostęp, dobre logowanie:\nlogin: roman\nuri: roman/\npo dopasowaniu w $1 i %1 mamy roman i sprawdzamy czy $1:%1 jest zgodne z roman:roman, a jest więc po negacji nie blokujemy dostępu. Skoro dostępu nie blokujemy to roman może dostać się do swoich plików. Finalna konfiguracja Zostało przedstawienie całościowo konfiguracji:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user RewriteEngine on RewriteRule ^$ /%{REMOTE_USER} [R,L] RewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Duże i małe litery wpisywane w loginie przez userów Zerknij tutaj.\n","permalink":"https://gagor.pro/2012/09/apache-ograniczenie-dostepu-dla-zalogowanych-uzytkownikow-z-mod_rewrite-i-mod_auth_basic/","summary":"Niedawno trafiłem na ciekawy problem w mod_rewrite - by przekierowywać użytkowników logujących się jednym z modułów mod_auth_basic do dedykowanych im katalogów, równocześnie blokując dostęp do katalogów innych użytkowników. Nie brzmi to jakoś strasznie ale problem okazał się być całkiem nietrywialnym. Teoretyczne rozwiązanie sprowadzało się do wyszukania loginu użytkownika ze ścieżki URI i porównania z nazwą użytkownika ze zmiennej %{REMOTE_USER} - jeśli wartości się różnią to Forbidden. Ale szybko okazało się że w RewriteCond zmienne z dopasowań można podstawiać tylko w pierwszym parametrze i że o ile można RewriteCond\u0026rsquo;y połączyć wyrażeniami logicznymi typu AND/OR to nie ma możliwości porównania czy dopasowania z kolejnych RewriteCond\u0026rsquo;ów są identyczne.","title":"Apache: ograniczenie dostępu dla zalogowanych użytkowników z mod_rewrite i mod_auth_basic"},{"content":"Wybór dobrego X-terminala to w życiu admina prawie jak wybór żony\u0026hellip; spędza się wspólnie dużo czasu i miło gdy estetycznie wygląda, robi to co chcemy, itd\u0026hellip; 😉\nNie lubię gnome-terminal'a bo domyślnie binduje F10 co wnerwia mnie w midnight commanderze, stąd szukałem i szukałem i jak dotychczas najbardziej podpasował mi unicode-rxvt. Można uruchamiać go po prostu jako urxvt lub uruchomić demona urxvtd po zalogowaniu i potem odpalać tylko klienta urxvtc. Druga metoda skutkuje natychmiastowym startem terminala, wiec gdy podbinduję go sobie pod F12 mam terminal zawsze pod ręką w mniej niż sekundę. Dodatkowo fajnie wygląda z półprzezroczystością i nie ma żadnych dodatkowych menu/gadżetów.\nSkróty klawiaturowe Wygodne skróty klawiaturowe to kolejny atut tego terminala, najczęściej korzystam z:\nshift+dół - otwarcie nowej karty, shift+lewo/shift+prawo - przejście na kartę w lewo/prawo, ctrl+lewo/ctrl+prawo - przeniesienie karty w lewo/prawo, ctrl+d - zamknij kartę. Więcej ciekawych informacji o tym terminalu można znaleźć tutaj\u0026thinsp; external link .\nKonfig dla urxvt cat \u0026gt; ~/.Xdefaults \u0026lt;\u0026lt;SRC visualBell: False Xft.antialias: true Xft.hinting: true Xft.hintstyle: 0 Xft.dpi: 75 urxvt*termName: rxvt-unicode urxvt*geometry: 110x35 urxvt*background: rgba:2000/2000/2000/dddd urxvt*foreground: white urxvt*depth: 32 urxvt*fading: 40 urxvt*shading: 40 # fajny font ale w nowszych Ubuntu dziwnie zachowuje sie kursor ;-( #urxvt*font: xft:Terminus:pixelsize=14 #urxvt*font: xft:Monospace:pixelsize=12 urxvt*font: xft:Ubuntu Mono:pixelsize=14 urxvt*scrollBar: false urxvt*saveLines: 30000 urxvt*tintColor: gray urxvt*perl-ext-common: default,matcher,selection-autotransform,tabbed,selection-pastebin # urlLauncher otwiera wpisaną przeglądarkę # po kliknięciu środkowym klawiszem myszy #urxvt*urlLauncher: firefox urxvt*urlLauncher: chromium-browser # to już nie jest config dla urxvt # ale przeważnie też go dorzucam xterm*geometry: 110x35 xterm*background: black xterm*foreground: grey xterm*fading: 90 xterm*shading: 50 xterm*inheritPixmap: true xterm*font: -misc-fixed-medium-r-*-*-12-*-*-*-*--iso10646-1 xterm*scrollBar: false xterm*saveLines: 30000 xterm*tintColor: gray SRC P.S. Jeśli urxvt wydaje Ci się zbyt spartański to zerknij na terminator\u0026rsquo;a\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2012/09/unicode-rxvt-moje-ustawienia/","summary":"Wybór dobrego X-terminala to w życiu admina prawie jak wybór żony\u0026hellip; spędza się wspólnie dużo czasu i miło gdy estetycznie wygląda, robi to co chcemy, itd\u0026hellip; 😉\nNie lubię gnome-terminal'a bo domyślnie binduje F10 co wnerwia mnie w midnight commanderze, stąd szukałem i szukałem i jak dotychczas najbardziej podpasował mi unicode-rxvt. Można uruchamiać go po prostu jako urxvt lub uruchomić demona urxvtd po zalogowaniu i potem odpalać tylko klienta urxvtc. Druga metoda skutkuje natychmiastowym startem terminala, wiec gdy podbinduję go sobie pod F12 mam terminal zawsze pod ręką w mniej niż sekundę.","title":"unicode-rxvt - moje ustawienia"},{"content":"Jeżeli chcemy by po ponownym uruchomieniu w czasie startu zostały sprawdzone wszystkie dyski narzędziem to można to osiągnąć na dwa sposoby. Zawsze gdy tego potrzebuję zastanawiam się tylko jaki plik trzeba było utworzyć\u0026hellip; forcefsck, fsck, fsckforce\u0026hellip; więc notuję 😉\nUtworzenie pliku /forcefsck Pierwsza metoda polega na utworzeniu pliku forcefsck w głównym katalog, robimy to poniższym poleceniem:\nsudo touch /forcefsck Polecenie shutdown Druga metoda wykorzystuje parametr -F polecenia shutdown ale nie działa na wszystkich dystrybucjach (w takiej sytuacji patrz pierwsza metoda):\nsudo shutdown -rF now ","permalink":"https://gagor.pro/2012/09/wymuszenie-fsck-po-restarcie/","summary":"Jeżeli chcemy by po ponownym uruchomieniu w czasie startu zostały sprawdzone wszystkie dyski narzędziem to można to osiągnąć na dwa sposoby. Zawsze gdy tego potrzebuję zastanawiam się tylko jaki plik trzeba było utworzyć\u0026hellip; forcefsck, fsck, fsckforce\u0026hellip; więc notuję 😉\nUtworzenie pliku /forcefsck Pierwsza metoda polega na utworzeniu pliku forcefsck w głównym katalog, robimy to poniższym poleceniem:\nsudo touch /forcefsck Polecenie shutdown Druga metoda wykorzystuje parametr -F polecenia shutdown ale nie działa na wszystkich dystrybucjach (w takiej sytuacji patrz pierwsza metoda):","title":"Wymuszenie fsck po restarcie"},{"content":"Znajomi co jakiś czas pytają mnie: jak nazywa się ta aplikacja, którą masz na telefonie do\u0026hellip;? Z jakiego programu do poczty korzystasz na tel\u0026hellip;? itd\u0026hellip;\nPytacie - więc macie \u0026#x1f603;\nDGT GTD\u0026thinsp; external link Bardzo przydatna lista TODO. Stworzona z myślą o metodzie Getting Things Done i bardzo ułatwia pamiętanie u różnych zadaniach. Posiada też bardzo wygodny widżet, na którym możemy podglądnąć nasze najbliższe zadania.\nOpera Mini\u0026thinsp; external link Wbudowana przeglądarka jest niezła, ale Opera Mini kompresuje mocno strony wykorzystując pośredniczące serwery proxy co znacznie obniża koszty transmisji danych. Od niedawna posiada tez prymitywny ale niezgorszy czytnik RSS\u0026rsquo;ów.\nWiFinder\u0026thinsp; external link Prosty i funkcjonalny skaner sieci WiFi - wolę go zamiast domyślnego narzędzie do wyszukiwania i podłączania sieci WiFi.\nWiFi Analyzer\u0026thinsp; external link Bardziej zaawansowany skaner, pokazujący moc sygnały do poszczególnych AP, jakość danego kanału. Szpanerska aplikacja przy kolegach adminach 😉\nK-9 Mail\u0026thinsp; external link Klient pocztowy - dość zaawansowany funkcjonalnie ale z bardzo prostym interfejsem (choć początkowa konfiguracja bywa nieco \u0026ldquo;tricky\u0026rdquo;).\nAdFree Android\u0026thinsp; external link Na root\u0026rsquo;owanym telefonie trzeba to mieć - aplikacja blokuje reklamy w większości popularnych aplikacji.\nAdvanced Task Killer\u0026thinsp; external link Bardzo przydatny programik - szczególnie na słabszych telefonach (jak mój). Zabija aplikacje działające w tle, zwalniając pamięć i odciążając procesor. Dzięki temu nawet cienki telefon działa całkiem żwawo. Dostajemy również widżet, który za jednym dotknięciem zabija wszystko 😉\nTransportoid\u0026thinsp; external link Rozkład jazdy komunikacji miejskiej dla wielu polskich miast. Można sprawdzić połączenia danej linii, odjazdy z danego przystanku, wyszukać połączenia. Jesteśmy automatycznie informowani o zmianach rozkładu, który można z poziomu aplikacji pobrać (bez żadnych rejestracji).\n","permalink":"https://gagor.pro/2012/09/moje-ulubione-aplikacje-na-androida/","summary":"Znajomi co jakiś czas pytają mnie: jak nazywa się ta aplikacja, którą masz na telefonie do\u0026hellip;? Z jakiego programu do poczty korzystasz na tel\u0026hellip;? itd\u0026hellip;\nPytacie - więc macie \u0026#x1f603;\nDGT GTD\u0026thinsp; external link Bardzo przydatna lista TODO. Stworzona z myślą o metodzie Getting Things Done i bardzo ułatwia pamiętanie u różnych zadaniach. Posiada też bardzo wygodny widżet, na którym możemy podglądnąć nasze najbliższe zadania.\nOpera Mini\u0026thinsp; external link Wbudowana przeglądarka jest niezła, ale Opera Mini kompresuje mocno strony wykorzystując pośredniczące serwery proxy co znacznie obniża koszty transmisji danych.","title":"Moje ulubione aplikacje na Android’a"},{"content":"Jedną z rzeczy, które podobają mi się w maszynach wirtualnych Xen jest możliwość zrobienia backupu całego obrazu i szybkie odzyskanie już w trakcie ciężkiej awarii. Gdy dodatkowo korzysta się z LVM\u0026rsquo;a to można na chwilę wyłączyć DomU, utworzyć snapshot jego dysków, uruchomić DomU i w trakcie działania robić spójny backup ze snapshot\u0026rsquo;a. Dzięki takiemu mechanizmowi serwer jest niedostępny przez kilkanaście sekund, a backup spójny jakby został wykonany przy całkowicie wyłączonej maszynie. Taki backup sprowadza się do kilku poleceń które można oskryptować np.:\nlvcreate -L1000M -s -n volumendomu-snap /dev/vg/volumendomu dd if=/dev/vg/volumendomu | gzip -9 \u0026gt; backup.img.gz Problem pojawia się przy próbie montowania takiego snapshot\u0026rsquo;a by uzyskać dostęp do plików gdy na volumenie LVM zostanie utworzona partycja i dopiero ona formatowana (domyślnie przy ext3/4). Czyli potrzebujemy zamontować partycję z volumenu LVM ale ta nie jest wprost widoczna (nie ma urządzenia np. /dev/vg/volumendomu1).\nTen sam problem pojawia się przy dostępie do partycji \u0026ldquo;zaszytych\u0026rdquo; w obrazie zrzuconym narzędziem dd z całego dysku, np.:\ndd if=/dev/sda of=/mnt/backups/somewhere.img W obu przypadkach w obrazie/wolumenie jest zaszyta partycja i przy próbie montowania dostaniemy tylko monit o nieznanym typie systemu plików.\nBy wylistować partycje wewnątrz obrazu lub wolumenu najwygodniej posłużymć się narzędziem parted :\nsudo parted -s /mnt/backups/somewhere.img \u0026#34;unit B print\u0026#34; Model: (file) Dysk /mnt/backups/somewhere.img: 500105740288B Rozmiar sektora (logiczny/fizyczny): 512B/512B Tablica partycji: msdos Numer Początek Koniec Rozmiar Typ System plików Flaga 1 1048576B 500105740287B 500104691712B primary ntfs Parted może działać albo w trybie interaktywnym albo razem z parametrem -s podajemy na końcu skrypt z poleceniami które mają zostać podane. Powyższe wywołanie zmienia jednostki z kilo/megabajtów na bajty (dokładnie tego potrzebujemy jako offset - nic nie będziemy musieli przeliczać).\nTeraz możemy próbować zamontować daną partycję korzystając z parametru offset w mount, np. tak:\nsudo mount -o loop,ro,offset=1048576 -t ntfs /mnt/backups/somewhere.img /mnt/test Jeśli posiadamy stosunkowo aktualne wersje jajka i pakietu util-linux to powyższa sztuczka powinna się udać. Jeśli takowych nie posiadamy to możemy mieć problemy przy próbie montowania kolejnych partycji. Wtedy może być potrzebne rozpakowania pojedynczej partycji z obrazu poleceniem dd.\n","permalink":"https://gagor.pro/2012/09/montowanie-partycji-z-obrazu-dysku/","summary":"Jedną z rzeczy, które podobają mi się w maszynach wirtualnych Xen jest możliwość zrobienia backupu całego obrazu i szybkie odzyskanie już w trakcie ciężkiej awarii. Gdy dodatkowo korzysta się z LVM\u0026rsquo;a to można na chwilę wyłączyć DomU, utworzyć snapshot jego dysków, uruchomić DomU i w trakcie działania robić spójny backup ze snapshot\u0026rsquo;a. Dzięki takiemu mechanizmowi serwer jest niedostępny przez kilkanaście sekund, a backup spójny jakby został wykonany przy całkowicie wyłączonej maszynie.","title":"Montowanie partycji z obrazu dysku"},{"content":"Kiedyś poproszono mnie o przeszukanie wszystkich plików php na serwerze webowym po kątem wywołania pewnej funkcji. Oczywiste wydało mi się użycie rekurencyjnie grep\u0026rsquo;a, więc:\ngrep -R \u0026#34;JAKAS_FUNKCJA\u0026#34; /var/www/*.php Ale szybko okazało się że grep dopasowuje maskę *.php również do katalogów, więc nie przeszukiwał katalogów które nie kończyły się na .php ehhh\u0026hellip;..\nDrugie podejście okazało się trafniejsze - najpierw poleceniem find wyszukuję wszystkie pliki php, a dopiero później grepuję (wypisując nazwę pliku i numer linii):\nfind /var/www/ -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;JAKAS_FUNKCJA\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; Przykładowo wynik:\nfind /var/www -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;eval(\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; /var/www/*****/wp-admin/press-this.php:208: var my_src = eval( /var/www/*****/wp-admin/press-this.php:219: var my_src = eval( /var/www/*****/wp-admin/press-this.php:402: eval(data); /var/www/*****/wp-admin/includes/class-pclzip.php:4063:// eval(\u0026#39;$v_result = \u0026#39;.$p_options[PCLZIP_CB_PRE_EXTRACT].\u0026#39;(PCLZIP_CB_PRE_EXTRACT, $v_local_header);\u0026#39;); /var/www/*****/wp-includes/class-phpmailer.php:1916: //TODO using /e (equivalent to eval()) is probably not a good idea /var/www/*****/wp-includes/class-json.php:22: * Javascript, and can be directly eval()\u0026#39;ed with no further parsing /var/www/*****/wp-includes/functions.php:190: if ( doubleval($bytes) \u0026gt;= $mag ) ","permalink":"https://gagor.pro/2012/08/przeszukiwanie-plikow-danego-typu-pod-katem-tekstu/","summary":"Kiedyś poproszono mnie o przeszukanie wszystkich plików php na serwerze webowym po kątem wywołania pewnej funkcji. Oczywiste wydało mi się użycie rekurencyjnie grep\u0026rsquo;a, więc:\ngrep -R \u0026#34;JAKAS_FUNKCJA\u0026#34; /var/www/*.php Ale szybko okazało się że grep dopasowuje maskę *.php również do katalogów, więc nie przeszukiwał katalogów które nie kończyły się na .php ehhh\u0026hellip;..\nDrugie podejście okazało się trafniejsze - najpierw poleceniem find wyszukuję wszystkie pliki php, a dopiero później grepuję (wypisując nazwę pliku i numer linii):","title":"Przeszukiwanie plików danego typu pod kątem tekstu"},{"content":"To raczej nie jest podstawowy konfig i próżno szukać go na stronie WordPress\u0026rsquo;a, więc odradzam tę zabawę jeśli nie zna się zbyt dobrze nginx\u0026rsquo;a.\nPonieważ serwerek, na którym działa stronka to sprzęcik z Atomem 330 i mocy na CPU zbyt wiele nie ma to popularne pluginy (np. W3 Total Cache) potencjalnie zwiększające wydajność tak na prawdę zmulały stronkę jeszcze bardziej. Pluginów sprawdziłem kilka i każdorazowo efekt był podobny - stronka działała wolniej niż bez ich pomocy.\nDruga sprawa to zwiększony ruch - w takiej konfiguracji już przy kilku osobach równocześnie przeglądających blog, serwerek zwyczajnie nie radził sobie z dynamicznym generowaniem strony.\nPomysł na rozwiązanie problemu z wydajnością polegał na odpaleniu cache\u0026rsquo;ującego reverse proxy przed właściwą stroną, z krótkim okresem ważności cache\u0026rsquo;u (max kilka sekund) tak by przy dużym obciążeniu strony serwować głównie z cache\u0026rsquo;u (tylko co pewien czas ktoś będzie miał niefart i będzie musiał zaczekać na wygenerowanie strony), przy czym komentarze i panel administracyjny działają z pominięciam cache\u0026rsquo;owania (czyli każdorazowo trafiają przez proxy do aplikacji).\nWażne jednak by osoba wysyłająca komentarz mogła wynik swojego działania zobaczyć od razu na stronie. Ponieważ komentarze wysyłane są metodą HTTP POST to w momencie odebrania takiego połączenia będzie ustawiane ciasteczko dezaktywujące cache dla danego połączenia na kilka sekund (do momentu jego wygaśnięcia).\nPoniżej plik konfiguracyjny, który należy zapisać np. w: /etc/nginx/sites-available/wordpress\n# na początek ustawiamy lokalizację dla cache\u0026#39;u proxy_cache_path /var/cache/nginx/wordpress levels=1:2 keys_zone=WORDPRESS:10m inactive=24h max_size=100m; # nie chcę stronki z www na początku więc cały ruch przekierowują # na stronkę bez www server { listen 10.0.1.2:80; server_name www.example.com; rewrite ^ http://example.com$request_uri? permanent; } # tutaj ma miejsce magia - główny host obsługujący stronkę # to tak na prawdę cache\u0026#39;ujące proxy serwujące okresowo # generowane pliki server { listen 10.0.1.2:80 default; access_log /var/log/nginx/wordpress.access.log; server_name example.com; # ten rewrite przerzuca do panelu admina nawet jeśli # na końcu nie wpiszemy ukośnika # bez niego też to działa ale przekierowanie jest przetwarzane # przez skrypt stronki i działa wolniej rewrite ^/wp-admin$ /wp-admin/ last; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s # (moja stronka nie obsługuje zbyt dużego ruchu i rzadko # jest modyfikowana - np. przez komentarze - więc 60s jest OK, # na bardziej aktywnych stronkach można się pokusić o ustawienie # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to # cache zapewni obsługę nawet kilku tys. zapytań na sekundę proxy_cache_valid 200 60s; # informacje dla backendu na jakiego host się wbijamy # i z jakiego \u0026#34;prawdziwego\u0026#34; IP proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # możemy ukryć niektóre nagłówki np. by nie podpowiadać # z jakich pluginów korzystamy w WordPressie proxy_hide_header X-Powered-By; # kilka ustawień timeout\u0026#39;ów proxy_connect_timeout 60; proxy_read_timeout 120; proxy_send_timeout 120; # Ważne - poniższa opcja ustawia w jaki sposób generowane są # nazwy plików w cache\u0026#39;u, # dodając np. kolejne zmienne możemy zróżnicować cache dla # pewnych grup użytkowników proxy_cache_key \u0026#34;$scheme$request_method$host$request_uri\u0026#34;; # domyślna lokalizacja location / { # ustawiamy domyślną wartość zmiennej set $no_cache \u0026#34;\u0026#34;; # If non GET/HEAD, don\u0026#39;t cache \u0026amp; mark user as uncacheable for 1 second via cookie # jeśli metoda inna niż GET/HEAD to oznacz usera przez cookie jako niecachowanego # na czas 60s (ustawiany poniżej) if ($request_method !~ ^(GET|HEAD)$) { set $no_cache \u0026#34;1\u0026#34;; } # jeśli zalogowany to nie cache\u0026#39;uj if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache \u0026#34;1\u0026#34;; } # jeżeli któryś z wcześniejszych warunków jest spełniony # to ustawiamy cookie, które poinformuje nas by nie cachować # kolejnych zapytań # (z powodu \u0026#34;dziwnego\u0026#34; zachowania if w nginx\u0026#39;ie ustawienie # tego bezpośrednio we wcześniejszych warunkach nie działa) if ($no_cache = \u0026#34;1\u0026#34;) { add_header Set-Cookie \u0026#34;_mcnc=1; Max-Age=61; Path=/\u0026#34;; add_header X-Microcachable \u0026#34;0\u0026#34;; } # jeśli cookie jest ustawione to pomijamy cache i serwujemy # świeżą treść if ($http_cookie ~* \u0026#34;_mcnc\u0026#34;) { set $no_cache \u0026#34;1\u0026#34;; } # dwie poniższe opcje zapewniają pominięcia cache\u0026#39;owania # w przypadku gdy wystąpi któryś z wcześniejszych warunków proxy_no_cache $no_cache; proxy_cache_bypass $no_cache; # Serwujemy cache jeśli strona jest obecnie odświeżana # lub wystąpi błąd proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; # pliki większe niż 1M nie będą cache\u0026#39;owane proxy_max_temp_file_size 1M; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s # (moja stronka nie obsługuje zbyt dużego ruchu i rzadko # jest modyfikowana - np. przez komentarze - więc 60s jest OK, # na bardziej aktywnych stronkach można się pokusić o ustawienie # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to # cache zapewni obsługę nawet kilku tys. zapytań na sekundę proxy_cache_valid 200 60s; # zmieniamy domyślny klucz cache\u0026#39;owania tak by uwzględniał # naszą zmienną proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; # wskazujemy konkretną lokalizację cache\u0026#39;u proxy_cache WORDPRESS; # podajemy lokalizację backendu (nie widziałem sensu # by udostępniać go na zewnętrznym adresie) proxy_pass http://127.0.0.1:81; # można ustawić dodatkowo cache\u0026#39;owanie strony w przeglądarce # (całkiem niezależnie od tego co będzie w cache\u0026#39;u na serwerze) expires 60s; } # dla panelu administracyjnego ustawiamy proxy bez cache\u0026#39;u location ~* wp\\-(admin|login) { # dostęp do panelu administracyjnego dodatkowo chronimy # hasłem - po co? # bo w tym katalogu są różne fajne skrypty, w których już # nie raz znaleziono dziury auth_basic \u0026#34;Go Away\u0026#34;; auth_basic_user_file htpasswd; # proxy bez cache\u0026#39;u proxy_pass http://127.0.0.1:81; } # statykę cache\u0026#39;ujemy mocniej niż treści dynamiczne # a czemu nie puszczam jej bezpośrednio? bo wyplute przez # backend zostaną skompresowane i w takiej postaci zachowają # się w cache\u0026#39;u - gdybym serwował je bezpośrednio to nginx # kompresowałby np. css\u0026#39;y/js\u0026#39;y przy każdym dostępie do nich location ~* \\.(jpg|png|gif|jpeg|css|js|mp3|wav|swf|mov|doc|pdf|xls|ppt|docx|pptx|xlsx)$ { # cache\u0026#39;ujemy statykę przez 2 godziny proxy_cache_valid 200 120m; # dodatkowo ustawiamy długie cache\u0026#39;owanie w przeglądarkach expires 864000; # puszczamy wszystko w proxy + cache proxy_pass http://127.0.0.1:81; proxy_cache WORDPRESS; # wyłączam logowanie dostępu do statyki nawet w przypadku błędów # to mało istotne log_not_found off; access_log off; } # jeszcze inaczej ustawiam cache dla RSS\u0026#39;ów location ~* \\/[^\\/]+\\/(feed|\\.xml)\\/? { # cache\u0026#39;ujemy RSS\u0026#39;y przez 45 minut if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache 1; } proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; proxy_cache_valid 200 45m; proxy_cache MYSITE; proxy_pass http://127.0.0.1:81; } } # to teraz konfiguracja serwera serwującego treści dynamiczne server { # nasłuchujemy lokalnie bo z zewnątrz strona dostępna # jest przez proxy listen 127.0.0.1:81; error_log /var/log/nginx/mysite.error.log; root /var/www/wordpress; index index.php; # logujemy prawdziwe IP dzięki odpowiednim nagłówkom # przesyłanym przez proxy set_real_ip_from 127.0.0.0/24; real_ip_header X-Real-IP; # tutaj praktycznie klasyka - z tym że zamiast na końcu # wskazywać index.php robię najpierw kilka rewrite\u0026#39;ów # np. dla przeniesionych stron, itp location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /main http://example.com/about/? permanent; rewrite /projekty http://example.com/category/projects/? permanent; rewrite /tag/gd http://example.com? permanent; rewrite /category/hobby http://roman.com? permanent; rewrite ^ /index.php last; } # na bardziej obleganych stronach limitowanie wyszukiwania może # pomóc, ale to nie mój przypadek # location /search { limit_req zone=mysitesearch burst=3 nodelay; rewrite ^ /index.php; } location ~* \\.(?:ico|css|js|gif|jpe?g|png)$ { # cache\u0026#39;owanie atrybutów statycznych plików open_file_cache max=1000 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; # maksymalne cache\u0026#39;owanie statyki w przeglądarkach expires max; } # no i na końcu obsługa skryptów php location ~* \\.php$ { # albo plik istnieje i go serwujemy albo dajemy Forbidden try_files $uri =403; # kilka standardowych ustawień include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # blokujemy możliwość wykonywania skryptów z katalogu upload # (nawet jeśli komuś uda się je wepchnąć) if ($uri !~ \u0026#34;^/wp-content/uploads/\u0026#34;) { fastcgi_pass php-fastcgi; } # informacje dla proxy jak długo może cache\u0026#39;ować add_header Cache-Control \u0026#34;max-age:60, public\u0026#34;; expires 60s; } # blokuję dostęp do plików zaczynających się od kropki location ~ /\\. { access_log off; log_not_found off; deny all; } # wyłączam logowanie do nieistotnych plików location = /robots.txt { allow all; access_log off; log_not_found off; } location = /favicon.ico { access_log off; log_not_found off; } } Konfiguracja potrzebuje jednego folderu na cache, do którego dostęp do zapisu ma nginx (użytkownik na którym działa proces):\nmkdir -p /var/cache/nginx/wordpress chown -R www-data:www-data /var/cache/nginx/wordpress A żeby zaczął działać trzeba go \u0026ldquo;włączyć\u0026rdquo; i przeładować nginx\u0026rsquo;a:\ncd /etc/nginx/sites-available/ ln -s wordpress /etc/nginx/sites-enabled/wordpress service nginx reload Jedyna rzecz, której brakuje w tym configu to konfiguracja backend\u0026rsquo;u do PHP\u0026rsquo;a (u mnie nazwana php-fastcgi) - może kiedyś zrobię HOWTO o konfiguracji NGINX+PHP, ale na tą chwilę zakładam że sobie poradzisz \u0026#x1f603;\nBenchmark Sprawdźmy jak to wygląda teraz:\nab -n 1000 -c 10 https://gagor.pl/ This is ApacheBench, Version 2.3 \u0026lt; $Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking gagor.pl (be patient) Completed 100 requests Completed 200 requests Completed 300 requests Completed 400 requests Completed 500 requests Completed 600 requests Completed 700 requests Completed 800 requests Completed 900 requests Completed 1000 requests Finished 1000 requests Server Software: nginx Server Hostname: gagor.pl Server Port: 80 Document Path: / Document Length: 36263 bytes Concurrency Level: 10 Time taken for tests: 8.716 seconds Complete requests: 1000 Failed requests: 22 (Connect: 0, Receive: 0, Length: 22, Exceptions: 0) Write errors: 0 Total transferred: 36601094 bytes HTML transferred: 36263088 bytes Requests per second: 114.73 [#/sec] (mean) Time per request: 87.159 [ms] (mean) Time per request: 8.716 [ms] (mean, across all concurrent requests) Transfer rate: 4100.91 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 1 16 7.1 15 55 Processing: 17 67 97.0 53 789 Waiting: 5 28 55.6 19 456 Total: 37 83 98.0 68 803 Percentage of the requests served within a certain time (ms) 50% 68 66% 74 75% 78 80% 80 90% 87 95% 95 98% 665 99% 755 100% 803 (longest request) Dla mnie bomba \u0026#x1f603;\nPomysł na taki rodzaj cache\u0026rsquo;owania zaczerpnąłem stąd\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2012/06/nginx-konfiguracja-pod-wordpressa/","summary":"To raczej nie jest podstawowy konfig i próżno szukać go na stronie WordPress\u0026rsquo;a, więc odradzam tę zabawę jeśli nie zna się zbyt dobrze nginx\u0026rsquo;a.\nPonieważ serwerek, na którym działa stronka to sprzęcik z Atomem 330 i mocy na CPU zbyt wiele nie ma to popularne pluginy (np. W3 Total Cache) potencjalnie zwiększające wydajność tak na prawdę zmulały stronkę jeszcze bardziej. Pluginów sprawdziłem kilka i każdorazowo efekt był podobny - stronka działała wolniej niż bez ich pomocy.","title":"Nginx - konfiguracja pod WordPress’a"},{"content":"Pisałem już HOWTO o konfiguracji Xen\u0026rsquo;a ale nie opisałem jak się bawić wirtualkami gdy Xen\u0026rsquo;a już mamy. To nadrabiam.\nTworzenie i usuwanie maszyn wirtualnych Do tworzenia/niszczenia DomU wykorzystuję pakiet xen-tools dostarczający m.in. dwa narzędzia:\nxen-create - dla którego przygotowałem dość skomplikowaną konfigurację przy okazji wcześniejszego posta: Instalacja i konfiguracja DomU\u0026thinsp; external link . Przykład użycia: xen-create --hostname example-domu --ip 10.0.0.77 \\ --gateway 10.0.0.1 --broadcast 10.0.0.255 --netmask 255.255.255.0 \\ --bridge br10 --vcpus 2 --memory=2G xen-delete-image - narzędzie do kasowania wirtualnych maszyn. Maszyna musi być wyłączona aby można było ją usunąć. Narzędzie to kasuje plik konfiguracyjny maszyny wirtualnej oraz przydzielone jej volumeny lvm. Przykład użycia: xen-delete-image nazwamaszyny Zarządzanie maszynami wirtualnymi Do uruchamiania, wyłączanie, resetowania (i ogólnie zarządzania) maszynami wirtualnymi służy tylko jedno polecenie: xm z różnymi parametrami:\nxm list- listuje uruchomione w danej chwili wirtualne maszyny, wyświetlając przydzieloną im ilość pamięci, procesorów, stan (uruchomiona, zawieszona), czas działania (uptime). Dla objaśnienia Domain-0 (zwane też Dom-O) to hyperwisor czyli fizyczna maszyna na której uruchomione są wirtualki. xm top - polecenie wyświetla dokładne dane chwilowego zużycia zasobów dla różnych wirtualnych maszyn i Dom-0. xm create nazwapliku.cfg - uruchamia maszynę wirtualną zgodnie z instrukcjami zawartymi w pliku konfiguracyjnym (przydzielone dyski, pamięć, etc). xm shutdown nazwamaszyny - wysyła sygnał wyłączenia maszyny wirtualnej i wraca do wiersza poleceń. Dokładnie to polecenie wysyła sygnał ACPI równoważny przyciśnięciu przycisku Power na obudowie komputera - system operacyjny wykrywa to zdarzenie i zaczyna się wyłączać. Jest to zalecana instrukcja do wyłączania wirtualek. Należy pamiętać, że po wykonaniu tego polecenia jeszcze przez kilka/kilkanaście sekund maszyna działa - do puki nie skończy się wyłączać. xm shutdown -w nazwamaszyny - działa jak powyższe polecenie, ale dodatkowo czeka aż maszyna wirtualna zostanie wyłączona a przydzielone jej zasoby zwolnione. Gdy to polecenie skończy się wykonywać mamy pewność, że maszyna jest już wyłączona. xm destroy nazwamaszyny - polecenie do twardego resetu maszyny wirtualnej. Najpierw odbierany jest czas procesora dla maszyny, potem zwalniana pamięć i zarezerwowane uchwyty. Wykorzystując to polecenie może dojść do utraty danych lub uszkodzenia OS‘u na wirtualnej maszynie. xm reboot nazwamaszyny - restartuje maszynę wirtualną w bezpieczny sposób (czyli wysyła sygnał ACPI do wyłączenia i startuje DomU). Gdy zmodyfikujemy plik konfiguracyjny danego DomU nie wystarczy wywołać xm reboot - przeważnie potrzeba położyć maszynę i ponownie ją uruchomić, np. tak: xm shutdown -w maszyna \u0026amp;\u0026amp; xm create maszyna.cfg xm pause nazwamaszyny - pauzuje wirtualną maszynę, zamrażając ją w obecnym stanie razem z pamięcią itd. xm unpause nazwamaszyny - uruchamia zapauzowaną wcześniej maszynę wirtualną. Działa odwrotnie do polecenia powyżej. xm console nazwamaszyny - polecenie działa jak „podpięcie monitora” do fizycznej maszyny, na pierwszy terminal. Bardzo przydatne zaraz po utworzeniu wirtualki jak również w różnych sytuacjach kryzysowych 😉 Jest jeszcze kilka innych poleceń np. dodających na gorąco urządzenia blokowe ale ich działanie mocno zależy od wersji Xen\u0026rsquo;a i jajka.\n","permalink":"https://gagor.pro/2012/06/xen-podstawowe-polecenia/","summary":"Pisałem już HOWTO o konfiguracji Xen\u0026rsquo;a ale nie opisałem jak się bawić wirtualkami gdy Xen\u0026rsquo;a już mamy. To nadrabiam.\nTworzenie i usuwanie maszyn wirtualnych Do tworzenia/niszczenia DomU wykorzystuję pakiet xen-tools dostarczający m.in. dwa narzędzia:\nxen-create - dla którego przygotowałem dość skomplikowaną konfigurację przy okazji wcześniejszego posta: Instalacja i konfiguracja DomU\u0026thinsp; external link . Przykład użycia: xen-create --hostname example-domu --ip 10.0.0.77 \\ --gateway 10.0.0.1 --broadcast 10.0.0.255 --netmask 255.255.255.0 \\ --bridge br10 --vcpus 2 --memory=2G xen-delete-image - narzędzie do kasowania wirtualnych maszyn.","title":"Xen - Podstawowe polecenia"},{"content":"Ustawienie domyślnego vhosta w nginx\u0026rsquo;ie jest ładnie opisane w dokumentacji i początkowo wydawało się dobrze działać ale gdy wykorzystałem tą konfigurację na serwerze z wieloma adresami IP i nasłuchiwaniem na porcie 80 (bez podania IP) to zachowywało się to dość dziwnie (przeważnie nie ładowało tej strony którą chciałem). Od teraz tworzę konfigurację domyślnego vhosta dla każdego z dostępnych adresów IP. Powiem szczerze że nie miałem czasu na głębsze zbadanie tego zachowania i wykorzystałem rozwiązanie, które działało w każdym przypadku czyli po jednym konfigu na IP + przekierowanie na ogólną stronę.\nserver { listen 10.0.0.100:80 default_server; server_name _; server_name_in_redirect off; rewrite ^ http://www.gagor.pl permanent; } ","permalink":"https://gagor.pro/2012/06/nginx-ustawienie-domyslnego-vhosta/","summary":"Ustawienie domyślnego vhosta w nginx\u0026rsquo;ie jest ładnie opisane w dokumentacji i początkowo wydawało się dobrze działać ale gdy wykorzystałem tą konfigurację na serwerze z wieloma adresami IP i nasłuchiwaniem na porcie 80 (bez podania IP) to zachowywało się to dość dziwnie (przeważnie nie ładowało tej strony którą chciałem). Od teraz tworzę konfigurację domyślnego vhosta dla każdego z dostępnych adresów IP. Powiem szczerze że nie miałem czasu na głębsze zbadanie tego zachowania i wykorzystałem rozwiązanie, które działało w każdym przypadku czyli po jednym konfigu na IP + przekierowanie na ogólną stronę.","title":"Nginx - ustawienie domyślnego vhosta"},{"content":"VLAN\u0026rsquo;y są prostą metodą na separację sieci. Gdy mamy wiele sieci może zajść potrzeba by poszczególne DomU miały dostęp do różnych VLAN\u0026rsquo;ów (czasem nawet wielu równocześnie). Jeżeli serwer z Dom0 posiada minimum giga-bitową kartę sieciową (a najlepiej kilka) to powinniśmy być w stanie z godziwą jakością udostępnić systemom DomU różne VLAN\u0026rsquo;y z interfejsów gospodarza.\nZaprezentowane poniżej skrypty zapożyczyłem z tej strony: http://renial.net/weblog/2007/02/27/xen-vlan\u0026thinsp; external link Na początek musimy zainstalować pakiet vlan:\napt-get install vlan Później w pliku /etc/xen/xend-config.sxp ustawiamy taka skrypt dla konfiguracji sieci:\n(network-script network-multi-vlan) Tworzymy plik /etc/xen/scripts/network-multi-vlan i wpisujemy w nim (no dobra - komentarze można pominąć):\n#!/bin/sh #=================================================================== # Xen vlan bridge start/stop script. # Xend calls a network script when it starts. # The script name to use is defined in /etc/xen/xend-config.sxp # in the network-script field. # # This script creates multiple bridges to segregate individual # domUs to separate VLANs. Customize to fit your needs. # # Usage: # # network-multi-vlan (start|stop|status) # #=================================================================== dir=$(dirname \u0026#34;$0\u0026#34;) # Poniższa linijka pozwala udostępnić dany interfejs sieciowy # w całości jako domyślny bridge - jeśli chcemy się ograniczyć # wyłącznie do bridge\u0026#39;y na VLAN\u0026#39;ach to możemy tą linijkę # zakomentować (jak ja) \u0026#34;$dir/network-bridge\u0026#34; \u0026#34;$@\u0026#34; vifnum=0 netdev=eth0 # No i teraz odpalamy kolejne VLAN\u0026#39;y na poszczególnych interfejsach. # Brak parametru netdev jest równoznaczny wybraniu netdev=eth0 # Parametr VLAN jest... hm... samoopisujący \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=10 netdev=eth1 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=11 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=23 netdev=eth2 Pobieramy skrypt network-bridge-vlan do lokalizacji: /etc/xen/scripts/network-bridge-vlan.\ncd /tmp wget https://gagor.pl/wp-content/uploads/2012/06/network-bridge-vlan.gz gunzip network-bridge-vlan.gz mv network-bridge-vlan /etc/xen/scripts/network-bridge-vlan Źródła http://wiki.xensource.com/xenwiki/XenDom0VLANstoDomUVirtualNICs\u0026thinsp; external link - ogólne schematy konfiguracji VLAN\u0026rsquo;ów na Xen\u0026rsquo;ie, http://renial.net/weblog/2007/02/27/xen-vlan\u0026thinsp; external link - skrypty do konfiguracji VLAN\u0026rsquo;ów pochodzą z tej strony. ","permalink":"https://gagor.pro/2012/06/xen-konfiguracja-interfejsu-sieciowego-dom0-jako-brdigea-dla-roznych-vlanow/","summary":"VLAN\u0026rsquo;y są prostą metodą na separację sieci. Gdy mamy wiele sieci może zajść potrzeba by poszczególne DomU miały dostęp do różnych VLAN\u0026rsquo;ów (czasem nawet wielu równocześnie). Jeżeli serwer z Dom0 posiada minimum giga-bitową kartę sieciową (a najlepiej kilka) to powinniśmy być w stanie z godziwą jakością udostępnić systemom DomU różne VLAN\u0026rsquo;y z interfejsów gospodarza.\nZaprezentowane poniżej skrypty zapożyczyłem z tej strony: http://renial.net/weblog/2007/02/27/xen-vlan\u0026thinsp; external link Na początek musimy zainstalować pakiet vlan:","title":"Xen - Konfiguracja interfejsu sieciowego Dom0 jako brdige’a dla VLAN’ów"},{"content":"Ostatnio trafiłem na ciekawy problem, który wielokrotnie rozwiązywałem w nginx\u0026rsquo;ie ale tym razem musiałem zrobić to w Apache. Pewna stronka działa sobie na HTTPS\u0026rsquo;ie i chciałem by wszystkie powiązane z nią pliki były serwowane z jej adresu szyfrowanym połączeniem by nie pojawiały się w przeglądarce monity że \u0026ldquo;część ruchu nie jest szyfrowana\u0026rdquo;. Tyle że część potrzebnych plików była już obecnie serwowana na innym serwerze (w innej domenie) poprzez HTTP.\nMogłem albo skopiować te pliki i wykombinować jakiś mechanizm synchronizujący albo wykorzystać proxy + cache. Drugie rozwiązanie wydało mi się prostsze i ładniejsze \u0026#x1f603;\nNa początek włączamy w Apache\u0026rsquo;m odpowiednie moduły:\na2enmod proxy a2enmod proxy_http Teraz przykładowa konfiguracja vhosta:\n\u0026lt;VirtualHost *:80\u0026gt; #podstawowa konfiguracja vhosta ServerAdmin webmaster@example.com ServerName www.example.com ServerAlias example.com # wyłącza działanie Apache\u0026#39;go jako przekazującego proxy (forwarding proxy) ProxyRequests Off # nie chciałem by błędy HTTP z backendowego serwera były przekazywane # zamiast nich będą błędy Apache\u0026#39;go ProxyErrorOverride On # przykład prostego reverse proxy - wystarczą dwa poniższe polecenia # ProxyPass proxuje ruch do danego serwera pod wskazanym URL\u0026#39;em ProxyPass /stats/ http://google-anal.com/ # ProxyPassReverse modyfikuje nagłówki odpowiedzi ze zdalnego serwera # tak by odpowiedź wyglądała na wysłaną z lokalnego serwera ProxyPassReverse /stats/ http://google-anal.com/ # ciekawszy przykład proxowania z dodatkowymi ustawieniami cachowania # najpierw konfiguracja cache\u0026#39;u dyskowego \u0026lt;IfModule mod_disk_cache.c\u0026gt; # CacheRoot to wymagany parametr - ścieżka w której znajduje się cache CacheRoot /var/cache/apache2/mod_disk_cache/example # ponieważ przewidywałem że cache\u0026#39;owanych będzie kilka GB małych plików # to by listowanie ich w cache\u0026#39;u było efektywne warto wykorzystać wielo # poziomowe zagłębienie katalogów - wtedy na każdym poziomie, w danym # katalogu będzie stosunkowo mało plików, indeksy mniejsze, listowanie # szybsze CacheDirLevels 5 CacheDirLength 2 # teraz czas na konfiguracje cache\u0026#39;u \u0026lt;IfModule mod_cache.c\u0026gt; # pozwolę sobie na zignorowanie nagłówków Expires/Cache-Control # z aplikacji sam lepiej wiem że te pliki nie zmieniają się # zbyt często CacheIgnoreCacheControl On # pliki stracą ważność w cache\u0026#39;u po tygodniu CacheDefaultExpire 604800 # ponieważ zasoby nie różnią się dla różnych zalogowanych użytkowników # zignoruję cookies\u0026#39;y CacheIgnoreHeaders Set-Cookie # nie chcę by proxy weryfikowało czy pojawiła się nowa wersja obrazka # bo raczej rzadko pojawiają się zmiany CacheIgnoreNoLastMod On # cache uruchamiany dla dwóch \u0026#34;subkatalogów\u0026#34; # http://example.com/images oraz http://example.com/files CacheEnable disk /images CacheEnable disk /files \u0026lt;/IfModule\u0026gt; \u0026lt;/IfModule\u0026gt; # włączamy mod_rewrite - będzie za chwilkę potrzebny RewriteEngine on # poprzednio do uruchomienia proxy wykorzystałem opcję ProxyPass, # ale często potrzebujemy bardziej zaawansowanego przekierowania # i wtedy warto wykorzystać mod_rewrite do modyfikacji URL\u0026#39;i w locie # koniecznie z flagą [P] RewriteRule ^/images/(.+)/(.+) http://10.0.0.100:8080/example-images/get.php?id=$1\u0026amp;width=$2 [P] \u0026lt;Location /images\u0026gt; # jak powyżej - modyfikacja zwrotnych nagłówków ProxyPassReverse /example-images/ # kilka nagłówków z backendu ukrywam by nie były przekazywane dalej Header unset Server Header unset Expires Header unset ETag # akurat nagłówek Via można wyłączyć w konfiguracji modułu proxy # ale ponieważ bywa przydatny przy debugowaniu to w niektórych miejscach # wolę gdy jest ustawiony - a w innych nie Header unset Via # ręczne ustawienie nagłówka Cache-Control i zezwolenie # na cachowanie przez inne proxy lub przeglądarki Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # i drugie przekierowanie RewriteRule ^/files/(.+)/(.+) http://10.0.0.100:8080/example-files/$1/$2 [P] \u0026lt;Location /files\u0026gt; ProxyPassReverse /example-files/ Header unset Server Header unset Via Header unset ETag Header unset Expires Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # standardowa konfiguracja DocumentRoot /var/www/example \u0026lt;Directory /var/www/example\u0026gt; Options -Indexes FollowSymLinks Includes AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; LogLevel warn CustomLog /var/log/apache2/example_access.log combined \u0026lt;/VirtualHost\u0026gt; ","permalink":"https://gagor.pro/2012/06/apache-reverse-proxy-z-cacheowaniem/","summary":"Ostatnio trafiłem na ciekawy problem, który wielokrotnie rozwiązywałem w nginx\u0026rsquo;ie ale tym razem musiałem zrobić to w Apache. Pewna stronka działa sobie na HTTPS\u0026rsquo;ie i chciałem by wszystkie powiązane z nią pliki były serwowane z jej adresu szyfrowanym połączeniem by nie pojawiały się w przeglądarce monity że \u0026ldquo;część ruchu nie jest szyfrowana\u0026rdquo;. Tyle że część potrzebnych plików była już obecnie serwowana na innym serwerze (w innej domenie) poprzez HTTP.\nMogłem albo skopiować te pliki i wykombinować jakiś mechanizm synchronizujący albo wykorzystać proxy + cache.","title":"Apache - reverse proxy z cache’owaniem"},{"content":"Gdy tworzymy pierwszą aplikację webową, która umożliwia upload plików przeważnie lądują one lokalnie w pewniej lokalizacji. Gdy druga aplikacja potrzebuje dostępu do tych plików wystarczy podać ścieżkę. Problemy zaczynają się gdy aplikacji jest kilka i rozmieszczonych na kilku serwerach. Można korzystać z sieciowych systemów plików ale to często nie jest zbyt wygodne - ciężko odpowiednio ustawić uprawnienia by pewne aplikacje miały dostęp do zapisu plików a inne nie, trzeba skonfigurować dany katalog w kilku miejscach w konfiguracji serwera WWW aby serwować pliki itp\u0026hellip;\nJeżeli w utrzymaniu swoich aplikacji dociera się do takiego momentu to kolejne pomysły zakładają wykorzystanie bazy danych do przechowywania plików. Ale chwila googlania i już wiemy że bazy typu SQL średnio radzą sobie z przetwarzaniem tak dużych rekordów jak pliki. Kolejny etap to sprawdzenie co mogą nam zaoferować bazy NoSQL.\nW tym miejscu przeczytałem wiele różnych artykułów i ostatecznie zastanawiałem się czy wybrać MongoDB czy CouchDB. Oba projekty są bardzo podobne a główną ich zaletą jest łatwość wykorzystania w istniejących aplikacjach webowych. Tutaj szczególnie CouchDB wypada bardzo dobrze bo zarządzanie i dostęp do bazy odbywa się standardowym protokołem HTTP - polecenia wydaje się POST\u0026rsquo;ami, a dane pobiera GET\u0026rsquo;ami. Dzięki takiej budowie łatwo można schować CouchDB za reverse proxy i serwować np. pliki uploadowane przez użytkowników wprost z bazy - bez dorabiania dodatkowych interfejsów. Bardzo łatwo też obsługuje się bazę z poziomu aplikacji AJAX. Więc w moim przypadku wypadło na CouchDB.\nInstalacja Instalacja na Debianie jest banalna i sprowadza się do:\napt-get install -y couchdb TADAM! Mamy działające CouchDB. Domyślnie baza nasłuchuje na adresie 127.0.0.1 i porcie 5984.\nDodatkowo Couch posiada webowy interfejs (zwany Futon\u0026rsquo;em) zarządzający dostępny na tym samym porcie pod adresem, np. http://127.0.0.1:5984/_utils/\nOptymalizacja NODELAY W moim przypadku CouchDB służy do przechowywania zarówno bardzo wielu małych plików, jak i kilku całkiem sporych. W przypadku bardzo małych plików CouchDB w wersji 0.11 czeka z wysłaniem odpowiedzi od razu bo być może uda się \u0026ldquo;dorzucić coś jeszcze\u0026rdquo;. Takie zachowanie może powodować opóźnienia przy pobieraniu małych plików, warto więc zmienić w pliku /etc/couchdb/local.ini taką opcję:\n[httpd] socket_options = [{nodelay, true}] Ustawienie opcji TCP NODELAY wyłączy opóźnienie przy wysyłaniu małych plików.\nIdentyfikatory Warto zastanowić się nad długością i schematem identyfikatorów dla rekordów. Domyślnie mają one 32 bajty co przy małej liczbie elementów w bazie jest mocną przesadą. Rozmiar identyfikatorów znacznie wpływa na rozmiar bazy i jej wydajność - dlatego czasem warto opracować własny schemat generowanych identyfikatorów. Przykładowo jeśli identyfikator będzie generowany z cyfr oraz dużych i małych liter to dla 3 znaków możemy wygenerować identyfikatory dla ponad 260 tys. elementów, dla 4 znaków już ponad 14 milionów co powinno wystarczyć dla średniej wielkości bazy.\nUstawienia bezpieczeństwa Trochę mnie zaskoczyło podejście do bezpieczeństwa w bazie CouchDB - domyślnie po instalacji baza działa w trybie Admin Party, czyli każda osoba która wejdzie do zarządzania bez logowania ma uprawnienia administratora \u0026#x1f604;\nMnie ten stan rzeczy nie bardzo odpowiadał więc:\nodpalamy interfejs zarządzający - Futon i w prawym dolnym rogu szukamy tekstu: \u0026ldquo;Welcome to Admin Party! Everyone is admin. Fix this\u0026rdquo; - klikamy na \u0026ldquo;Fix this\u0026rdquo;, w okienku które się pojawi podajemy login i hasło administratora.\nPo ustawieniu konta administratora dalsze musimy zadbać o ustawienie odpowiednich uprawnień dla każdej nowo tworzonej bazy, czyli bazy tworzą się dostępem dla wszystkich ale możemy ograniczyć np. zapis/odczyt dla pewnych grup użytkowników.\nZabezpieczenie bazy użytkowników Po ustawieniu pierwszego użytkownika kolejna rzecz, o którą powinniśmy zadbać do dostępna dla każdego do odczytu baza użytkowników. Najlepiej gdy tylko administratorzy będą mieli do niej dostęp. By to osiągnąć:\nw Futonie wchodzimy do bazy _users i klikamy przycisk \u0026ldquo;Security\u0026rdquo; na górze strony, w okienku które się pojawi w polu Readers -\u0026gt; Roles (pole na samym dole) wpisujemy [\u0026quot;admin\u0026quot;] Od teraz tylko administratorzy mają dostęp do bazy _users.\nUstawienie bazy jako tylko do odczytu To co chciałem osiągnąć korzystając z CouchDB to jakieś repozytorium, do którego wrzucać mogą wybrańcy a czytać wszyscy (np. taki CDN dla stronki itp) ale otrzymanie takiego rezultatu jest nieco\u0026hellip; nieintuicyjne. By to osiągnąć:\nw bazie którą chcemy ustawić jako tylko do odczytu wchodzimy w Security i w polu Admin Roles (drugie z góry) wpisujemy [\u0026quot;admin\u0026quot;] - to zablokuje dostęp do panelu Security i możliwości modyfikowania design dokumentów, nadal możliwe jest jednak dodawanie, modyfikowani i kasowanie dokumentów,\nby zablokować dostęp do zapisu w CouchDB trzeba wykorzystać funkcję validate_doc_update która będzie wywoływana przy każdej próbie dostępu do pojedynczego dokumentu, by z niej skorzystać tworzymy nowy pusty dokument, zmieniamy pole _id dokumentu na _design/auth dodajemy pole nazwane language z wartością javascript dodajemy kolejne pole nazwane validate_doc_update z wartością: function(newDoc, oldDoc, userCtx) { if (userCtx.roles.indexOf(\u0026#39;_admin\u0026#39;) !== -1) { return; } else { throw({forbidden: \u0026#39;Nie jesteś administratorem!\u0026#39;}); } } zapisujemy dokument klikając na \u0026ldquo;Save Document\u0026rdquo;\n","permalink":"https://gagor.pro/2012/06/couchdb-instalacja-i-wstepna-konfiguracja/","summary":"Gdy tworzymy pierwszą aplikację webową, która umożliwia upload plików przeważnie lądują one lokalnie w pewniej lokalizacji. Gdy druga aplikacja potrzebuje dostępu do tych plików wystarczy podać ścieżkę. Problemy zaczynają się gdy aplikacji jest kilka i rozmieszczonych na kilku serwerach. Można korzystać z sieciowych systemów plików ale to często nie jest zbyt wygodne - ciężko odpowiednio ustawić uprawnienia by pewne aplikacje miały dostęp do zapisu plików a inne nie, trzeba skonfigurować dany katalog w kilku miejscach w konfiguracji serwera WWW aby serwować pliki itp\u0026hellip;","title":"CouchDB - Instalacja i wstępna konfiguracja"},{"content":"Używam LVM\u0026rsquo;a zarówno na desktopie jak i wielu serwerach bo bardzo podoba mi się możliwość powiększenia akurat tej partycji, na której brakuje miejsca. O ile pamiętam jak powiększyć partycję XFS (xfs_growfs /punkt/montowania) to zawsze mam problem jak to zrobić na EXT3/4, więc notuję.\nPowiększenie wolumenu LVM (np. o 10 gigabajtów):\nlvextend -L+10G /dev/vggroup/vol Zwiększenie rozmiaru systemu plików do nowego rozmiaru wolumenu:\nresize2fs /dev/vggroup/vol Powyższe polecenie można wykonać na zamontowanym zasobie - online.\n","permalink":"https://gagor.pro/2012/06/dynamiczna-zmiana-rozmiaru-partycji-ext4-na-lvmie/","summary":"Używam LVM\u0026rsquo;a zarówno na desktopie jak i wielu serwerach bo bardzo podoba mi się możliwość powiększenia akurat tej partycji, na której brakuje miejsca. O ile pamiętam jak powiększyć partycję XFS (xfs_growfs /punkt/montowania) to zawsze mam problem jak to zrobić na EXT3/4, więc notuję.\nPowiększenie wolumenu LVM (np. o 10 gigabajtów):\nlvextend -L+10G /dev/vggroup/vol Zwiększenie rozmiaru systemu plików do nowego rozmiaru wolumenu:\nresize2fs /dev/vggroup/vol Powyższe polecenie można wykonać na zamontowanym zasobie - online.","title":"Dynamiczna zmiana rozmiaru partycji EXT4 na LVM’ie"},{"content":"W tym poście nie rozpiszę się zbytnio - wrzucam tylko config od którego zaczynam konfigurację nginx\u0026rsquo;a.\nuser www-data; worker_processes 4; pid /var/run/nginx.pid; events { worker_connections 1024; ## zaakceptuj tak dużo połączeń jak to możliwe multi_accept on; ## epoll jest preferowany na jajkach od 2.6 ## http://www.kegel.com/c10k.html#nb.epoll use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## opcje TCP sendfile on; tcp_nopush on; tcp_nodelay on; ## maksymalny rozmiar zapytnia client_max_body_size 10m; ## timeout\u0026#39;y client_body_timeout 60; client_header_timeout 60; keepalive_timeout 10; send_timeout 60; ## kompresja gzip on; gzip_static on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml; ## bezpieczeństwo ## security by obscurity - ukrywamy wersję nginx\u0026#39;a server_tokens off; ignore_invalid_headers on; ## resetuj zbyt długie połączenia - powinno pomóc na slowlorisa reset_timedout_connection on; ## włączenie ochrony przed clickjackingiem - uruchamiam to per vhost ## https://developer.mozilla.org/en/The_X-FRAME-OPTIONS_response_header #add_header X-Frame-Options SAMEORIGIN; ## potrzebne zeby zbudowac mape ponizej map_hash_bucket_size 256; ## blacklist\u0026#39;a botów i referer\u0026#39;ów include blacklist.conf; ## # jeśli połączenie na HTTPS\u0026#39;a to ustawiamy zmienną by mogło być obsłużone ## map $scheme $server_https { default off; https on; } include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } No i jeszcze zawartość blacklist.conf - na początek wystarczy a można rozszerzyć pod siebie:\n## blokowanie agentów map $http_user_agent $bad_bot { default 0; libwww-perl 1; ~(?i)(httrack|htmlparser|libwww) 1; } ## blokowanie refererów map $http_referer $bad_referer { default 0; ~(?i)(babes|click|diamond|forsale|girl|jewelry|love|nudit|organic|poker|porn|poweroversoftware|sex|teen|webcam|zippo|casino|replica) 1; } ","permalink":"https://gagor.pro/2012/06/nginx-moj-domyslny-config/","summary":"W tym poście nie rozpiszę się zbytnio - wrzucam tylko config od którego zaczynam konfigurację nginx\u0026rsquo;a.\nuser www-data; worker_processes 4; pid /var/run/nginx.pid; events { worker_connections 1024; ## zaakceptuj tak dużo połączeń jak to możliwe multi_accept on; ## epoll jest preferowany na jajkach od 2.6 ## http://www.kegel.com/c10k.html#nb.epoll use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## opcje TCP sendfile on; tcp_nopush on; tcp_nodelay on; ## maksymalny rozmiar zapytnia client_max_body_size 10m; ## timeout\u0026#39;y client_body_timeout 60; client_header_timeout 60; keepalive_timeout 10; send_timeout 60; ## kompresja gzip on; gzip_static on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml; ## bezpieczeństwo ## security by obscurity - ukrywamy wersję nginx\u0026#39;a server_tokens off; ignore_invalid_headers on; ## resetuj zbyt długie połączenia - powinno pomóc na slowlorisa reset_timedout_connection on; ## włączenie ochrony przed clickjackingiem - uruchamiam to per vhost ## https://developer.","title":"Nginx - mój domyślny config"},{"content":"Jeśli po aktualizacji firmware na swoim firewall\u0026rsquo;u do wersji MR3 natrafisz na komunikat Warning: SQL Logging is not enabled przy dostępie do logów to prawdopodobnie musisz zmienić źródło logów dla interfejsu gui. Poniżej polecenie CLI i możliwe opcje:\nconfig log gui set logdevice {memory | disk | fortianalyzer} end Ja potrzebowałem ustawić tę opcję na fortianalyzer by uzyskać dostęp do moich logów.\n","permalink":"https://gagor.pro/2012/04/fortigate-warning-sql-logging-is-not-enabled/","summary":"Jeśli po aktualizacji firmware na swoim firewall\u0026rsquo;u do wersji MR3 natrafisz na komunikat Warning: SQL Logging is not enabled przy dostępie do logów to prawdopodobnie musisz zmienić źródło logów dla interfejsu gui. Poniżej polecenie CLI i możliwe opcje:\nconfig log gui set logdevice {memory | disk | fortianalyzer} end Ja potrzebowałem ustawić tę opcję na fortianalyzer by uzyskać dostęp do moich logów.","title":"Fortigate: Warning: SQL Logging is not enabled"},{"content":"Chciałem zaimportować mój certyfikat self-signed do Nokii E72 by nie krzyczała przy sprawdzaniu poczty. Potrzebowałem certyfikatu w formacie DER, a miałem w PEM - chwilę szukałem jak dokonać konwersji, więc ku pamięci zapisuję kilka gotowych poleceń:\nKonwersja certyfikatu z PEM na DER openssl x509 -in in.crt -inform PEM -out out.crt -outform DER Konwersja certyfikatu z DER na PEM openssl x509 -in in.crt -inform DER -out out.crt -outform DER Konwersja klucza z formatu PEM na DER openssl rsa -in in.crt -inform PEM -out out.crt -outform DER Konwersja klucza z formatu DER na PEM openssl rsa -in in.crt -inform DER -out out.crt -outform PEM Po konwersji certyfikat w formacie DER wystarczy wrzucić na kartę i otworzyć z menadżera plików, zainstalować.\n","permalink":"https://gagor.pro/2012/04/konwersja-formatu-certyfikatu-dla-telefonow-nokia/","summary":"Chciałem zaimportować mój certyfikat self-signed do Nokii E72 by nie krzyczała przy sprawdzaniu poczty. Potrzebowałem certyfikatu w formacie DER, a miałem w PEM - chwilę szukałem jak dokonać konwersji, więc ku pamięci zapisuję kilka gotowych poleceń:\nKonwersja certyfikatu z PEM na DER openssl x509 -in in.crt -inform PEM -out out.crt -outform DER Konwersja certyfikatu z DER na PEM openssl x509 -in in.crt -inform DER -out out.crt -outform DER Konwersja klucza z formatu PEM na DER openssl rsa -in in.","title":"Konwersja formatu certyfikatu dla telefonów Nokia"},{"content":"Czasami odpalam klona jakiegoś systemu by później po drobnych zmianach uczynić go osobnym bytem. Jednym z kroków po odtworzeniu systemu jest wygenerowanie nowego zestawu kluczy dla serwera OpenSSH (by mój klient ssh nie siał warning\u0026rsquo;ami). Można to wykonać tak:\nNajpierw kasujemy obecne klucze:\nrm /etc/ssh/ssh_host_* Teraz generujemy nowe:\ndpkg-reconfigure openssh-server I na koniec restartujemy usługę by załadować nowy zestaw kluczy (nie powinno to zerwać obecnej sesji, ale dla pewności lepiej zadanie odpalić w screen\u0026rsquo;ie):\nservice ssh restart Jeżeli zmienialiśmy klucze dla obecnego hosta to konieczne może być usunięcie nieaktualnego wpisu z ~/.ssh/known_hosts.\n","permalink":"https://gagor.pro/2012/02/ponowne-wygenerowanie-kluczy-serwera-openssh/","summary":"Czasami odpalam klona jakiegoś systemu by później po drobnych zmianach uczynić go osobnym bytem. Jednym z kroków po odtworzeniu systemu jest wygenerowanie nowego zestawu kluczy dla serwera OpenSSH (by mój klient ssh nie siał warning\u0026rsquo;ami). Można to wykonać tak:\nNajpierw kasujemy obecne klucze:\nrm /etc/ssh/ssh_host_* Teraz generujemy nowe:\ndpkg-reconfigure openssh-server I na koniec restartujemy usługę by załadować nowy zestaw kluczy (nie powinno to zerwać obecnej sesji, ale dla pewności lepiej zadanie odpalić w screen\u0026rsquo;ie):","title":"Ponowne wygenerowanie kluczy serwera OpenSSH"},{"content":"Aby wybrane systemy DomU startowały automatycznie po restarcie hypervisora należy podlinkować ich pliki konfiguracyjne w katalogu /etc/xen/auto po uprzednim jego utworzeniu. Przykładowo:\nmkdir /etc/xen/auto ln -s /etc/xen/example.cfg /etc/xen/auto/example.cfg Od teraz DomU example będzie startować automatycznie.\n","permalink":"https://gagor.pro/2012/02/xen-ustawienie-autostartu-domu/","summary":"Aby wybrane systemy DomU startowały automatycznie po restarcie hypervisora należy podlinkować ich pliki konfiguracyjne w katalogu /etc/xen/auto po uprzednim jego utworzeniu. Przykładowo:\nmkdir /etc/xen/auto ln -s /etc/xen/example.cfg /etc/xen/auto/example.cfg Od teraz DomU example będzie startować automatycznie.","title":"Xen - ustawienie autostartu DomU"},{"content":"Jeżeli zdecydowaliśmy się na systemu DomU w obrazach to możemy korzystać z live migration. By uruchomić jej obsługę, trzeba w pliku /etc/xen/xend-config.sxp odkomentować odpowiednie linie i ustawić adres IP:\n(xend-relocation-server yes) (xend-relocation-port 8002) (xend-relocation-address \u0026#39;10.0.10.91\u0026#39;) Wykonywanie migracji xm migrate --live nazwa-domu nazwa.lub.ip.zdalnego.hosta ","permalink":"https://gagor.pro/2012/02/xen-wlaczenie-live-migration/","summary":"Jeżeli zdecydowaliśmy się na systemu DomU w obrazach to możemy korzystać z live migration. By uruchomić jej obsługę, trzeba w pliku /etc/xen/xend-config.sxp odkomentować odpowiednie linie i ustawić adres IP:\n(xend-relocation-server yes) (xend-relocation-port 8002) (xend-relocation-address \u0026#39;10.0.10.91\u0026#39;) Wykonywanie migracji xm migrate --live nazwa-domu nazwa.lub.ip.zdalnego.hosta ","title":"Xen - Włączenie Live Migration"},{"content":"Ostatnio pisałem o konfiguracji Dom0\u0026thinsp; external link - dzisiaj napiszę o uruchamianiu DomU.\nDo instalacji DomU wykorzystuję skrypty z pakietu xen-tools, można go zainstalować poleceniem:\napt-get install xen-tools Oczywiście aby wszystko działało fajnie musimy ustawić kilka domyślnych opcji, robimy to edytując plik /etc/xen-tools/xen-tools.conf. Lecimy po kolei:\n# Virtual machine disks are created as logical volumes in # volume group \u0026#39;universe\u0026#39; (hint: LVM storage is much faster # than file) lvm = universe Osobiście korzystam z LVM\u0026rsquo;a który zgodnie z hint\u0026rsquo;em jest znacznie szybszy od plików obrazów. Daje do tego bardzo wygodne możliwość w zarządzaniu rozmiarami wolumenów przydzielonych DomU, możliwość tworzenia snapshotów (np. na potrzeby backupu). Postawienie LVM\u0026rsquo;a w skrócie wygląda tak:\napt-get install lvm2 pvcreate /dev/sdb1 # to tylko przyklad - samodzielnie ustal partycje :) vgcreate universe /dev/sdb1 vgchange -a y universe Pozostałe opcje i bardziej skomplikowane przypadki tworzenia LVM\u0026rsquo;a można znaleźć tutaj: http://tldp.org/HOWTO/LVM-HOWTO/commontask.html\nUstalamy domyślne parametry dla tworzonych DomU - oczywiście zawsze możemy je nadpisać poprzez podanie innej wartości w linii poleceń, np. -memory 4Gb:\nsize = 8Gb # Disk image size. memory = 512Mb # Memory size swap = 1Gb # Swap size xfs_options = noatime,nodiratime Domyślnie nie ma konfiguracji dla systemu plików ext4, wystarczy dodać w odpowiednich miejscach:\nfs = ext4 ext4_options = noatime,nodiratime,errors=remount-ro Ustawienia sieci:\n# Default gateway and netmask for new VMs gateway = 10.0.10.1 netmask = 255.255.255.0 broadcast = 192.168.3.255 Warto ustawić domyślne ustawienia sieci - choć ja osobiście wolę zawsze podawać te parametry ręcznie.\n# When creating an image, interactively setup root password passwd = 1 Dzięki tej opcji na koniec tworzenia DomU zostaniemy poproszeni o podanie hasła dla root\u0026rsquo;a.\n# This is most useful on 64 bit host machines, for other systems it # doesn\u0026#39;t need to be used. # arch = amd64 Domyślna architektura dla tworzonych DomU - można przedefiniować z linii poleceń.\n# Uncomment if you wish newly created images to boot once they\u0026#39;ve been # created. # boot = 0 Ja nie lubię gdy DomU uruchamiają się automatycznie po przygotowaniu bo często ręcznie modyfikuję konfigurację i dopiero uruchamiam wirtualkę - stąd 0.\n# Let xen-create-image use pygrub, so that the grub from the VM is used, # which means you no longer need to store kernels outside the VMs. # Keeps things very flexible. pygrub=1 role = pygrub, myconfig Ta opcja powoduje że DomU będzie posiadać własny kernel, który może być aktualizowany niezależnie od kernela hypervisora. Włączenie tej opcji wymaga dodatkowej roli (pygrub) która zainstaluje wymagane pakiety, etc. Niestety roli tej nie ma w standardowej instalacji więc zamieszczam poniżej plik znaleziony w sieci. Edytujemy plik /etc/xen-tools/role.d/pygrub:\n#!/bin/sh # # Configure the new image to be suitable for booting via pygrub # # Wejn # -- # http://wejn.org/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Update APT lists. # chroot ${prefix} /usr/bin/apt-get update # # Install the packages # set -e installDebianPackage ${prefix} perl installDebianPackage ${prefix} libklibc installDebianPackage ${prefix} klibc-utils installDebianPackage ${prefix} initramfs-tools installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #chroot ${prefix} /usr/bin/dpkg -l | grep linux-image-xen-amd64 #if [ $? -ne 0 ]; then # installDebianPackage ${prefix} linux-image-2.6-xen-686 #else # installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #fi # Force initrd if none exists echo ${prefix}/boot/initrd* | grep -q 2\\\\.6 if [ $? -ne 0 ]; then chroot ${prefix} update-initramfs -c -k `ls -1 ${prefix}/lib/modules/ | head -n 1` fi # Generate grub menu.lst LNZ=`basename \\`ls -1 ${prefix}/boot/vmlinuz*|tail -n 1\\`` RD=`basename \\`ls -1 ${prefix}/boot/initrd*|tail -n 1\\`` mkdir -p ${prefix}/boot/grub cat - \u0026lt; ${prefix}/boot/grub/menu.lst default 0 timeout 1 title Debian root (hd0,0) kernel /boot/$LNZ root=/dev/xvda2 ro initrd /boot/$RD EOF Jeżeli na DomU mamy zamiar główny system plików sformatować jako XFS to pygrub nie będzie potrafił się z niego zbootować - dobrze za to działa z ext3 i ext4.\nRola myconfig do wstępnej konfiguracji systemu DomU Rola myconfig to mój własny zestaw skryptów, który konfiguruje DomU w taki sposób jak lubię - bym nie musiał każdorazowo tracić czasu na ustawienie nowego systemu pod siebie. Zawartość poniżej:\n#!/bin/sh # # Configure DomU to my needs # # Tomasz Gagor # -- # https://gagor.pl/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Moje ulubione zrodla w sources.list # echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list # Update APT lists. chroot ${prefix} /usr/bin/apt-get update # # Zainstaluj ulubione paczki # set -e installDebianPackage ${prefix} ssh installDebianPackage ${prefix} vim installDebianPackage ${prefix} mc installDebianPackage ${prefix} bash-completion installDebianPackage ${prefix} ethtool installDebianPackage ${prefix} less installDebianPackage ${prefix} screen installDebianPackage ${prefix} postfix installDebianPackage ${prefix} apticron # # Usun paczki ktorych nie potrzebuje lub nie lubie # # PPP removeDebianPackage ${prefix} pppconfig removeDebianPackage ${prefix} pppoeconf removeDebianPackage ${prefix} pppoe removeDebianPackage ${prefix} ppp removeDebianPackage ${prefix} libpcap0.7 # edytory removeDebianPackage ${prefix} nano removeDebianPackage ${prefix} ed removeDebianPackage ${prefix} nvi # inne #removeDebianPackage ${prefix} tasksel tasksel-data #removeDebianPackage ${prefix} pciutils #removeDebianPackage ${prefix} fdutils #removeDebianPackage ${prefix} cpio # skypt automatyczne czyszczacy cache apta # zmniejsza rozmiar backupow echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh echo \u0026#39;apt-get clean\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh chmod +x ${prefix}/etc/cron.daily/apt-get-clean.sh # ethtool - zwieksza wydajnosc wirtualizowanych urzadzen sieciowych echo \u0026#34;post-up ethtool -K eth0 tx off\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/network/interfaces # konfiguracja vima tak jak lubie chroot ${prefix} /usr/sbin/update-alternatives --set editor /usr/bin/vim.basic cat \u0026lt; ${prefix}/etc/vim/vimrc.local syntax on set background=dark if has(\u0026#34;autocmd\u0026#34;) filetype plugin indent on endif set showmatch VIMRC # konfiguracja bash-autocompletion cat \u0026lt;\u0026gt; ${prefix}/etc/bash.bashrc # enable bash completion in interactive shells if [ -f /etc/bash_completion ]; then . /etc/bash_completion fi BASHCOMP echo \u0026#39;source /etc/bash.bashrc\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/profile # instalacja i konfiguracja postfixa # poniżej wpisz własny alias dla hostmastera echo \u0026#34;root: hostmaster@mojadomena.pl\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/aliases chroot ${prefix} /usr/bin/newaliases chroot ${prefix} /usr/sbin/postconf -e \u0026#34;myhostname = `cat ${prefix}/etc/hostname`\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#34;mydestination = `cat ${prefix}/etc/hostname`, `cat ${prefix}/etc/hostname`.in.veracomp.pl, localhost.localdomain, localhost\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;relayhost = mail.example.pl\u0026#39; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;myorigin = /etc/mailname\u0026#39; # dostosuj domenę pod siebie echo \u0026#34;`cat ${prefix}/etc/hostname`.internal.example.pl\u0026#34; \u0026gt; ${prefix}/etc/mailname # konfiguracja locales chroot ${prefix} /usr/sbin/locale-gen # konfiguracja timezone echo \u0026#39;Europe/Warsaw\u0026#39; \u0026gt; ${prefix}/etc/timezone chroot ${prefix} dpkg-reconfigure -f noninteractive tzdata # konfiguracja mc i authorized_keys ssh takie jak na dom0 cp -r /root/.mc/ ${prefix}/root/ cp -r /root/.ssh/ ${prefix}/root/ ","permalink":"https://gagor.pro/2012/02/xen-na-squeeze-instalowanie-i-konfiguracja-hostow-gosci-domu/","summary":"Ostatnio pisałem o konfiguracji Dom0\u0026thinsp; external link - dzisiaj napiszę o uruchamianiu DomU.\nDo instalacji DomU wykorzystuję skrypty z pakietu xen-tools, można go zainstalować poleceniem:\napt-get install xen-tools Oczywiście aby wszystko działało fajnie musimy ustawić kilka domyślnych opcji, robimy to edytując plik /etc/xen-tools/xen-tools.conf. Lecimy po kolei:\n# Virtual machine disks are created as logical volumes in # volume group \u0026#39;universe\u0026#39; (hint: LVM storage is much faster # than file) lvm = universe Osobiście korzystam z LVM\u0026rsquo;a który zgodnie z hint\u0026rsquo;em jest znacznie szybszy od plików obrazów.","title":"Xen na Squeeze - Instalowanie i konfiguracja hostów gości (DomU)"},{"content":"Bardzo udany przepis - gdzieś musiałem go udokumentować by nie zginął :)\nSkładniki 1 duża kaczka (2-2,5 kg) 4 ząbki czosnku 1 kg jabłek (w zależności od kaczki mogą wystarczyć 3 większe sztuki) sok z cytryny do skropienia jabłek 200 g suszonej żurawiny pół butelki czerwonego wytrawnego wina rozmaryn 4 łyżki miodu cynamon kardamon sól pieprz Sposób wykonania Kaczkę nacieramy solą, pieprzem, rozmarynem oraz roztartym czosnkiem i pozostawiamy na min. 3-4 godziny w lodówce (można też przygotować ją wieczorem by była gotowa na następny dzień).\nJabłka obieramy, usuwamy z nich nasiona i kroimy w średniej wielkości kostkę. Do jabłek dodajemy żurawinę, skrapiamy całość sokiem z cytryny, po czym dodajemy cynamon, kardamon i miód.\nPo wyjęciu z lodówki wypełniamy wnętrze kaczki wcześniej przygotowanym farszem i zaszywamy. Do rozgrzanego piekarnika (170-190 stopni) wstawiamy kaczkę umieszczoną w naczyniu pod przykryciem. Po około 1,5 godziny wyjmujemy kaczkę z piekarnika i z obu stron nacinamy skórę pomiędzy udami a korpusem - umożliwi to wypłynięcie zebranego pod skórą tłuszczu. Co 30 minut należy kaczkę podlewać powstałym sosem.\nCzas pieczenia kaczki zależy od jej wagi. Ważąca 1,5-2 kg sztukę należy piec od 90 minut do 2 godzin. Na godzinę przed końcem pieczenia wlewamy czerwone wino (ok. pół butelki). Na ostatnie 20 minut zdejmujemy pokrywę i przełączamy piekarnik na zapiekanie od góry by skórka mogła się zrumienić.\nWynik Całkiem puściło moje kiepskie szycie ale smaku to nie popsuło :)\nŹródło: Ugotowani\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/02/kaczka-pieczona-z-zurawina-i-jablkami/","summary":"Bardzo udany przepis - gdzieś musiałem go udokumentować by nie zginął :)\nSkładniki 1 duża kaczka (2-2,5 kg) 4 ząbki czosnku 1 kg jabłek (w zależności od kaczki mogą wystarczyć 3 większe sztuki) sok z cytryny do skropienia jabłek 200 g suszonej żurawiny pół butelki czerwonego wytrawnego wina rozmaryn 4 łyżki miodu cynamon kardamon sól pieprz Sposób wykonania Kaczkę nacieramy solą, pieprzem, rozmarynem oraz roztartym czosnkiem i pozostawiamy na min. 3-4 godziny w lodówce (można też przygotować ją wieczorem by była gotowa na następny dzień).","title":"Kaczka pieczona z żurawiną i jabłkami"},{"content":"Po co mi to? Wiele razy miałem do czynienia z serwerami na których działało kilka/kilkanaście usług równocześnie, np. Apache (kilka stronek, webmail, phpmyadmin, itp), Postfix/Exim (poczta i żeby było fajnie to na kontach systemowych), Samba (jakieś zasoby dla pracowników), MySQL (baza dla stronek), PostgreSQL (bo jedna stronka potrzebowała), itd\u0026hellip;. Przy takiej konfiguracji pomijane są kwestie separacji usług zewnętrznych/wewnętrznych - no ale firma/instytucja mała nie ma sensu kasy na 3 kolejne serwery wydawać skoro działa\u0026hellip;.\nTaka konfiguracja nie jest zbyt bezpieczna i ma wiele wad:\nciężko backupować tak duży system w postaci obrazów, a odzyskiwanie z backupów całości będzie trwać wieki, słaba separacja usług ułatwia ataki na zasoby wewnętrzne (np. dziura w stronie może pozwolić na dostęp do plików z Samby), trudno rozgraniczyć zasoby sprzętowe (procesor, pamięć) konkretnym usługom, trudno jest migrować usługi na inne serwery. Wirtualizacja pozwala na zminimalizowanie tych problemów:\nbackupować można pojedyncze usługi (działające na różnych maszynach wirtualnych) np. z pomocą snapshotów z minimalnym czasem przerwy w działaniu, oddzielenie usług w osobnych maszynach wirtualnych pozwala \u0026ldquo;zamknąć\u0026rdquo; napastnika w przypadku kompromitacji którejś z usług (choć zdarzały się błędy w implementacji maszyn wirtualnych pozwalające \u0026ldquo;wyskoczyć\u0026rdquo; z wirtualki), możemy przydzielać pamięć i konkretne rdzenie procesora danym maszynom wirtualnym, live migration pozwala w locie przenieść maszynę wirtualną na inny serwer. Xen jest szczególnie dobrym wyborem jeżeli zamierzamy wirtualizować systemy Linux\u0026rsquo;owe. Można je uruchamiać w trybie para-wirtualizacji, która w mniejszym stopniu niż pełna wirtualizacja obciąża CPU.\nW trybie pełnej wirtualizacji można zainstalować praktycznie dowolny system (również Windows) choć jest to tryb mniej wydajny.\nUwaga Post ten jest próbą zebrania w jednym miejscu różnych moich doświadczeń z Xen\u0026rsquo;em ale nie miałem czasu na testową instalację według poniższego tekstu, więc mogłem gdzieś coś zamieszać. Dlatego proszę o komentowanie jeśli coś nie zadziała.\nKonfiguracja hypervisor\u0026rsquo;a (Dom0) Jeżeli zastanawiasz się czy stawiać Xen\u0026rsquo;a na 32 czy 64-bitowym systemie to wybierz 64-bit. Na systemie 64-bitowym można uruchamiać systemy gości 32 i 64-bitowe (w przeciwieństwie do hypervisora 32-bitowego) i nie będzie problemów z obsługą dużej ilości pamięci.\nZaczynamy od instalacji hypervisor\u0026rsquo;a, zmodyfikowanej do działania z Xen wersji jądra i pakietu podstawowych narzędzi. Wszystko co potrzebne jest w jednym metapakiecie (xen-linux-system ale gdyby nie chciał iść to lepiej wskazać konkretną wersję):\napt-get install xen-linux-system-2.6-xen-amd64 By mieć dostęp do pełnej wirtualizacji (np. by uruchamiać systemy Windows) należy doinstalować jeszcze jedną paczkę (jeśli nie potrzebujemy to zawsze można doinstalować później):\napt-get install xen-qemu-dm-4.0 Squeeze domyślnie korzysta z GRUB\u0026rsquo;a 2 i niezbyt przeze mnie lubianej metody wykrywania systemów, która po zainstalowaniu jajka dla Xen\u0026rsquo;a nadal będzie domyślnie startować podstawowy kernel. By to zmienić proponuję przenieść wykrywanie domyślnego systemu na pozycję późniejszą niż Xen\u0026rsquo;a - zmieniamy nazwę pliku:\nmv /etc/grub.d/10_linux /etc/grub.d/21_linux Ponadto jeżeli zamierzamy instalować DomU na LVM\u0026rsquo;ie to na pewno zainteresuje nas wyłączenie opcji OS prober by DomU nie były wykrywane jako zainstalowane lokalnie systemy (wyłączy to też wykrywanie Windowsa jeśli jest zainstalowany na tej samej maszynie).\nBy wyłączyć OS prober\u0026rsquo;a otwórz plik /etc/default/grub i dodaj na końcu:\n# Wylaczenie OS prober\u0026#39;a by nie wykrywac maszyn wirtualnych na # logicznych wolumenach LVM i nie pokazywac ich w menu grub\u0026#39;a GRUB_DISABLE_OS_PROBER=true Jeżeli chcesz dodać jakieś dodatkowe parametry przy bootowaniu Xen\u0026rsquo;a dodaj poniższe zmienne w /etc/default/grub (przypisane wartości nie są prawidłowe i jeśli nic nie potrzebujesz dodawać to pomiń ten krok):\n# parametry dla wszystkich opcji startowych Xen\u0026#39;a (rowniez recovery) GRUB_CMDLINE_XEN=\u0026#34;something\u0026#34; # dodatkowe parametry dla opcji non-recovery GRUB_CMDLINE_XEN_DEFAULT=\u0026#34;something else\u0026#34; Po modyfikacji opcji gruba musimy wykonać aktualizację poleceniem:\nupdate-grub Domyślnym zachowaniem dom0 Xen\u0026rsquo;a przy wyłączaniu bądź restartowaniu jest próba zahibernowania wszystkich działających domU. Gdy nie przewidzimy takiej sytuacji i nie mamy wystarczającej ilości wolnego miejsca w /var często proces ten kończy się błędami. Zdarzają się też problemy z prawidłową obsługą hibernacji po stronie systemów działających w domU, wtedy po uruchomieniu dom0 i próbie załadowania zapisanych stanów maszyn kończymy z wiszącymi wirtualkami.\nNa dobrą sprawę równie dobrym zachowaniem byłoby wyłączenie wszystkich domU i zaczekanie aż to nastąpi a po starcie dom0 uruchomienie ich od zera. Może cały proces będzie trwać chwilę dłużej ale mamy za to większą pewność że przejdzie bez niespodzianek. By ustawić takie zachowanie Xen\u0026rsquo;a edytujemy /etc/default/xendomains:\nXENDOMAINS_RESTORE=false XENDOMAINS_SAVE= Aby systemy domU miały dostęp do sieci należy w pliku /etc/xen/xend-config.sxp upewnić się że network-script ustawione jest na network-bridge.\n(network-script \u0026#39;network-bridge antispoof=yes\u0026#39;) Przy takim ustawieniu systemu domU będą korzystać z głównego interfejsu dom0 aby uzyskać dostęp do sieci. Dla wielu będzie to niezbyt optymalne rozwiązanie ale na początek wystarczy (niecierpliwi mogą zerknąć do dokumentacji Xen\u0026rsquo;a aby sprawdzić inne opcje XenNetworking\u0026thinsp; external link ).\nDodatkowa opcja antispoof=yes aktywuje firewall, który uniemożliwi systemom domU przypisywanie sobie adresów innych systemów domU. By to działało w konfiguracji maszyny wirtualnej w definicji interfejsu (vif) należy przypisać adres IP danemu systemowi domU.\nNa moim systemie nie wszystkie skrypty dla konfiguracji sieci miały atrybut do wykonywania co skutkowało błędami przy uruchamianiu domU (np. \u0026ldquo;missing vif-script, or network-script, in the Xen configuration file\u0026rdquo;). By temu zapobiec ustawmy wszystkim skryptom odpowiednie uprawnienia:\nchmod +x /etc/xen/scripts/* W pliku /etc/xen/xend-config.sxp można ponadto ustawić jak dużo pamięci i procesorów rezerwujemy dla systemu dom0. Ilość pamięci dla dom0 można ograniczyć dodając opcję dom0_mem dla kernela w zmiennej GRUB_CMDLINE_XEN w konfiguracji gruba.\nNiektórzy zalecają wyłączenie dom0-balloning (czyli zajmowania przez dom0 całej pozostałej pamięci po uruchomieniu systemów domU), a zamiast tego przydzielenie minimalnej ilości pamięci, np. tak:\n(dom0-min-mem 1024) (enable-dom0-ballooning no) Ja osobiście wolę pozostawić balloning włączony - po co blokować ram skoro nikt inny z niego nie skorzysta, a tak to przynajmniej jest więcej na bufory dyskowe 😉\nUstawienie konsoli By mieć dostęp do wyjścia z GRUB\u0026rsquo;a, Xen hypervisor\u0026rsquo;a, kernela i getty poprzez VGA i konsolę należy dokonać zmian w pliku /etc/default/grub jak poniżej:\nGRUB_SERIAL_COMMAND=\u0026#34;serial --unit=0 --speed=9600 --word=8 --parity=no --stop=1\u0026#34; GRUB_TERMINAL=\u0026#34;console serial\u0026#34; GRUB_TIMEOUT=5 GRUB_CMDLINE_XEN=\u0026#34;com1=9600,8n1 console=com1,vga\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;console=tty0 console=hvc0\u0026#34; A następnie w /etc/inittab aktualizujemy linijki:\n1:2345:respawn:/sbin/getty 38400 hvc0 2:23:respawn:/sbin/getty 38400 tty1 # NO getty on ttyS0! W ten sposób tty1 pokazuje wyjście VGA, a hvc0 kosolę. Standardowo po modyfikacji opcji gruba musimy wykonać aktualizację poleceniem:\nupdate-grub Jeśli nic nie skopaliśmy to po restarcie powinniśmy otrzymać hypervisora gotowego do uruchamiania DomU.\nŹródła http://wiki.debian.org/Xen\u0026thinsp; external link ","permalink":"https://gagor.pro/2012/02/xen-na-squeeze-instalacja-i-konfiguracja-hypervisora/","summary":"Po co mi to? Wiele razy miałem do czynienia z serwerami na których działało kilka/kilkanaście usług równocześnie, np. Apache (kilka stronek, webmail, phpmyadmin, itp), Postfix/Exim (poczta i żeby było fajnie to na kontach systemowych), Samba (jakieś zasoby dla pracowników), MySQL (baza dla stronek), PostgreSQL (bo jedna stronka potrzebowała), itd\u0026hellip;. Przy takiej konfiguracji pomijane są kwestie separacji usług zewnętrznych/wewnętrznych - no ale firma/instytucja mała nie ma sensu kasy na 3 kolejne serwery wydawać skoro działa\u0026hellip;.","title":"Xen na Squeeze - instalacja i konfiguracja hypervisor’a"},{"content":"Jakiś czas temu korzystałem z preload\u0026rsquo;a\u0026thinsp; external link który sam uczył się jakie aplikacje odpalam i te programy ładował już podczas startu - przeważnie nieco spowalnia to start systemu ale gdy już się załaduje to programy, które uruchamiam jako pierwsze startują \u0026ldquo;z kopa\u0026rdquo;. Od jakiegoś czasu popularniejszy jest instalowany domyślnie w Ubuntu ureadahead - pełni on podobną funkcję jak preload.\nMożna zmusić ureadahead do ponownego wygenerowania nowej listy programów wczytywanych przy starcie do cache a oto jak zrobić:\nNależy skasować pliki z rozszerzeniem pack w /var/lib/ureadahead/: sudo rm /var/lib/ureadahead/*.pack Można ustawić autmatyczne logowanie. Restartujemy system. Szybko logujemy się do systemu i uruchomiamy aplikacje, które chcemy aby szybciej startowały. Gdy wszystko się już załaduje sprawdzamy czy załadowały się wszystkie programy, na których nam zależało, np.: sudo ureadahead --dump | grep firefox 6.Jeżeli nie załadowały się wszystkie programy to w pliku /etc/init/ureadahead.conf zwiększamy wartość w linii:\npre-stop exec sleep 45 na np. 90 i wracamy do punktu 1.\n","permalink":"https://gagor.pro/2012/01/wstepne-ladowanie-programow-przy-starcie-z-ureadahead/","summary":"Jakiś czas temu korzystałem z preload\u0026rsquo;a\u0026thinsp; external link który sam uczył się jakie aplikacje odpalam i te programy ładował już podczas startu - przeważnie nieco spowalnia to start systemu ale gdy już się załaduje to programy, które uruchamiam jako pierwsze startują \u0026ldquo;z kopa\u0026rdquo;. Od jakiegoś czasu popularniejszy jest instalowany domyślnie w Ubuntu ureadahead - pełni on podobną funkcję jak preload.\nMożna zmusić ureadahead do ponownego wygenerowania nowej listy programów wczytywanych przy starcie do cache a oto jak zrobić:","title":"Wstępne ładowanie programów przy starcie z ureadahead"},{"content":"Aby umożliwić odwiedzającym nasze strony cachowanie obrazków (tak by nie musieli pobierać ich każdorazowo bo przecież nie zmieniają się aż tak często) konieczne jest ustawienie nagłówków: Cache-Control, Expires dla odpowiednich typów plików. W Apachem jest do tego dedykowany moduł - mod_expires. W Debianie dostarczany jest on bez domyślnej globalnej konfiguracji - a ja lubię gdy cacheuje mi się większość statyki. Zawsze można dostosować czas cachowania pod siebie względem określonego typu pliku, np. dla Java Scriptów ustawić na 1 dzień gdy często się zmieniają. Można też w samej aplikacji zmieniać ścieżkę do pliku by wymusić odświeżenie (lub podawać ścieżkę z jakimś losowym identyfikatorem wycinanym przez mod_rewrite).\nCache-Control, Expires Najpierw tworzymy globalny config (oczywiście można sobie robić też taki per host - jak kto woli):\ncat \u0026gt; /etc/apache2/mods-available/expires.conf \u0026lt; \u0026lt;CONF \u0026lt;IfModule mod_expires.c\u0026gt; # włączamy moduł ExpiresActive on # cacheowanie typowych obrazów przez 3 miesiące ExpiresByType image/jpg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/gif \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/jpeg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/png \u0026#34;access plus 3 months\u0026#34; # cacheowanie CSS i JavaScript przez 2-1 miesiąca ExpiresByType text/css \u0026#34;access plus 2 month\u0026#34; ExpiresByType text/javascript \u0026#34;access plus 1 day\u0026#34; ExpiresByType application/javascript \u0026#34;access plus 1 day\u0026#34; # domyślna wartość cachowania ExpiresDefault \u0026#34;access plus 3 months\u0026#34; CONF Dokładniejsze wyjaśnienie składni można znaleźć tutaj. Teraz aktywujemy moduł i restartujemy Apache\u0026rsquo;go (nie jestem pewien czy sam reload by wystarczył):\na2enmod expires service apache2 restart I tyle - jeśli sprawdzimy nagłówki (Firebug w Firefoxie albo Narzędzia programistyczne w Chromie) dla odpowiednich typów plików to powinny one zawierać przykładowo takie wartości:\nCache-Control: max-age=7776000 Expires: Thu, 19 Apr 2012 15:48:13 GMT FileETag Ustawiając powyższą konfigurację warto wyłączyć generowanie nagłówka ETag. Co prawda w dawnym zamyśle miał to być mechanizm pozwalający zbliżony do cachowania a dający 100% pewności że nowy plik zostanie pobrany a aktualne w kopie w cache już nie. Polega na obliczeniu hasha na podstawie \u0026ldquo;pewnych\u0026rdquo; danych o pliku (np. INode, data modyfikacji, rozmiar) - jeżeli posiadasz aktualny hash to nie musisz pobierać danych.\nPomysł na początku nie był zły\u0026hellip; ale obecne strony często udostępniają nawet kilkadziesiąt (i więcej) niedużych plików. By pobrać potrzebne do wygenerowania ETag\u0026rsquo;a dane trzeba wywołać operację stat na pliku co przy wielu równoczesnych odwołaniach potrafi mocno obciążyć dyski.\nNa serwerach ze średnim i dużym obciążeniem warto wyłączyć generowanie nagłówka w ten sposób:\necho \u0026#34;FileETag None\u0026#34; \u0026gt;\u0026gt; /etc/apache2/httpd.conf service apache2 reload ","permalink":"https://gagor.pro/2012/01/apache-mod_expires-konfiguracja/","summary":"Aby umożliwić odwiedzającym nasze strony cachowanie obrazków (tak by nie musieli pobierać ich każdorazowo bo przecież nie zmieniają się aż tak często) konieczne jest ustawienie nagłówków: Cache-Control, Expires dla odpowiednich typów plików. W Apachem jest do tego dedykowany moduł - mod_expires. W Debianie dostarczany jest on bez domyślnej globalnej konfiguracji - a ja lubię gdy cacheuje mi się większość statyki. Zawsze można dostosować czas cachowania pod siebie względem określonego typu pliku, np.","title":"Apache mod_expires konfiguracja"},{"content":"Objaw przeważnie jest taki: łączysz się po ssh podając klucz/hasło i czekasz nawet i 10 sekund aż pojawi się prompt. Po połączeniu wszystkie polecenia działają z normalną szybkością. Brzmi znajomo? 😉\nTaki objaw przeważnie jest skutkiem problemów z działaniem DNS\u0026rsquo;a po stronie klienta lub serwera. Warto sprawdzić poleceniami host/dig/nslookup po obu stronach jak dużo czasu potrzeba na rozwiązanie nazw. Najlepiej rozwiązać problem z DNS\u0026rsquo;em ustawiając szybkie serwery ale gdy nie mamy takiej możliwości to po stronie serwera można ustawić w /etc/sshd_config opcję:\nUseDNS no I restart ssh:\nservice ssh restart Spowoduje to wyłączenie znacznej części zapytań DNS po stronie serwera (w tym sprawdzanie reverse DNS\u0026rsquo;a dla hosta klienta w momencie łączenia). Na kilku serwerach z kiepskim DNS\u0026rsquo;em opcja ta \u0026ldquo;daje niezłego kopa\u0026rdquo;.\n","permalink":"https://gagor.pro/2012/01/dlugie-oczekiwanie-na-nawiazanie-polaczenia-ssh/","summary":"Objaw przeważnie jest taki: łączysz się po ssh podając klucz/hasło i czekasz nawet i 10 sekund aż pojawi się prompt. Po połączeniu wszystkie polecenia działają z normalną szybkością. Brzmi znajomo? 😉\nTaki objaw przeważnie jest skutkiem problemów z działaniem DNS\u0026rsquo;a po stronie klienta lub serwera. Warto sprawdzić poleceniami host/dig/nslookup po obu stronach jak dużo czasu potrzeba na rozwiązanie nazw. Najlepiej rozwiązać problem z DNS\u0026rsquo;em ustawiając szybkie serwery ale gdy nie mamy takiej możliwości to po stronie serwera można ustawić w /etc/sshd_config opcję:","title":"Długie oczekiwanie na nawiązanie połączenia ssh"},{"content":"Zawsze gdy potrzebuję zesniffować coś na żywo na Fortigate\u0026rsquo;ach muszę przeszukać Knowledge Base\u0026thinsp; external link by przypomnieć sobie wszystkie polecenia do tego potrzebne. Tym razem robię notatki \u0026#x1f603;\nSniffowanie diagnose debug enable diagnose debug flow filter addr ip address clear clear filter daddr dest ip address dport destination port negate inverse filter port port proto protocol number saddr source ip address sport source port vd index of virtual domain # np. diagnose debug flow filter saddr 10.10.80.3 diagnose debug flow filter daddr 8.8.8.8 diagnose debug flow filter dport 53 # wyświetl wyniki na konsoli diagnose debug flow show console enable # opcjonalne: wyświetla nazwy funkcji np. odwołania do routingu, itp diagnose debug flow show function-name enable # uruchomienie sniffowania - warto podać na końcu jakaś wartość # by sniffowanie zakończyło się po takiej liczbie pakietów # w przeciwnym wypadku wyniki będą się wypisywać na konsoli # aż uda nam się na oślep wyłączyć sniffowanie diagnose debug flow trace start 100 # zresetowanie filtrowania flow diagnose debug reset # wyłączenie sniffowania diagnose debug disable Diagnostyka tuneli IP-Sec # tutaj jest dużo prościej, najpierw włączamy debuga diagnose debug enable # a potem diagnose debug application ike 2 # lub dla bardzo, bardzo szczegółowych logów diagnose debug application ike -1 Niestety nie ma tutaj możliwości filtrowania (albo jeszcze o tym nie wiem), więc jeśli mamy dużo aktywnych tuneli to najlepiej zbierać wypisywane komunikaty do pliku i dopiero przeglądać.\n","permalink":"https://gagor.pro/2012/01/sniffowanie-w-fortios/","summary":"Zawsze gdy potrzebuję zesniffować coś na żywo na Fortigate\u0026rsquo;ach muszę przeszukać Knowledge Base\u0026thinsp; external link by przypomnieć sobie wszystkie polecenia do tego potrzebne. Tym razem robię notatki \u0026#x1f603;\nSniffowanie diagnose debug enable diagnose debug flow filter addr ip address clear clear filter daddr dest ip address dport destination port negate inverse filter port port proto protocol number saddr source ip address sport source port vd index of virtual domain # np.","title":"Sniffowanie w FortiOS"},{"content":"Co jakiś czas powtarza się sytuacja, gdy muszę zaktualizować jakiś serwerek z Lennym do Squeeze\u0026rsquo;a i za każdym razem muszę googlać za odpowiednimi źródłami, które paczki najpierw, etc\u0026hellip; Więc sobie zebrałem wszystko w poniższym poście.\nW razie wątpliwości patrz tutaj: http://www.debian.org/releases/squeeze/releasenotes\nZrób backup konfiguracji.\nTrzeba zaktualizować źródła by wskazywały na squeeze\u0026rsquo;a (poniższe polecenie nadpisze Twoje obecne repozytoria):\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb http://security.debian.org/ squeeze/updates main contrib non-free deb-src http://security.debian.org/ squeeze/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free SRC Teraz trzeba odświeżyć repozytoria:\nsudo apt-get update Proponuję pobrać też pliki by podczas aktualizacji wszystkie leżały w cache\u0026rsquo;u - na wypadek gdyby nagle padło łącze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz można zaktualizować kluczowe paczki:\nsudo apt-get install apt dpkg I aktualizacja całego systemu:\nsudo apt-get dist-upgrade I można brać się do łatania bugów w starej konfiguracji\u0026hellip; 😉\n","permalink":"https://gagor.pro/2012/01/upgrade-debian-lenny-do-squeeze/","summary":"Co jakiś czas powtarza się sytuacja, gdy muszę zaktualizować jakiś serwerek z Lennym do Squeeze\u0026rsquo;a i za każdym razem muszę googlać za odpowiednimi źródłami, które paczki najpierw, etc\u0026hellip; Więc sobie zebrałem wszystko w poniższym poście.\nW razie wątpliwości patrz tutaj: http://www.debian.org/releases/squeeze/releasenotes\nZrób backup konfiguracji.\nTrzeba zaktualizować źródła by wskazywały na squeeze\u0026rsquo;a (poniższe polecenie nadpisze Twoje obecne repozytoria):\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb http://security.","title":"Upgrade Debian Lenny do Squeeze"},{"content":"Jeżeli jesteś właścicielem Skody Fabii I to wcześniej czy później Twój BAT-Mobil zgłosi któreś z ostrzeżeń serwisowych:\nOIL - pojawia się przeważnie co 10 tys. km i wtedy gdy jeszcze nie trzeba wymieniać oleju w silniku \u0026#x1f604; service INSP - nie wiem jak często się pojawia a oznacza mniej więcej \u0026ldquo;czas wesprzeć finansowo lokalny autoryzowany serwis\u0026rdquo;. Kasowanie ostrzeżenia \u0026ldquo;OIL\u0026rdquo; Przy wyłączonym silniku wciskamy i przytrzymujemy przycisk kasowania przebiegu dziennego/pokrętło ustawiania godziny, od teraz nazywany po prostu \u0026lsquo;przyciskiem\u0026rsquo;. Cały czas przytrzymując \u0026lsquo;przycisk\u0026rsquo;, włączamy zapłon. Powinniśmy zobaczyć napis OIL, puszczamy \u0026lsquo;przycisk\u0026rsquo;. Przekręcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, aż zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Możemy wyłączyć zapłon. Kasowanie ostrzeżenia \u0026ldquo;service INSP\u0026rdquo; Wykonujemy czynności z punktów 1-3 podanych powyżej. Wciskamy \u0026lsquo;przycisk\u0026rsquo;, powinniśmy zobaczyć napis INSP, puszczamy przycisk. Przekręcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, aż zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Możemy wyłączyć zapłon. U mnie zadziałało, mam nadzieję, że się komuś przyda.\n","permalink":"https://gagor.pro/2012/01/skoda-fabia-kasownie-ostrzezen-oil-i-service-insp/","summary":"Jeżeli jesteś właścicielem Skody Fabii I to wcześniej czy później Twój BAT-Mobil zgłosi któreś z ostrzeżeń serwisowych:\nOIL - pojawia się przeważnie co 10 tys. km i wtedy gdy jeszcze nie trzeba wymieniać oleju w silniku \u0026#x1f604; service INSP - nie wiem jak często się pojawia a oznacza mniej więcej \u0026ldquo;czas wesprzeć finansowo lokalny autoryzowany serwis\u0026rdquo;. Kasowanie ostrzeżenia \u0026ldquo;OIL\u0026rdquo; Przy wyłączonym silniku wciskamy i przytrzymujemy przycisk kasowania przebiegu dziennego/pokrętło ustawiania godziny, od teraz nazywany po prostu \u0026lsquo;przyciskiem\u0026rsquo;.","title":"Skoda Fabia - Kasownie ostrzeżeń OIL i service INSP"},{"content":"Uruchamiamy SciTE i klikamy menu Options -\u0026gt; Open User Options File, wpisujemy dane:\n# domyślne korzystanie z fontów o stałej szerokości font.base=$(font.monospace) font.small=$(font.monospace) font.comment=$(font.monospace) font.text=$(font.monospace) font.text.comment=$(font.monospace) font.embedded.base=$(font.monospace) font.embedded.comment=$(font.monospace) font.vbs=$(font.monospace) # numerowanie wierszy line.margin.visible=1 line.margin.width=3+ # ikonki toolbara z tematu systemowego toolbar.usestockicons=1 # zaznaczanie blokowe rectangular.selection.modifier=8 # ustawienia wgłębień kodu indent.size=4 use.tabs=1 indent.automatic=1 Więcej opcji tutaj: http://www.scintilla.org/SciTEDoc.html\nSam pewnie jeszcze nie raz zaktualizuję ten wpis \u0026#x1f604;\n","permalink":"https://gagor.pro/2012/01/moj-domyslny-config-dla-scite/","summary":"Uruchamiamy SciTE i klikamy menu Options -\u0026gt; Open User Options File, wpisujemy dane:\n# domyślne korzystanie z fontów o stałej szerokości font.base=$(font.monospace) font.small=$(font.monospace) font.comment=$(font.monospace) font.text=$(font.monospace) font.text.comment=$(font.monospace) font.embedded.base=$(font.monospace) font.embedded.comment=$(font.monospace) font.vbs=$(font.monospace) # numerowanie wierszy line.margin.visible=1 line.margin.width=3+ # ikonki toolbara z tematu systemowego toolbar.usestockicons=1 # zaznaczanie blokowe rectangular.selection.modifier=8 # ustawienia wgłębień kodu indent.size=4 use.tabs=1 indent.automatic=1 Więcej opcji tutaj: http://www.scintilla.org/SciTEDoc.html\nSam pewnie jeszcze nie raz zaktualizuję ten wpis \u0026#x1f604;","title":"Mój domyślny config dla SciTE"},{"content":"W Gajim\u0026rsquo;ie od dawna brakowało mi wygodnego przeszukiwania po liście kontaktów (takiego jakie ma się pojawić już niebawem w wersji 0.15 - z półtora roku już na to czekam\u0026hellip;). W między czasie znalazłem chwilę by pobawić się Empathy - brzydki, nie ma przeglądarki usług XMPP, ale wyszukiwanie na rosterze jest dokładnie takie jakiego szukałem (w miarę wpisywania znaków zawęża listę kontaktów by pasowały do wpisywanego wzorca).\nTyle że skróty klawiaturowe w tym programie zwyczajnie mnie rozwalają - przez lata przyzwyczaiłem się że okienka czatu można zamknąć Escape\u0026rsquo;m - a tutaj nawet nie ma opcji, która pozwalała by na taką zmianę zachowania.\nRozwiązaniem jest edycja z root\u0026rsquo;a pliku /usr/share/empathy/empathy-chat-window.ui i zamiana linii:\n\u0026lt;accelerator key=\u0026#34;W\u0026#34; modifiers=\u0026lt;wbr\u0026gt;\u0026#34;GDK_CONTROL_\u0026lt;/wbr\u0026gt;\u0026lt;wbr\u0026gt;MASK\u0026#34;/\u0026gt;\u0026lt;/wbr\u0026gt; na:\n\u0026lt;accelerator key=\u0026#34;Escape\u0026#34; /\u0026gt; Ta da! Restart komunikatora i da się z nim żyć.\nŹródło: https://bugs.launchpad.net/ubuntu/+source/empathy/+bug/486508\u0026thinsp; external link ","permalink":"https://gagor.pro/2011/12/empathy-zamykanie-okienka-chatu-przyciskiem-escape/","summary":"W Gajim\u0026rsquo;ie od dawna brakowało mi wygodnego przeszukiwania po liście kontaktów (takiego jakie ma się pojawić już niebawem w wersji 0.15 - z półtora roku już na to czekam\u0026hellip;). W między czasie znalazłem chwilę by pobawić się Empathy - brzydki, nie ma przeglądarki usług XMPP, ale wyszukiwanie na rosterze jest dokładnie takie jakiego szukałem (w miarę wpisywania znaków zawęża listę kontaktów by pasowały do wpisywanego wzorca).\nTyle że skróty klawiaturowe w tym programie zwyczajnie mnie rozwalają - przez lata przyzwyczaiłem się że okienka czatu można zamknąć Escape\u0026rsquo;m - a tutaj nawet nie ma opcji, która pozwalała by na taką zmianę zachowania.","title":"Empathy - zamykanie okienka chatu przyciskiem Escape"},{"content":"Można znaleźć wiele tutoriali jakimi narzędziami wykonywać backupy. W większości przypadków absolutnie wystarczający okaże się flexbackup. Bardziej wymagający wykorzystają BackupPC, Baculę lub Amandę. Narzędzia te pozwalają wykonać kopie pełne, różnicowe, przyrostowe - kompresując je dla zaoszczędzenia miejsca.\nWszystko fajnie - ale problemy pojawiają się przy dostępie do tych danych. Żeby odzyskać plik zmodyfikowany dzisiaj trzeba rozpakować najpierw kopię pełną, potem różnicową, przyrostową by wreszcie wyciągnąć plik z wczoraj\u0026hellip; hmm ten też jest skopany. No to lecimy jeszcze raz\u0026hellip;\nDodatkowo jeżeli danych jest kilkadziesiąt GB to cały proces wielokrotnej dekompresji może trwać nawet kilka godzin. Zacząłem szukać alternatywnej metody backupowania i trafiłem na ciekawy artykuł na podstawie, którego opracowałem własny skrypt do backupu: http://www.mikerubel.org/computers/rsync_snapshots/\u0026thinsp; external link #!/bin/bash # zdalny serwer z którego chcemy wykonać backup RHOST=\u0026#34;server.test.pl\u0026#34; # zasoby rsync ze zdalnego serwera które zostaną zsynchronizowane BACKUPDIRS=(\u0026#34;backup\u0026#34; \u0026#34;home\u0026#34; \u0026#34;mails\u0026#34;) # lokalny katalog do którego trafi backup DSTDIR=\u0026#34;/srv/backup/server\u0026#34; # maksymalny czas przechowywania backupów wyrażony w dniach MAXAGE=33 # dodatkowe opcje dla rsynca (--verbose można zastąpić --quiet # łatwiej zauważyć wtedy błędy), można włączyć kompresje # w przypadku synchronizacji ze zdalnej lokalizacji OPTIONS=\u0026#34;--del --verbose\u0026#34; # plik z hasłem dla rsync\u0026#39;a - zabezpieczenie marne ale w izolowanej # sieci dopuszczalne PASS=\u0026#34;/root/.rsync-passwd\u0026#34; DATE=`date +\u0026#39;%Y.%m.%d\u0026#39;` YESTERDAY=`ls -1 $DSTDIR | sort | tail -n1` LINKDEST=$DSTDIR/$YESTERDAY # prosty mechanizm lock\u0026#39;a by uniemożliwić wielokrotne uruchomienie # skryptu, np. w sytuacji gdy nie zdąży wykonać się pełny backup # przed kolejnym wywołaniem if [ -f \u0026#34;/tmp/server_sync\u0026#34; -o -f \u0026#34;/tmp/server_sync_block\u0026#34; ]; then echo \u0026#34;Another sync is still running!\u0026#34; exit 1 fi touch /tmp/server_sync # wykonujemy kolejno kopie for DIR in ${BACKUPDIRS[@]}; do mkdir -p $DSTDIR/$DATE/$DIR # sprawdzenie czy mamy wcześniejszy backup, # jak mamy to robimy snapshot if [ -d \u0026#34;$DSTDIR/$YESTERDAY/$DIR\u0026#34; ]; then rsync -a --link-dest=$LINKDEST/$DIR \\ backup@$RHOST::archive/$DIR/ \\ $DSTDIR/$DATE/$DIR/ \\ --password-file=$PASS $OPTIONS else # jak nie mamy to robimy nowy rsync -a backup@$RHOST::archive/$DIR/ \\ $DSTDIR/$DATE/$DIR/ \\ --password-file=$PASS $OPTIONS fi # sprawdzenie czy synchronizacja się udała # jeśli się nie udała to możemy chcieć skasować niedokończony # backup by kolejny nie musiał być \u0026#34;prawie pełnym\u0026#34; # plik /tmp/server_sync_block trzeba skasować ręcznie if [ $? -ne 0 -a $? -ne 24 ]; then echo \u0026#34;Something was wrong becase rsync return $?\u0026#34; touch /tmp/server_sync_block exit 2 fi done # zwalnianie lock\u0026#39;a rm -f /tmp/server_sync # kasowanie najstarszych backupów find $DSTDIR -maxdepth 1 -type d -mtime +$MAXAGE \\ -print0 | xargs -0 -r rm -r ","permalink":"https://gagor.pro/2011/12/automatyczne-backupy-w-stylu-snapshot-z-rsynciem/","summary":"Można znaleźć wiele tutoriali jakimi narzędziami wykonywać backupy. W większości przypadków absolutnie wystarczający okaże się flexbackup. Bardziej wymagający wykorzystają BackupPC, Baculę lub Amandę. Narzędzia te pozwalają wykonać kopie pełne, różnicowe, przyrostowe - kompresując je dla zaoszczędzenia miejsca.\nWszystko fajnie - ale problemy pojawiają się przy dostępie do tych danych. Żeby odzyskać plik zmodyfikowany dzisiaj trzeba rozpakować najpierw kopię pełną, potem różnicową, przyrostową by wreszcie wyciągnąć plik z wczoraj\u0026hellip; hmm ten też jest skopany.","title":"Automatyczne backupy w stylu snapshot z rsync’iem"},{"content":"Zdarzyło mi się bawić sprzętowymi Firewallami firmy Fortigate - chcąc sprawdzić działanie pewnych funkcji potrzebowałem uruchomić dwa/trzy pudełka na osobnych łączach. Pomysł polegał na próbie zmuszenia pudełek do współpracy z modemem iPlus na USB.\nDrugim fajnym zastosowaniem tego triku jest możliwość wykorzystania iPlusa jako \u0026ldquo;zapasowego łącza\u0026rdquo; w przypadku awarii głównego.\nDzięki pomocy inżyniera Fortigate szybko udało mi się zebrać potrzebne do działania parametry, które należy uruchomić poprzez command line (telnet/ssh).\nJeżeli chcemy uruchomić modem dla konkretnego vdom\u0026rsquo;u to należy go najpierw wybrać, np:\nconfig vdom edit root Jeśli nie korzystamy z vdom\u0026rsquo;ów to krok ten możemy pominąć. Dalej:\nconfig system modem set status enable set pin-init at+cpin=xxxx set auto-dial enable set idle-timer 6 set redial 2 set phone1 *99# set username1 ppp set passwd1 ppp set extra-init1 at+cgdcont=1,\\\u0026#34;IP\\\u0026#34;,\\\u0026#34;\u0026lt;a href=\u0026#34;http://www.plusgsm.pl%5C\u0026#34;\u0026gt;www.plusgsm.pl\\\u0026lt;/a\u0026gt;\u0026#34; end Przy czym w opcji pin-init zamiast xxxx należy podać PIN karty SIM.\nNa koniec potrzebny jest restart urządzenia:\nexecute reboot Po restarcie będziemy mieć do dyspozycji nowy interfejs (modem). Automatycznie zostanie ustawione dns proxy.\nTestowałem to na modemach Huawei E156G i Huawei E173 i różnych urządzeniach FortiGate z firmware w wersji MR2.\n","permalink":"https://gagor.pro/2011/12/konfiguracja-modemu-usb-iplus-na-urzadzeniach-fortigate/","summary":"Zdarzyło mi się bawić sprzętowymi Firewallami firmy Fortigate - chcąc sprawdzić działanie pewnych funkcji potrzebowałem uruchomić dwa/trzy pudełka na osobnych łączach. Pomysł polegał na próbie zmuszenia pudełek do współpracy z modemem iPlus na USB.\nDrugim fajnym zastosowaniem tego triku jest możliwość wykorzystania iPlusa jako \u0026ldquo;zapasowego łącza\u0026rdquo; w przypadku awarii głównego.\nDzięki pomocy inżyniera Fortigate szybko udało mi się zebrać potrzebne do działania parametry, które należy uruchomić poprzez command line (telnet/ssh).","title":"Konfiguracja modemu USB iPlus na urządzeniach FortiGate"},{"content":"Wcześniej czy później zawsze pojawia się potrzeba zoptymalizowania naszej bazy MySQL. Przedstawię kilka zmian w konfiguracji, które powinny zwiększyć wydajność w większości przypadków.\nMyISAM - key_buffer_size Najprostszą optymalizacją baz/tabel z mechanizmem MyISAM jest odpowiednie dobranie bufora na cache dla kluczy i indeksów (dane nigdy nie są cachowane). Poniższe zapytanie pozwala oszacować zalecany rozmiar cache\u0026rsquo;u:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.4999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_key_buffer_size FROM (SELECT LEAST(POWER(2,32),KBS1) KBS FROM (SELECT SUM(index_length) KBS1 FROM information_schema.tables WHERE engine=\u0026#39;MyISAM\u0026#39; AND table_schema NOT IN (\u0026#39;information_schema\u0026#39;,\u0026#39;mysql\u0026#39;)) AA ) A, (SELECT 2 PowerOf1024) B; Wynik określa zalecany rozmiar bufora (parametr key_buffer_size w pliku /etc/mysql/my.cnf) dla bieżącego stanu bazy - warto ciut dodać na zapas. Na systemach 32 bitowych parametr key_buffer_size może przyjmować maksymalnie 4GB, na 64 bitowych maksymalnie 8GB.\n[mysqld] key_buffer_size=xxxM InnoDB - innodb_buffer_pool_size W przypadku InnoDB cachowane mogą być zarówno dane jak i klucze/indeksy a rozmiar bufora określa parametr innodb_buffer_pool_size. Zalecaną minimalną wartość tego parametru możemy oszacować zapytaniem:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.49999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_innodb_buffer_pool_size FROM (SELECT SUM(data_length+index_length) KBS FROM information_schema.tables WHERE engine=\u0026#39;InnoDB\u0026#39;) A, (SELECT 2 PowerOf1024) B; W przypadku InnoDB po zmianie wartości innodb_buffer_pool_size konieczne jest również ustawienie innodb_log_file_size na 25% wartości innodb_buffer_pool_size lub 2047M (należy wybrać mniejszą z wartości). By wygenerować pliki log o nowych rozmiarach musimy postępować według poniższej instrukcji:\nDopisujemy w pliku my.cnf parametry: [mysqld] innodb_buffer_pool_size=xxxM innodb_log_file_size=25% xxxM lub 2047M Wyłączamy bazę: invoke-rc.d mysql stop Kasujemy obecnie pliki log: rm /var/lib/mysql/ib_logfile[01] Uruchamiamy bazę: invoke-rc.d mysql start Po starcie bazy zostaną wygenerowane nowe pliki log o nowych rozmiarach. InnoDB - kompaktowanie plików W domyślnej konfiguracji MySQL dla baz InnoDB (na Debianie na bank, przypuszczam że na innych distro jest podobnie) wszystkie tabele, indeksy, metadane tabel i inne dane dotyczące table InnoDB przechowywane są w jednym pliku: /var/lib/mysql/ibdata1\nNie jest to optymalne ustawienie szczególnie gdy mamy dużo baz i o znacznych rozmiarach - wykonywanie wielu równoczesnych operacji na jednym gigantycznym pliku potrafi mocno przymulić.\nPróba kompaktowania/optymalizowania tabel InnoDB nie powoduje zmniejszenia tego pliku - bo gdy próbujemy optymalizować tabele InnoDB powoduje to:\nułożenie danych i indeksów wewnątrz pliku ibdata1 w sposób ciągły, wzrost rozmiaru pliku ibdata1 ponieważ powyższe dane dopisywane są na jego końcu. Niewiele osób spodziewa się takiego rezultatu. Można zmniejszyć rozmiar tego pliku wyłączając dane tabel i ich indeksy do osobnych plików ale proces ten wymaga pełnego backupu bazy i jej odtworzenia, wiąże się więc z chwilowym (a w przypadku dużych baz - dłuższym) przestojem.\nRobimy pełny backup bazy, np. poleceniem: mysqldump --all-databases --single-transaction -uroot -p \u0026gt; my-dump.sql (dump\u0026rsquo;a można dodatkowo skompresować gzipem dodając pipe\u0026rsquo;a - przyspieszy to odzyskiwanie przez zmniejszenie ilości danych potrzebnych do odczytania z dysku)\nKasujemy wszystkie bazy z wyjątkiem schematu mysql (de facto powinno wystarczyć skasowanie i odzyskanie tylko baz korzystających z tabel InnoDB)\nWyłączamy usługę:\ninvoke-rc.d mysql stop Dodajemy w pliku /etc/mysql/my.cnf parametry: [mysqld] innodb_file_per_table innodb_log_file_size=25% xG innodb_buffer_pool_size=xG pierwsza opcja powoduje właściwe rozdzielenie tabel InnoDB do różnych plików (dodanie tej opcji na serwerze na którym znajdują się bazy InnoDB spowoduje ich uszkodzenie - odradzam), drugą i trzecią linijkę konfigurujemy według wcześniejszej instrukcji o innodb_buffer_pool_size, możemy też na czas przywracania danych dodać opcję bulk_insert_buffer_size=256M, skróci to czas potrzebny na przywrócenie bazy. Kasujemy pliki: ibdata1, ib_logfile0 and ib_logfile1: rm /var/lib/mysql/ibdata1 rm /var/lib/mysql/ib_logfile[01] Uruchamiamy serwer MySQL: invoke-rc.d mysql start Przywracamy wszystkie bazy z dump\u0026rsquo;a: cat my-dump.sql | mysql -uroot -p Plik ibdata1 urośnie ale od tej pory będzie zawierać wyłącznie metadane tabel a poszczególne tabele i indeksy będą przechowywane w osobnych plikach, np. tabela.ibd. Teraz optymalizowanie tabel InnoDB będzie powodować zmniejszenie rozmiarów plików *.ibd a plik ibdata1 nie będzie tak obciążony.\nZmiana metody flush\u0026rsquo;a W niektórych przypadkach ustawienie opcji innodb_flush_method na wartość O_DIRECT może poprawić wydajność, choć w innych wydajność może się pogorszyć (na forach sugerowano występowanie problemu na dedykowanych macierzach). Można bezpiecznie włączyć tę opcję i wykonać kilka bechmarków:\n[mysqld] innodb_flush_method=O_DIRECT Cache wyników zapytań Sprawdźmy najpierw czy cachowanie jest włączone (u mnie było to domyślne ustawienie), wydajemy zapytanie:\nSHOW VARIABLES LIKE \u0026#39;query_cache_type\u0026#39;; Możliwe są 3 ustawienia:\nON (query_cache_type = 1) - cachowanie wszystkich zapytań, OFF (query_cache_type = 0) - cachowanie na żądanie, DEMAND (query_cache_type = 2) - cachowanie wyłączone. W przypadku opcji DEMAND cachowanie jest włączane jeżeli po SELECT\u0026rsquo;cie dodamy SQL_CACHE. Mi osobiście najbardziej odpowiada opcja z cachowaniem wszystkiego.\nNastępnie należy ustawić w pliku my.cnf poniższe zmienne według potrzeb:\nquery_cache_limit = 2M query_cache_size = 32M Pierwsza opcja ustala maksymalny rozmiar pojedynczego zapytanie, które będzie cachowane - zapytania większe nie będą cachowane. Druga opcja ustala rozmiar całego bufora na cache (przeważnie więcej działa lepiej).\nMyISAM - unikanie repair with keycache Gdy nasza baza urośnie i będziemy mieć w niej tabele o rozmiarze przekraczającym 2GB to da się zauważyć że pewne operacja jak np. zakładanie indeksu, optymalizacja, naprawa - trwają cholernie długo. Można to szczególnie odczuć właśnie w momencie przekraczania rozmiaru 2GB i tak utworzenie indeksu na tabeli o rozmiarze 1,9GB trwa powiedzmy kilkanaście/kilkadziesiąt minut, a ta sama operacja na bazie o rozmiarze 2,1GB może zająć nawet kilka godzin.\nPrzyczynę łatwo namierzyć obserwując wynik polecenia:\nSHOW PROCESSLIST; w trakcie operacji na \u0026ldquo;małej\u0026rdquo; i \u0026ldquo;dużej\u0026rdquo; tabeli. \u0026ldquo;Mała\u0026rdquo; zatrzymuje się na dłużej na operacji Repair By Sorting, a \u0026ldquo;duża\u0026rdquo; kona godzinami na Repair With Keycache. Właśnie różnica w działaniu obu mechanizmów sortowania daje w kość:\nrepair by sorting - wykorzystuje do sortowania wiele plików tymczasowych i wymaga sporo wolnego miejsca w katalogu ustawionym w opcji tmpdir (domyślnie ustawionej na /tmp) - jeżeli miejsca będzie za mało to wybierany będzie mechanizm \u0026ldquo;repair with keycache\u0026rdquo;, repair with keycache - wykorzystuje do sortowania bardzo mały bufor (u mnie 8MB), jest ok 10~20 krotnie wolniejszy niż \u0026ldquo;repair by sorting\u0026rdquo; a do tego tworzy mniej optymalne indeksy. O tym który z mechanizmów zostanie wybrany decyduje opcja myisam_max_sort_file_size - zmienna ta ma domyślnie wartość 2GB i właśnie dlatego problemy pojawiają się po przekroczeniu tego rozmiaru. Proponuję ustawić ją sporo powyżej rozmiaru największych tablic - oczywiście jeśli miejsce w temp\u0026rsquo;ie pozwoli na to, np:\nmyisam_max_sort_file_size=8GB Przy takim ustawieniu warto mieć w /tmp minimum drugie tyle wolnego miejsca.\nŹródło http://dba.stackexchange.com/questions/3163/mysql-5-1-innodb-configuration-24gb-ram-bi-xeon-high-load\u0026thinsp; external link ","permalink":"https://gagor.pro/2011/12/mysql-proste-metody-optymalizacji/","summary":"Wcześniej czy później zawsze pojawia się potrzeba zoptymalizowania naszej bazy MySQL. Przedstawię kilka zmian w konfiguracji, które powinny zwiększyć wydajność w większości przypadków.\nMyISAM - key_buffer_size Najprostszą optymalizacją baz/tabel z mechanizmem MyISAM jest odpowiednie dobranie bufora na cache dla kluczy i indeksów (dane nigdy nie są cachowane). Poniższe zapytanie pozwala oszacować zalecany rozmiar cache\u0026rsquo;u:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.4999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_key_buffer_size FROM (SELECT LEAST(POWER(2,32),KBS1) KBS FROM (SELECT SUM(index_length) KBS1 FROM information_schema.","title":"MySQL - Proste metody optymalizacji"},{"content":"Domyślna konfiguracja fail2ban\u0026rsquo;a (na Debianie) nie zawiera reguł pozwalających na blokowanie prób włamań na skrzynki POP/IMAP dla dovecota (no chyba że korzystamy z saslauthd). Można szybko utworzyć własny zestaw filtrów co przedstawię poniżej.\nTworzymy plik: /etc/fail2ban/filter.d/dovecot.conf\n[Definition] failregex = (?: pop3-login|imap-login): .*(?:Authentication failure|Aborted login \\(auth failed|Aborted login \\(tried to use disabled|Disconnected \\(auth failed|Aborted login \\(\\d+ authentication attempts).*rip=(?P\u0026lt;host\u0026gt;\\S*),.* ignoreregex = Później dopisujemy na końcu pliku: /etc/fail2ban/jail.conf\n[dovecot] enabled = true filter = dovecot port = pop3,pop3s,imap,imaps logpath = /var/log/mail.log maxretry = 20 # te dwa poniżej wedle uznania - ja mam dobrze ustawione default\u0026#39;y #findtime = 1200 #bantime = 1200 Zostało zrestartować fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart Tip na bazie dokumentacji: http://wiki.dovecot.org/HowTo/Fail2Ban\u0026thinsp; external link ","permalink":"https://gagor.pro/2011/11/fail2ban-regulki-dla-dovecota/","summary":"Domyślna konfiguracja fail2ban\u0026rsquo;a (na Debianie) nie zawiera reguł pozwalających na blokowanie prób włamań na skrzynki POP/IMAP dla dovecota (no chyba że korzystamy z saslauthd). Można szybko utworzyć własny zestaw filtrów co przedstawię poniżej.\nTworzymy plik: /etc/fail2ban/filter.d/dovecot.conf\n[Definition] failregex = (?: pop3-login|imap-login): .*(?:Authentication failure|Aborted login \\(auth failed|Aborted login \\(tried to use disabled|Disconnected \\(auth failed|Aborted login \\(\\d+ authentication attempts).*rip=(?P\u0026lt;host\u0026gt;\\S*),.* ignoreregex = Później dopisujemy na końcu pliku: /etc/fail2ban/jail.conf\n[dovecot] enabled = true filter = dovecot port = pop3,pop3s,imap,imaps logpath = /var/log/mail.","title":"fail2ban - regułki dla dovecot’a"},{"content":"Gdy już ustawimy reverse proxy przed Apache szybko można zauważyć że w logach zamiast adresów IP zdalnych użytkowników pojawia się tylko jeden adres: adres naszego proxy. Również z poziomu php\u0026rsquo;a jako adres klienta widać IP naszego proxy.\nBy poradzić sobie z tym problemem trzeba na serwerze reverse proxy ustawić przekazywanie informacji o oryginalnym adresie IP klienta w nagłówku X-Forwarded-For. W przypadku gdy reverse proxy działa na nginx\u0026rsquo;e wystarczy dodać taki wpis:\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; Teraz w trzeba zainstalować moduł mod_rpaf dla Apachego, który to zajmie się interpretacją nagłówka i podmianą IP proxy prawdziwym IP. Na Debianie wystarczy wpisać:\napt-get install libapache2-mod-rpaf Po instalacji należy w pliku /etc/apache2/mods-available/rpaf.conf w opcji RPAFproxy_ips dopisać adresy IP serwerów proxy, np (oczywiście wpisz swoje adresy):\nRPAFproxy_ips 127.0.0.1 10.24.0.5 Ważne by były to zaufane adresy IP - bo w tym miejscu pozwalamy by z tych lokalizacji możliwe było nadpisanie adresu IP np. w logach. Jeżeli pozwolimy na modyfikację adresów IP z zewnątrz to atakujący może wykorzystać to by nadpisać swój prawdziwy adres fałszywym.\nPozostało uruchomić moduł i zrestartować Apachego aby go załadował:\na2enmod rpaf invoke-rc.d apache2 restart Teraz zarówno w logach Apache\u0026rsquo;go jak i skryptach PHP\u0026rsquo;a będzie przekazywane rzeczywiste IP użytkownika.\n","permalink":"https://gagor.pro/2011/11/x-forwarded-for-mod_rpaf-logowanie-rzeczywistych-adresow-ip-na-apache-za-reverse-proxy/","summary":"Gdy już ustawimy reverse proxy przed Apache szybko można zauważyć że w logach zamiast adresów IP zdalnych użytkowników pojawia się tylko jeden adres: adres naszego proxy. Również z poziomu php\u0026rsquo;a jako adres klienta widać IP naszego proxy.\nBy poradzić sobie z tym problemem trzeba na serwerze reverse proxy ustawić przekazywanie informacji o oryginalnym adresie IP klienta w nagłówku X-Forwarded-For. W przypadku gdy reverse proxy działa na nginx\u0026rsquo;e wystarczy dodać taki wpis:","title":"X-Forwarded-For + mod_rpaf - logowanie rzeczywistych adresów IP na Apache za reverse proxy"},{"content":"Przez pewien czas korzystałem z eAcceleratora do przyspieszenia działania stron pisanych w PHP\u0026rsquo;ie ale czasem bywał niestabilny. Aktualizacje pojawiały się rzadko a od czasu do czasu miewałem problemy ze stabilnością tej wtyczki na kilku bardziej skomplikowanych aplikacjach. Zdarzało się że pomimo zmiany kodu w skrypcie php, eAccelerator serwował wciąż stary plik - konieczny był restart Apache\u0026rsquo;go by wszystko działało jak trzeba.\nZacząłem szukać alternatywy i trafiłem na dwa moduły:\nAPC\u0026thinsp; external link (czyli Alternative PHP Cache), który ma być nawet domyślnie wbudowany w PHP od wersji 5.4, XCache\u0026thinsp; external link - projekt rozwijany przez jednego z programistów lighttpd. Byłem ciekaw wydajności poszczególnych rozwiązań względem siebie, więc przygotowałem małe środowisko testowe składające się z 4 maszyn wirtualnych (działających pod kontrolą VirtualBox\u0026rsquo;a):\nApache 2.2 + PHP 5.3 Apache 2.2 + PHP 5.3 + eAccelerator 0.9.6.1 (instrukcja instalacji tutaj\u0026thinsp; external link ) Apache 2.2 + PHP 5.3 + APC 3.1.3p1 (pod Debianem paczka php-apc) Apache 2.2 + PHP 5.3 + XCache 1.3.0 (pod Debianem paczka php5-xcache) Najpierw przygotowałem pierwszą maszynę wraz z konfiguracją MySQL\u0026rsquo;a i domyślną instalacją WordPress\u0026rsquo;a 3.2 by test był w miarę miarodajny. Kolejne maszynki to klony tej pierwszej, plus zainstalowane i domyślnie skonfigurowane kolejne rozszerzenia. Każdemu z rozszerzeń przyznałem 32 MB pamięci na cache.\nMetodyka testu Do automatyzacji testu posłużył mi skrypt w bash\u0026rsquo;u uruchamiający ab dla 1000 zapytań z kolejno rosnącą liczbą równoległych połączeń. Pozwoli to na porównanie wydajności optymalizatorów przy mniejszym i większym obciążeniu. Uruchomienie testu dla czystej instalacji bez dodatku pokaże jak duży przyrost wydajności daje się uzyskać.\nJako systemy testowe posłużyły mi wirtualki uruchomione pod kontrolą VirtualBOX\u0026rsquo;a z zainstalowanym aktualnym Debianem Squeeze. Przydzieliłem im po 1 GB RAM\u0026rsquo;u i 2 rdzenie CPU. Wirtualki te raczej nie są highend\u0026rsquo;em ale do ogólnego porównania optymalizatorów będą w zupełności wystarczające.\nWyniki Już na pierwszy rzut oka widać że opłaca się zainstalować dowolny optymalizator bo ich wydajność jest zbliżona a w stosunku do czystej instalacji pozwalają obsłużyć prawie czterokrotnie większy ruch. Przy czym system bez opcode cacher\u0026rsquo;a nie pokonał granicy 70 zapytań na sekundę - zaczął swapować i nie ukończył kolejnych testów.\nPoniżej wykres przedstawiający ilość obsługiwanych żądań w zależności od ilości równoczesnych połączeń: I jeszcze jeden wykres, na którym porównałem wydajność poszczególnych optymalizatorów względem czystego PHP\u0026rsquo;a Wychodzi na to że przez większość testu eAccelerator był najszybszy, gdzieniegdzie przeplatając się z APC. XCache nieznacznie ale na całej długości poniżej dwóch wcześniejszych. Całościowe różnice pomiędzy optymalizatorami przeważnie nie przekraczały 3 zapytań/sekundę - więc różnice pomiędzy nimi są rzędu 1~2%. Można na tej podstawie wywnioskować że wydajność jest tak zbliżona iż nie powinna być jedynym kryterium wyboru optymalizatora dla naszego systemu.\nPoniżej postaram się zebrać subiektywne oceny poszczególnych rozwiązań by dostarczyć dodatkowych argumentów.\neAccelerator eAccelerator był najszybszy w teście ale miewałem z nim problemy (kilka razy ale\u0026hellip;) stąd nie jest moim faworytem.\nZalety:\nzdecydowanie najszybszy, jest stosunkowo aktywnie rozwijany, dość stabilny, wbudowany encoder i dekoder skryptów (do dystrybucji kodu bez źródeł w postaci skompilowanej). Wady:\nbrak paczek w repozytoriach Debiana - ręczna kompilacja nie jest ciężka ale gdy trzeba go utrzymać na 30 serwerach to przestaje być zabawnie, pomimo że pojawiają się nowe wersje to ostatnio miałem problemy z pobraniem ich ze strony projektu - szukanie paczek \u0026ldquo;gdzieś\u0026rdquo; na sieci nie wydaje mi się bezpieczne, miałem problem z pewną dużą aplikacją, nie działała stabilnie pod eAcceleratorem, eAccelerator powstał na bazie kodu Turck MMCache (ten nie jest już rozwijany) -istnieją pewne wątpliwości licencyjne co do jego kodu\u0026hellip; PHP APC APC pod względem wydajności nieznacznie ustępuje eAcceleratorowi. Ciekawą funkcją udostępnianą przez APC jest obsługa RFC1867 (File Upload Progress hook handler). Jest też pewna potwierdzona plotka mówiąca o włączeniu kodu APC do PHP\u0026rsquo;a 6. Teoretycznie jeżeli przesiądziemy się już teraz na APC to później powinno pójść łatwiej\u0026hellip;\nZalety:\ndość szybki, aktywnie rozwijany, bardzo stabilny, dostępna paczka w repozytoriach Debiana (i z tego co wiem na wielu innych systemach też przeważnie wystarczają domyślne repozytoria) obsługa RFC1867 (upload progress), zostanie włączony do PHP od wersji 5.4, APC udostępnia API umożliwiające tworzenie własnych obiektów w pamięci cache współdzielonych pomiędzy zapytaniami np. by nie pobierać za każdym razem właściwości profilu z bazy, listy użytkowników, itp (coś w stylu memcached). dostępny jest ze skryptem apc.php, który pozwala zarządzać obiektami w cache, czyścić itp. Wady:\npodobno bywa problematyczny w konfiguracji z fast-cgi (choć u mnie działa), przy mocno zapchanym cache\u0026rsquo;u czyszczenie go potrafiło się zwiesić. XCache Ostatni projekt rozwijany jest przez jednego z programistów lighttpd. W chwili obecnej wydaje się być dość dojrzałym i wystarczająco stabilnym do produkcyjnego użycia. Choć gdy próbowałem z niego korzystać jakiś rok/dwa temu to miałem sporo losowych padów - niezależnych od obciążenia serwera.\nZalety:\nstabilny, aktywnie rozwijany z kilkoma gałęziami (stable/unstable/devel) - możemy wybrać czy potrzebujemy funkcji czy stabilności. Wady:\nnieznacznie, ale jednak najniższa wydajność, miałem z nim mało styczności a szybko zraziłem się do kiepskiej stabilności - obecnie wydaje się że nie stanowi to problemu. Podsumowanie i mój wybór Do testu celowo wybrałem WordPress\u0026rsquo;a jako dość duży i wystarczająco skomplikowany projekt - jeśli on będzie działać stabilnie to większość mniejszych też powinna\u0026hellip; Ku mojemu zaskoczeniu żaden z optymalizatorów nie sypnął błędami. Dziwiło mnie to bo jeszcze jakiś czas temu eAccelerator czasami losowo mi się sypał - działał przez tydzień i nagle zgon w sobotę po południu\u0026hellip; Później próbowałem XCache i było podobnie\u0026hellip; tylko gorzej bo problemy występowały częściej. APC testowałem jako ostatnie ale w wykorzystywanych przeze mnie aplikacjach zachowywał się bardzo stabilnie i przewidywalni. Jedyny problem z wieszaniem się podczas czyszczenia/usuwania elementu z cache\u0026rsquo;u można obejść stosunkowo szybkim restartem Apachego - skuteczne i efekt ten sam \u0026#x1f603; Na jednym z serwerów testuję APC w trybie fast-cgi od około dwóch miesięcy i jak na razie nie mogę narzekać (może w wolnej chwili uzupełnię to zestawienie o testy w trybie fast-cgi).\nObecnie w większości administrowanych przeze mnie serwerów z PHP\u0026rsquo;em standardowo instaluję APC. Wybór jest dla mnie tym bardziej oczywisty że paczka ta jest dostępna w standardowych repozytoriach (łatwość aktualizacji itd) - nie ma zatem potrzeby jak w przypadku eAcceleratora instalowania wielu paczek z zależnościami by móc skompilować 1 moduł.\nDodatkową zaletą jest fakt że APC ma być standardowo wbudowany w kolejne wersje PHP\u0026rsquo;a - jeżeli w rozwijanych aplikacjach już teraz zwróci się uwagę na integrację z tym rozwiązaniem to w przyszłości migracja nie powinna przysporzyć problemów.\nJeżeli się wahasz - wybierz APC. Jeżeli w Twoim środowisku okaże się niestabilne zawsze możesz spróbować dwóch pozostałych rozwiązań.\nPrzydatne linki (część potwierdza moje obserwacje, są też testy z drupalem):\nhttp://php.net/manual/en/book.apc.php\u0026thinsp; external link http://xcache.lighttpd.net/\u0026thinsp; external link http://eaccelerator.net/\u0026thinsp; external link (w chwili pisanie strona nie działała \u0026#x1f603;)\nhttp://www.ducea.com/2006/10/30/php-accelerators/\u0026thinsp; external link http://2bits.com/articles/benchmarking-drupal-with-php-op-code-caches-apc-eaccelerator-and-xcache-compared.html\u0026thinsp; external link http://2bits.com/articles/benchmarking-apc-vs-eaccelerator-using-drupal.html\u0026thinsp; external link http://hostingfu.com/article/increasing-php-application-performance-xcache\u0026thinsp; external link ","permalink":"https://gagor.pro/2011/11/porownanie-optymalizatorow-php-eaccelerator-php-apc-xcache/","summary":"Przez pewien czas korzystałem z eAcceleratora do przyspieszenia działania stron pisanych w PHP\u0026rsquo;ie ale czasem bywał niestabilny. Aktualizacje pojawiały się rzadko a od czasu do czasu miewałem problemy ze stabilnością tej wtyczki na kilku bardziej skomplikowanych aplikacjach. Zdarzało się że pomimo zmiany kodu w skrypcie php, eAccelerator serwował wciąż stary plik - konieczny był restart Apache\u0026rsquo;go by wszystko działało jak trzeba.\nZacząłem szukać alternatywy i trafiłem na dwa moduły:\nAPC\u0026thinsp; external link (czyli Alternative PHP Cache), który ma być nawet domyślnie wbudowany w PHP od wersji 5.","title":"Porównanie optymalizatorów PHP - eAccelerator, PHP APC, XCache"},{"content":"Administrowałem do tej pory głównie darmowymi distro, ale gdzieś tam ukradkiem wkradło się kilka \u0026ldquo;siusiaków\u0026rdquo; (aka SUSE Linux Enterprise Server). Żyłem w utopijnym przekonaniu że skoro się za nie płaci to powinno się z nimi łatwiej współpracować\u0026hellip; w przypadku instalacji aktualizacji (a w szczególności SP) nie było to aż takie proste.\nPrzywykłem w darmowych dystrybucjach że gdy pojawiała się nowszą \u0026ldquo;większa wersja\u0026rdquo; to po prostu można było jednym poleceniem zaktualizować wszystkie pakiety. Źródła aktualizowały się automatycznie (lub prawie automatycznie) - później trzeba było połatać ewentualne zmiany w plikach konfiguracyjnych. W SUSE jest ciut inaczej\u0026hellip; 😉\nInstalacja SP1 na SLES\u0026rsquo;ie 11 Instrukcja jest dla SLES\u0026rsquo;a jedenastki (o ile pamiętam dziesiątkę aktualizowało się inaczej) i Service Pack\u0026rsquo;a 1 - ale powinna zadziałać również w przypadku każdego kolejnego SP. Zaczynamy!\nZ root\u0026rsquo;a uruchamiamy polecenia:\nzypper ref -s zypper up -t patch zypper up -t patch (Nie pomyliłem się - drugie polecenie należy uruchomić dwa razy - SIC!)\nPierwsze polecenie odświeży informacje o dostępnych usługach i repozytoriach.\nDrugie polecenie zainstaluje aktualizacje dla programów zarządzających paczkami w systemi, a kolejne wywołanie powinno zainstalować pozostałe dostępne aktualizacje. Podwójne wywołanie zypper up -t patch znajduje się w oficjalnej instrukcji - w nieoficjalnej znalezionej w sieci proponowano by uruchamiać to polecenie do puki nie będzie miało już nic więcej do zaktualizowania.\nPo wydaniu powyższych poleceń, w systemie (a dokładnie w plikach /etc/products.d/*.prod) powinny pojawić się informacje o dostępnych pakietach narzędzi migracyjnych. By je wylistować należy wydać polecenie:\ngrep \u0026#39;\u0026lt;product\u0026gt;\u0026#39; /etc/products.d/*.prod U mnie dało to taki wynik:\n\u0026lt;product\u0026gt;sle-sdk-SP1-migration\u0026lt;/product\u0026gt; \u0026lt;product\u0026gt;SUSE_SLES-SP1-migration\u0026lt;/product\u0026gt; Jeżeli u Ciebie to polecenie nic nie zwróciło tzn. że nie ma dostępnych aktualizacji lub że zbyt mało razy uruchomiono zypper up -t patch \u0026#x1f603;\nPowyższe \u0026ldquo;produkty\u0026rdquo; - należy zainstalować poleceniem:\nzypper in -t product sle-sdk-SP1-migration SUSE_SLES-SP1-migration Aby zaktualizować system musimy mieć dostęp do podstawowego repozytorium z nowszymi wersjami pakietów - uzyskamy go rejestrując się:\nsuse_register -d 2 -L /root/.suse_register.log Jeżeli nie zapomnieliśmy o przedłużeniu licencji i rejestracja przebiegła pomyślnie to możemy odświeżyć zawartość nowych repozytoriów i usług:\nzypper ref -s Teraz wylistujmy dostępne repozytoria, poleceniem:\nzypper lr BARDZO WAŻNE: musimy wyłączyć stare repozytoria (dla systemu bez SP) i włączyć nowe repozytoria dla systemu z SP1 - jeżeli tylko włączymy nowe repozytoria to po kolejnej aktualizacji systemu mogą zainstalować się paczki w starszych wersjach rozwalając system! Sprawdzałem osobiście i rzeczywiście tak jest \u0026#x1f603;\nWłączanie/wyłączanie repozytoriów umożliwiają polecenia:\nzypper mr -disable repozytorium_do_wylaczenia zypper mr -enable repozytorium_do_wlaczenia Dopiero teraz system jest gotowy do aktualizacji, którą przeprowadzamy poleceniem (jeżeli instalujemy zdalnie warto odpalić je spod screen‘a):\nzypper dup Zypper zapyta czy chcemy usunąć zainstalowane wcześniej \u0026ldquo;produkty migracyjne\u0026rdquo; i zaktualizować pozostałe pakiety - należy potwierdzić (oczywiście jeśli jesteśmy absolutnie pewni i mamy backup \u0026#x1f603; ).\nZNÓW WAŻNE: po zakończonej aktualizacji należy ponownie zarejestrować siusiaka aby usunąć repozytoria z aktualizacjami dla czystej wersji 11 i zastąpić je repami dla wersji z SP 1:\nsuse_register -d 2 -L /root/.suse_register.log Teraz możemy zrestartować system by przeładowało się jajko i wszystkie usługi - jeżeli wszystko poszło po naszej myśli to powita nas SLES 11 SP 1.\nPrawda że proste a cały proces wręcz oczywisty?\n","permalink":"https://gagor.pro/2011/10/sles-11-instalacja-service-packa/","summary":"Administrowałem do tej pory głównie darmowymi distro, ale gdzieś tam ukradkiem wkradło się kilka \u0026ldquo;siusiaków\u0026rdquo; (aka SUSE Linux Enterprise Server). Żyłem w utopijnym przekonaniu że skoro się za nie płaci to powinno się z nimi łatwiej współpracować\u0026hellip; w przypadku instalacji aktualizacji (a w szczególności SP) nie było to aż takie proste.\nPrzywykłem w darmowych dystrybucjach że gdy pojawiała się nowszą \u0026ldquo;większa wersja\u0026rdquo; to po prostu można było jednym poleceniem zaktualizować wszystkie pakiety.","title":"SLES 11 - instalacja Service Pack’a"},{"content":"Bardzo często konfigurując usługi dostępne publicznie poświęca się sporo czasu na maksymalne zwiększenie bezpieczeństwa przez \u0026ldquo;dopieszczanie\u0026rdquo; konfiguracji (certyfikaty z mocnym szyfrowaniem, ochronę pewnych stron hasłem, dostęp do SSH tylko kluczami, itd.) ale całkowicie pomija się przygotowanie systemu aktywnie monitorującego błędne próby autoryzacji. Oczywiście nie można umniejszać wagi pierwszego z wymienionych etapów ale zdecydowanie nie powinno pomijać się też tego drugiego. Przecież każdy admin chciałby wiedzieć gdy ktoś próbuje włamać się na jego serwer (FTP, HTTP, SSH, itp.) - tylko ilu z Nas zadaje sobie trud by uruchomić taki system?\nWielu z nas uważa że przygotowanie takiego mechanizmu jest zbyt pracochłonne, trudne, nie teraz, nie mam czasu, itd\u0026hellip;. Zapominamy przy tym że gotowy system zostanie oddany w ręce użyszkodników a Ci na pewno będą z niego korzystać wbrew wszelkim zasadom bezpieczeństwa \u0026#x1f603;\nNa przykład taki Roman z loginem romek ustawi hasło do poczty abc123 - wirusy i boty zamiast próbować łamać hasło jednego usera bardzo często próbują wbić się na popularne loginy wykorzystując proste hasła - zaskakujące jak często się im to udaje. A później Roman ma pretensje do admina że jego znajomi dostali po 10000 maili z wirusami\u0026hellip;\nTymczasem w wielu przypadkach wystarczy fail2ban w praktycznie podstawowej konfiguracji by ochronić typowe usługi przed:\natakami brute force jak w powyższym przykładnie (SSH, Apache, vsftpd, proftpd, Postfix, SASL Auth, etc), atakiem pocztowego bot netu (blokowanie hostów generujących dużą ilość błędów), złymi crawler\u0026rsquo;ami czy narzędziami skanującymi strony WWW, ataki flood na Bind DNS. Narzędzie napisane jest w Perl\u0026rsquo;u i działa jako demon zapisując dane z syslog\u0026rsquo;a w małych paczkach i okresowo uruchamiając testy sprawdzające - wpływa to korzystnie na wydajność (bo nie musi analizować całodniowych logów jak prymitywniejsze narzędzia). Modułowa konfiguracja pozwala łatwo dodać filtry (wykrywające niezdefiniowane w standardzie zdarzenia) lub akcje (np. zamiast wycinać spam bota przez iptables można go wrzucić do naszego prywatnego RBL\u0026rsquo;a). W innym poście podałem przykład dodania reguł dla dovecot\u0026rsquo;a .\nPomimo iż narzędzie wydaje się zbytnio nie obciążać systemu to raczej nie odważyłbym się go zainstalować na mocno obciążonym serwerze, gdzie generują się miliony linii logów dziennie. W takiej sytuacji potrzebne jest inne rozwiązanie.\nInstalacja W moim przypadku na Debianie leci to typowo:\napt-get install fail2ban Aplikację można też dość łatwo zainstalować ze źródeł - zalezności nie ma zbyt dużo.\nKonfiguracja Gdy już zainstalujemy fail2ban\u0026rsquo;a otwieramy główny plik konfiguracyjny /etc/fail2ban/jail.conf. Idąc od początku warto zainteresować się opcjami:\n# poniżej należy umieścić listę adresów IP, sieci CIDR, z których # chcemy ignorować zagrożenia ignoreip = 127.0.0.1 10.3.4.0/24 # domyślny czas BAN\u0026#39;a - można nadpisać w konfiguracji danej usługi bantime = 36000 # domyślna ilość wykrytych akcji, po których zostanie dojdzie # do BAN\u0026#39;a maxretry = 3 # adres osoby, która ma być informowana o nałożeniu/zdjęciu BAN\u0026#39;a destemail = moj@mail.pl # domyślna akcja, predefiniowane są 3 możliwości: # action_ - tylko BAN # action_mw - BAN i powiadomienie mailem z danymi WHOIS # action_mwl - jak wyżej plus linie z loga action = %(action_mwl)s Dalej w konfigu znajdują się przygotowane definicje poszczególnych usług i ich konfiguracja, np. SSH:\n[ssh] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 5 bantime = 3600 [ssh-ddos] enabled = true port = ssh filter = sshd-ddos logpath = /var/log/auth.log maxretry = 6 Jak wspominałem wcześniej w danym bloku można wpisać inne niż domyślne wartości dla opcji bantime i maxretry. Opcja filter wskazuje na nazwę pliku z definicją filtru, np: /etc/fail2ban/filter.d/sshd.conf. W tej lokalizacji możemy dodawać też własne filtry, trzeba tylko znać wyrażenia regularne. W opcji port możemy zmienić port, na którym działa usługa (by banowanie działało), korzystając przy tym z nazw dostępnych w pliku /etc/services lub bezpośrednio wpisując numer portu.\nDomyślnie nie jest włączona ochrona żadnej z usług. By ją włączyć należy zmienić w stosownym bloku:\nenabled = false na:\nenabled = true i przeładować fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart I na początek wystarczy. W kilku banalnych krokach uruchomiliśmy system monitorujący logi i blokujący próby włamań.\nOczywiście włączając ochronę danej usługi dobrze zapoznać się z regułami zapisanymi w danym filtrze - by nie być zaskoczonym gdy sypnie mailami 😉\nWyłączenie powiadamiania o starcie fail2ban\u0026rsquo;a Jeżeli włączymy powiadamianie mailem o banach jako gratis fail2ban będzie powiadamiać nas mailem o właczeniu i wyłączeniu ochrony dla każdej z usług - na wypadek gdyby ktoś bez naszej wiedzy zechciał go wyłączyć. Dla wielu może to być irytujące zachowanie.\nBy wyłączyć powiadomienia trzeba w pliku akcji - u mnie w: /etc/fail2ban/action.d/mail-whois-lines.conf wyczyścić opcje actionstart i actionstop by wyglądały jak poniżej:\nactionstop = actioncheck = ","permalink":"https://gagor.pro/2011/10/ochrona-uslug-przed-atakami-brute-force-z-fail2banem/","summary":"Bardzo często konfigurując usługi dostępne publicznie poświęca się sporo czasu na maksymalne zwiększenie bezpieczeństwa przez \u0026ldquo;dopieszczanie\u0026rdquo; konfiguracji (certyfikaty z mocnym szyfrowaniem, ochronę pewnych stron hasłem, dostęp do SSH tylko kluczami, itd.) ale całkowicie pomija się przygotowanie systemu aktywnie monitorującego błędne próby autoryzacji. Oczywiście nie można umniejszać wagi pierwszego z wymienionych etapów ale zdecydowanie nie powinno pomijać się też tego drugiego. Przecież każdy admin chciałby wiedzieć gdy ktoś próbuje włamać się na jego serwer (FTP, HTTP, SSH, itp.","title":"Ochrona usług przed atakami brute force z fail2ban’em"},{"content":"Jeżeli administrujesz nawet niedużym serwerem pocztowym na pewno masz świadomość, że nie jesteś w stanie monitorować logów na bieżąco. Ciężko jest wyłapać np. problem w komunikacji z pewną domeną. Ciężko też oszacować skalę ruchu na serwerze zarówno pod kątem ilości jak i wolumenu maili. Trudno wybrać domeny, dla których warto by zrezygnować z greylistingu, itd, itp\u0026hellip;\nNa szczęście dostępne jest narzędzie pflogsumm, które wygeneruje nam dość wyczerpujące statystyki z logów postfix\u0026rsquo;a. Bardzo przydatne przy codziennym przeglądzie \u0026ldquo;stanu zdrowia\u0026rdquo; serwera pocztowego.\nPrzykładowy wycinek statystyk z pewnego małego serwerka prezentuje się tak:\nPrzykładowy raport Postfix log summaries for Jul 4 Grand Totals ------------ messages 1158 received 1261 delivered 0 forwarded 5 deferred (50 deferrals) 2 bounced 392 rejected (23%) 0 reject warnings 0 held 0 discarded (0%) 164898k bytes received 242985k bytes delivered 201 senders 77 sending hosts/domains 354 recipients 51 recipient hosts/domains Per-Hour Traffic Summary time received delivered deferred bounced rejected -------------------------------------------------------------------- 0000-0100 26 28 2 0 7 0100-0200 14 18 3 0 10 0200-0300 4 4 1 0 8 0300-0400 6 6 1 0 8 0400-0500 4 4 0 0 8 0500-0600 2 2 1 0 9 0600-0700 8 8 1 0 9 0700-0800 16 18 1 0 10 0800-0900 58 60 1 0 8 0900-1000 104 110 5 0 17 1000-1100 132 152 2 0 18 1100-1200 106 106 1 0 31 1200-1300 64 70 2 0 9 1300-1400 112 132 2 0 14 1400-1500 98 106 1 0 78 1500-1600 86 88 2 0 32 1600-1700 56 56 3 0 23 1700-1800 58 77 5 2 19 1800-1900 36 36 3 0 16 1900-2000 26 26 2 0 24 2000-2100 48 50 3 0 9 2100-2200 32 42 2 0 10 2200-2300 34 34 3 0 10 2300-2400 28 28 3 0 5 ... Host/Domain Summary: Message Delivery sent cnt bytes defers avg dly max dly host/domain -------- ------- ------- ------- ------- ----------- 132 5688k 0 1.7 s 11.0 s gmail.com 104 2633k 0 4.6 s 2.8 m wp.pl 68 1525k 0 1.3 s 9.4 s interia.pl 42 744k 21 1.1 s 83.6 h o2.pl 30 89891 0 0.7 s 2.6 s op.pl 29 6677k 1 16.1 s 7.4 m poczta.onet.pl 26 540k 0 1.9 s 6.7 s poczta.fm ... Host/Domain Summary: Messages Received msg cnt bytes host/domain -------- ------- ----------- 50 4142k gmail.com 46 491259 facebookmail.com 38 1446k wp.pl 22 13520k interia.pl 14 675k o2.pl 10 105377 poczta.fm 10 57713 hotmail.com ... i dużo więcej... Instalacja na Debianie:\nInstalacja apt-get install pflogsumm Testowo polecenie można uruchomić w następujący sposób:\nPrzykładowe wywołanie sudo pflogsumm -i -d yesterday /var/log/mail.log /var/log/mail.log.1 W moim przypadku, logi przewijam codziennie ok 2:00 w nocy, dlatego podaję dwie ścieżki do plików log (bieżącego i wczorajszego) by mi te dwie godzinki nie umknęły 😉\nPowyższe polecenie wypisze na standardowe wyjście statystyki w postaci ładnie sformatowanych tekstowych tabel. Warto przyjrzeć się innym parametrom polecenia - można dzięki nim zrezygnować ze statystyk, które nas nie interesują, bądź zmienić domyślną kolejność.\nTeraz warto uruchomić okresowe raportowanie. Edytujemy crona:\nCrontab sudo crontab -e Na generowanie statystyk warto wybrać godzinę mniejszego obciążenia serwera (@daily oznacza północ), bo proces ich przygotowania dość mocno obciąży CPU. Wpisujemy polecenie wraz z interesującymi nas parametrami:\nPrzykład wywołania z Cron\u0026#39;a @daily /usr/sbin/pflogsumm -i --problems_first --no_bounce_detail \\ --no_deferral_detail -d yesterday \\ /var/log/mail.log /var/log/mail.log.1 | \\ mail -e -s \u0026#34;Statystyki poczty na `uname -n`\u0026#34; postmaster Kolejnego dnia powinniśmy otrzymać nasze statystyki.\nEnjoyed? ","permalink":"https://gagor.pro/2011/09/pflogsum-statystyki-poczty-dla-postfixa/","summary":"Jeżeli administrujesz nawet niedużym serwerem pocztowym na pewno masz świadomość, że nie jesteś w stanie monitorować logów na bieżąco. Ciężko jest wyłapać np. problem w komunikacji z pewną domeną. Ciężko też oszacować skalę ruchu na serwerze zarówno pod kątem ilości jak i wolumenu maili. Trudno wybrać domeny, dla których warto by zrezygnować z greylistingu, itd, itp\u0026hellip;\nNa szczęście dostępne jest narzędzie pflogsumm, które wygeneruje nam dość wyczerpujące statystyki z logów postfix\u0026rsquo;a.","title":"pflogsumm - statystyki poczty dla postfix’a"},{"content":"Miałem ostatnio dziwną przygodę: pewien serwer do backupu gdzie ląduje dużo małych plików i dodatkowo tworzonych jest sporo hardlinków zaliczył pada. Co prawda starałem się go grzecznie położyć z pomocą Magic SysRq ale ponieważ nie wiedziałem co było przyczyną awarii fsck wydawał się wskazany.\nPodczas próby uruchomienia fsck.ext4 na systemie plików o rozmiarze ok 14TB z kilkuset milionami plików po kilkudziesięciu sekundach otrzymywałem komunikat:\nBłąd podczas przydzielania struktury icount: Memory allocation failed\nGooglając dowiedziałem się że problem jest znany i występuje podczas sprawdzania bardzo dużych systemów plików z dużą ilością hardlinków. To akurat idealnie mój przypadek\u0026hellip; Przeważnie zdarza się to na systemach 32 bitowych gdy fsck próbuje zaalokować powyżej 2GB pamięci. Jest to górny limit możliwej do wykorzystania przez pojedynczy proces pamięci dla architektury 32 bitowej - nie można go przeskoczyć nawet stosując PAE. Ale mój system jest 64 bitowy, 8GB RAM\u0026rsquo;u, 8GB swap\u0026rsquo;a - nie jest dobrze jeśli brakuje pamięci ;-/\nZgodnie z sugestiami Teo Tso by zmniejszyć zapotrzebowanie fsck na pamięć można zmusić go by informacje o inodach i właściwościach katalogów przechowywał w tymczasowym katalogu. By wskazać katalog tymczasowy należy utworzyć plik: /etc/e2fsck.conf z zawartością:\n[scratch_files] directory = /var/cache/e2fsck Katalog trzeba utworzyć ręcznie:\nmkdir /var/cache/e2fsck Warto zadbać o kilka/kilkanaście gigabajtów wolnego miejsca w tym katalogu - a najlepiej na czas sprawdzania podmontować jakiś zasób z fizycznie innego dysku niż sprawdzany. Ja wykorzystałem w tym celu kilku gigabajtowego pen drive\u0026rsquo;a.\nTeraz można odpalić fsck\u0026rsquo;a (odpalając zdalnie warto zrobić to w screen\u0026rsquo;ie):\nfsck.ext4 -f -C0 /dev/md1 Parametr -f wymusi sprawdzenie, a -C0 wyświetli pasek postępu co przy długim sprawdzeniu da nam jakieś wyobrażenie postępu. Tak uruchomiony fsck działa zdecydowanie wolniej niż przy domyślnych ustawieniach ale przynajmniej powinien się wykonać.\n# fsck.ext4 -fD -C0 /dev/md1 e2fsck 1.41.12 (17-May-2010) Przebieg 1: Sprawdzanie i-węzłów, bloków i rozmiarów /dev/md1: |=========== | 19.4% Opis podobnego problemu ze wsparciem Teo Tso: http://www.linux-archive.org/ext3-users/103464-2gb-memory-limit-running-fsck-6tb-device.html\u0026thinsp; external link ","permalink":"https://gagor.pro/2011/09/fsck-ext4-blad-podczas-przydzielania-struktury-icount-memory-allocation-failed/","summary":"Miałem ostatnio dziwną przygodę: pewien serwer do backupu gdzie ląduje dużo małych plików i dodatkowo tworzonych jest sporo hardlinków zaliczył pada. Co prawda starałem się go grzecznie położyć z pomocą Magic SysRq ale ponieważ nie wiedziałem co było przyczyną awarii fsck wydawał się wskazany.\nPodczas próby uruchomienia fsck.ext4 na systemie plików o rozmiarze ok 14TB z kilkuset milionami plików po kilkudziesięciu sekundach otrzymywałem komunikat:\nBłąd podczas przydzielania struktury icount: Memory allocation failed","title":"fsck.ext4 - Błąd podczas przydzielania struktury icount: Memory allocation failed"},{"content":"Pomimo iż Linux uchodzi za stabilne środowisko to raz na jakiś czas trafi się ciężka zwiecha - z powodu przeciążenia, awarii sprzętu\u0026hellip; nieistotne\u0026hellip;\nZałóżmy że licho wzięło za cel główny serwer plików lub bazę danych dla wielu, wielu stron internetowych. Dostać się po ssh nie możemy bo lecą timeout\u0026rsquo;y, a siedząc bezpośrednio przy klawiaturze konsola nie reaguje. Mimo to coś ostro daje po dyskach, więc ewentualny twardy reset to na bank utrata części plików\u0026hellip; jeśli system po nim w ogóle wstanie\u0026hellip; \u0026#x1f611;\nJeśli powyższa historyjka wygląda znajomo to zdecydowanie warto czytać dalej.\nW wielu dystrybucjach kernel standardowo jest skompilowany z opcją CONFIG_MAGIC_SYSRQ - opcja opisana jest jako szczególnie przydatna dla developerów jądra ale i nam może się przysłużyć jako ostatnia deska ratunku przed twardym resetem.\nAby wywołać funkcję trzeba przytrzymać na klawiaturze: Lewy ALT + PrintScrn/SysRq i przycisk określający funkcję. Kilka z możliwych funkcji to:\nr - przełącz klawiaturę z trybu RAW do XLATE (w wolnym tłumaczeniu: odzyskaj obsługę klawiatury od X\u0026rsquo;ów), e - wyślij SIGTERM do wszystkich procesów z wyjątkiem init\u0026rsquo;a, i - wyślij SIGKILL do wszystkich procesów z wyjątkiem init\u0026rsquo;a, s - wywołaj sync dla wszystkich zamontowanych zasobów (czyli zapisz wszystkie niezapisane dotąd transakcje dyskowe), u - przemontuj wszystkie zamontowane zasoby to trybu tylko do odczytu, b - natychmiast uruchom ponownie system, bez odmontowania partycji i bez synchronizacji dysków. To tylko część funkcji, ale wykonanie ich w kolejności w jakiej zostały tu wypisane (czyli reisub) powinno skutkować STOSUNKOWO bezpiecznym resetem, po którym nawet nie powinno być potrzebne skanowanie dysków fsck\u0026rsquo;iem. Każda z funkcji potrzebuje kilka/kilkanaście sekund na wykonanie (szczególnie warto zaczekać by po sync\u0026rsquo;u przestała migać kontrolka aktywności dysków twardych) - nie warto się spieszyć. Po ostatnim wywołaniu system powinien się zrestartować - zdarzyło mi się tylko kilka razy by procedura ta zawiodła.\nMetodę można też wykorzystać dla serwerów zdalnych, które np. z jakiegoś powodu nie chcą się zrestartować w standardowy sposób. Można to zrobić wysyłając poszczególne polecenia do /proc/sysrq-trigger, np. tak:\necho b \u0026gt; /proc/sysrq-trigger Na zdalnych maszynach i VPS\u0026rsquo;ach wygodniejsze może być skonfigurowanie demona sysrqd.\nDla zainteresowanych, więcej funkcji Magic SysRq (głównie diagnostycznych) i dokładniejszy opis można znaleźć na stronie wiki\u0026thinsp; external link .\n","permalink":"https://gagor.pro/2011/09/magic-sysrq-bezpieczny-reset-linuxa/","summary":"Pomimo iż Linux uchodzi za stabilne środowisko to raz na jakiś czas trafi się ciężka zwiecha - z powodu przeciążenia, awarii sprzętu\u0026hellip; nieistotne\u0026hellip;\nZałóżmy że licho wzięło za cel główny serwer plików lub bazę danych dla wielu, wielu stron internetowych. Dostać się po ssh nie możemy bo lecą timeout\u0026rsquo;y, a siedząc bezpośrednio przy klawiaturze konsola nie reaguje. Mimo to coś ostro daje po dyskach, więc ewentualny twardy reset to na bank utrata części plików\u0026hellip; jeśli system po nim w ogóle wstanie\u0026hellip; \u0026#x1f611;","title":"Magic SysRq - bezpieczny reset Linux’a"},{"content":"Wielu administratorów gdy zaczyna swoją przygodę zarządza jedną/dwoma maszynami\u0026hellip; Po pewnym czasie jest ich już kilka\u0026hellip; W którymś momencie dostrzega się zalety wirtualizacji i na kilku maszynach fizycznych działa kilkanaście czy kilkadziesiąt maszyn wirtualnych. W takiej sytuacji pobieranie aktualizacji dla wszystkich maszyn potrafi mocno zabić łącze.\nI w tym momencie zaczynamy się zastanawiać czy może nie warto byłoby zrobić własnego mirror\u0026rsquo;a paczek dla naszego ulubionego distro\u0026hellip; do prywatnego użytku\u0026hellip; synchronizowanego w nocy by nikomu nie przeszkadzać\u0026hellip; i dostępnego nawet gdy będziemy offline\u0026hellip; Zaczynamy liczyć miejsce i okazuje się że repozytorium Debiana dla architektury i386 to prawie 60GB (sic!), no ale mamy kilka maszynek z arch amd64 i tutaj też prawie 60GB - auć. W tym miejscu wielu dochodzi do wniosku że to jeszcze nie pora na własnego mirror\u0026rsquo;a \u0026#x1f603;\nRozwiązaniem tego dylematu jest wykorzystanie approx\u0026rsquo;a, który działa jako cachujące proxy dla repozytoriów Debiania i Ubuntu. Pośredniczy on w pobieraniu plików zapisując kopię każdego w lokalnym cache\u0026rsquo;u. Czyli pierwsze pobranie zasysa paczkę z sieci, a kolejne odwołania do approx\u0026rsquo;a serwują już tę pobraną kopię. Ponadto przyjemne jest zachowanie approx\u0026rsquo;a, który samodzielnie utrzymuje \u0026ldquo;czystość\u0026rdquo; cachu usuwając starsze paczki i aktualizując nowe. Druga wielka zaleta tej aplikacji to lokalne składowanie tylko tych paczek które pobieramy - bo mało kto potrzebuje całego repozytorium - drastycznie zmniejsza to rozmiar takiego mirror\u0026rsquo;a. U mnie mirror ma raptem kilkaset megabajtów.\nInstalacja i konfiguracja Ponieważ paczka jest w domyślnych repozytoriach wystarczy:\napt-get install approx i po chwili mamy działające proxy.\nDomyślnie pliki składowane są w: /var/cache/approx - warto zadbać o odrobinę wolnego miejsca w tej lokalizacji.\nNależy jeszcze skonfigurować odpowiednie repozytoria w pliku konfiguracyjnym: /etc/approx/approx.conf - u mnie wygląda on tak:\ndebian http://ftp.pl.debian.org/debian security http://security.debian.org/debian-security volatile http://volatile.debian.org/debian-volatile backports http://backports.debian.org/debian-backports # The following are the default parameter values, so there is # no need to uncomment them unless you want a different value. # See approx.conf(5) for details. $interface any $port 8080 $max_wait 30 #$max_rate unlimited #$user approx #$group approx #$syslog daemon #$pdiffs true #$verbose false #$debug true Po instalacji approx działa na porcie 8080 o czym trzeba pamiętać przy podawaniu URL\u0026rsquo;i w sources.list - mi to odpowiadało bo na porcie 80-tym mam odpalony serwer WWW pod jakieś śmieci\u0026hellip; Ale Wam może odpowiadać uruchomienie approx\u0026rsquo;a na 80-tce.\nTeraz restartujemy approx\u0026rsquo;a by przeładować konfigurację:\ninvoke-rc.d approx restart Zostało skonfigurować plik /etc/apt/sources.list tak by wskazywał na nasze proxy. Należy to zrobić na każdej maszynce w naszej sieci by cała zabawa miała sens.\nW przypadku Lenny\u0026rsquo;ego powinien on wyglądać mniej więcej tak:\ndeb http://approx.costam.pl:8080/debian lenny main non-free contrib deb-src http://approx.costam.pl:8080/debian lenny main non-free contrib deb http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb-src http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb-src http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb http://approx.costam.pl:8080/backports lenny-backports-sloppy main W przypadku Squeeze\u0026rsquo;a tak:\ndeb http://approx.costam.pl:8080/debian squeeze main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze main non-free contrib deb http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb-src http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb http://approx.costam.pl:8080/backports squeeze-backports main No i na koniec sprawdzamy czy wszystko działa:\napt-get update Możemy spróbować coś zainstalować i zobaczyć jak się spisze proxy.\n","permalink":"https://gagor.pro/2011/09/approx-cachujace-proxy-dla-repozytoriow-debiana/","summary":"Wielu administratorów gdy zaczyna swoją przygodę zarządza jedną/dwoma maszynami\u0026hellip; Po pewnym czasie jest ich już kilka\u0026hellip; W którymś momencie dostrzega się zalety wirtualizacji i na kilku maszynach fizycznych działa kilkanaście czy kilkadziesiąt maszyn wirtualnych. W takiej sytuacji pobieranie aktualizacji dla wszystkich maszyn potrafi mocno zabić łącze.\nI w tym momencie zaczynamy się zastanawiać czy może nie warto byłoby zrobić własnego mirror\u0026rsquo;a paczek dla naszego ulubionego distro\u0026hellip; do prywatnego użytku\u0026hellip; synchronizowanego w nocy by nikomu nie przeszkadzać\u0026hellip; i dostępnego nawet gdy będziemy offline\u0026hellip; Zaczynamy liczyć miejsce i okazuje się że repozytorium Debiana dla architektury i386 to prawie 60GB (sic!","title":"approx - cachujące proxy dla repozytoriów Debiana"},{"content":"Linux bardzo agresywnie wykorzystuje wolną pamięć RAM do buforowania danych odczytywanych z dysków (inode\u0026rsquo;ów, plików, itd\u0026hellip;). Ma to niebagatelny wpływ na zwiększenie szybkości uruchamiania programów które już raz zostały uruchomione. Jednak nie zawsze jest to pożądane zachowanie, np. testując szybkość uruchomienia/wykonywania tworzonej przez nas aplikacji - buforowanie zmienia czas ładowania aplikacji przy kolejnych uruchomieniach. Dobrze byłoby móc wymusić zwolnienie buforów by każdy start programu miał porównywalne \u0026ldquo;warunki startowe\u0026rdquo;.\nNa szczęście można to zrobić w prosty sposób:\nsync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches Polecenie to zwolni niewykorzystywany cache pliku stronicowania, katalogów i inodów. Wcześniejsze uruchomienie sync pozwala zwolnić większą ilość buforów przez wymuszenie zapisania otwartych plików.\nInne możliwe warianty to:\nzwolnienie cache pliku stronicowania: sync \u0026amp;\u0026amp; echo 1 \u0026gt; /proc/sys/vm/drop_caches zwolnienie cache cache katalogów i inodów: sync \u0026amp;\u0026amp; echo 2 \u0026gt; /proc/sys/vm/drop_caches Opcja ta dostępna jest w jajkach od wersji 2.6.16.\n","permalink":"https://gagor.pro/2011/09/wymuszenie-zwolnienia-pamieci-buforow-dyskowych-na-linuxie/","summary":"Linux bardzo agresywnie wykorzystuje wolną pamięć RAM do buforowania danych odczytywanych z dysków (inode\u0026rsquo;ów, plików, itd\u0026hellip;). Ma to niebagatelny wpływ na zwiększenie szybkości uruchamiania programów które już raz zostały uruchomione. Jednak nie zawsze jest to pożądane zachowanie, np. testując szybkość uruchomienia/wykonywania tworzonej przez nas aplikacji - buforowanie zmienia czas ładowania aplikacji przy kolejnych uruchomieniach. Dobrze byłoby móc wymusić zwolnienie buforów by każdy start programu miał porównywalne \u0026ldquo;warunki startowe\u0026rdquo;.\nNa szczęście można to zrobić w prosty sposób:","title":"Wymuszenie zwolnienia pamięci buforów dyskowych na Linux’ie"},{"content":"Od jakiegoś czasu dostępny jest w sieci skrypt slowloris.pl\u0026thinsp; external link pozwalający z pojedynczego komputera wykonać atak DOS na zdalny serwer WWW. Atak polega na uruchomieniu wielu równoczesnych sesji i bardzo wolnym wysyłaniu komunikatów HTTP. Atakujący udaje \u0026ldquo;klienta z wolnym łączem\u0026rdquo; równocześnie uruchamiając kolejne sesje by po pewnym czasie zająć wszystkie dostępne. Serwer WWW przestaje wtedy odpowiadać na zapytania od innych klientów. Dodatkowo na źle wyskalowanych serwerach duża liczba procesów Apachego może spowodować swapowanie i błędy braku pamięci.\nW zależności od wydajności atakowanej maszyny by doprowadzić do jej zablokowania potrzeba przeważnie od kilkunastu do kilkudziesięciu sekund. Tak przeprowadzony atak DOS nie wymaga botnetu czy super wydajnego sprzętu - zwykły lapciak w zupełności wystarczy.\nNa chwilę obecną są już co najmniej dwie metody ochrony Apachego przed takim atakiem: mod_antiloris\u0026thinsp; external link i mod_reqtimeout\u0026thinsp; external link . Pierwszy dostępny jako moduł do ręcznej kompilacji dla starszych wersji Apache (np. w Debianie Lennym). Drugi dostępny jest w standardzie od wersji 2.2.15 (np. w Debianie Squeeze).\nmod_antiloris pozwala na limitowanie ilości równoczesnych sesji dla zdalnego klienta. W przypadku przekroczenia dozwolonej liczby zrywane jest połączenie. Ma to swoje wady, np. gdy z naszego serwera WWW korzysta jakaś duża NAT\u0026rsquo;owana sieć to przy większej liczbie połączeń zostaną zablokowani \u0026ldquo;dobrzy\u0026rdquo; klienci. Trudne jest też właściwe ustawienie maksymalnej liczby połączeń - domyślnie ustawiona jest wartość 5. Ale niektóre przeglądarki (lub wtyczki do nich) podnoszą limit równoczesnych połączeń per serwer do 8.\nmod_reqtimeout pozwala na określenie po jakim czasie przerwać połączenie w przypadku nie utrzymywania wystarczającego przepływu danych. Skracając - pozwala precyzyjnie wyciąć \u0026ldquo;wolnych\u0026rdquo; klientów. Moduł ten monitoruje każdą sesję z osobna przez co nie ma zagrożenia blokowania sieci NAT czy \u0026ldquo;agresywnie\u0026rdquo; ustawionych przeglądarek.\nInstalacja i konfiguracja mod_antiloris (Apache do wersji 2.2.14) Najpierw musimy pobrać potrzebne zależności:\napt-get install gcc apache2-prefork-dev Później pobieramy mod\u0026rsquo;a i rozpakowujemy:\nwget \u0026#34;ftp://ftp.monshouwer.eu/pub/linux/mod_antiloris/mod_antiloris-0.4.tar.bz2\u0026#34; tar xvf mod_antiloris-0.4.tar.bz2 cd mod_antiloris-0.4 Jeżeli mamy taką potrzebę możemy wyedytować plik mod_antiloris.c i podnieść limit połączeń:\n#define antiloris_MAX_PER_IP 5 Do kompilacji modułu wykorzystamy narzędzie apxs2, które skompiluje moduł jako dynamicznie ładowalny:\napxs2 -c mod_antiloris.c Teraz możemy skopiować moduł do katalogu z innymi modułami:\nsudo cp .libs/mod_antiloris.so /usr/lib/apache2/modules/mod_antiloris.so Musimy też utworzyć plik konfiguracyjny, który będzie ładować mod\u0026rsquo;a:\nsudo su -c \u0026#34;echo \u0026#39;LoadModule antiloris_module /usr/lib/apache2/modules/mod_antiloris.so\u0026#39; \u0026gt; /etc/apache2/mods-available/antiloris.load\u0026#34; Włączamy moduł:\nsudo a2enmod antiloris Na koniec musimy przeładować Apache\u0026rsquo;go:\nsudo service apache2 reload Instalacja i konfiguracja mod_reqtimeout (Apache od wersji 2.2.15) Ponieważ moduł jest dostępny wystarczy go uaktywnić i przeładować Apachego:\nsudo a2enmod reqtimeout sudo service apache2 reload Domyślnie w Squeeze dostępny jest plik konfiguracyjny z poniższymi wartościami:\nRequestReadTimeout header=20-40,minrate=500 RequestReadTimeout body=10,minrate=500 Pierwsza opcja oznacza: zezwalaj na wysyłanie zapytania przez co najmniej 20 sekund i zwiększaj limit do maksymalnie 40 sekund po 1 sekundzie za każde przesłane 500 bajtów.\nDruga opcja oznacza: zezwalaj na pobieranie przez co najmniej 10 sekund i zwiększaj limit czasu w nieskończoność po 1 sekundzie za każde pobrane 500 bajtów.\nDomyślne ustawienia są sensowne i powinny wystarczyć w większości przypadków. Dodatkowe dopieszczenie tych opcji może być potrzebne na serwerach wysyłających bądź odbierających dość duże pliki lub w przypadku \u0026ldquo;spersonalizowanych\u0026rdquo; ataków DOS.\nTest działania Skoro mamy już \u0026ldquo;tarczę\u0026rdquo; warto sprawdzić czy działa. W tym celu pobieramy slowlorisa:\nwget \u0026#34;http://ha.ckers.org/slowloris/slowloris.pl\u0026#34; chmod +x slowloris.pl I możemy uruchomić test:\nperl slowloris.pl -dns twojserwer.pl -port 80 -timeout 1 -num 300 -cache Jeżeli po dwóch minutach (w zależności od konfiguracji sprzętowej serwera) strona odpowiada i można się na nią bez problemów dostać to znaczy że nasz wysiłek się opłacił.\nDalszym krokami wartymi podjęcia może być analiza logów np. z użyciem fail2ban\u0026thinsp; external link i wycinanie na firewallu bardziej uciążliwych gości.\n","permalink":"https://gagor.pro/2011/09/zabezpieczenie-apachego-na-debianie-przed-slowlorisem/","summary":"Od jakiegoś czasu dostępny jest w sieci skrypt slowloris.pl\u0026thinsp; external link pozwalający z pojedynczego komputera wykonać atak DOS na zdalny serwer WWW. Atak polega na uruchomieniu wielu równoczesnych sesji i bardzo wolnym wysyłaniu komunikatów HTTP. Atakujący udaje \u0026ldquo;klienta z wolnym łączem\u0026rdquo; równocześnie uruchamiając kolejne sesje by po pewnym czasie zająć wszystkie dostępne. Serwer WWW przestaje wtedy odpowiadać na zapytania od innych klientów. Dodatkowo na źle wyskalowanych serwerach duża liczba procesów Apachego może spowodować swapowanie i błędy braku pamięci.","title":"Zabezpieczenie Apachego na Debianie przed slowloris’em"},{"content":"Na jednym z serwerów zauważyłem dziwny wzrost obciążenia. Tzw. LOAD od kilku dni po woli rósł. top pokazywał że dwa rdzenie CPU czekają na dane z dysku - tzw. io wait na poziomie 80~90% ale żaden proces w znaczącym stopniu nie obciążał CPU.\nJest kilka narzędzi (iostat, wmstat), które pozwalają monitorować obciążenie dysków ale ja nie szukałem informacji czy i w jakim stopniu dyski są obciążone - wiedziałem że są. Chciałem dowiedzieć się który proces generuje to obciążenie - by móc go ubić \u0026#x1f603;\nPrzydatny okazał się programik iotop - który działa jak top ale sortuje procesy w zależności od generowanego przez nie obciążenia dysków - właśnie tego szukałem:\nProgram jest w standardowych repozytoriach Debiana i można go zainstalować w ten sposób:\napt-get install iotop ","permalink":"https://gagor.pro/2011/09/sprawdzenie-ktory-proces-obciaza-dyski/","summary":"Na jednym z serwerów zauważyłem dziwny wzrost obciążenia. Tzw. LOAD od kilku dni po woli rósł. top pokazywał że dwa rdzenie CPU czekają na dane z dysku - tzw. io wait na poziomie 80~90% ale żaden proces w znaczącym stopniu nie obciążał CPU.\nJest kilka narzędzi (iostat, wmstat), które pozwalają monitorować obciążenie dysków ale ja nie szukałem informacji czy i w jakim stopniu dyski są obciążone - wiedziałem że są. Chciałem dowiedzieć się który proces generuje to obciążenie - by móc go ubić \u0026#x1f603;","title":"Sprawdzenie który proces obciąża dyski"},{"content":"Onego czasu próbowałem znaleźć coś co ułatwiłoby mi rysowanie prostych wykresów w PHP\u0026rsquo;ie inaczej niż z palca w GD. Kumpel polecił mi JPGraph.\nJPGraph to świetna sprawa, do generowania statystyk jak chociażby na mojej stronie, ale biblioteka potrafi dużo więcej\u0026hellip;\nZałóżmy, że ze stronki zbieramy do bazy takie rzeczy jak: datę, adres IP, ilość połączeń z tego adresu. Prosta tabela (przykład w Postgre SQL\u0026rsquo;u):\nCREATE TABLE wizyty ( pid serial NOT NULL, \u0026#34;data\u0026#34; date NOT NULL DEFAULT (\u0026#39;now\u0026#39;::text)::date, odslony integer NOT NULL DEFAULT 1, CONSTRAINT visits_pkey PRIMARY KEY (id) ); Dane z takiej tabeli można łatwo wyciągnąć jednym select\u0026rsquo;em:\nSELECT date_part(\u0026#39;day\u0026#39;, \u0026#34;data\u0026#34;) AS x, sum(odslony) AS y FROM wizyty WHERE date \u0026gt; (\u0026#39;today\u0026#39;::date - \u0026#39;30 days\u0026#39;::interval) GROUP BY \u0026#34;data\u0026#34; ORDER BY \u0026#34;data\u0026#34; LIMIT 30 Powyższe zapytanie wyciągnie nam z kolumny z datą tylko dzień miesiąca, który posłuży za etykietę (oś X) oraz sumę odsłon z danego dnia (oś Y), posortowane według daty i tylko z ostanich 30 dni.\nW zależności od sposobu dostępu i pobierania wyników, trzeba je ładnie zapisać w dwóch tablicach. Ja to robię w prostej pętli:\nforeach ($query-\u0026gt;result() as $row) { $x[] = $row-\u0026gt;x; $y[] = $row-\u0026gt;y; } Mając dane została zabawa z ustawieniem oczekiwanych opcji wykresu:\n/* bez tego ani rusz */ include_once \u0026#39;jpgraph/jpgraph.php\u0026#39;; include_once \u0026#39;jpgraph/jpgraph_bar.php\u0026#39;; /* szerokość */ $width = 400; /* wysokość */ $height = 300; $graph = new Graph($width, $height); /* oś X tekst, oś Y wartości całkowite */ $graph-\u0026gt;SetScale(\u0026#34;textint\u0026#34;); /* kolorowa ramka */ $graph-\u0026gt;SetFrame(true, \u0026#39;#222222\u0026#39;); $graph-\u0026gt;SetMarginColor(\u0026#39;#222222\u0026#39;); /* tytuł wykresu, trochę konfiguracji fontów i kolorów */ $graph-\u0026gt;title-\u0026gt;Set(\u0026#39;Sumaryczna liczba odsłon\u0026#39;); $graph-\u0026gt;title-\u0026gt;SetFont(FF_VERDANA,FS_BOLD,10); $graph-\u0026gt;title-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* podobnież jak powyżej dla osi X */ $graph-\u0026gt;xaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;xaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;yaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;yaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* dodatkowe ozdobniki ;) */ $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SupressZeroLabel(false); $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;xaxis-\u0026gt;SetTickLabels($x); /* genrujemy właściwy wykres, w tym przypadku słupkowy */ $bplot = new BarPlot($y); $bplot-\u0026gt;SetWidth(0.6); $bplot-\u0026gt;SetFillColor(\u0026#39;#D01A71@0.25\u0026#39;); $bplot-\u0026gt;SetColor(\u0026#39;gray\u0026#39;); /* dodajemy wykres do obiektu $graph (który to może pomieścić i wyświetlić wiele wykresów, co z resztą pokazane jest poniżej) */ $graph-\u0026gt;Add($bplot); /* teraz wykres liniowy */ $lplot = new LinePlot($y); /* z wypełnieniem pod wykresem */ $lplot-\u0026gt;SetFillColor(\u0026#39;skyblue@0.5\u0026#39;); $lplot-\u0026gt;SetColor(\u0026#39;navy@0.7\u0026#39;); $lplot-\u0026gt;SetBarCenter(); /* typ znacznika na łącznikach pomiędzy kolejnymi słupkami */ $lplot-\u0026gt;mark-\u0026gt;SetType(MARK_SQUARE); $lplot-\u0026gt;mark-\u0026gt;SetColor(\u0026#39;blue@0.5\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetFillColor(\u0026#39;lightblue\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetSize(6); /* wrzucamy kolejny wykres do obrazu */ $graph-\u0026gt;Add($lplot); /* a teraz magia... */ $graph-\u0026gt;Stroke(); Co to robi\u0026hellip; Ano dokładnie taki wykres jak ten poniżej:\nCo prawda ten wykres nie odpowiada w 100% podanemu źródłu. To statystyka z mojej strony, która niebieską linią zaznacza volumen pobranych danych - przykład jest nieco uproszczony.\nNa pierwszy rzut oka powyższy skrypt wydaje się dość długi i jest tam dużo nie do końca jasnych opcji, ale najfajniejsze jest to, że tak na prawdę nie trzeba się tego uczyć. Biblioteka domyślnie dostarczana jest z masą przykładów. Wystarczy wybrać wykres podobny do tego, który sami chcemy zrobić - skopiować większość kodu. Znaleźć inne wykresy, których cechy chcemy powielić i po kilku próbach skleimy takiego Frankenstein\u0026rsquo;a jakiego świat nie widział wcześniej ;-D\nMój wykres akurat ma trochę zmieniony kolor tła ramki i fonty, tak aby lepiej wkomponował się w moją stronę - reszta bazuje na dwóch przykładach z dokumentacji. Na zrobienie pierwszego wykresu z wykorzystaniem JPGraph potrzebowałem ok. godziny. Teraz wystarcza mi 10 minut \u0026#x1f603;\n","permalink":"https://gagor.pro/2011/08/jpgraph-wykresy-z-phpa/","summary":"Onego czasu próbowałem znaleźć coś co ułatwiłoby mi rysowanie prostych wykresów w PHP\u0026rsquo;ie inaczej niż z palca w GD. Kumpel polecił mi JPGraph.\nJPGraph to świetna sprawa, do generowania statystyk jak chociażby na mojej stronie, ale biblioteka potrafi dużo więcej\u0026hellip;\nZałóżmy, że ze stronki zbieramy do bazy takie rzeczy jak: datę, adres IP, ilość połączeń z tego adresu. Prosta tabela (przykład w Postgre SQL\u0026rsquo;u):\nCREATE TABLE wizyty ( pid serial NOT NULL, \u0026#34;data\u0026#34; date NOT NULL DEFAULT (\u0026#39;now\u0026#39;::text)::date, odslony integer NOT NULL DEFAULT 1, CONSTRAINT visits_pkey PRIMARY KEY (id) ); Dane z takiej tabeli można łatwo wyciągnąć jednym select\u0026rsquo;em:","title":"JPGraph, wykresy z PHP’a"},{"content":"Tak się składa, że Debian ze względu na stosunkowo rzadkie wydawanie kolejnych wersji szybko staje się niezbyt świeży a dostępne w nim pakiety często nie spełaniają naszych oczekiwań. Nie ma najnowszej wersji Subversion\u0026hellip; Nie ma mod_security itd, itp\u0026hellip;\nRozwiązaniem tego problemu może być instalacja pakietów z testowej gałęzi ale można polec na zależnościach. Można też kompilować ze źródeł\u0026hellip; Tak czy siak w obu przypadkach aktualizacja i utrzymanie tak zmodyfikowanego systemu byłoby jak wrzód na zadku.\nNa szczęście jest prostsze rozwiązanie. System backportów - czyli repozytorium dostarczające możliwie najnowsze wersje pakietów dla gałęzi stabilnej. Dodając jedno źródło można zainstalować subversion, mod_security i inne, a przy tym równocześnie nie rozwalić sobie systemu.\nKonfiguracja - Squeeze Najpierw trzeba dodać dodatkowe źródło pakietów:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ squeeze-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Konfiguracja - Lenny Najpierw trzeba dodać dodatkowe źródło pakietów:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ lenny-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Ponadto w Lennym dostępne jest drugie repozytorium backportów tzw. lenny-backports-sloppy - to repozytorium nie gwarantuje bezproblemowej aktualizacji do Squeeze\u0026rsquo;a ale powinny się tam znaleźć nowsze wersje pakietów niż w przypadku podstawowego repo.\nDodatkowym krokiem w przypadku Lennego, aczkolwiek zalecanym jest ustawienie tzw. pinningu dla backportów, aby pakiety zainstalowane z nich były aktualizowane. Robimy to wklepując:\necho \u0026#34;Package: *\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin: release a=lenny-backports\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin-Priority: 200\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences Odświeżanie repozytoriów Teraz odświeżamy repozytoria:\napt-get update Instalacja pakietów z backportów Instalacja pakietów z backportów wymaga wymuszenia ich użycia, dzięki czemu jedynie wybrane przez nas pakiety zostaną zainstalowane w nowszych wersjach. Robimy to przykładowo tak:\napt-get -t squeeze-backports install subversion lub\napt-get -t lenny-backports install subversion To tyle. Możemy korzystać z aktualnych wersji paczek.\n","permalink":"https://gagor.pro/2011/08/konfiguracja-backportow-na-debianie/","summary":"Tak się składa, że Debian ze względu na stosunkowo rzadkie wydawanie kolejnych wersji szybko staje się niezbyt świeży a dostępne w nim pakiety często nie spełaniają naszych oczekiwań. Nie ma najnowszej wersji Subversion\u0026hellip; Nie ma mod_security itd, itp\u0026hellip;\nRozwiązaniem tego problemu może być instalacja pakietów z testowej gałęzi ale można polec na zależnościach. Można też kompilować ze źródeł\u0026hellip; Tak czy siak w obu przypadkach aktualizacja i utrzymanie tak zmodyfikowanego systemu byłoby jak wrzód na zadku.","title":"Konfiguracja backportów na Debianie"},{"content":"Przy okazji wykonywania kilku drobnych optymalizacji swojej stronki natknąłem się na eAccelerator\u0026rsquo;a. Ciekawy projekt, który w sposobie działania przypomina Zend Optimizer\u0026rsquo;a ale ma jedną zasadniczą zaletę - jest darmowy \u0026#x1f603;\nNiestety nie ma go w repozytoriach Debiana, więc trzeba go sobie skompilować - cały proces jest dość prosty. Zaczynamy od pobrania najświeższej paczki, obecnie jest to wersja 0.9.5.3:\nPobierz eAccelerator\u0026thinsp; external link (ostatnio miałem problem z tym linkiem więc proponuję pogooglać)\nPobieramy i rozpakowujemy pliki:\ntar xvfj eaccelerator-0.9.5.3.tar.bz2 cd eaccelerator-0.9.5.3 Do kompilacji eAccelerator\u0026rsquo;a potrzebujemy paru paczek, które możemy zainstalować tak:\napt-get install build-essential php5-dev W katalogu ze źródłami klepiemy (prawie standardowo):\nphpize ./configure make make install No i eAccelerator jest już zainstalowany w naszym systemie. Pozostało wygenerowanie konfiguracji PHP aby moduł był automatycznie ładowany oraz ustawienie podstawowych parametrów konfiguracyjnych.\nW przypadku Debiana domyślna konfiguracja modułów PHP przechowywana jest w katalogu/etc/php5/conf.d tam też zapiszemy naszą konfigurację jako eaccelerator.ini. Tworzymy plik naszym ulubionym edytorem:\nvim /etc/php5/conf.d/eaccelerator.ini W pliku wpisujemy:\nextension=\u0026#34;eaccelerator.so\u0026#34; eaccelerator.shm_size=\u0026#34;128\u0026#34; eaccelerator.cache_dir=\u0026#34;/tmp/eaccelerator\u0026#34; eaccelerator.enable=\u0026#34;1\u0026#34; eaccelerator.optimizer=\u0026#34;1\u0026#34; eaccelerator.check_mtime=\u0026#34;1\u0026#34; eaccelerator.debug=\u0026#34;0\u0026#34; eaccelerator.filter=\u0026#34;\u0026#34; eaccelerator.shm_max=\u0026#34;0\u0026#34; eaccelerator.shm_ttl=\u0026#34;0\u0026#34; eaccelerator.shm_prune_period=\u0026#34;0\u0026#34; eaccelerator.shm_only=\u0026#34;0\u0026#34; eaccelerator.compress=\u0026#34;1\u0026#34; eaccelerator.compress_level=\u0026#34;9\u0026#34; extension - określa plik modułu, czasami może być konieczne podanie pełnej ścieżki,\neaccelerator.shm_size - określa ilość pamięci współdzielonej, którą eAccelerator rezerwuje do przechowywani skompilowanych skryptów. Ja dałem 128MB bo mój serwerek ma 2GB RAM-u, ale przypuszczam że dla mniej obciążonych maszyn wystarczy 16~32MB. Nadając temu parametrowi wartość 0 ufamy programistom 😉\neaccelerator.cache_dir - ustawiamy katalog, w którym będzie zapisywane to co nie będzie się już mieścić w pamięci współdzielonej. Ja mam akurat sporą osobną partycję na /tmp, ale równie dobrze nadaje się /var/cache/eaccelerator czy /var/tmp/eaccelerator - ustawiamy jak nam wygodnie,\nSkoro już piszę o katalogu na skrypty to warto go utworzyć i przypisać mu odpowiednie uprawnienia (ja ustawiam je tak aby tylko Apache miał dostęp):\nmkdir /tmp/eaccelerator chown -R www-data:www-data /tmp/eaccelerator chmod 0770 /tmp/eaccelerator No to jeszcze restart Apache\u0026rsquo;a i możemy sprawdzać jak działa:\ninvoke-rc.d apache2 restart Najszybszym sposobem sprawdzenia czy moduł się ładuje jest sprawdzenie wyniku poleceniaphp -v - powinno to wyglądać jak poniżej:\nphp -v Zend Engine v2.2.0, Copyright (c) 1998-2008 Zend Technologies with eAccelerator v0.9.5.3, Copyright (c) 2004-2006 eAccelerator, \\ by eAccelerator Podobny wynik można uzyskać z pomocą funkcji phpinfo() i prostego skryput PHP do jej wywołania.\nCzy warto? Na koniec trochę benchmarków aby sprawdzić czy cała zabawa jest warta świeczki \u0026#x1f603;\nWykorzystam standardowe narzędzie dostępne z Apache\u0026rsquo;m: ab (Apache benchmark):\nab -n 1000 -c 10 http://mojastrona.pl/ U mnie dało to następujące wyniki - z wyłączonym eAccelerator\u0026rsquo;em:\nConcurrency Level: 10 Time taken for tests: 8.648 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12836000 bytes HTML transferred: 12612000 bytes Requests per second: 115.64 [#/sec] (mean) Time per request: 86.477 [ms] (mean) Time per request: 8.648 [ms] (mean, across all concurrent requests) Transfer rate: 1449.54 [Kbytes/sec] received Oraz z włączonym:\nConcurrency Level: 10 Time taken for tests: 3.663 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12840344 bytes HTML transferred: 12616120 bytes Requests per second: 272.97 [#/sec] (mean) Time per request: 36.634 [ms] (mean) Time per request: 3.663 [ms] (mean, across all concurrent requests) Transfer rate: 3422.90 [Kbytes/sec] received Wynik jest całkiem niezły, ponad dwa razy szybciej. A w niektórych przypadkach udaje się wyciągnąć więcej (nawet 5~10 krotnie). Tak, czy siak - warto!\n","permalink":"https://gagor.pro/2011/08/optymalizacja-php-z-eacceleratorem/","summary":"Przy okazji wykonywania kilku drobnych optymalizacji swojej stronki natknąłem się na eAccelerator\u0026rsquo;a. Ciekawy projekt, który w sposobie działania przypomina Zend Optimizer\u0026rsquo;a ale ma jedną zasadniczą zaletę - jest darmowy \u0026#x1f603;\nNiestety nie ma go w repozytoriach Debiana, więc trzeba go sobie skompilować - cały proces jest dość prosty. Zaczynamy od pobrania najświeższej paczki, obecnie jest to wersja 0.9.5.3:\nPobierz eAccelerator\u0026thinsp; external link (ostatnio miałem problem z tym linkiem więc proponuję pogooglać)","title":"Optymalizacja PHP z eAccelerator’em"},{"content":"Co prawda na swojej stronie zrobiłem kilka podstawowych statystyk i coś tam sobie loguję do bazy danych, ale gdyby się chwilę zastanowić to przecież to samo robi serwer www - wrzuca do logów każde zapytanie HTTP, kod błędu, nazwę agenta, itd. Dublowanie tych danych nie jest najbardziej optymalne.\nStąd też chwilę pogooglałem i znalazłem świetny Open Source\u0026rsquo;owy projekt: AWStats, który jest webowym analizatorem logów dla serwerów HTTP, FTP i SMTP.\nInstalacja i konfiguracja Najpierw instalacja, na moim Debianie leci to tak:\nsudo apt-get install awstats Teraz trzeba się chwilę zastanowić nad konfiguracją serwera i celem, który chcemy osiągnąć:\nczy staty będą dostępne publicznie? czy tylko dla ograniczonego grona zainsteresowanych (np. w pewnej sieci)? a może zabezpieczenie hasłem? Wiedząc, że AWStats działają jako skrypt CGI wystawianie takiego serwisu \u0026ldquo;na świat\u0026rdquo; nie wydaje mi się bezpiecznym rozwiązaniem. Wolę np. skonfigurować serwis tak aby był dostępny tylko w LAN\u0026rsquo;ie, gdzie mam większą władzę i szybciej poradzę sobie z namierzeniem i zablokowaniem ewentualnego napastnika 😉\nOpiszę tylko dwa pierwsze przypadki (jak ktoś chce hasło to szybko znajdzie jak je ustawić) - pierwsza dla leniwych, druga dla ambitnych 😉\nPrzygotowania Bez względu na wybraną metodę konfiguracji (leniwą, bądź nie) do działania serwisu potrzebny jest włączony w Apache moduł CGI. Jest tak w domyślnej konfiguracji ale jeśli nie masz pewności to odpal:\na2enmod cgi Musi też być zdefiniowany katalog ze skryptami CGI z uaktywnioną interpretacją CGI - domyślnie w pliku /etc/apache2/sites-available w pliku default jest poniższa konfiguracja:\nScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; Na potrzeby metody leniwej jest to wystarczająca konfiguracja, dla ambitnych zalecam trochę inne umiejscowienie tego kodu.\nMetoda dla leniwych Ponieważ pakiet AWStats instaluje się przykładową konfiguracją można bardzo szybko uruchomić staty, wystarczy skopiować jeden plik:\ncp /usr/share/doc/awstats/examples/apache.conf /etc/apache2/conf.d/awstats No i tyle 😉\nMetoda dla ambitnych Ambitnym zalecam nieco inną konfigurację: z ograniczeniem dostępu do statystyk wyłącznie z LAN\u0026rsquo;u i tak samo z dostępem do skryptów CGI. W moim przypadku żaden z wystawianych przezemnie serwisów nie korzysta z CGI, więc udostępnianie tych skryptów wszystkim \u0026ldquo;zainteresowanym\u0026rdquo; nie ma sensu.\nProponuję wykorzystać taki lub podobny plik konfiguracyjny dla hosta serwującego statystyki:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName staty.domena.pl ServerAdmin webmaster@domena.pl DocumentRoot /var/www/stats/ \u0026lt;Directory /\u0026gt; Options FollowSymLinks AllowOverride None \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/www/stats/\u0026gt; Options -Indexes FollowSymLinks MultiViews AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/lib/awstats\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; Alias /awstats-icon/ /usr/share/awstats/icon/ \u0026lt;Directory /usr/share/awstats/icon\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ErrorLog /var/log/apache2/error.log LogLevel warn CustomLog /var/log/apache2/access.log combined \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On RewriteCond $1 !^$ RewriteCond %{REQUEST_URI} !.*cgi-bin RewriteCond %{REQUEST_URI} !.*awstats.pl RewriteRule /(.*)/? /cgi-bin/awstats.pl?config=$1 [PT] RewriteRule ^/awstats.pl(.*?) /cgi-bin/awstats.pl$1 [QSA,R,L] \u0026lt;/IfModule\u0026gt; \u0026lt;/VirtualHost\u0026gt; Z ważnych rzeczy do personalizacji:\nServerName - wpisz swoją nazwę serwisu, Allow from 192.168.1.0/24 - zamień na adres/adresy, które Tobie odpowiadają, mod_rewrite - ostatnich kilka linijek wykorzystuje mod_rewrite do uproszczenia odwołań do statystyk, wystarczy wtedy wpisać adres np. tak: http://staty.domena.pl/nazwa.domeny.ktora.nas.interesuje.pl\u0026thinsp; external link plik zapisujemy jako /etc/apache2/sites-available/awstats Teraz zostało nam uaktywnienie site\u0026rsquo;a:\na2ensite awstats Część wspólna konfiguracji Teraz tworzymy katalog na skrypt, który wypisze nam dostępne statystyki:\nmkdir /var/www/stats chown -R www-data:www-data /var/www/stats/ Właściwa konfiguracja AWStats Pliki konfiguracyjne AWStats znajdują się w katalogu /etc/awstats. Jest ich dokładnie 2 szt.:\nawstats.conf awstats.conf.local Plik awstats.conf zawiera przykładową konfigurację wystarczającą do odpalenia statystyk dla pojedynczego hosta. Z kolei plik awstats.conf.local jest miejscem gdzie można wrzucić wspólną konfigurację dla kilku plików hostów.\nJeżeli mamy wiele hostów (a taki przypadek tutaj omawiam) to wygodniej będzie nam wrzucić cały plik awstats.conf do awstats.conf.local i w kolejnych plikach konfiguracyjnych zmieniać tylko parametry rozróżniające poszczególne hosty. Robimy więc tak:\nmv /etc/awstats/awstats.conf.local /etc/awstats/awstats.conf.local.orig mv /etc/awstats/awstats.conf /etc/awstats/awstats.conf.local Teraz musimy zmienić kilka linijek w pliku awstats.conf.local:\n# musimy odszukać i zakomentować poniższe linie LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; SiteDomain=\u0026#34;\u0026#34; HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # w ten sposób #LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; #SiteDomain=\u0026#34;\u0026#34; #HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; #Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # dodatkowo odszukujemy linię LogFormat=4 # i zamieniamy na LogFormat=1 Teraz możemy utworzyć pliki konfiguracyjne dla naszych vhostów raptem w kilku linijkach, np.:\nLogFile=\u0026#34;/var/log/apache2/access.log\u0026#34; SiteDomain=\u0026#34;domena.pl\u0026#34; HostAliases=\u0026#34;www.domena.pl\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; Oczywiście trzeba wpisać własną lokalizację pliku access.log. W powyższym przypadku jest to lokalizacja domyślna, wspólna dla wszystkich vhostów - rozróżnienie ruchu do poszczególnych vhostów następuje dzięki podaniu parametrów SiteDomain (podstawowej domeny danej strony) oraz HostAliases (innych domen wskazujących na tego samego vhosta).\nOstatnim elementem jest załadowanie pliku ze wspólną konfiguracją.\nZmiana uprawnień do logów Aby umożliwić dostęp AWStats do logów serwera Apache musimy wykonać dwie czynności. Na początek zmiana atrybutu dla aktualnego pliku log:\nchmod o+r /var/log/apache2/access.log Później musimy zadbać aby logi po rotacji przez logrotate również zachowywały atrybuty, oraz aby przed rotacją AWStats wygenerowało statystyki, których nie zebrało wcześniej. W tym celu zmieniamy plik /etc/logrotate.d/apache2 tak by wyglądał jak poniżej:\n/var/log/apache2/*.log { weekly missingok rotate 52 compress delaycompress notifempty create 644 root adm sharedscripts prerotate /usr/share/doc/awstats/examples/awstats-update endscript postrotate if [ -f /var/run/apache2.pid ]; then /etc/init.d/apache2 restart \u0026gt; /dev/null fi endscript } Przeładowanie konfiguracji Apache Po tych wszystkich zmianach w konfiguracji musimy zrestartować Apache:\ninvoke-rc.d apache2 restart Testy konfiguracji Aby sprawdzić konfigurację AWStats spróbujemy wejść na stronę:http://staty.domena.pl/cgi-bin/awstats.pl?config=domena.pl\nJeżeli na żadnym z etapów nie popełniliśmy błędu to naszym oczom powinny ukazać się \u0026ldquo;wspaniałe i upragnione statystyki\u0026rdquo; \u0026#x1f603;\nJedyną delikatną wadą awstats jest \u0026ldquo;brzydki\u0026rdquo; i długi link z cgi w środku\u0026hellip; Nieco mnie to irytowało, więc przysiadłem chwilę przy mod_rewrite i przygotowałem regułki (były podane w konfiguracji dla ambitnych), które pozwalają rozpocząć przeglądanie statystyk z uproszczonego linku postaci:\nhttp://staty.domena.pl/domena.pl\u0026thinsp; external link Proste, czyste i klarowne, bez zbędnych śmieci.\nCo prawda koniec tutora, ale nie koniec samej konfiguracji - proponuję aby przejrzeć przykładowy plik z konfiguracją i zapoznać się z zawartymi tam opcjami.\n","permalink":"https://gagor.pro/2011/08/statystyki-odwiedzin-dla-wielu-serwisow-z-awstats/","summary":"Co prawda na swojej stronie zrobiłem kilka podstawowych statystyk i coś tam sobie loguję do bazy danych, ale gdyby się chwilę zastanowić to przecież to samo robi serwer www - wrzuca do logów każde zapytanie HTTP, kod błędu, nazwę agenta, itd. Dublowanie tych danych nie jest najbardziej optymalne.\nStąd też chwilę pogooglałem i znalazłem świetny Open Source\u0026rsquo;owy projekt: AWStats, który jest webowym analizatorem logów dla serwerów HTTP, FTP i SMTP.","title":"Statystyki odwiedzin dla wielu serwisów z AWStats"},{"content":"Certyfikaty oparte o SSL stanowią obecnie podstawę bezpieczeństwa wielu usług sieciowych zaczynając od HTTP, przez POPS, IMAPS, itd\u0026hellip; Niestety zakupienie certyfikatu w organizacjach jak VeriSgin czy Thawte jest dość kosztowe, a jeżeli potrzebujemy kilka certyfikatów to często na lokalne potrzeby jest to po prostu nie opłacalne.\nPostaram się przedstawić wersję \u0026ldquo;ekonomiczną\u0026rdquo; certyfikacji \u0026#x1f603;\nGenerowanie Certificate Signing Request Pierwszym etapem generowania certyfikatu jest przygotowanie Certificate Signing Request, czyli czegoś w rodzaju \u0026ldquo;prośby\u0026rdquo; o certyfikat. Nasza \u0026ldquo;prośba\u0026rdquo; po podpisaniu przez centrum autoryzacyjne stanie się certyfikatem.\nDo wygenerowania Request\u0026rsquo;u potrzebny jest najpierw klucz prywatny, który generujemy np. tak:\nopenssl genrsa -aes256 -out priv.key 4096 Powyższe polecenie poprosi nas dwa razy o hasło, które trzeba zapamiętać (albo zapisać i zamknąć w sejfie) - zalecam wykorzystanie pseudolosowego ciągu znaków o długości minimum 10 znaków.\nPo genrsa możemy użyć kilku opcji wybierając w ten sposób algorytm szyfrujący - do wyboru są: des, des3, aes128, aes192, aes256. Ja wybrałem 256-bitowego AES\u0026rsquo;a - najmocniejszy z tych algorytmów.\nOstatni parametr do długość klucza. Można użyć innej wartości np. 1024. Klucz wygeneruje się szybciej ale będzie prostszy do złamania. Z moich doświadczeń wynika, że niektóre aplikacje mogą mieć problem z obsługą długiego klucza (np. jak użyty w tym przykładzie), wtedy użycie mniejszej wartości może być konieczne (lepsze słabsze zabezpieczenie niż żadne).\nWarto ograniczyć uprawnienia do wygenerowanego klucza prywatnego tak by tylko właściciel miał do niego dostęp:\nchmod 400 priv.key Przy okazji tworzenia klucza prywatnego warto wspomnieć, że jeżeli będziemy chcieli go użyć w tak przygotowanej postaci np. w Apache\u0026rsquo;m to przy każdym starcie Apache będzie nas prosić o podanie hasła odbezpieczającego klucz prywatny. Niby to bezpieczne ale z drugiej strony przez takie zabezpieczenie Apache nie podniesie się samodzielnie np. po awarii. Rozwiązaniem jest przygotowanie odszyfrowanej wersji klucza prywatnego, z której będzie korzystał Apache. Robi się to tak:\nopenssl rsa -in priv.key -out priv.unsecure.key chmod 400 priv.unsecure.key Klucz priv.unsecure.key podany w konfiguracji programu nie będzie wymagał hasła. Koniecznie należy uniemożliwić dostęp do tego pliku wszystkim z wyjątkiem root\u0026rsquo;a!\nopenssl req -new -key priv.key -out request.csr Co dalej? Ok. Mamy już request\u0026rsquo;a, warto w tym miejscu wspomnieć co możemy z nim zrobić:\nMożemy taki plik przesłać do centrum autoryzacji (CA) celem podpisania i wygenerowania certyfiaktu. Klucze publiczne CA takich jak Thawte, VeriSign, itp są dołączane do przeglądarek internetowych, OS\u0026rsquo;ów, etc. dzięki temu strony internetowe (bądź inne usługi) zabezpieczone takimi certyfikatami weryfikują się bez żadnej dodatkowej akcji ze strony użytkownika. Warto wziąć pod uwagę to rozwiązanie, jeżeli np. chcemy zabezpieczyć sklep internetowy - my odpuścimy to rozwiązanie ze względu na koszty \u0026#x1f603; Drugą opcją jest możliwość samodzielnego podpisania certyfikatu tworząc tzw. SelfSigned Certificate. To zalecana opcja jeżeli mamy tylko jedną stronę/usługę. Użytkownik raz doda sobie nasz certyfikat jako zaufany i będzie mógł korzystać z szyfrowania do woli. Metoda ta robi się problematyczna gdy trzeba zarządzać większą liczbą certyfikatów. Ostatnia opcja to utworzenie własnego CA (czyli specjalnego certyfikatu), którym będziemy podpisywać wygenerowane przez nas Requesty. Ma to tę zaletę, że wystarczy rozdystrybuować klucz publiczny CA i będzie to wystarczające do weryfikacji wszystkich naszych certyfikatów. Tworzenie certyfikatów SelfSigned Zaczniemy od prostszej wersji czyli podpiszemy nasz request wygenerowanym przez nas wcześniej kluczem prywatnym tworzac tzw. SelfSigned Certificate. Aby to zrobić potrzebne jest takie polecenie:\nopenssl x509 -req -days 365 -in request.csr \\ -signkey priv.key -out certificate.crt Tak oto wygenerowaliśmy certyfikat certificate.crt ważny przez 365 dni (tutaj dowolność ustawień ale 1 rok to sensowny okres).\nWłaściwości certyfikatu można sprawdzić np. tak:\nopenssl x509 -noout -text -in certificate.crt Tworzenie własnego CA Najpierw należy utworzyć klucz prywatny naszego CA, robi się to dokładnie tak samo jak w przypadku generowanego wcześniej klucza prywatnego dla serwera. Później musimy wygenerować requesta (analogicznie jak w przypadku certyfikatu SelfSigned). Czyli dwa polecenia:\nopenssl genrsa -aes256 -out ca.key 4096 openssl req -new -x509 -days 365 \\ -key ca.key -out ca.crt Skoro mamy już certyfikat naszego CA, możemy podpisać wcześniej wygenerowany Request dla serwera:\nopenssl x509 -req -days 365 -in request.csr -CA ca.crt \\ -CAkey ca.key -set_serial 01 -out certificate.crt Ogólnie podpisywanie przebiega podobnie jak przy certyfikatach SelfSigned ale jak widać wykorzystywane są klucze naszego CA i pojawia się nowy parametr: set_serial, którego wartość musi być inna dla każdego podpisanego przez to CA certyfikatu. Najprościej aby serial przyjmował numer kolejno wygenerowanego certyfikatu.\nJeżeli wiemy, że będziemy generować więcej tego typu certyfikatów warto przygotować sobie skrypt, który zadba o prawidłową wartość pola serial.\nPodsumowanie Certyfikaty przygotowane w ten sposób można wrzucić do Apache, postfix\u0026rsquo;a, itd\u0026hellip; i w ten sposób szyfrując ruch.\n","permalink":"https://gagor.pro/2011/08/certyfikaty-selfsigned/","summary":"Certyfikaty oparte o SSL stanowią obecnie podstawę bezpieczeństwa wielu usług sieciowych zaczynając od HTTP, przez POPS, IMAPS, itd\u0026hellip; Niestety zakupienie certyfikatu w organizacjach jak VeriSgin czy Thawte jest dość kosztowe, a jeżeli potrzebujemy kilka certyfikatów to często na lokalne potrzeby jest to po prostu nie opłacalne.\nPostaram się przedstawić wersję \u0026ldquo;ekonomiczną\u0026rdquo; certyfikacji \u0026#x1f603;\nGenerowanie Certificate Signing Request Pierwszym etapem generowania certyfikatu jest przygotowanie Certificate Signing Request, czyli czegoś w rodzaju \u0026ldquo;prośby\u0026rdquo; o certyfikat.","title":"Certyfikaty SelfSigned"},{"content":"Mój serwer pocztowy działa od jakiegoś czasu na dynamicznym IP (dobre bo tanie\u0026hellip;) i przeważnie nie ma z tym problemów. Postarałem się jak mogłem ustawiając SPF\u0026rsquo;a i DomainKeys aby uwiarygodnić go u większych dostawców poczty.\nNiestety wszystko to diabli biorą w momencie gdy wygasa mi leasse DHCP i dostaję nowe IP po jakimś spamerze/zombiaku. Wisi takie w 2-3 większych RBL\u0026rsquo;ach i o dostarczaniu poczty można zapomnieć. Miło gdy jeszcze zdalny MTA zechce odesłać zwrotkę \u0026ldquo;zróbta coś bo wisisz w RBL\u0026rsquo;u takim a takim\u0026hellip;\u0026rdquo;, ale zdecydowania niefajnie gdy wysyłasz pocztę a ona od razu leci do /dev/null rblcheck Poszperałem trochę i znalazłem fajne narzędzie aka rblcheck, które sprawdza domyślnie kilka RBL\u0026rsquo;i. Można też dodać kolejne jako parametry. Wygląda to mniej więcej tak:\n$ rblcheck 89.76.116.114 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net Jak widać IP wisi w jednym z RBL\u0026rsquo;i.\nMożna odpytać RBL\u0026rsquo;e o konkretny powód znalezienia się na liście (o ile funkcja taka jest obsługiwana):\n$ rblcheck 89.76.116.114 -t 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net: \\ Dynamic IP Addresses See: http://www.sorbs.net/lookup.shtml?89.76.116.114 W tym przypadku nie było się czym przejmować. Ta lista zawiera wszystkie IP dynamiczne, więc nic dziwnego że nasze dynamiczne też tam jest. Nawet jeżeli ktoś będzie korzystać z tej listy to raczej nie na zasadzie odcinania się od dynamicznych IP, ale w ramach punktowania wiarygodności danego adresu. Z naszej strony za bardzo nie jesteśmy w stanie nic z tym zrobić (bo nie zamierzamy dopłacić za statyczne IP), więc tak musi być.\nAutomatyzacja Najpierw zainstalowałem rblcheck‘a:\nsudo apt-get install rblcheck Później w /usr/local/sbin/check_rbls.sh utworzyłem skrypt sprawdzający kilka interesujących mnie RBL\u0026rsquo;i:\n#!/bin/bash IP=`host mojadomena.pl | head -n1 | awk \u0026#39;{print $4}\u0026#39;` RBLCHECK=\u0026#34;rblcheck -t -s dul.dnsbl.sorbs.net \\ -s abuse.rfc-ignorant.org \\ -s postmaster.rfc-ignorant.org \\ -s dsn.rfc-ignorant.org \\ -s ix.dnsbl.manitu.net \\ -s rhsbl.ahbl.org\u0026#34; $RBLCHECK $IP | awk \u0026#39;{if($2 != \u0026#34;not\u0026#34;) print $0 }\u0026#39; Skrypt ten podlinkowałem aby uruchamiał się co godzinę:\nln -s /usr/local/sbin/check_rbls.sh /etc/cron.hourly/ Dzięki temu prostemu skryptowie jeżeli dostanę IP wiszące w tych kilku RBL\u0026rsquo;ach to stosunkowo szybko (maksymalnie w ciągu godziny) się o tym dowiem. Jeżeli natomiast IP będzie czyste to nie będę dostawać zbędnych maili. Do tego listę sprawdzanych RBL\u0026rsquo;i można bardzo łatwo powiększyć o kolejne w razie takiej potrzeby.\n","permalink":"https://gagor.pro/2011/08/dynamiczne-ip-i-rble/","summary":"Mój serwer pocztowy działa od jakiegoś czasu na dynamicznym IP (dobre bo tanie\u0026hellip;) i przeważnie nie ma z tym problemów. Postarałem się jak mogłem ustawiając SPF\u0026rsquo;a i DomainKeys aby uwiarygodnić go u większych dostawców poczty.\nNiestety wszystko to diabli biorą w momencie gdy wygasa mi leasse DHCP i dostaję nowe IP po jakimś spamerze/zombiaku. Wisi takie w 2-3 większych RBL\u0026rsquo;ach i o dostarczaniu poczty można zapomnieć. Miło gdy jeszcze zdalny MTA zechce odesłać zwrotkę \u0026ldquo;zróbta coś bo wisisz w RBL\u0026rsquo;u takim a takim\u0026hellip;\u0026rdquo;, ale zdecydowania niefajnie gdy wysyłasz pocztę a ona od razu leci do /dev/null rblcheck Poszperałem trochę i znalazłem fajne narzędzie aka rblcheck, które sprawdza domyślnie kilka RBL\u0026rsquo;i.","title":"Dynamiczne IP i RBL’e"},{"content":"Klastrowanie to może zbyt dumnie powiedziane. Rozwiązanie to wyszukałem gdy chcąc skonfigurować dwa serwery apache do współpracy na rzecz jednego serwisu okazało się, że sejse trzymane są tylko przez jeden serwer a drugi nic o nich nie wie. To oczywiście nie pozwalało na prawidłowe działanie jakiegokolwiek serwisu korzystającego z sesji.\nPomysł jest taki, że zastępujemy domyśny mechanizm przechowywania sesji w plikach na dysku mechanizmem memcache. Ponieważ memcached działa jako usługa sieciowa, różne serwery mogą się odwoływać do puli memcached i odczytywać zapisane w niej dane. W przypadku sesji - nie jest ważne, kto ją utworzył - bo po jej wysłaniu do puli memcached staje się dostępna dla wszystkich klientów php z niej korzystających.\nJednym z pierwszych pytań nasuwających się do takiej kofiguracji jest: a co jeśli serwer memcached padnie? W chwili gdy wiele serwerów apache zależy od jednego serwera memcached jego awaria unieruchamia kaskadowo wszystkie.\nDlatego wykorzystałem konfigurację z dwoma serwerami memcached. Gdy php zapisuje dane w puli memcached dane są wysyłane do wszystkich podanych serwerów (w tym przypadku dwóch). A odczytywanie polega na odpytaniu pierwszego podanego serwera, a jeśli to się nie uda to drugiego. Układ nie jest idealny (jak mamy dwa działające serwery to aż się prosi o loadbalancing) ale zmniejsza prawdopodobieństwo, że awaria pojedynczego elementu położy wszystko.\nZ moim problemem moża było sobie poradzić też inaczej, np. zmieniając mechanizm sesji na stronie na taki, który korzysta z bazy danych. O ile w przypadku jednego serwisu nie jest to duży kłopot, to już przy kilku/kilkunastu byłoby to już spore przedsięwzięcie.\nWażną zaletą tego rozwiązanie jest fakt, że nie są wymagane żadne zmiany w istniejących serwisach. Po zmianie mechanizmu w konfiguracji php wszystko powinno działać bez zmian.\nInstalacja/Konfiguracja Najpierw trzeba zainstalować i uruchomić memcached oraz rozszerzenie php5-memcachedo php\u0026rsquo;a, które da nam możliwość korzystania z niego. U mnie robi się to tak:\napt-get install memcached php5-memcache Teraz trzeba by wyedytować konfigurację /etc/memcached.conf. Poniżej wycinek z tego pliku z opcjami, które należy ustawić:\n# Maksymalna wartość pamięci w MB jaka może być # wykorzystana przez demona. # Warto dostosować do swoich potrzeb. -m 128 # Interfejs, na którym nasłuchiwać będzie usługa. # Ja dla wygody wybiorę wszystkie :) -l 0.0.0.0 # można też dostosować port do nasłuchiwania -p 11211 # użytkownika nieuprzywilejowanego # (memcached domyślnie startuje jako root) -u nobody Wprowadzamy i zapisujemy zmiany. Trzeba zrestartować serwer memcached uruchamiając:\ninvoke-rc.d memcached restart Do tego momentu musimy powtórzyć konfigurację na drugiej maszynie (bo inaczej po co klastrować sesje).\nTeraz trzeba skonfiguraować php\u0026rsquo;a aby zamiast używać sesji zapisywanych do plików, korzystał z memcached. Edytujemy php.ini, u mnie akurat w lokalizacji:/etc/php5/apache2/php.ini. Odszukujemy opcje:\nsession.save_handler = files ;session.save_path = /var/lib/php5 I zamieniamy na:\nsession.save_handler = memcache ; adresy oczywiście należy dostosować do własnych ustawień session.save_path = \u0026#34;tcp://localhost:11211, tcp://remotehost:11211\u0026#34; W kolejnym kroku edytujemy plik konfiguracyjny rozszerzenia php\u0026rsquo;a dla memcache w /etc/php5/conf.d/memcache.ini dodając takie ustawienia:\nextension=memcache.so [memcache] memcache.dbpath=\u0026#34;/var/lib/memcache\u0026#34; memcache.maxreclevel=0 memcache.maxfiles=0 memcache.archivememlim=0 memcache.maxfilesize=0 memcache.maxratio=0 ; to jedyna wymagana opcja - resztę można dostosować pod siebie ; albo zostawić domyślnie memcache.allow_failover=1 memcache.max_failover_attempts=20 memcache.default_port=11211 memcache.chunk_size=8192 memcache.hash_strategy=standard memcache.hash_function=\u0026#34;crc32\u0026#34; Więcej po poszczególnych opcjach można się dowiedzieć z dokumentacji php\u0026thinsp; external link .\nTest Jeżeli zrobiliśmy wszystko jak trzeba to sesje powinny zapisywać się z pamięci memcachedi dystrybuować na wszystkie wpisane w polu save_path serwery. Możemy to sprawdzić wykorzystując np. taki skrypt:\n\u0026lt;?php session_start(); print \u0026#34;Opcja save_handler: \u0026#34; . ini_get(\u0026#34;session.save_handler\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; print \u0026#34;Opcja save_path: \u0026#34; . ini_get(\u0026#34;session.save_path\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; if(isset($_SESSION[\u0026#39;testowa\u0026#39;])) { print \u0026#34;Testowa sesja jest już ustawiona: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } else { $_SESSION[\u0026#39;testowa\u0026#39;] = \u0026#34;i wygląda, że działa dobrze\u0026#34;; print \u0026#34;Ustawiamy testową sesją wartością: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } ?\u0026gt; Wystarczy odświeżyć skrypt kilka razy. Za pierwszym razem sesja zostanie ustawiona a kolejne odświeżenia będą już zwracały jej wartość.\nProponuję też wyłączyć jeden z serwerów memcached aby sprawdzić czy php poprawnie odwoła się do drugiego serwera.\n","permalink":"https://gagor.pro/2011/08/klastrowanie-sesji-php-z-memcached/","summary":"Klastrowanie to może zbyt dumnie powiedziane. Rozwiązanie to wyszukałem gdy chcąc skonfigurować dwa serwery apache do współpracy na rzecz jednego serwisu okazało się, że sejse trzymane są tylko przez jeden serwer a drugi nic o nich nie wie. To oczywiście nie pozwalało na prawidłowe działanie jakiegokolwiek serwisu korzystającego z sesji.\nPomysł jest taki, że zastępujemy domyśny mechanizm przechowywania sesji w plikach na dysku mechanizmem memcache. Ponieważ memcached działa jako usługa sieciowa, różne serwery mogą się odwoływać do puli memcached i odczytywać zapisane w niej dane.","title":"Klastrowanie sesji PHP z memcached"},{"content":"Instalacja serwera MySQL na Debianie jest niezwykle prosta i sprowadza się do jednego polecenia:\nsudo apt-get install mysql-server Polecenie to zainstaluje i uruchomi usługę serwerową MySQL. W czasie instalacji będziemy proszeni o podanie hasła dla root\u0026rsquo;a (które oczywiście dobrze jest zapamiętać bądź zapisać).\nTak zainstalowana baza nasłuchuje na lokalnym porcie (localhost:3306) umożliwiająć dostęp wyłącznie root\u0026rsquo;owi. Jest to bardzo bezpieczna konfiguracja\u0026hellip; Ale jeśli nie mamy zamiaru na tej samej maszynie instalować oprogramowania zarządzającego to nie zawsze jest to wygodne, tym bardziej gdy przykładowo mamy działającego phpmyadmin\u0026rsquo;a na jakimś serwerze www. W takim przypadku pierwszą rzeczą, którą robię jest udostępnienie dostępu zdalnego dla root\u0026rsquo;a. Warto zaznaczyć że uprawnienia root\u0026rsquo;a można nadać dowolnemu użytkownikowi (np. romanowi) co jest dużo bezpieczniejszą konfiguracją niż działanie bezpośrednio na koncie root\u0026rsquo;a (którego nazwa jest powszechnie znana).\nDostęp zdalny dla root\u0026rsquo;a Aby umożliwić zdalne zalogowanie się do bazy z uprawnieniami root\u0026rsquo;a trzeba ustawić odpowiednie GRANT\u0026rsquo;y, robimy to tak:\nmysql -u root -p mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;roman\u0026#39;@\u0026#39;%\u0026#39; \u0026gt; IDENTIFIED BY \u0026#39;haslo dla zdalnego roota\u0026#39; WITH GRANT OPTION; mysql\u0026gt; FLUSH PRIVILEGES; mysql\u0026gt; exit Pierwsze polecenie połączy nas z bazą prosząc o hasło podane w czasie instalacji.\nKolejne to polecenia SQL\u0026rsquo;owe, które pozwalają użytkownikowi ‘roman\u0026rsquo; łączącemu się z hosta ‘%\u0026rsquo; (dowolnego) identyfikującemu się hasłem ‘haslo dla zdalnego roota\u0026rsquo;. Jeżeli chcemy ograniczyć dostęp do tylko jednego zdalnego adresu to zamiast znaku procenta wpisujemy ten adres IP. Tak dodany użytkownik ma też prawo nadawania uprawnień (GRANT OPTION). Polecenie FLUSH PRIVILEGES przeładowuje uprawnienia - umożliwiając logowanie z podanymi wcześniej uprawnieniami.\nPozostało nam zmienić ustawienia serwera tak aby nasłuchiwał nie tylko na localhoście. W tym celu edytujemy plik /etc/mysql/my.cnf:\nsudo vim /etc/mysql/my.cnf Odszukujemy następującą linię:\nbind-address = 127.0.0.1 Linię tę możemy zakomentować co będzie skutkować nasłuchiwaniem przez serwer na wszystkich skonfigurowanych adresach IP (taki sam efekt da wpisanie w polu adresu 0.0.0.0). Można też wpisać tylko jeden adres IP w przypadku gdy na serwerze jest ich kilka i nie chcemy aby serwer był dostępny na wszystkich.\nOstatnim krokiem jest zrestartowania serwera MySQL aby zadziałały wprowadzone w pliku konfiguracyjnym zmiany. Można to zrobić tak:\ninvoke-rc.d mysql restart Jeżeli właśnie założyłeś i udostępniłeś nowy serwer bazodanowy MySQL to oszczędź sobie pracy w przyszłości i od razu ustaw przechowywanie tabel InnoDB w osobnych plikach .\n","permalink":"https://gagor.pro/2011/08/mysql-na-szybko/","summary":"Instalacja serwera MySQL na Debianie jest niezwykle prosta i sprowadza się do jednego polecenia:\nsudo apt-get install mysql-server Polecenie to zainstaluje i uruchomi usługę serwerową MySQL. W czasie instalacji będziemy proszeni o podanie hasła dla root\u0026rsquo;a (które oczywiście dobrze jest zapamiętać bądź zapisać).\nTak zainstalowana baza nasłuchuje na lokalnym porcie (localhost:3306) umożliwiająć dostęp wyłącznie root\u0026rsquo;owi. Jest to bardzo bezpieczna konfiguracja\u0026hellip; Ale jeśli nie mamy zamiaru na tej samej maszynie instalować oprogramowania zarządzającego to nie zawsze jest to wygodne, tym bardziej gdy przykładowo mamy działającego phpmyadmin\u0026rsquo;a na jakimś serwerze www.","title":"MySQL - dostęp zdalny na szybko"},{"content":"Większość systemów plików w linuksie pozwala na ustawienie quoty na dwóch poziomach: na użytkownika lub na grupę użytkowników. W wielu przypadkach taki podział jest sensowny i wystarczający. Ale zdarzają się scenariusze, w których to za mało.\nDobrym przykładem jest serwer FTP z wirtualnymi kontami użytkowników. Czyli usługa serwera działa jako pewien nieuprzywilejowany użytkownik systemowy (przeważnie ftp) przypisany do nieuprzywilejowanej grupy (np. nogroup). Konta użytkowników serwera FTP są zdefiniowane w bazie danych lub serwerze LDAP. Takie konta nazywa się wirtualnymi ponieważ po autoryzacji w pewnym systemie (bazie danych, LDAP\u0026rsquo;ie) działają z uprawnieniami pewnego systemowego konta (w tym przypadku ftp) - nie ma więc odzwierciedlenia pomiędzy użytkownikami korzystającymi z ftp a kontami systemowymi.\nProblemem w takim układzie jest to, że nie można rozróżnić poszczególnych użytkowników wirtualnych, ponieważ wszystkie operacje są wykonywane przez systemowego użytkownika ftp i to on jest właścicielem wszystich plików utworzonych przez użytkowników wirtualnych.\nRozwiązania są dwa:\nsewer FTP ma zaimplementowaną obsługę quot dla użytkowników wirtualnych, z czego możemy skorzystać, tworzymy quotę na katalog domowy użytkownika wirtualnego. Mnie bardziej odpowiadała druga metoda.\nO ile mi wiadomo w ext3 nie można tworzyć quoty per katalog, na moje szczęscie taką możliwość oferuje xfs, z którego korzystałem.\nJeżeli jeszcze nie masz dysku sformatowanego jako xfs to jest to dobry moment aby to zrobić:\n# sdaX zastap nazwa urzadzenia ktore TY chcesz sformatowac mkfs.xfs /dev/sdaX # aby dzialala quoata per katalog trzeba zamonowac # dysk z opcja prjquota mount /dev/sdaX /mnt/ftp -o prjquota Logika quot per katalog w xfs\u0026rsquo;ie jest taka, że najpierw tworzymy projekt z powiązanym z nim id, a później wiążemy poprzez id projekt z katalogiem. Przez co quota nałożona na projekt ma zastosowanie do katalogu. Konfiguracja projektów i przypisanych im katalogów znajduje się w dwóch plikach: /etc/projid i /etc/projects.\nUtwórzymy teraz projekt i konfigurację dla niego:\n# nazwa projektu i jego id - w naszym przypadku # konto uzytkownika romana :) echo ftproman:10 \u0026gt;\u0026gt; /etc/projid # projektowi z id 10 przypisujemy katalog /mnt/ftp/roman echo 10:/mnt/ftp/roman \u0026gt;\u0026gt; /etc/projects Pozostało uruchomić quotę wpisująć komendy:\n# wiazemy projekt z punktem montowania xfs_quota -x -c \u0026#39;poject -s ftproman\u0026#39; /mnt/ftp # ustawiamy wlasiwa quote 1 gigabajt dla projektu xfs_quota -x -c \u0026#39;limit -p bhard=1g ftproman\u0026#39; /mnt/ftp To tyle - quota na katalog jest założona i działa. Aby zobaczyć jaka część miejsca jest już wykorzystana wystarczy uruchomić polecenie:\nxfs_quota -x -c \u0026#39;report -h /mnt/ftp\u0026#39; Project quota on /mnt/ftp (/dev/sda3) Blocks Project ID Used Soft Hard Warn/Grace ---------- --------------------------------- ftproman 1,5M 0 1G 00 [------] Dla kolejnych użytkowników musimy tworzyć kolejne wpisy.\n","permalink":"https://gagor.pro/2011/08/quota-na-katalog-w-xfsie/","summary":"Większość systemów plików w linuksie pozwala na ustawienie quoty na dwóch poziomach: na użytkownika lub na grupę użytkowników. W wielu przypadkach taki podział jest sensowny i wystarczający. Ale zdarzają się scenariusze, w których to za mało.\nDobrym przykładem jest serwer FTP z wirtualnymi kontami użytkowników. Czyli usługa serwera działa jako pewien nieuprzywilejowany użytkownik systemowy (przeważnie ftp) przypisany do nieuprzywilejowanej grupy (np. nogroup). Konta użytkowników serwera FTP są zdefiniowane w bazie danych lub serwerze LDAP.","title":"Quota na katalog w XFS’ie"},{"content":"Pewnego popołudnia przeczytałem artykuł o tęczowych tabelach i pod jego wpływem zacząłem kombinować jak zrobić coś takiego ale swojego.\nDo bazy wrzuciłem cały słownik z aspell\u0026rsquo;a dla języka polskiego (słowa z ogonkami i bez), angielskiego, kilka słowników z popularnymi hasłami, długą listę haseł z yourock i kilka innych. Sporo się nakombinowałem aby wygenerować dużo kombinacji szczególnie pod kątem polskich haseł (wszystkie: misiaczki, dupeczki, itp\u0026hellip;) razem z różnymi popularnymi modyfikacjami (typu: misiaczki1, DUPECZKI, etc\u0026hellip;). Na tym etapie miałem już wystarczająco dużo haseł aby bawić się dalej.\nZacząłem kombinować z różnymi długościami łańcuchów - niestety funkcja redukująca zmniejszała nieco entropię kolejnych hashy w łańcuchu, co dla długich łańcuchów mocno zmniejszało wydajność przeszukiwania bazy. Ostatecznie postawiłem na bardzo krótkie łańcuchy (a nuż się coś tafi a przynajmniej nie będzie mocno opuźniać wyszukiwań).\nObecnie baza ma ponad 200 milionów haseł i dla nich wygenerowane łańcuchy o długości 10 ogniw. Co daje teoretycznie możliwość sprawdzenia ok 2 miliardów hashy. Wydaje się to sporo ale w stosunku do innych baz dostępnych w sieci jest to kropla w morzu. A co do innych baz\u0026hellip; pomyślałem, że skoro ktoś już zrobił taką bazę dobrze to można by to wykorzystać. I tak dorzuciłem funkcję \u0026ldquo;obsysania\u0026rdquo; innych bazy hashy w przypadku gdy moja danego hasha nie potrafi rozpoznać. Taki zabieg (którego implementacja zajęła mi 1 popołudnie) wdrożony dla kilku różnych wyszukiwarek znacznie poprawił efektywność rozpoznawania haseł.\nBazę postanowiłem udostępnić w postaci strony www - jak na razie ze stosunkowo liberalnymi zasadami korzystania (czyli bez ograniczeń na liczbę wyszukiwań, konieczności rejestracji itp).\nZ bazy można korzystać pod poniższym linkiem: RainbowDB Wyłączyłem tą stronkę po dobrych dwóch/trzech latach działania - ruch jest przekierowany na nowy projekt Hybrid Rainbow DB , w którym oprócz tęczowych tablic zaimplementowałem hybrydową tęczową tablicę - nowa aplikacja sprawdza o wiele więcej haseł.\n","permalink":"https://gagor.pro/2011/08/rainbowdb/","summary":"Pewnego popołudnia przeczytałem artykuł o tęczowych tabelach i pod jego wpływem zacząłem kombinować jak zrobić coś takiego ale swojego.\nDo bazy wrzuciłem cały słownik z aspell\u0026rsquo;a dla języka polskiego (słowa z ogonkami i bez), angielskiego, kilka słowników z popularnymi hasłami, długą listę haseł z yourock i kilka innych. Sporo się nakombinowałem aby wygenerować dużo kombinacji szczególnie pod kątem polskich haseł (wszystkie: misiaczki, dupeczki, itp\u0026hellip;) razem z różnymi popularnymi modyfikacjami (typu: misiaczki1, DUPECZKI, etc\u0026hellip;).","title":"RainbowDB"},{"content":"Jeżeli tu zaglądasz pewnie zdarzyło Ci się kiedyś, że przykładowo wygrzebujesz jakiś stary serwer i nie masz pojęcia co na nim było, ani do czego służyło, czy jeszcze działa\u0026hellip; Albo jeszcze inaczej - serwer działał tak długo, że wszystkie osoby znające hasło na root\u0026rsquo;a przeszły na emeryturę lub zmarły\u0026hellip; Nieistotne \u0026#x1f603;\nJest pewna prosta sztuczka, pozwalająca wbić się na konto root\u0026rsquo;a nie znając hasła - dając nam możliwość jego zmiany. Potrzebne dwa restarty ale za to nie trzeba korzystać z żadnychlive cd.\nNa początek zmuszamy serwer do restartu - mieć nadzieję, że maszyna obsługuje ACPI i delikatne wciśnięcie przycisku power subtelnie ją wyłączy. Jeśli to nie zadziała to kojarzą mi się tylko brzydkie rzeczy \u0026#x1f603; Gdy po restarcie załaduje się grub na domyślnej opcji bootowania wybieramy edycję wciskając \u0026ldquo;e\u0026rdquo;. Wybieramy linię zaczynającą się od kernel i znów wybieramy edycję wciskając \u0026ldquo;e\u0026rdquo;. Jeżeli znajduje się tam parametr ro to zastępujemy go rw i dopisujemy na końcu init=/bin/bash Wbijamy \u0026ldquo;enter\u0026rdquo; zapisując zmieniony wiersz. Bootujemy się z tak zmienionej konfiguracji wciskając \u0026ldquo;b\u0026rdquo;. Po chwili system zamiast wystartować init\u0026rsquo;a i uruchamiać usługi, ląduje w bash\u0026rsquo;u z uprawnieniami root\u0026rsquo;a. A skoro mamy root\u0026rsquo;a to możemy wpisać passwd i zmienić rootowi hasło \u0026#x1f603; Teraz już tylko reboot i startujemy system normalnie - hasło root\u0026rsquo;a powinno działać. Niestety ta prosta sztuczka nie działa na wszystkich linux\u0026rsquo;ach - szczególnie tych wykorzystujących initramfs-tools. Na tych systemach trzeba ciut więcej pokombinować ale przynajmniej ma się jakiś punkt wyjścia.\n","permalink":"https://gagor.pro/2011/08/wlam-na-lokalne-konto-roota/","summary":"Jeżeli tu zaglądasz pewnie zdarzyło Ci się kiedyś, że przykładowo wygrzebujesz jakiś stary serwer i nie masz pojęcia co na nim było, ani do czego służyło, czy jeszcze działa\u0026hellip; Albo jeszcze inaczej - serwer działał tak długo, że wszystkie osoby znające hasło na root\u0026rsquo;a przeszły na emeryturę lub zmarły\u0026hellip; Nieistotne \u0026#x1f603;\nJest pewna prosta sztuczka, pozwalająca wbić się na konto root\u0026rsquo;a nie znając hasła - dając nam możliwość jego zmiany. Potrzebne dwa restarty ale za to nie trzeba korzystać z żadnychlive cd.","title":"Włam na lokalne konto root’a"},{"content":"Kiedyś potrzebowałem w ramach testu obciążeniowego wysłać dużo wiadomości z załącznikami. Chciałem to zrobić na szybko z shell\u0026rsquo;a i tutaj chwilę musiałem pogooglać aby znaleźć działające polecenie. To co znalazłem wygląda tak:\n(echo \u0026#34;testowa wiadomosc\u0026#34;; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test\u0026#34; testowy@mail.pl Wiedząc już jak wysyłać maile z załącznikami, mały mail bombing mogłem zrobić tak:\nfor i in `seq 1 100`; do (cat tekst.txt; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test $i\u0026#34; testowy@mail.pl; done ","permalink":"https://gagor.pro/2011/08/wysylanie-zalacznikow-poleceniem-mail/","summary":"Kiedyś potrzebowałem w ramach testu obciążeniowego wysłać dużo wiadomości z załącznikami. Chciałem to zrobić na szybko z shell\u0026rsquo;a i tutaj chwilę musiałem pogooglać aby znaleźć działające polecenie. To co znalazłem wygląda tak:\n(echo \u0026#34;testowa wiadomosc\u0026#34;; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test\u0026#34; testowy@mail.pl Wiedząc już jak wysyłać maile z załącznikami, mały mail bombing mogłem zrobić tak:\nfor i in `seq 1 100`; do (cat tekst.txt; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test $i\u0026#34; testowy@mail.","title":"Wysyłanie załączników poleceniem mail"},{"content":"My name is Tomasz Gągor.\nI\u0026rsquo;m a passionate day-to-day DevOps practitioner. I enjoy working with anything Linux-driven, especially containerized web services. I\u0026rsquo;m a security enthusiast who loves managing and scaling cloud systems.\nI\u0026rsquo;m a generalist who has delved into many diverse topics throughout my career. I\u0026rsquo;ve discussed the advantages of one DB engine over another and how to tune them for better performance with DBAs. With network specialists, I\u0026rsquo;ve even debugged IKE protocol using raw packets. Somehow (I\u0026rsquo;m not exactly sure how), I\u0026rsquo;ve gained a reputation as a Java performance tuning specialist!\nWhile I\u0026rsquo;m not a regular developer, I\u0026rsquo;ve written applications in Python, Ruby, Groovy, and Perl. I particularly enjoy using Makefiles for simple day-to-day automation. I\u0026rsquo;m addicted to the shell, which is one of the main reasons I consider myself incompatible with Windows. I automate extensively and have a voodoo-magic-script for almost any task.\nAs I step into leadership roles, I\u0026rsquo;ve begun to focus more on the human aspects of IT organizations. Many problems can be solved without simply adding more technology. Building clean processes can often reduce the number of annoyances or support requests much more effectively than purchasing another million-dollar product.\nI often act as a catalyst, encouraging those who may be hesitant or lack confidence to embrace change and reap its benefits. While we can\u0026rsquo;t control everything, let\u0026rsquo;s focus on what we can impact.\nAll pages on my site are licensed under CC BY-SA 4.0.\n","permalink":"https://gagor.pro/about/","summary":"My name is Tomasz Gągor.\nI\u0026rsquo;m a passionate day-to-day DevOps practitioner. I enjoy working with anything Linux-driven, especially containerized web services. I\u0026rsquo;m a security enthusiast who loves managing and scaling cloud systems.\nI\u0026rsquo;m a generalist who has delved into many diverse topics throughout my career. I\u0026rsquo;ve discussed the advantages of one DB engine over another and how to tune them for better performance with DBAs. With network specialists, I\u0026rsquo;ve even debugged IKE protocol using raw packets.","title":"Few words about me"},{"content":" Rzecz o istocie informatykiAlgorytmika\nAuthors: David Harel, Yishai Feldman\nempik.com ","permalink":"https://gagor.pro/books/2010/algorytmika/","summary":" Rzecz o istocie informatykiAlgorytmika\nAuthors: David Harel, Yishai Feldman\nempik.com ","title":"Rzecz o istocie informatyki"},{"content":" Więcej perełek oprogramowaniaWyznania programisty\nAuthor: Jon Bentley\ngandalf.com.pl ","permalink":"https://gagor.pro/books/2010/wiecej-perelek-oprogramowania/","summary":" Więcej perełek oprogramowaniaWyznania programisty\nAuthor: Jon Bentley\ngandalf.com.pl ","title":"Więcej perełek oprogramowania"},{"content":" Jeszcze krótsza historia czasuAuthor: Stephen Hawking\namazon.pl ","permalink":"https://gagor.pro/books/2009/krotka-historia-czasu/","summary":" Jeszcze krótsza historia czasuAuthor: Stephen Hawking\namazon.pl ","title":"Jeszcze krótsza historia czasu"},{"content":" Perełki oprogramowaniaSeria Klasyka informatyki\nAuthor: Jon Bentley\nhelion.pl ","permalink":"https://gagor.pro/books/2009/perelki-oprogramowania/","summary":" Perełki oprogramowaniaSeria Klasyka informatyki\nAuthor: Jon Bentley\nhelion.pl ","title":"Perełki oprogramowania"},{"content":" Zarządzanie czasemStrategie dla administratorów systemów\nAuthor: Thomas Limoncelli\nhelion.pl ","permalink":"https://gagor.pro/books/2008/zarzadzanie-czasem/","summary":" Zarządzanie czasemStrategie dla administratorów systemów\nAuthor: Thomas Limoncelli\nhelion.pl ","title":"Zarządzanie czasem"},{"content":" GłębiaAuthor: Lincoln Child\namazon.comempik.com ","permalink":"https://gagor.pro/books/2008/glebia/","summary":" GłębiaAuthor: Lincoln Child\namazon.comempik.com ","title":"Głębia"},{"content":" Bezpieczeństwo sieciNarzędzia\nAuthors: Nitesh Dhanjani, Justin Clarke\nhelion.pl ","permalink":"https://gagor.pro/books/2007/bezpieczenstwo-sieci/","summary":" Bezpieczeństwo sieciNarzędzia\nAuthors: Nitesh Dhanjani, Justin Clarke\nhelion.pl ","title":"Bezpieczeństwo sieci"},{"content":" Kurs Szybkiego CzytaniaAuthor: Jamruszkiewicz Jolanta\nempik.com ","permalink":"https://gagor.pro/books/2006/kurs-szybkiego-czytania/","summary":" Kurs Szybkiego CzytaniaAuthor: Jamruszkiewicz Jolanta\nempik.com ","title":"Kurs Szybkiego Czytania"},{"content":" Władca PierścieniDrużyna pierścienia. Tom 1\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://gagor.pro/books/2005/lotr1/","summary":" Władca PierścieniDrużyna pierścienia. Tom 1\nAuthor: J. R. R. Tolkien\nempik.com ","title":"Władca Pierścieni"},{"content":" Władca PierścieniDwie wieże. Tom 2\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://gagor.pro/books/2005/lotr2/","summary":" Władca PierścieniDwie wieże. Tom 2\nAuthor: J. R. R. Tolkien\nempik.com ","title":"Władca Pierścieni"},{"content":" Władca PierścieniPowrót króla. Tom 3\nAuthor: J. R. R. Tolkien\nempik.com ","permalink":"https://gagor.pro/books/2005/lotr3/","summary":" Władca PierścieniPowrót króla. Tom 3\nAuthor: J. R. R. Tolkien\nempik.com ","title":"Władca Pierścieni"},{"content":" HobbitCzyli tam i z powrotem\nAuthor: J.R.R. Tolkien\nempik.com ","permalink":"https://gagor.pro/books/2005/hobbit/","summary":" HobbitCzyli tam i z powrotem\nAuthor: J.R.R. Tolkien\nempik.com ","title":"Hobbit"},{"content":" Kukułcze jajoEkscytujące łowy na nieuchwytnego hackera\nAuthor: Clifford Stoll\nlubimyczytac.plamazon.pl ","permalink":"https://gagor.pro/books/2000/kukulcze-jajo/","summary":" Kukułcze jajoEkscytujące łowy na nieuchwytnego hackera\nAuthor: Clifford Stoll\nlubimyczytac.plamazon.pl ","title":"Kukułcze jajo"},{"content":"","permalink":"https://gagor.pro/bookshelf/","summary":"","title":"My bookshelf"}]