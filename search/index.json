[{"content":" Welcome on my site, enojoy!\n ","href":"/","title":"Home"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/docker/","title":"Docker"},{"content":" TL;DR\nCentOS base images sucks! They\u0026rsquo;re old, not updated for months!\n As a professional DevOps I concern about a lot of things\u0026hellip; but security is always close to the top of the list. With Docker build environments and deployments became much more stable, which often is a result of just being stale ;/\nI\u0026rsquo;ve been talking about this for long time but it\u0026rsquo;s still hard for people to believe it. Let\u0026rsquo;s check then few, most popular images and when were they last time updated. I wrote a script for that, so feel free to check your list:\n Results show that not all images are frequently updated and if we get a little bit deeper and check how many packages require upgrade:\n   Image Creation date Age (in days) Packages to upgrade     centos:7 2020-11-14T00:20:04.644613188Z 75 16   centos:8 2020-12-08T00:22:53.076477777Z 51 12   debian:9 2021-01-12T00:35:06.08981705Z 16 0   debian:10 2021-01-12T00:32:37.071722022Z 16 0   ubuntu:18.04 2021-01-21T03:38:05.801776526Z 7 2   ubuntu:20.04 2021-01-21T03:38:23.37559427Z 7 4   alpine:3.11 2020-12-17T00:19:49.284211148Z 42 0   alpine:3.12 2020-12-17T00:19:42.11518025Z 42 0   alpine:3.13 2021-01-15T02:23:51.238454884Z 13 5   node:10 2021-01-27T20:32:54.257201224Z 0 13   node:12 2021-01-12T10:36:27.349274428Z 15 13   node:14 2021-01-12T10:33:48.195283512Z 15 13   node:15 2021-01-27T20:29:39.779176105Z 0 13   openjdk:8 2021-01-21T02:40:05.312239007Z 7 0   openjdk:11 2021-01-21T02:38:20.819671373Z 7 0   openjdk:15 2021-01-20T00:45:36.664060993Z 8 ?    Personally, I consider running yum upgrade or apt upgrade/apt dist-upgrade in Dockerfile as anti-pattern - instead builds should be running so frequently to automatically pull all new upgrades from base images. That\u0026rsquo;s theory, but with images like CentOS, you have to do it or risk running your software on unpatched and potentially unsecure system. That\u0026rsquo;s why I don\u0026rsquo;t like CentOS images as a base in general, they just suck from this perspective.\nThere\u0026rsquo;s also another issue here - running those upgrades makes your image just bigger. Sometimes significantly bigger. That\u0026rsquo;s not what I expect from base images\n","href":"/2021/01/how-old-are-docker-official-images/","title":"How old are Docker Official Images?"},{"content":"","href":"/posts/","title":"Posts"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/authors/timor/","title":"timor"},{"content":"","href":"/categories/tip/","title":"Tip"},{"content":"I started my blog on custom (written by my) engine, but as I didn\u0026rsquo;t had enough time to enhance it I switched to Wordpress. I\u0026rsquo;ve been using Wordpress as an engine of my blog for past 8~9 years. I have small VPS with PHP + Nginx and you can find a lot of configuration examples from my config on this site 😄\nThere was a time, when I was really satisfied by what it provides. Not only because of features, but also beacause I was able to play with insane configuration options (check out my caching reverse proxy config). For me it was opportuninty to excel with my skills.\nBut things change. I don\u0026rsquo;t have that much time to carry this server configuration, keep it properly updated and play with new features. Actually I don\u0026rsquo;t need that anymore as I\u0026rsquo;ve been there, I saw it already\u0026hellip;\nI\u0026rsquo;ve been thinking about switching to static page generator and putting my blog in eg. github pages for really long time. I even almost completely migrated it to pelican (after playing for a while with Octopress too). Eventually I\u0026rsquo;ve found Hugo and I loved it!\nAnd here we are, I switched blog to Hugo. I will try to share why I choose this configuration soon and how I handle it.\nKeep warm, be positvie, stay negative!\n","href":"/2020/10/bye-bye-wordpress/","title":"Bye Bye Wordpress!"},{"content":"","href":"/tags/hugo/","title":"hugo"},{"content":"","href":"/categories/off-topic/","title":"Off-topic"},{"content":"","href":"/tags/bash/","title":"bash"},{"content":"","href":"/categories/howto/","title":"HOWTO"},{"content":"","href":"/tags/linux/","title":"Linux"},{"content":"","href":"/tags/macos/","title":"MacOS"},{"content":"Few years ago I moved from Linux desktop to MacOS for my business, day to day work. There were 2 main reasons for that:\n Corporations don\u0026rsquo;t like Linux - they can\u0026rsquo;t manage it, they can\u0026rsquo;t support it, so they blocked it with \u0026ldquo;Security policy\u0026rdquo;, ISO20001, or other nonsense. Actually they\u0026rsquo;re partially right but in different place - many business collaboration applications don\u0026rsquo;t work well on LInux (or they don\u0026rsquo;t work at all)  Skype for Business - there\u0026rsquo;s open source alternative but to get full support you have to pay for additional codecs (as far as I remember) - it\u0026rsquo;s not working stable even in paid version Outlook and calendar support - I love Thunderbird and I use it for years, but calendar invitations didn\u0026rsquo;t work nice (honestly, they didn\u0026rsquo;t work nice even between different Outlook versions\u0026hellip;) Corporate VPN apps - Christ, I always was able to get it working eventually, but\u0026hellip; why bother    I\u0026rsquo;m older, maybe lazier, maybe smarter - I don\u0026rsquo;t like to spend my time resolving problems that don\u0026rsquo;t give me any value. That\u0026rsquo;s how I switched to MacOS - for business purposes only. Privately I still prefer Linux.\nAfter the switch I\u0026rsquo;ve found some differences. Annoying stuff like different behavior of home/end buttons, etc. So right now, on every Mac that I\u0026rsquo;m working with, I\u0026rsquo;m making it to work more like Linux desktop. I\u0026rsquo;ve found those information useful to few my friends too. I decided to publish this because I received too many questions about what to do, how to start?\nIf you think I\u0026rsquo;m missing something important or I did something really bad way - please comment, I will updated it.\nFAQ  How to make screnshots (full screen/partial/desktop recording)? How to change screenshot save localisation? Keyboard and keyboard shortcuts\u0026hellip; Special function keys do not work from external keyboard (it\u0026rsquo;s not Mac compatible ) Keyboard shortcuts are totally different than on Windows or Linux Problem with bash completion on linux boxes Bash completion do not work well on Mac, there are no completions for hosts configured in ssh_config or /etc/hosts How to add bash completion for docker? SSH agent for key management does not work by default How to automatically unlock private SSH keys on login (how to keep SSH private key password in OS X Keychain)? How to write to multiple terminal panes in iTerm2? Blurry fonts on external monitors Jump word left/right (Ctrl+left/right) shortcut don\u0026rsquo;t work on console (iTerm2) Packages on MacOS are outdated and updates arrive later than on Linux End/Home keys behave differently on MacOS I can\u0026rsquo;t use X forwarding with MacOS ssh client and Linux on second end I have problems working with terminator on Mac   How to make screnshots (full screen/partial/desktop recording)? https://support.apple.com/pl-pl/HT201361\nHow to change screenshot save localisation? By default screenshots are saved on desktop which will turn into mess quickly. It\u0026rsquo;s possible to change default save localization for created screenshots: https://discussions.apple.com/docs/DOC-9081\nKeyboard and keyboard shortcuts\u0026hellip; Polish keyboard layout is terrific, location of tilde and backslash buttons cause both Left Shift and Enter to be really far from normal hands position - in my case it\u0026rsquo;s causing pain in hands after few hours of use Another problem is location of Right Alt, it\u0026rsquo;s hidden deeply under hand during writing so it\u0026rsquo;s not convenient to write polish letters like ąśłóćź, etc. Maybe it will be possible to remap few keys to make this layout more usable but right now experience is terrific.\nThis is really big issue. Because on Mac Win/CMD key is used a lot switch to normal keyboard doesn\u0026rsquo;t help. Use of most common shortcuts really overload my thumbs.\nBest solutions I\u0026rsquo;ve found is Karabiner-Elements. It allow to remap keys (ex. switch right alt/cmd) and you can define different options per device (internal/external keyboard). It\u0026rsquo;s also very useful to make standard PC keyboards to be mapped like Apple keyboard.\nSpecial function keys do not work from external keyboard (it\u0026rsquo;s not Mac compatible ) I don\u0026rsquo;t know if it\u0026rsquo;s possible to configure them. With Karabiner-Elements it\u0026rsquo;s possible to add support for some of them.\nKeyboard shortcuts are totally different than on Windows or Linux Here you could find introduction to most typical shortcuts: https://www.apple.com/support/pages/shortcuts/body.html No other way - you have to learn them.\nFew of my favorites, I use everyday:\n Cmd + Space - Spotlight search - think about it like ‘Win' key in Gnome 3, you can start writing app or file name to start/open it Ctrl + Left/Right - switch Desktop on specific screen (full screen apps use \u0026ldquo;whole desktop\u0026rdquo; so it\u0026rsquo;s easy way to see what you have there or start new empty desktop) Ctrl + Up - shows all active windows, desktops, etc. Useful if you\u0026rsquo;re searching specific window  Problem with bash completion on linux boxes -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8) I solved it by marking option in iTerm2 to always set language system variables.\nBash completion do not work well on Mac, there are no completions for hosts configured in ssh_config or /etc/hosts I initially tried this one: http://davidalger.com/development/bash-completion-on-os-x-with-brew/ - it generally works but only for some common tools, ex, svn requires manual download of:\ncurl -L http://svn.apache.org/repos/asf/subversion/trunk/tools/client-side/bash_completion -o /usr/local/etc/bash_completion.d/svn Right now I thing that Brew makes this even easier, because it\u0026rsquo;s installing a lot of bash_completion configs.\nHow to add bash completion for docker? Those two commands will solve problem:\ncurl -L https://raw.githubusercontent.com/docker/compose/master/contrib/completion/bash/docker-compose -o /usr/local/etc/bash_completion.d/docker-compose curl -L https://raw.githubusercontent.com/docker/docker-ce/blob/master/components/cli/contrib/completion/bash/docker -o /usr/local/etc/bash_completion.d/docker SSH agent for key management does not work by default There is a Keychain application installed on MacOS by default, it\u0026rsquo;s responsible for storing keys and managing access to them. To add ssh key to Keychain you have to run:\nssh-add -K and provide password to unlock key.\nSadly this works only one time, I have to manually add key to keychain every time I login by:\nssh-add ~/.ssh/id_rsa How to automatically unlock private SSH keys on login (how to keep SSH private key password in OS X Keychain)? It\u0026rsquo;s all nice described here: https://apple.stackexchange.com/a/250572\nHow to write to multiple terminal panes in iTerm2? use cmd + shift + i to write to all panes on all tabs, or cmd + alt + i to write to all panes on current tab only\nBlurry fonts on external monitors Fonts on external monitors are really blurry - they\u0026rsquo;re badly anti-aliased or hinting is bad. I\u0026rsquo;ve found, that MacOS disable hinting on external monitors. It\u0026rsquo;s possible to enable it back. Check below and play with it to get what would work for you.\nhttps://www.howtogeek.com/358596/how-to-fix-blurry-fonts-on-macos-mojave-with-subpixel-antialiasing/\nJump word left/right (Ctrl+left/right) shortcut don\u0026rsquo;t work on console (iTerm2) You have to configure special escape sequences for Alt+left/right, described here: http://apple.stackexchange.com/a/136931\nPackages on MacOS are outdated and updates arrive later than on Linux Yes, that\u0026rsquo;s sad true. When I have Ansible 2.3 on Jenkins server for MacOS only version 2.2 was available. Version 2.3 will arrive but some time later. This is causing problems in compatibility of code (newer features/options on Jenkins cause problems during deployment and I\u0026rsquo;m not able to test this all on my workstation before real release). Another problem connected to that is that some command line tool on Mac have different switches than on Linux, ex. date -rfc-3339=s is not available, causing scripts to broke on Mac when working on Linux, this also makes testing harder.\nMacOS also use quite old version of bash. As a result .bashrc won\u0026rsquo;t be parsed, you have to put everything to .bash_profile which will slow down starting of each new terminal session (ex. python virtual envs can add significant delay).\nEnd/Home keys behave differently on MacOS Generally you won\u0026rsquo;t find Home/End keys on typical MacBook keyboad - by default on Mac you have Command + Right keyboard shortcut to mimic End, and Command + Left to mimic Home.\nBut\u0026hellip; by default Home/End will move you to the end of page, not line. If you want this behavior back in most of your apps you could try to change keybinding:\nOne option is to create ~/Library/KeyBindings/ and save a property list like this as ~/Library/KeyBindings/DefaultKeyBinding.dict:\n{ \u0026#34;\\UF729\u0026#34; = moveToBeginningOfLine:; \u0026#34;\\UF72B\u0026#34; = moveToEndOfLine:; \u0026#34;$\\UF729\u0026#34; = moveToBeginningOfLineAndModifySelection:; \u0026#34;$\\UF72B\u0026#34; = moveToEndOfLineAndModifySelection:; } Quit and reopen applications to apply the changes. Note that DefaultKeyBinding.dict is not supported by some applications like Xcode or Firefox.\nhttps://apple.stackexchange.com/questions/18016/can-i-change-the-behavior-of-the-home-and-end-keys-on-an-apple-keyboard-with-num\nI can\u0026rsquo;t use X forwarding with MacOS ssh client and Linux on second end There\u0026rsquo;s additional X11 server app that you can install on MacOS (it\u0026rsquo;s called XQuartz. I tried it for short time but I don\u0026rsquo;t need it anymore.\nI have problems working with terminator on Mac For example:\n it\u0026rsquo;s running as python process but it\u0026rsquo;s not available in Lunchpad (not easy to switch with Cmd + Tab keyboard shortcuts are different than on Linux, so this is not making switch easier  I\u0026rsquo;ve found iTerm2, which is \u0026ldquo;state of the art\u0026rdquo; terminal for MacOS. It\u0026rsquo;s popular, well supported and feature complete.\nUseful key shortcuts:\n Cmd + T - new tab Cmd + D - spit vertically Cmd + Shift + D - split horizontally Cmd + Opt + Left/Right/Up/Down - move between shell windows (after split) Cmd + Left/Right - prev/next tab Ctrl + Cmd + Left/Right/Up/Down - change size of windows after split  ","href":"/2020/01/moving-from-linux-to-macos-first-steps/","title":"Moving from Linux to MacOS – first steps"},{"content":"I had stragne statistics on one memcached servers. I had to look what it\u0026rsquo;s doing there. I found such commands that may be used to sniff, extract and make statistics from running memcached server.\nDebug GET commands tcpflow -c dst port 11211 | cut -b46- | grep ^get cut command will remove 46 bytes at beginning of every string (src, dst, port). You may need to adjust numeric parameter for cut to leave commands only. Output should look like:\nget myapp-cache-theme_registry get myapp-cache_field get myapp-cache-schema ... Debug everything except GET commands tcpflow -c dst port 11211 | cut -b46- | grep -v ^get Check transfer Add pv -r at the end to calculate transfer:\n$ tcpflow -c dst port 11211 | cut -b46- | grep ^get | pv -r \u0026gt; /dev/null tcpflow[11140]: listening on eth0 [8.25kB/s] Check command rates To check command rates not the transfer add -l param (stands for lines) to pv command:\n$ tcpflow -c dst port 11211 | cut -b46- | grep -v ^get | pv -rl \u0026gt; /dev/null tcpflow[9271]: listening on eth0 [1.51k/s] That should be enough for you to start debugging 😄\nSource: http://www.streppone.it/cosimo/blog/tag/memcached/\n","href":"/2016/07/debuging-commands-running-on-memcached/","title":"Debuging commands running on memcached"},{"content":"It happen to me all the time that one of developers notifies me about some kind of problem that I can\u0026rsquo;t confirm from my account. Sometimes it was because of bad ssh keys configuration, other times file permissions, mostly such stuff. It\u0026rsquo;s sometimes convenient to \u0026ldquo;enter into someone\u0026rsquo;s shoes\u0026rdquo; to see what\u0026rsquo;s going on there.\nIf you\u0026rsquo;re root on machine you may do that like this:\nsu developer - Easy one but that\u0026rsquo;s not enough for all cases. When you use bastion host (or similar solutions) sometimes users have connection problems and it\u0026rsquo;s harder to check. When such user have ForwardAgent ssh option enabled you may stole this session to check login problems. After you switch to such user, you may wan\u0026rsquo;t to hide history (it\u0026rsquo;s optional 😉 ) - disable history like that:\nexport HISTFILESIZE=0 export HISTSIZE=0 unset HISTFILE Now you may stole ssh session, but first check if you have your dev is logged on:\n$ ls -la /tmp/ | grep ssh drwx------ 2 root root 4096 Apr 27 20:56 ssh-crYKv29798 drwx------ 2 developer developer 4096 Apr 27 18:03 ssh-cVXFo28108 Export SSH_AUTH_SOCK with path to developer\u0026rsquo;s agent socket:\nSSH_AUTH_SOCK=/tmp/ssh-cVXFo28108/agent.28108 Finally you may try to login via ssh as developer and see with his eyes what\u0026rsquo;s now working.\n","href":"/2016/04/how-to-stole-ssh-session-when-youre-root/","title":"How to stole ssh session when you’re root"},{"content":"Virtualenvs in python are cheap but from time to time you will install something with pip on your system and when time comes removing all this crap could be difficult. I found this bash snippet that will uninstall package with all dependencies:\nfor dep in $(pip show python-neutronclient | grep Requires | sed \u0026#39;s/Requires: //g; s/,//g\u0026#39;) ; do sudo pip uninstall -y $dep ; done pip uninstall -y python-neutronclient Source: http://stackoverflow.com/a/32698209/4828478\n","href":"/2016/04/pip-uninstall-package-with-dependencies/","title":"pip - uninstall package with dependencies"},{"content":"","href":"/tags/python/","title":"Python"},{"content":"","href":"/tags/backup/","title":"backup"},{"content":"I\u0026rsquo;ve been using standard MySQL dumps as backup technique on my VPS for few years. It works fine and backups were usable few times when I needed them. But in other places I\u0026rsquo;m using xtrabackup. It\u0026rsquo;s faster when crating backups and a lot faster when restoring them - they\u0026rsquo;re binary so there is no need to reevaluate all SQL create tables/inserts/etc. Backups also include my.cnf config file so restoring on other machine should be easy.\nAfter I switched from MariaDB to Percona I have Percona repos configured, so I will use latest version of xtrabackup.\napt-get install -y percona-xtrabackup Prerequisities xtrabackup requires configured user to be able to make backups. One way is to write user and password in plaintext in ~/.my.cnf. Another is using mysql_config_editor to generate ~/.mylogin.cnf file with encrypted credentials. To be honest I didn\u0026rsquo;t check what kind of security provides this encryption but it feels better than keeping password in plaintext.\nI do not want to create new user for this task - I just used debian-sys-maint user. Check password for this user like this:\ngrep password /etc/mysql/debian.cnf Now create encrypted file:\nmysql_config_editor set --login-path=client --host=localhost --user=debian-sys-maint --password Hit enter and copy/paste password. File .mylogin.cnf should be created with binary content. We may check this with:\n# mysql_config_editor print [client] user = debian-sys-maint password = ***** host = localhost Looks OK.\nBackuping Now backup script. I placed it directly in cron.daily dir ex. /etc/cron.daily/zz-percona-backup with content:\n#!/bin/bash DATE=`date +%F-%H%M%S` DIR=/backup/xtrabackup DST=$DIR/${DATE}.tar.xz # this will produce directories with compresses files # mkdir -p $DST # xtrabackup --backup --compress --target-dir=$DST # this will produce tar.xz archives xtrabackup --backup --stream=tar | xz -9 \u0026gt; $DST # delete files older than 30 days find $DIR -type f -mtime +30 -delete I prefer to have single archive with backup because I\u0026rsquo;m transferring those files to my NAS (for security). But for local backups directories are more convenient and faster when restoring. Also tar archives have to be decompressed with -ioption.\nRestoring First time I saw it it scared me a little but after all worked fine and without problems\u0026hellip;\nservice mysql stop rm -rf /var/lib/mysql mkdir /var/lib/mysql Now prepare backup, if you used directory backups it\u0026rsquo;s easy:\nxtrabackup --decompress --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --prepare --target-dir=/backup/xtrabackup/2016-03-14-214233 xtrabackup --copy-back --target-dir=/backup/xtrabackup/2016-03-14-214233 But if you used tar archives it\u0026rsquo;s little more messy\u0026hellip; You have to create temporary dir and extract archive there:\nmkdir /tmp/restore tar -xvif /backup/xtrabackup/2016-03-14-214233.tar.xz -C /tmp/restore xtrabackup --prepare --target-dir=/tmp/restore xtrabackup --copy-back --target-dir=/tmp/restore We have to fix ownership of restored files and db may be started:\nchown -R mysql:mysql /var/lib/mysql service mysql start If your backup is huge you should reorder commands to shutdown database after backup decompression.\nSource: https://www.percona.com/doc/percona-xtrabackup/2.3/xtrabackup_bin/xtrabackup_binary.html http://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html https://www.percona.com/doc/percona-xtrabackup/2.1/innobackupex/streaming_backups_innobackupex.html\n","href":"/2016/04/daily-mysql-backups-with-xtrabackup/","title":"Daily MySQL backups with xtrabackup"},{"content":"","href":"/tags/mysql/","title":"MySQL"},{"content":"","href":"/tags/percona/","title":"Percona"},{"content":"","href":"/tags/ssh/","title":"ssh"},{"content":"When you deploy your application in cloud you don\u0026rsquo;t need and don\u0026rsquo;t want your hosts exposed via SSH to the world. Malware scans whole network for easy SSH access and when find something will try some brute force attacks, overloading such machines. It\u0026rsquo;s easier to have one exposed, but secured host, that doesn\u0026rsquo;t host anything and is used as proxy/gateway to access our infrastructure- it\u0026rsquo;s called bastion host.\nAnsible is quite easy to integrate with bastion host configuration. We will need custom ansible.cfg and ssh_config file. So let\u0026rsquo;s start with ssh_config:\nHost bastion Hostname ip.xxx.xxx.xxx.xxx.or.host.name User ubuntu IdentityFile ~/.ssh/id_rsa PasswordAuthentication no ForwardAgent yes ServerAliveInterval 60 TCPKeepAlive yes ControlMaster auto ControlPath ~/.ssh/ansible-%r@%h:%p ControlPersist 15m ProxyCommand none LogLevel QUIET Host * User ubuntu IdentityFile ~/.ssh/id_rsa ServerAliveInterval 60 TCPKeepAlive yes ProxyCommand ssh -q -A ubuntu@bastion nc %h %p LogLevel QUIET StrictHostKeyChecking no Now I will describe what most important options mean. For bastion:\n User - I\u0026rsquo;m using Ubuntu kickstarted on cloud as bastion host with it\u0026rsquo;s default user. Never use root here - you don\u0026rsquo;t need that ForwardAgent yes - we want to forward our ssh keys through bastion to destination hosts, ServerAliveInterval 60 - this is like keepalive connection, ssh will send small ping/pong packets every 60 seconds so your connection won\u0026rsquo;t hung/terminate after long time, ControlMaster auto - we will open one connection to bastion host and multiplex other ssh connections through it, connection will be opened for ControlPersist time, ControlPath - this have to be configured same way like in ansible.cfg, ProxyCommand none - we\u0026rsquo;re setting ProxyCommand for all hosts but we need it disabled for bastion,``  Default hosts configuration:\n ProxyCommand ssh -q -A ubuntu@bastion nc %h %p - this is what makes all magic, it will pipe your ssh connection via bastion to destination host, StrictHostKeyChecking no - this options shouldn\u0026rsquo;t be there for production but it\u0026rsquo;s useful at beginning when you create and destroy machines few times before you test everything. Normally this will cause notifications about ssh key changes, but you\u0026rsquo;re aware of that - you just recreated those machines.  I\u0026rsquo;ve found examples without netcat but was unable to get them working - this one worked for me really well.\nTo test if connections work fine use this configuration like:\nssh -F ssh_config bastion ssh -F ssh_config other.host.behind.bastion And now ansible.cfg:\n[defaults] forks=20 [ssh_connection] ssh_args = -F ./ssh_config -o ControlMaster=auto -o ControlPersist=5m -o LogLevel=QUIET control_path = ~/.ssh/ansible-%%r@%%h:%%p pipelining=True Most important section here is in ssh_args where we\u0026rsquo;re pointing to ssh_config file in current dir with -F option. I also have to reenter configuration for multiplexing here - it wasn\u0026rsquo;t working with ssh only configuration. control_path option have to use same paths like ssh_config (% signs are escaped with %%).\nYou should be able to run ansible/ansible-playbook commands normally now - all traffic will be forwarded through bastion.\nIt\u0026rsquo;s good time now to install fail2ban on bastion and maybe reconfigure it to run ssh on crazy high port 🙂\nSources http://alexbilbie.com/2014/07/using-ansible-with-a-bastion-host/ http://blog.scottlowe.org/2015/12/24/running-ansible-through-ssh-bastion-host/ https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\n","href":"/2016/04/use-bastion-host-with-ansible/","title":"Use bastion host with Ansible"},{"content":"","href":"/tags/debian/","title":"Debian"},{"content":"Lately I was searching for mobile notebook that I could use for remote work. I checked f ThinkPad series but they were huge bricks that have nothing in common with \u0026lsquo;mobile\u0026rsquo; word. Then I saw ASUS Zenbook that I didn\u0026rsquo;t take into account before and it was exactly what I was searching for.\nConfiguration of Skylake based notebook right now is not straightforward - there are still glitches and small bugs that are waiting to be fixed. I want to sum up what I\u0026rsquo;ve done after installation. I started with fresh Ubuntu 16.04 to get Debian based distro with possibly latest kernel and patches.\nSome SSD tweaks Change mount options for filesystems on SSD from:\n/dev/mapper/ubuntu--vg-root / ext4 errors=remount-ro 0 1 to:\n/dev/mapper/ubuntu--vg-root / ext4 discard,noatime,errors=remount-ro 0 1 And move /tmp to RAM with this additional line in /etc/fstab:\ntmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0 Now add deadline scheduler for root disk - edit /etc/rc.local and add this line before exit 0:\necho deadline \u0026gt; /sys/block/sda/queue/scheduler echo 1 \u0026gt; /sys/block/sda/queue/iosched/fifo_batch I have configured swap but I don\u0026rsquo;t want to use it too much and setting low swappines sysctl option will help. Run this as root:\necho \u0026#34;vm.swappiness = 1\u0026#34; \u0026gt; /etc/sysctl.conf.d/90-swappines.conf sysctl -p /etc/sysctl.conf.d/90-swappines.conf Power usage tweaks I\u0026rsquo;ve installed laptop-mode-tools to achieve lower power usage on battery. So:\napt-get install -y laptop-mode-tools By default it\u0026rsquo;s cutting hard CPU performance on battery (half performance, no turbo) so I fixed this by changing /etc/laptop-mode/conf.d/intel_pstate.conf section On battery:\n#On battery BATT_INTEL_PSTATE_PERF_MIN_PCT=0 # Minimum performance, in percent BATT_INTEL_PSTATE_PERF_MAX_PCT=100 # Maximum performance, in percent BATT_INTEL_PSTATE_NO_TURBO=0 # Disable \u0026#34;Turbo Boost\u0026#34;? Laptop mode tools won\u0026rsquo;t start automatically so we may integrate them with pm-utils (that are already installed on Ubuntu) to get it running when needed. We have to create new config file:\nsudo touch /etc/pm/sleep.d/10-laptop-mode-tools sudo chmod a+x /etc/pm/sleep.d/10-laptop-mode-tools with content like this:\ncase $1 in hibernate) /etc/init.d/laptop-mode stop ;; suspend) /etc/init.d/laptop-mode stop ;; thaw)d /etc/init.d/laptop-mode start ;; resume) /etc/init.d/laptop-mode start ;; *) echo Something is not right. ;; esac Now I will enable ALPM for SATA in AHCI mode optimizations:\necho SATA_ALPM_ENABLE=true | sudo tee /etc/pm/config.d/sata_alpm And some kernel parameters in /etc/default/grub:\nGRUB_CMDLINE_LINUX=\u0026#34;pcie_aspm=force drm.vblankoffdelay=1 i915.semaphores=1\u0026#34; and update-grub with:\nupdate-grub You may use powertop to nail power heavy processes. There is also powerstat to benchmark power usage through time - I have:\nsudo pm-powersave true powerstat ...... Summary: System: 4.49 Watts on average with standard deviation 0.46 It\u0026rsquo;s really nice. I should be able to run about 8~9h! Sweet!\nSources https://www.reddit.com/r/linux/comments/3ia8ta/review_of_ubuntu_on_asus_ux305fa/\nhttps://help.ubuntu.com/community/PowerManagement/ReducedPower#Using_less_power_with_laptop-mode-tools\nhttps://help.ubuntu.com/community/AsusZenbook\nhttps://wiki.ubuntu.com/Kernel/PowerManagementALPM\nDisable touchpad when writing It\u0026rsquo;s crazy annoying when you tap touchpad during writing text and lose focus on editor window. There is solution for that, it\u0026rsquo;s even installed by default on Ubuntu and it\u0026rsquo;s called: syndaemon. It\u0026rsquo;s started by default like this:\nsyndaemon -i 1.0 -t -K -R 1 second feels too small for me. I will adjust it to 2s. There is no easy way to do this. I created script to run on login:\n#!/bin/bash killall syndaemon syndaemon -d -i 2.0 -t -K -R Now better 🙂\nVD-PAU I installed vdpauinfo tool to see if it\u0026rsquo;s working:\napt-get install -y vdpauinfo It wasn\u0026rsquo;t:\nvdpauinfo display: :0 screen: 0 Failed to open VDPAU backend libvdpau_va_gl.so: cannot open shared object file: No such file or directory Error creating VDPAU device: 1 I checked this library and couldn\u0026rsquo;t find it - it wasn\u0026rsquo;t installed. Easy fix:\napt-get install -y libvdpau-va-gl1 Check again:\nvdpauinfo display: :0 screen: 0 libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 API version: 1 Information string: OpenGL/VAAPI/libswscale backend for VDPAU Video surface: name width height types ------------------------------------------- 420 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 422 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 444 1920 1080 NV12 YV12 UYVY YUYV Y8U8V8A8 V8U8Y8A8 Decoder capabilities: name level macbs width height ---------------------------------------------------- MPEG1 --- not supported --- MPEG2_SIMPLE --- not supported --- MPEG2_MAIN --- not supported --- H264_BASELINE 51 16384 2048 2048 H264_MAIN 51 16384 2048 2048 H264_HIGH 51 16384 2048 2048 VC1_SIMPLE --- not supported --- VC1_MAIN --- not supported --- VC1_ADVANCED --- not supported --- MPEG4_PART2_SP --- not supported --- MPEG4_PART2_ASP --- not supported --- DIVX4_QMOBILE --- not supported --- DIVX4_MOBILE --- not supported --- DIVX4_HOME_THEATER --- not supported --- DIVX4_HD_1080P --- not supported --- DIVX5_QMOBILE --- not supported --- DIVX5_MOBILE --- not supported --- DIVX5_HOME_THEATER --- not supported --- DIVX5_HD_1080P --- not supported --- H264_CONSTRAINED_BASELINE 51 16384 2048 2048 H264_EXTENDED --- not supported --- H264_PROGRESSIVE_HIGH --- not supported --- H264_CONSTRAINED_HIGH --- not supported --- H264_HIGH_444_PREDICTIVE --- not supported --- HEVC_MAIN --- not supported --- HEVC_MAIN_10 --- not supported --- HEVC_MAIN_STILL --- not supported --- HEVC_MAIN_12 --- not supported --- HEVC_MAIN_444 --- not supported --- Output surface: name width height nat types ---------------------------------------------------- B8G8R8A8 8192 8192 y R8G8B8A8 8192 8192 y R10G10B10A2 8192 8192 y B10G10R10A2 8192 8192 y A8 8192 8192 y Bitmap surface: name width height ------------------------------ B8G8R8A8 8192 8192 R8G8B8A8 8192 8192 R10G10B10A2 8192 8192 B10G10R10A2 8192 8192 A8 8192 8192 Video mixer: feature name sup ------------------------------------ DEINTERLACE_TEMPORAL - DEINTERLACE_TEMPORAL_SPATIAL - INVERSE_TELECINE - NOISE_REDUCTION - SHARPNESS - LUMA_KEY - HIGH QUALITY SCALING - L1 - HIGH QUALITY SCALING - L2 - HIGH QUALITY SCALING - L3 - HIGH QUALITY SCALING - L4 - HIGH QUALITY SCALING - L5 - HIGH QUALITY SCALING - L6 - HIGH QUALITY SCALING - L7 - HIGH QUALITY SCALING - L8 - HIGH QUALITY SCALING - L9 - parameter name sup min max ----------------------------------------------------- VIDEO_SURFACE_WIDTH - VIDEO_SURFACE_HEIGHT - CHROMA_TYPE - LAYERS - attribute name sup min max ----------------------------------------------------- BACKGROUND_COLOR - CSC_MATRIX - NOISE_REDUCTION_LEVEL - SHARPNESS_LEVEL - LUMA_KEY_MIN_LUMA - LUMA_KEY_MAX_LUMA - Looks better now\u0026hellip; But not impressive, there\u0026rsquo;s only H264 support.\nI\u0026rsquo;ve tried it in VLC but it was crashing from time to time the whole VLC (leaving it running in background). Time to test VA-API 🙂\nVA-API Like earlier I have to install one tool to see what we have: vainfo\napt-get install -y vainfo Checking what we have on system:\nvainfo libva info: VA-API version 0.39.0 libva info: va_getDriverName() returns 0 libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so libva info: Found init function __vaDriverInit_0_39 libva info: va_openDriver() returns 0 vainfo: VA-API version: 0.39 (libva 1.7.0) vainfo: Driver version: Intel i965 driver for Intel(R) Skylake - 1.7.0 vainfo: Supported profile and entrypoints VAProfileMPEG2Simple :\tVAEntrypointVLD VAProfileMPEG2Simple :\tVAEntrypointEncSlice VAProfileMPEG2Main :\tVAEntrypointVLD VAProfileMPEG2Main :\tVAEntrypointEncSlice VAProfileH264ConstrainedBaseline:\tVAEntrypointVLD VAProfileH264ConstrainedBaseline:\tVAEntrypointEncSlice VAProfileH264Main :\tVAEntrypointVLD VAProfileH264Main :\tVAEntrypointEncSlice VAProfileH264High :\tVAEntrypointVLD VAProfileH264High :\tVAEntrypointEncSlice VAProfileH264MultiviewHigh :\tVAEntrypointVLD VAProfileH264MultiviewHigh :\tVAEntrypointEncSlice VAProfileH264StereoHigh :\tVAEntrypointVLD VAProfileH264StereoHigh :\tVAEntrypointEncSlice VAProfileVC1Simple :\tVAEntrypointVLD VAProfileVC1Main :\tVAEntrypointVLD VAProfileVC1Advanced :\tVAEntrypointVLD VAProfileNone :\tVAEntrypointVideoProc VAProfileJPEGBaseline :\tVAEntrypointVLD VAProfileJPEGBaseline :\tVAEntrypointEncPicture VAProfileVP8Version0_3 :\tVAEntrypointVLD VAProfileVP8Version0_3 :\tVAEntrypointEncSlice VAProfileHEVCMain :\tVAEntrypointVLD VAProfileHEVCMain :\tVAEntrypointEncSlice It requires package i965-va-driver to work but on my system it was installed (probably during VDPAU installation as dependency).\nIt was working almost fine\u0026hellip; In VLC on my machine VA-API on X11 was drawing through all desktops. VA-API DRM was working better\u0026hellip; But crashed my X11 server after few minutes of watching ;/\nOpenCL You may thing: for what the hell you need OpenCL on such tiny machine? I doesn\u0026rsquo;t care - I want it 🙂\nFirst install clinfo package:\napt-get install -y clinfo And run it:\nclinfo Number of platforms 0 Not too much 😀\nFor Intel GPU/CPU OpenCL support we will need beignet package:\napt-get install -y beignet clinfo Number of platforms 1 Platform Name Intel Gen OCL Driver Platform Vendor Intel Platform Version OpenCL 1.2 beignet 1.1.1 Platform Profile FULL_PROFILE Platform Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_spir cl_khr_icd Platform Extensions function suffix Intel Platform Name Intel Gen OCL Driver Number of devices 1 Device Name Intel(R) HD Graphics Skylake ULX GT2 Device Vendor Intel Device Vendor ID 0x8086 Device Version OpenCL 1.2 beignet 1.1.1 Driver Version 1.1.1 Device OpenCL C Version OpenCL C 1.2 beignet 1.1.1 Device Type GPU Device Profile FULL_PROFILE Max compute units 24 Max clock frequency 1000MHz Device Partition (core) Max number of sub-devices 1 Supported partition types None, None, None Max work item dimensions 3 Max work item sizes 512x512x512 Max work group size 512 Preferred work group size multiple 16 Preferred / native vector sizes char 16 / 8 short 8 / 8 int 4 / 4 long 2 / 2 half 0 / 8 (cl_khr_fp16) float 4 / 4 double 0 / 2 (n/a) Half-precision Floating-point support (cl_khr_fp16) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Single-precision Floating-point support (core) Denormals No Infinity and NANs Yes Round to nearest Yes Round to zero No Round to infinity No IEEE754-2008 fused multiply-add No Support is emulated in software No Correctly-rounded divide and sqrt operations No Double-precision Floating-point support (n/a) Address bits 32, Little-Endian Global memory size 2147483648 (2GiB) Error Correction support No Max memory allocation 1073741824 (1024MiB) Unified memory for Host and Device Yes Minimum alignment for any data type 128 bytes Alignment of base address 1024 bits (128 bytes) Global Memory cache type Read/Write Global Memory cache size 8192 Global Memory cache line 64 bytes Image support Yes Max number of samplers per kernel 16 Max size for 1D images from buffer 65536 pixels Max 1D or 2D image array size 2048 images Max 2D image size 8192x8192 pixels Max 3D image size 8192x8192x2048 pixels Max number of read image args 128 Max number of write image args 8 Local memory type Global Local memory size 65536 (64KiB) Max constant buffer size 134217728 (128MiB) Max number of constant args 8 Max size of kernel argument 1024 Queue properties Out-of-order execution No Profiling Yes Prefer user sync for interop Yes Profiling timer resolution 80ns Execution capabilities Run OpenCL kernels Yes Run native kernels Yes SPIR versions printf() buffer size 1048576 (1024KiB) Built-in kernels __cl_copy_region_align4;__cl_copy_region_align16;__cl_cpy_region_unalign_same_offset;__cl_copy_region_unalign_dst_offset;__cl_copy_region_unalign_src_offset;__cl_copy_buffer_rect;__cl_copy_image_1d_to_1d;__cl_copy_image_2d_to_2d;__cl_copy_image_3d_to_2d;__cl_copy_image_2d_to_3d;__cl_copy_image_3d_to_3d;__cl_copy_image_2d_to_buffer;__cl_copy_image_3d_to_buffer;__cl_copy_buffer_to_image_2d;__cl_copy_buffer_to_image_3d;__cl_fill_region_unalign;__cl_fill_region_align2;__cl_fill_region_align4;__cl_fill_region_align8_2;__cl_fill_region_align8_4;__cl_fill_region_align8_8;__cl_fill_region_align8_16;__cl_fill_region_align128;__cl_fill_image_1d;__cl_fill_image_1d_array;__cl_fill_image_2d;__cl_fill_image_2d_array;__cl_fill_image_3d; Device Available Yes Compiler Available Yes Linker Available Yes Device Extensions cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_fp16 cl_khr_spir cl_khr_icd NULL platform behavior clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...) Intel Gen OCL Driver clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...) Success [Intel] clCreateContext(NULL, ...) [default] Success [Intel] clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM) No devices found in platform clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL) Success (1) Platform Name Intel Gen OCL Driver Device Name Intel(R) HD Graphics Skylake ULX GT2 ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version 2.2.8 ICD loader Profile OpenCL 1.2 NOTE:\tyour OpenCL library declares to support OpenCL 1.2, but it seems to support up to OpenCL 2.1 too. Sources https://wiki.archlinux.org/index.php/GPGPU#Intel\nSuspend/Hibernate on lid close Default configuration of Ubuntu 16.04 was that after I close lid screen was blocked and LCD disabled. But system was still working normally - I strongly prefer to hibernate in such case and use no battery at all.\nI achieved that with systemd-logind. Edit /etc/systemd/logind.conf and uncomment line with HandleLidSwitch:\n[Login] HandleLidSwitch=suspend HandleLidSwitchDocked=ignore Now restart systemd-logind service with:\nsystemctl restart systemd-logind.service Problem with function keys Function keys were mostly working but not always like I expected. For example when I disable touchpad - it\u0026rsquo;s not disabled 🙂\nI found that module asus-nb-wmi is responssible for that and it\u0026rsquo;s still buggy. So I disabled it at all with:\necho \u0026#34;blacklist asus-nb-wmi\u0026#34; \u0026gt; /etc/modprobe.d/blacklist-ux305.conf Volume UP/DOWN/MUTE are still working fine - that\u0026rsquo;s enough for me. Rest could be configured with some keyboard shortcuts - more info here.\nTODO/Issues I still face some bugs:\n I could see occasional flickering from time to time. Rather after running notebook for some time than overheating/overloading it. This may be driver issue or maybe SNA acceleration method - I have to experiment a little to get this solved.\nLooks like disabling Virtualization support and VT-d in BIOS helped. It\u0026rsquo;s not final solution but for now I don\u0026rsquo;t need it\u0026hellip; A lot 😉 Tapping sometimes behave strange, for ex. tap to click stops to work and I have to use touchpad buttons for that. I think this may be related to syndaemon configuration because it started after I tuned it.\nIt was that. My hack for syndaemon broke touchpad. I will play with this a little more later. I like to use copy by selection and paste by middle click on my desktop - I\u0026rsquo;m addicted to this option but it\u0026rsquo;s not working on my laptop. I\u0026rsquo;m not sure if this will be convenient enough on touchpad to use.\nTo right click just tap with two fingers, to middle click (third button) tap with three fingers. Copy/paste is again easy like before.  If you found errors in my text of know better solutions for described problems, please let me know in comments.\n","href":"/2016/04/tweaking-asus-zenbook-ux305ca-on-linux/","title":"Tweaking ASUS Zenbook UX305CA on Linux"},{"content":"","href":"/tags/ubuntu/","title":"Ubuntu"},{"content":"","href":"/tags/zenbook/","title":"Zenbook"},{"content":"","href":"/tags/ipv6/","title":"IPv6"},{"content":"I try to use IPv6 where it\u0026rsquo;s available but it\u0026rsquo;s sometimes so hard\u0026hellip; It happen quite often that I can\u0026rsquo;t download packages from repos because they weren\u0026rsquo;t configured on IPv6 vhosts even when host is available via IPv6 address. For APT you may use this trick to force IPv4 connections only:\necho \u0026#39;Acquire::ForceIPv4 \u0026#34;true\u0026#34;;\u0026#39; \u0026gt; /etc/apt/apt.conf.d/99force-ipv4 If you need more than that, then gai.conf will allow you to filter where you will be connecting via IPv4 and where via IPv6 - in example bellow you will prefer IPv4 whenever it\u0026rsquo;s available:\necho \u0026#39;precedence ::ffff:0:0/96 100\u0026#39; \u0026gt;\u0026gt; /etc/gai.conf ","href":"/2016/03/prefer-ipv4-over-ipv6/","title":"Prefer IPv4 over IPv6"},{"content":"Sometimes it\u0026rsquo;s easier to use octal file permissions but they\u0026rsquo;re not so easy to list. I caught myself few times that I didn\u0026rsquo;t remember how to list them - so this is a reason for that note.\nstat -c \u0026#34;%a %n\u0026#34; * 755 bin 755 games 755 include Yes, it\u0026rsquo;s that easy 😃\nAnd here also with human readable attributes:\nstat -c \u0026#39;%A %a %n\u0026#39; * drwxr-xr-x 755 bin drwxr-xr-x 755 games drwxr-xr-x 755 include ","href":"/2016/02/list-octal-file-permissions-in-bash/","title":"List octal file permissions in bash"},{"content":"","href":"/tags/php/","title":"PHP"},{"content":"","href":"/tags/wordpress/","title":"Wordpress"},{"content":"I was configuring WordPress with HyperDB plugin on PHP 7.0 but the only I get were constant 500 errors. As I found here PHP 7.0 is not supported by HyperDB for now - it\u0026rsquo;s rely on mysql php extension but in PHP 7.0 there is only mysqli extension. But few folks fixed it and it\u0026rsquo;s possible to use it.\ncurl -O https://raw.githubusercontent.com/soulseekah/hyperdb-mysqli/master/db.php mv db.php /var/www/wordpress/wp-content/ And configure it ex. like this:\ncat \u0026lt;\u0026lt;DBCONFIG \u0026gt; /var/www/wordpress/db-config.php \u0026lt;?php \\$wpdb-\u0026gt;save_queries = false; \\$wpdb-\u0026gt;persistent = false; \\$wpdb-\u0026gt;max_connections = 10; \\$wpdb-\u0026gt;check_tcp_responsiveness = true; \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;master.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 1, \u0026#39;read\u0026#39; =\u0026gt; 1, )); \\$wpdb-\u0026gt;add_database(array( \u0026#39;host\u0026#39; =\u0026gt; \u0026#34;slave.db.host\u0026#34;, \u0026#39;user\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#34;random_password\u0026#34;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#34;wordpress\u0026#34;, \u0026#39;write\u0026#39; =\u0026gt; 0, \u0026#39;read\u0026#39; =\u0026gt; 1, )); DBCONFIG Now WordPress could handle crash of master database.\nSources https://www.digitalocean.com/community/tutorials/how-to-optimize-wordpress-performance-with-mysql-replication-on-ubuntu-14-04\n","href":"/2016/02/wordpress-with-hyperdb-on-php-7-0/","title":"WordPress with HyperDB on PHP 7.0"},{"content":"I\u0026rsquo;m playing a lot with Docker lately. Building images, and then rebuilding, and then building again\u0026hellip; It\u0026rsquo;s pretty boring. To automate this task a little I used inotify to build automatically after I changed any file. This trick could be used in many different situations.\nYou will need inotify-tools package:\nsudo apt-get install -y inotify-tools Then run something like this:\nwhile inotifywait -e modify -r .; do docker-compose build; done This commands will rebuild my Docker images after any file change in current directory. Use Ctrl+c to exit from loop.\n","href":"/2016/02/automatically-build-after-file-change/","title":"Automatically build after file change"},{"content":"I never tried it before but today I needed to install WordPress\u0026hellip; From command line only. And there is a way to do this with wp-cli.\nWP-CLI installation First some requirements (as root):\napt-get install php5-cli php5-mysql mysql-client curl And now installation of wp-cli (as root too):\ncurl -O https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar mv wp-cli.phar /usr/local/bin/wp Check if it\u0026rsquo;s working:\n$ wp --version WP-CLI 0.22.0 WordPress installation Now you should switch to user of your web application, ex. like this:\nsu - www-data -s /bin/bash And install WP:\nwp core download --path=/var/www/wordpress wp core config --path=/var/www/wordpress \\  --dbname=wordpress \\  --dbuser=wordpress \\  --dbpass=wordpresspass \\  --dbhost=localhost \\  --locale=pl_PL wp core install --path=/var/www/wordpress \\  --url=\u0026#34;http://example.com\u0026#34; \\  --title=\u0026#34;Example blog\u0026#34; \\  --admin_user=never_use_admin_here \\  --admin_password=admin_pass \\  --admin_email=admin@example.com \\  --skip-email Here you may find more about wp-cli configuration and commands.\n","href":"/2016/02/install-wordpress-from-command-line/","title":"Install WordPress from command-line"},{"content":"When I started playing with Docker I was running a lot of commands to build image, delete containers running on old image, run containers based on new image, etc\u0026hellip; A lot of log commands with links, volumes, etc\u0026hellip;\nThen I started searching for something to automate this task and here I get to docker-compse command, this is how you may install it:\npip install docker-compose And install additional bash completions (run as root):\ncurl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/bash/docker-compose \u0026gt; /etc/bash_completion.d/docker-compose Then you may prepare docker-compose.yml file like:\nweb: build: . command: php -S 0.0.0.0:8000 -t /code ports: - \u0026#34;8000:8000\u0026#34; links: - db volumes: - .:/code db: image: orchardup/mysql environment: MYSQL_DATABASE: wordpress More informations about syntax may be found here: https://docs.docker.com/compose/compose-file/\nAnd run such environment with:\ndocker-compose up Or to run this in background:\ndocker-compose up -d To stop and cleanup it use:\ndocker-compose stop \u0026amp;\u0026amp; docker-compose rm -f -v Other usable commands are:\ndocker-compose build --force-rm # to rebuild images and clean after docker-compose ps # to list containers I\u0026rsquo;m still playing with volumes in this but don\u0026rsquo;t have anything interesting enough to paste here - maybe later.\nSources https://docs.docker.com/compose/install/\nhttps://docs.docker.com/compose/completion/\n","href":"/2016/02/install-docker-compose/","title":"Install Docker Compose"},{"content":"I\u0026rsquo;ve played with Docker a little in it early days but didn\u0026rsquo;t stick for longer with it. It\u0026rsquo;s stable now so I wanted to check how it\u0026rsquo;s running now.\nI really can\u0026rsquo;t accept this method of installation:\ncurl -fsSL https://get.docker.com/ | sh I think that world is going to it\u0026rsquo;s end when I see such scritps\u0026hellip; I prefer to do this manually, knowing exactly what I have to do.\nInstall prerequisites:\napt-get update apt-get install -y apt-transport-https ca-certificates Purge old packages if you used them:\napt-get purge lxc-docker* apt-get purge docker.io* Add GPG key:\napt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add repo - use ONLY ONE repo appropriate for your system (lsb_release -a to check):\necho \u0026#34;deb https://apt.dockerproject.org/repo debian-wheezy main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-jessie main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo debian-stretch main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-precise main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list echo \u0026#34;deb https://apt.dockerproject.org/repo ubuntu-trusty main\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list Refresh repos and install Docker:\napt-get update apt-get install -y docker-engine Start service if it\u0026rsquo;s not running:\nservice docker start Grant access to docker service for non-root user I don\u0026rsquo;t like to use apps that require me to use root account. Docker even advice not to do so - service is running as root and you should add docker group and user to it to grant access to service socket:\ngroupadd docker gpasswd -a ${USER} docker service docker restart Now logout, login again and and you should be able to use docker command:\ndocker version docker info docker run hello-world Have fun 😃\nSources https://docs.docker.com/linux/\nhttps://docs.docker.com/engine/installation/\n","href":"/2016/02/manual-installation-of-docker-on-debian-ubuntu/","title":"Manual installation of Docker on Debian/Ubuntu"},{"content":"I started playing with Docker and here I will write some commands that where not so obvious at beginning 😃\nList running containers:\ndocker ps List also not running containers:\ndocker ps -a Remove all containers (be careful with that):\ndocker rm $(docker ps -a -q) Remove all images:\ndocker rmi $(docker images -q) Docker won\u0026rsquo;t remove any old volumes used by containers, so after some time you may be interested in deleting them all:\ndocker volume rm $(docker volume ls -q) Run container and enter bash:\ndocker run --name deb -t -i debian:jessie /bin/bash Show build logs from container:\ndocker logs deb Enter bash into running container:\ndocker exec -it deb /bin/bash Build image from Dockerfile in current directory:\ndocker build -t my_web . After playing a little with this command I started searching for something to automate my tasks a little, and found docker-compose - you may be interested in it too.\nSources https://techoverflow.net/blog/2013/10/22/docker-remove-all-images-and-containers/\n","href":"/2016/02/some-usefull-commands-in-docker/","title":"Some usefull commands in Docker"},{"content":"I was doing a lot of changes to my old posts, switched to HTTPS, etc. Sometimes it was useful to change some particular text in all my old posts at a time, but there is no such feature in WordPress. But WordPress runs on MySQL and I could use SQL query to update such posts.\nMake backup - it\u0026rsquo;s not required but strongly advised 😃\nNow use this query as template to replace in place whatever you need:\neYou should see something like:\nQuery OK, 157 rows affected (0.04 sec) Rows matched: 455 Changed: 157 Warnings: 0 This will remove \u0026lt;!--more--\u0026gt;from all posts (it\u0026rsquo;s used by WordPress to span article when showed on tag/category pages).\nAnother example to update all URLs to HTTPS:\nUPDATE wp_posts SET post_content = REPLACE(post_content, \u0026#34;http://gagor.pl\u0026#34;, \u0026#34;https://gagor.pl\u0026#34;); Be careful with that and make DB backup before you start.\n","href":"/2016/02/mass-replace-in-wordpress-posts-via-mysql-query/","title":"Mass replace in WordPress posts via MySQL query"},{"content":"","href":"/tags/silverlight/","title":"Silverlight"},{"content":"From few days I have access to UPC\u0026rsquo;s www.horizon.tv platform - until now it was useless on Linux. But there is Pipelight that will use Wine to emulate Silverlight on Linux and it\u0026rsquo;s working pretty well - you\u0026rsquo;re just few commands away from achieving that:\n# stop browser killall firefox # remove old version if you have it sudo apt-get remove pipelight Now configure repos and install packages:\nsudo apt-add-repository ppa:pipelight/stable sudo apt-get update sudo apt-get install --install-recommends pipelight-multi sudo pipelight-plugin --update Enable plugin (run it with sudo for system wide installation):\npipelight-plugin --enable silverlight Start Firefox and test if plugin is working here: http://bubblemark.com/silverlight2.html\nNow enter www.horizon.tv and try it yourself.\nP.S. It works only on Firefox because Chrome do not support NPAPI plugins anymore 😃\nSources http://www.webupd8.org/2013/08/pipelight-use-silverlight-in-your-linux.html\n","href":"/2016/02/use-www-horizon-tv-with-pipelight-silverlight-on-linux-ubuntu/","title":"Use www.horizon.tv with Pipelight/Silverlight on Linux/Ubuntu"},{"content":"I just bought new wifi card for my desktop computer. Like in topic, it\u0026rsquo;s Intel Dual Band Wireless-AC 7260 for Desktop.\nI was searching for card that:\n support AC standard have 5GHz network support (2,4GHz channels are cluttered heavily in my neighborhood have PCI/PCIx or USB3 connector is Linux friendly (no modules compilation by hand, support for aircrack-ng, kismet)  This one is the only I found that comply my expectations.\nI found time to play with kismet and aircrack-ng and it was working fine. Card works without problems on kernel 4.2.0. Highest transfer on my net I could get from my NAS - about 23 MB/s (megabytes per second) - much better than on my old N router (approx 6,5 MB/s).\nHere\u0026rsquo;s information from lspci -vvv:\n05:00.0 Network controller: Intel Corporation Wireless 7260 (rev 73) Subsystem: Intel Corporation Dual Band Wireless-AC 7260 Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast \u0026gt;TAbort- \u0026lt;TAbort- \u0026lt;MAbort- \u0026gt;SERR- \u0026lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 39 Region 0: Memory at f7c00000 (64-bit, non-prefetchable) [size=8K] Capabilities: [c8] Power Management version 3 Flags: PMEClk- DSI+ D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold+) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [d0] MSI: Enable+ Count=1/1 Maskable- 64bit+ Address: 00000000fee0400c Data: 4123 Capabilities: [40] Express (v2) Endpoint, MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0, Latency L0s \u0026lt;512ns, L1 unlimited ExtTag- AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd- ExtTag- PhantFunc- AuxPwr+ NoSnoop+ FLReset- MaxPayload 128 bytes, MaxReadReq 128 bytes DevSta: CorrErr+ UncorrErr- FatalErr- UnsuppReq- AuxPwr+ TransPend- LnkCap: Port #0, Speed 2.5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s \u0026lt;4us, L1 \u0026lt;32us ClockPM+ Surprise- LLActRep- BwNot- ASPMOptComp- LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk+ ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed 2.5GT/s, Width x1, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt- DevCap2: Completion Timeout: Range B, TimeoutDis+, LTR+, OBFF Via WAKE# DevCtl2: Completion Timeout: 16ms to 55ms, TimeoutDis-, LTR-, OBFF Disabled LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis- Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS- Compliance De-emphasis: -6dB LnkSta2: Current De-emphasis Level: -3.5dB, EqualizationComplete-, EqualizationPhase1- EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest- Capabilities: [100 v1] Advanced Error Reporting UESta: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UEMsk: DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol- UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol- CESta: RxErr- BadTLP+ BadDLLP- Rollover- Timeout- NonFatalErr+ CEMsk: RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr+ AERCap: First Error Pointer: 00, GenCap- CGenEn- ChkCap- ChkEn- Capabilities: [140 v1] Device Serial Number 7c-5c-f8-ff-xx-xx-xx-xx Capabilities: [14c v1] Latency Tolerance Reporting Max snoop latency: 0ns Max no snoop latency: 0ns Capabilities: [154 v1] Vendor Specific Information: ID=cafe Rev=1 Len=014 \u0026lt;?\u0026gt; Kernel driver in use: iwlwifi And iwconfig:\nwlp5s0 IEEE 802.11abgn ESSID:\u0026#34;cis5\u0026#34; Mode:Managed Frequency:5.5 GHz Access Point: 34:7A:60:XX:XX:XX Bit Rate=780 Mb/s Tx-Power=22 dBm Retry short limit:7 RTS thr:off Fragment thr:off Power Management:off Link Quality=60/70 Signal level=-50 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:7 Invalid misc:184 Missed beacon:0 If you\u0026rsquo;re thinking about buying any Linux friendly AC wifi card - this one is worth it\u0026rsquo;s price.\n","href":"/2016/02/intel-dual-band-wireless-ac-7260-for-desktop-on-linux/","title":"Intel Dual Band Wireless-AC 7260 for Desktop on Linux"},{"content":"","href":"/tags/ddos/","title":"DDoS"},{"content":"","href":"/tags/dos/","title":"DoS"},{"content":"","href":"/tags/iptables/","title":"iptables"},{"content":"I watched nice presentation about how Cloudflare protects itself against DoS. Most of us are not able to do that exactly like them but some of tips were general enough to be used on typical web front server.\nI took notes from this presentation and presented here. Thanks to Marek agreement I also reposted all examples (in easier to copy paste way).\nHowto prepare against ACK/FIN/RST/X-mas flood Use conntrack rule:\niptables -A INPUT --dst 1.2.3.4 -m conntrack --ctstate INVALID -j DROP which will only work with disabled tcp_loose setting (it\u0026rsquo;s by default enabled) with addition to sysctl:\nsysctl -w net.netfilter.nf_conntrack_tcp_loose=0 Howto prepare against SYN floods SYN flood is hard case - because when you use conntrack it will make your performance worst validating state for every new single packet.\nThe only way to get around this is to enable syncookies:\nsysctl -w net.ipv4.tcp_syncookies=1 sysctl -w net.ipv4.tcp_timestamps=1 Enabling syncookies will cause loose of some of connection informations, that are pretty useful like:\n window scaling factor ECN bit (Explicit Congestion Notification)  For that we will use tcp_timestamp option, that will use few bits from timestamp field to store some of this informations.\nThis still may be not efficient enough, but in kernel 4.4 there will be some update to how syncookies are served that should make it few times faster than with older one.\nRelated docs: https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\nHowto prepare against botnet attack Symptoms:\n concurrent connection count going up many sockets in orphaned state many sockets in time wait state  Solutions:\n Enable connlimit feature on conntrack to limit amount of concurrent connections to our service Use hashlimits to rate limit SYN packets per IP Use ipsets to efficiently block many IP/subnet addresses  manual blacklisting - feed IP blacklist from HTTP server logs supports subnets, timeouts automatic blacklisting hashlimits   Disable HTTP keep-alives to make this attack look more like SYN flood  This may still not work against DDoS because huge amount of bots won\u0026rsquo;t allow you to block them efficiently enough.\nSome exciting system tweaks and examples from this presentation I hope to find some time to merge them into template/script that could be used much easier - but first I have to play with these rules a little and test what will be most useful.\nNIC: Discard with flow steering ethtool -N eth3 flow-type udp4 dst-ip 129.168.254.30 dst-port 53 action -1 Flow steering for priority ethtool -X eth3 weight 0 1 1 1 1 1 1 1 1 1 1 ethtool -N eth3 flow-type tcp4 dst-port 22 action 0 SYN backlog size sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.tcp_max_syn_backlog=65535 It\u0026rsquo;s rounded to next power of two (in this case to 65536).\nSYN backlog decay sysctl -w net.ipv4.tcp_synack_retries=1 L7 connection count sysctl -w net.ipv4.tcp_max_orphans=262144 sysctl -w net.ipv4.tcp_orphan_retries=1 sysctl -w net.ipv4.tcp_max_tw_buckets=360000 sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w net.ipv4.tcp_fin_timeout=5 L3: u32 iptables -A INPUT \\  --dst 1.2.3.4 \\  --p udp -m udp --dport 53 \\  -m u32 --u32 \u0026#34;6\u0026amp;0xFF=0x6 \u0026amp;\u0026amp; 4\u0026amp;0x1FFF=0 \u0026amp;\u0026amp; 0\u0026gt;\u0026gt;22\u0026amp;0x3C@4=0x29\u0026#34; \\  -j DROP L4: Conntrack iptables -t raw -A PREROUTING \\  -i eth2 \\  --dst 1.2.3.4 \\  -j ACCEPT iptables -t raw -A PREROUTING \\  -i eth2 \\  -j NOTRACK iptables -A INPUT \\  --dst 1.2.3.4 \\  -m conntrack --ctstate INVALID \\  -j DROP Tuning conntrack sysctl -w net.netfilter.nf_conntrack_tcp_loose=0 sysctl -w net.netfilter.nf_conntrack_helper=0 sysctl -w net.nf_conntrack_max=2000000 echo 2500000 \u0026gt; /sys/module/nf_conntrack/parameters/hashsize More info about conntrack sysctl options: https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt\nL7: Connlimit iptables -t raw -A PREROUTING \\  -i eth2 \\  --dst 1.2.4.5 \\  -j ACCEPT iptables -A INPUT \\  --dst 1.2.3.4 \\  -p tcp -m tcp --dport 80 \\  -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN \\  -m connlimit \\  --connlimit-above 10 \\  --connlimit-mask 32 \\  --connlimit-saddr \\  -j DROP L7: ipset for blacklisting ipset -exist create ta_d335c5 hash:net family inet ipset add ta_d335c5 192.168.0.0/16 ipset add ta_d335c5 10.0.0/8 iptables -A INPUT \\  -m set --match-set ta_d335c5 src \\  -j DROP L7: being evil - TARPIT iptables -A INPUT \\  -m set --match-set ta_d335c5 src \\  -j TARPIT TARPIT target will imitate successful connection for the client (bot in this case) but without responding to it\u0026rsquo;s queries. It will cost that bot a lot more resources and time to timeout and drop this connection than when using DROP or REJECT here.\nL7: hashlimit for rate limiting iptables -A INPUT \\  --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\  --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\  -m hashlimit \\  --hashlimi-above 123/sec \\  --hashlimit-burst 5 \\  --hashlimit-mode srcip \\  --hashlimit-srcmask 24 \\  --hashlimit-name 341654b1d4af9bf \\  -j DROP L7: auto-blacklist ipset -exist create blacklist hash:net timeout 60 iptables -A INPUT \\  --dst 1.2.3.4 \\  -m set --match-set blaclist src \\  -j DROP iptables -A INPUT \\  --dst 1.2.3.4 -p tcp -m tcp --dport 80 \\  --tcp-flags FIN,SYN,RST,PSH,ACK,URG SYN \\  -m hashlimit \\  --hashlimit-above 100/sec \\  --hashlimit-mode srcip \\  --hashlimit-srcmask 24 \\  --hashlimit-name hl_blacklist \\  -j SET --add-set blacklist src L7+: payload in TCP - string iptables -A INPUT \\  --dst 1.2.3.4 \\  -p tcp --dport 80 \\  -m string \\  --hex-string 486f737777777777... \\  --from 231 --to 300 \\  -j DROP For more informations and explanations watch this great presentation:\nAnd here is the whole presentation with additional examples:\nhttps://speakerdeck.com/majek04/lessons-from-defending-the-indefensible\n","href":"/2016/02/prepare-for-dos-like-cloudflare-do/","title":"Prepare for DoS like Cloudflare do"},{"content":"","href":"/tags/security/","title":"Security"},{"content":"","href":"/tags/gnome/","title":"Gnome"},{"content":"I was looking at backup task running on my desk and saw that it\u0026rsquo;s spending a lot of time on ~/.local/share/zeitgeist directory. I checked and it had 4.6GB:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,4G activity.sqlite-wal 311M fts.index WTF? Fortunately I found here that I could easily delete some of this:\nzeitgeist-daemon --quit Now check that it\u0026rsquo;s not running:\nps axu | grep zeitgeist-daemon timor 9105 0.0 0.0 10756 2140 pts/5 S+ 19:01 0:00 grep --color=auto zeitgeist-daemon Nothing there so we may start deleting:\ncd ~/.local/share/zeitgeist rm -r fts.index rm activity.sqlite-wal rm activity.sqlite-shm Start zeitgeist-daemon again:\nzeitgeist-daemon --replace\u0026amp; Now check files sizes again:\ndu -sh ~/.local/share/zeitgeist/* 118M activity.sqlite 44M activity.sqlite.bck 32K activity.sqlite-shm 4,0K activity.sqlite-wal 640K fts.index Much better 😃\n","href":"/2016/02/zeitgeist-activity-sqlite-wal-getting-huge/","title":"Zeitgeist activity.sqlite-wal getting huge"},{"content":"","href":"/tags/nginx/","title":"nginx"},{"content":"There are many possible real life cases and not all optimization technics will be suitable for you but I hope it will be a good starting place.\nAlso you shouldn\u0026rsquo;t copy paste examples with faith that they will make your server fly 😃 You have to support your decisions with excessive tests and help of monitoring system (ex. Grafana).\nCache static and dynamic content Setting caching static and dynamic content strategy may offload your server from additional load from repetitive downloads of same, rarely updated files. This will make your site to load faster for frequent visitors.\nExample configuration:\nlocation ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf|m4v)$ { access_log off; log_not_found off; tcp_nodelay off; open_file_cache max=500 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; expires max; } For additional performance gain, you may:\n disable logging for static files, disable tcp_nodelay option - it\u0026rsquo;s useful to send a lot of small files (ideally smaller than single TCP packet - 1,5Kb), but images are rather big files and sending them all together will gain better performance, play with open_file_cache - it will take off some IO load, add long long expires.  Caching dynamic content is harder case. There are articles that are rarely updated and they may lay in cache forever but other pages are pretty dynamic and shouldn\u0026rsquo;t be cached for long. Even if caching dynamic content sounds scary for you it\u0026rsquo;s not. So called micro caching (caching for short period of time, like 1s) - is great solution for digg effect or slashdotting.\nLet say your page gets ten views per second and you will cache ever site for 1s, then you will be able to server 90% of requests from cache. Leaving precious CPU cycles for other tasks.\nCompress data On your page you should use filetypes that are efficiently compressed like: JPEG, PNG, MP3, etc. But all HTML, CSS, JS may be compressed too on the fly by web server, just enable options like that globally:\ngzip on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml image/svg+xml; You may also precompress these files stronger during build/deploy process and use gzip_static module to serve them without additional overhead for compression. Ex.:\ngzip_static on; Then use script like this to compress files:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; Files have to had same timestamp like original (not compressed) file to be used by Nginx.\nOptimize SSL/TLS New optimized versions of HTTP protocols like HTTP/2 or SPDY require HTTPS configuration (at least in browsers implementation). Then SSL/TLS high cost of every new HTTPS connection became crucial case for further optimizations.\nThere are few steps required for improved SSL/TLS performance.\nEnable SSL session caching Use ssl_session_cache directive to cache parameters used when securing each new connection, ex.:\nssl_session_cache builtin:1000 shared:SSL:10m; Enable SSL session tickets Tickets store information about specific SSL/TLS connection so connection may be reused without new handshake, ex.:\nssl_session_tickets on; Configure OCSP stapling for SSL This will lower handshaking time by caching SSL/TLS certificate informations. This is per site/certificate configuration, ex.:\nssl_stapling on; ssl_stapling_verify on; ssl_certificate /etc/ssl/certs/my_site_cert.crt; ssl_certificate_key /etc/ssl/private/my_site_key.key; ssl_trusted_certificate /etc/ssl/certs/authority_cert.pem; A ssl_trusted_certificate file have to point to trusted certificate chain file - root + intermediate certificates (this can be downloaded from your certificate provider site (sometimes you have to merge by yourself those files).\nExcessive article in this topic could be found here: https://raymii.org/s/tutorials/OCSP_Stapling_on_nginx.html\nImplement HTTP/2 or SPDY If you have HTTPS configured the only thing you have to do is to add two options on listen directive, ex.:\nlisten 443 ssl http2; # currently http2 is preferred against spdy;  # on SSL enabled vhost ssl on; You may also advertise for HTTP connection that you have newer protocol available, for that on HTTP connections use this header:\nadd_header Alternate-Protocol 443:npn-spdy/3; SPDY and HTTP/2 protocols use:\n headers compression, single, multiplexed connection (carrying pieces of multiple requests and responses at the same time) rather than multiple connection for every piece of web page.  After SPDY or HTTP/2 implementation you no longer need typical HTTP/1.1 optimizations like:\n domain sharding, resource (JS/CSS) merging, image sprites.  Tune other nginx performance options Access logs Disable access logs were you don\u0026rsquo;t need them, ex.: for static files. You may also use buffer and flush options with access_log directive, ex.:\naccess_log /var/log/nginx/access.log buffer=1m flush=10s; With buffer Nginx will hold that much data in memory before writing it to disk. flush tells Nginx how often it should write gathered logs to disk.\nProxy buffering Turning proxy buffering may impact performance of your reverse proxy.\nNormally when buffering is disabled, Nginx will pass response directly to client synchronously.\nWhen buffering is enable it will store response in memory set by proxy_buffer_size option and if response is too big it will be stored in temporary file.\nproxy_buffering on; proxy_buffer_size 16k; Keepalive for client and upstream connections] Every new connection costs some time for handshake and will add latency to requests. By using keepalive connections will be reused without this overhead.\nFor client connections:\nkeepalive_timeout = 120s; For upstream connections:\nupstream web_backend { server 127.0.0.1:80; server 10.0.0.2:80; keepalive 32; } Limit connections to some resources Some time users/bots overload your service by querying it to fast. You may limit allowed connections to protect your service in such case, ex.:\nlimit_conn_zone $binary_remote_addr zone=owncloud:1m; server { # ...  limit_conn owncloud 10; # ... } Adjust woker count Normally Nginx will start with only 1 worker process, you should adjust this variable to at the number of CPU\u0026rsquo;s, in case of quad core CPU use in main section:\nworker_processes 4; Use socket sharding In latest kernel and Nginx versions (at least 1.9.1) there is new feature of sockets sharding. This will offload management of new connections to kernel. Each worker will create a socket listener and kernel will assign new connections to them as they become available.\nlisten 80 reuseport; Thread pools Thread pools are solution for mostly long blocking IO operations that may block whole Nginx event queue (ex. when used with big files or slow storage).\nlocation / { root /storage; aio threads; } This will help a lot if you see many Nginx processes in D state, with high IO wait times.\nTune Linux for performance Backlog queue If you could see on your system connection that appear to be staling then you have to increase net.core.somaxconn. This system parameter describes the maximum number of backlogged sockets. Default is 128 so setting this to 1024 should be no big deal on any decent machine.\necho \u0026#34;net.core.somaxconn=1024\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf File descriptors If your system is serving a lot of connections you may get reach system wide open descriptor limit. Nginx uses up to two descriptors for each connection. Then you have to increase sys.fs.fs_max.\necho \u0026#34;sys.fs.fs_max=3191256\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf Ephemeral ports Nginx used as a proxy creates temporary (ephemeral) ports for each upstream server. On busy proxy servers this will result in many connection in TIME_WAIT state.\nSolution for that is to increase range of available ports by setting net.ipv4.ip_local_port_range. You may also benefit from lowering net.ipv4.tcp_fin_timeout setting (connection will be released faster, but be careful with that).\nUse reverse-proxy This with microcaching technic is worth separate article, I will add link here when it will be ready.\nSource:  http://www.fromdual.com/huge-amount-of-time-wait-connections https://www.nginx.com/blog/10-tips-for-10x-application-performance/ https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/ https://www.nginx.com/blog/thread-pools-boost-performance-9x/ https://tweaked.io/guide/kernel/ https://t37.net/nginx-optimization-understanding-sendfile-tcp_nodelay-and-tcp_nopush.html  ","href":"/2016/01/optimize-nginx-for-performance/","title":"Optimize Nginx for performance"},{"content":"","href":"/tags/xen/","title":"Xen"},{"content":"Sometime you need to make quick and dirty image backup of VM running on XenServer and this post is about such case 😃\nList machines:\nxl list Name ID Mem VCPUs State Time(s) Domain-0 0 4066 8 r----- 3526567.3 webfront1.example.com 1 4096 4 r----- 3186487.2 webfront2.example.com 2 2048 2 -b---- 920408.2 Now you may export one:\nxe vm-export vm=webfront1.example.com filename=/srv/backup/webfront.xva Export succeeded You may also use uuid for that - list machines with xe vm-list (best with less) and then:\nxe vm-export uuid=1234a43d-c5af-f1ef-b3c1-12347f63d84c filename=/srv/backup/webfront.xva That\u0026rsquo;s all!\n","href":"/2016/01/xenserver-export-vm-to-file/","title":"XenServer - export VM to file"},{"content":"","href":"/tags/curl/","title":"curl"},{"content":"","href":"/tags/nagios/","title":"Nagios"},{"content":"Sometimes deployment process or other havy task may cause some Nagios checks to rise below normal levels and bother admin. If this is expected and you want to add downtime on host/service during this task you may use this script:\n#!/bin/bash  function die { echo $1; exit 1; } if [[ $# -eq 0 ]] ; then die \u0026#34;Give hostname and time in minutes as parameter!\u0026#34; fi if [[ $# -eq 1 ]] ; then MINUTES=15 else MINUTES=$2 fi HOST=$1 NAGURL=http://nagios.example.com/nagios/cgi-bin/cmd.cgi USER=nagiosuser PASS=nagiospassword SERVICENAME=someservice COMMENT=\u0026#34;Deploying new code\u0026#34; export MINUTES echo \u0026#34;Scheduling downtime on $HOSTfor $MINUTESminutes...\u0026#34; # The following is urlencoded already STARTDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34;` # This gives us the date/time X minutes from now ENDDATE=`date \u0026#34;+%d-%m-%Y %H:%M:%S\u0026#34; -d \u0026#34;$MINUTESmin\u0026#34;` curl --silent --show-error \\  --data cmd_typ=56 \\  --data cmd_mod=2 \\  --data host=$HOST \\  --data-urlencode \u0026#34;service=$SERVICENAME\u0026#34; \\  --data-urlencode \u0026#34;com_data=$COMMENT\u0026#34; \\  --data trigger=0 \\  --data-urlencode \u0026#34;start_time=$STARTDATE\u0026#34; \\  --data-urlencode \u0026#34;end_time=$ENDDATE\u0026#34; \\  --data fixed=1 \\  --data hours=2 \\  --data minutes=0 \\  --data btnSubmit=Commit \\  --insecure \\  $NAGURL -u \u0026#34;$USER:$PASS\u0026#34;| grep -q \u0026#34;Your command request was successfully submitted to Nagios for processing.\u0026#34; || die \u0026#34;Failed to con tact nagios\u0026#34;; echo Scheduled downtime on nagios from $STARTDATE to $ENDDATE Threat this script as template with some tips:\n I you want to add downtime on service, then provide SERVICENAME and --data cmd_typ=56 \\. If you want downtime on whole host, just remove this line: --data-urlencode \u0026quot;service=$SERVICENAME\u0026quot; \\ and --data cmd_typ=86 \\ Another thing that in my example nagios page use basic auth for security, if your don\u0026rsquo;t use it, you may remove -u \u0026quot;$USER:$PASS\u0026quot; from parameters. If you get Start or end time not valid, then you have to adapt dates to your formats of dates accepted by Nagios (probably this depends on Nagios version or timezone configuration).  Sources http://stackoverflow.com/questions/6842683/how-to-set-downtime-for-any-specific-nagios-host-for-certain-time-from-commandli\n","href":"/2016/01/nagios-downtime-on-hostservice-from-command-line-with-curl/","title":"Nagios - downtime on host/service from command line with curl"},{"content":"","href":"/tags/collectd/","title":"CollectD"},{"content":"","href":"/tags/grafana/","title":"Grafana"},{"content":"Now when you have CollectD and InfluxDB installed you may configure Grafana 😃\nFirst configure repo with current Grafana version (select your distro):\ncurl https://packagecloud.io/gpg.key | sudo apt-key add - deb https://packagecloud.io/grafana/testing/debian/ wheezy main Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get update apt-get install -y apt-transport-https apt-get install -y grafana By default Grafana will use sqlite database to keep information about users, etc:\n[database] # Either \u0026#34;mysql\u0026#34;, \u0026#34;postgres\u0026#34; or \u0026#34;sqlite3\u0026#34;, it\u0026#39;s your choice ;type = sqlite3 ;host = 127.0.0.1:3306 ;name = grafana ;user = root ;password = If that\u0026rsquo;s ok for you, you may leave it as is. I prefer to configure MySQL database (create user, database, grant permissions to user):\n[database] type = mysql host = 127.0.0.1:3306 name = grafana user = grafana password = mydbpassword So Grafana should be running on port 3000 by default, now it\u0026rsquo;s time to connect ex.: http://localhost:3000 (use your host). Now click Data sources on left panel, then Add new on top panel and fill source data like below:\nBecause we didn\u0026rsquo;t set authorization for InfluxDB you may just type whatever login/password there. Now Test Connection and Save and you should be ready to play with Grafana.\nI also used scripted dashboard for Grafana to add easily statistics for my hosts, you may find it here: https://github.com/anryko/grafana-influx-dashboard\nSources http://docs.grafana.org/installation/debian/\n","href":"/2016/01/grafana-installation-and-configuraton-with-influxdb-and-collectd-on-debian-ubuntu/","title":"Grafana - installation and configuraton with InfluxDB and CollectD on Debian/Ubuntu"},{"content":"","href":"/tags/influxdb/","title":"InfluxDB"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nLet\u0026rsquo;s start! On project page there is no info about repo but it\u0026rsquo;s available, configure it:\ncurl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/debian wheezy stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf for Ubuntu use url like (of course selecting your version):\necho \u0026#34;deb https://repos.influxdata.com/ubuntu wily stable\u0026#34; \u0026gt; /etc/apt.sources.list.d/influxdb.conf Now install package (on wheezy I needed to install apt-transport-https to allow installation of packages from repo via HTTPS):\napt-get install -y apt-transport-https apt-get install -y influxdb Now edit /etc/influxdb/influxdb.conf and uncoment/fill [collectd] section like this:\n[collectd] enabled = true bind-address = \u0026#34;:8096\u0026#34; database = \u0026#34;collectd_db\u0026#34; typesdb = \u0026#34;/usr/share/collectd/types.db\u0026#34; You may adjust port to whatever suits you best. database sets InfluxDB database used to store collectd data, and typesdb is file from collectd package defining collectd metrics structure (this is location for Debian) - so you have collectd service installed earlier.\nNow you may check if InfluxDB is working fine by connecting to web admin panel, by standard on port 8083.\nSources https://github.com/influxdata/influxdb/issues/585\u0026quot;\nhttps://anomaly.io/collectd-metrics-to-influxdb/\n","href":"/2016/01/influxdb-installation-and-configuration-on-debianubuntu/","title":"InfluxDB - installation and configuration on Debian/Ubuntu"},{"content":"I wanted/needed some statistics on few my machines. I saw earlier grafana and was impressed so this was starting point. Then I started reading about graphite, carbon and whisper, and then… I found InfluxDB. Project is young but looks promising.\nInstallation of collectd is easy on Debian because packages are in default repo. One problem is that packages may be old, ex. on wheezy it version 5.1. But in backports/backports-sloppy you may find current 5.5, so enable backports first:\necho \u0026#34;deb http://http.debian.net/debian wheezy-backports main contrib non-free\u0026#34; \u0026gt; /etc/apt/sources.list.d/backports.list echo \u0026#34;deb http://http.debian.net/debian wheezy-backports-sloppy main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/backports.list Install package:\napt-get update apt-get install -y -t backports-sloppy collectd collectd-utils # or on recent system just apt-get install -y collectd collectd-utils Now edit configuration /etc/collectd/collectd.conf and add network section:\nLoadPlugin network \u0026lt;Plugin \u0026#34;network\u0026#34;\u0026gt; Server \u0026#34;localhost\u0026#34; \u0026#34;8096\u0026#34; \u0026lt;/Plugin\u0026gt; Use your InfluxDB hostname:port.\nNow select and add enable some plugins - list here and restart service:\nservice collectd restart That\u0026rsquo;s all - now install InfluxDB.\nSources https://anomaly.io/collectd-metrics-to-influxdb/\nhttp://backports.debian.org/Instructions/\n","href":"/2016/01/collectd-installation-and-configuration-with-influxdb-on-debianubuntu/","title":"CollectD - installation and configuration with InfluxDB on Debian/Ubuntu"},{"content":"","href":"/tags/letsencrypt/","title":"LetsEncrypt"},{"content":"From the first moment I heard about Let\u0026rsquo;s Encrypt I liked it and wanted to use it as fast as possible. But the more I read how they want to implement it, the more I dislike it.\nCurrent project with automatic configuration is not what I want to use at all. I have many very complicated configs and I do not trust such tools enough to use them. I like UNIX\u0026rsquo;s single purpose principle, tools should do one thing and do it well - nothing more.\nBut there is one neet tool that use Let\u0026rsquo;s Encrypt API only leaving all configuration for me, it\u0026rsquo;s acme-tiny python based script. I won\u0026rsquo;t copy/paste examples - documentation is written pretty well.\n","href":"/2016/01/lets-encrypt-without-auto-configuration/","title":"Let’s Encrypt - without auto configuration"},{"content":"","href":"/tags/openssl/","title":"OpenSSL"},{"content":"","href":"/tags/fail2ban/","title":"fail2ban"},{"content":"Lately I had a lot of brute force attacks on my WordPress blog. I used basic auth to /wp-admin part in nginx configuration to block this and as a better solution I wan\u0026rsquo;t to block source IPs at all on firewall.\nTo do this, place this filter code in /etc/fail2ban/filter.d/wp-login.conf:\n# WordPress brute force wp-login.php filter: # # Block IPs trying to authenticate in WordPress blog # # Matches e.g. # 178.218.54.109 - - [31/Dec/2015:10:39:34 +0100] \u0026#34;POST /wp-login.php HTTP/1.1\u0026#34; 401 188 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0\u0026#34; # [Definition] failregex = ^\u0026lt;HOST\u0026gt; .* \u0026#34;POST /wp-login.php ignoreregex = Then edit your /etc/fail2ban/jail.local and add:\n[wp-login] enabled = true port = http,https filter = wp-login logpath = /var/log/nginx/access.log maxretry = 3 Now restart fail2ban:\nservice fail2ban restart All done 😄\n","href":"/2015/12/fail2ban-block-wp-login-php-brute-force-attacks/","title":"fail2ban - block wp-login.php brute force attacks"},{"content":"","href":"/tags/ansible/","title":"Ansible"},{"content":"I have some Ansible roles to configure my vps, Raspberry Pi, etc. I like to test them before I broke something on my real, not clustered machines - I use Vagrant for that.\nBut with it I had one problem - in playbooks I define hosts as groups of severs ex. web for my vps:\n- hosts: web gather_facts: True sudo: True ... But testing machine wasn\u0026rsquo;t in this group and when I run vagrant I could only see:\n$ vagrant provision ==\u0026gt; default: Running provisioner: fix-no-tty (shell)... default: Running: inline script ==\u0026gt; default: Running provisioner: ansible... PLAY [web] ******************************************************************** skipping: no hosts matched PLAY RECAP ******************************************************************** To get rid of this I have to add default vagrant machine to my default group in Vagrantfile:\nconfig.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.groups = { \u0026#34;web\u0026#34; =\u0026gt; [\u0026#34;default\u0026#34;] } ansible.sudo = true ansible.limit = \u0026#34;all\u0026#34; ansible.playbook = \u0026#34;web.yml\u0026#34; end And that solved my problem 😄\n","href":"/2015/12/ansible-on-vagrant-skipping-no-hosts-matched/","title":"Ansible on Vagrant - skipping: no hosts matched"},{"content":"","href":"/tags/apache/","title":"Apache"},{"content":"Normally you want dynamic content to be fresh and not catchable. But sometimes it may be useful to cache it, like when you have website behind reverse proxy. To do this try something like this:\n\u0026lt;filesmatch \u0026#34;\\.(php|cgi|pl)$\u0026#34;\u0026gt; Header unset Pragma Header unset Expires Header set Cache-Control \u0026#34;max-age=3600, public\u0026#34; \u0026lt;/filesmatch\u0026gt; Sources http://www.askapache.com/htaccess/speed-up-your-site-with-caching-and-cache-control.html\n","href":"/2015/12/apache-force-caching-dynamic-php-content-with-mod_headers/","title":"Apache - Force caching dynamic PHP content with mod_headers"},{"content":"","href":"/tags/vagrant/","title":"Vagrant"},{"content":"","href":"/tags/mariadb/","title":"MariaDB"},{"content":"It will happen from time to time, that you\u0026rsquo;re on alien machine and have to brutally update things in db without knowing credentials. Example is for root (quite secure candidate to change because it shouldn\u0026rsquo;t be used in app 😃 ) but will work for any user.\n shutdown db  service mysql stop  create text file with command like this (update user accordingly) ex. in /tmp/pwchange.txt  SET PASSWORD FOR \u0026#34;root\u0026#34;@\u0026#34;localhost\u0026#34; = PASSWORD(\u0026#34;HereYourNewPassword\u0026#34;);  start mysqld with --init-file param  mysqld_safe --init-file=/tmp/pwchange.txt sometimes you may require to point configuration file ex. --defaults-file=/etc/mysql/my.cnf\n wait until it loads and kill mysql (ex. Ctrl+C / kill / etc) start mysql  service mysql start  delete file with password  rm -f /tmp/pwchange.txt You should be able to login with updated password.\nSources https://dev.mysql.com/doc/refman/5.5/en/resetting-permissions.html\n","href":"/2015/12/mysql-reset-root-password/","title":"MySQL - reset root password"},{"content":"I hate movies recorded on phone in vertical position. This just short tip how I dealt with with it last time:\nfor m in *.mp4 do avconv -i $m -vf \u0026#34;transpose=1\u0026#34; -codec:a copy -codec:v libx264 -preset slow -crf 23 rotated-$m done Other examples:\nhttp://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg\nhttp://superuser.com/questions/578321/how-to-flip-a-video-180°-vertical-upside-down-with-ffmpeg\n","href":"/2015/12/rotate-movies/","title":"Rotate movies"},{"content":"I had some passwords saved in remmina but like it always happen, I wasn\u0026rsquo;t been able to remember them when needed. Trying to restore them I found that they\u0026rsquo;re encrypted in .remmina directory.\nThen I used this script to the decrypt them:\nimport base64 from Crypto.Cipher import DES3 secret = base64.decodestring(\u0026#34;\u0026lt;STRING FROM remmina.prefs\u0026gt;\u0026#34;) password = base64.decodestring(\u0026#34;\u0026lt;STRING FROM XXXXXXX.remmina\u0026gt;\u0026#34;) print DES3.new(secret[:24], DES3.MODE_CBC, secret[24:]).decrypt(password) Sources http://askubuntu.com/questions/290824/how-to-extract-saved-password-from-remmina\n","href":"/2015/12/extract-password-saved-in-remmina/","title":"Extract password saved in remmina"},{"content":"After long break I\u0026rsquo;m thinking about writing more on my blog. I was reviewing my favorites/bookmarks and half of them was broken, so I can\u0026rsquo;t rely on them in case of knowledge management.\nI think I will write shorter, less descriptive articles just to be pointers to useful solutions from past.\n","href":"/2015/12/im-back/","title":"I’m back"},{"content":"Allow from IP without password prompt, and also allow from any address with password prompt\nOrder deny,allow Deny from all AuthName \u0026#34;htaccess password prompt\u0026#34; AuthUserFile /web/askapache.com/.htpasswd AuthType Basic Require valid-user Allow from 172.17.10.1 Satisfy Any Sources http://www.askapache.com/htaccess/apache-authentication-in-htaccess.html\n","href":"/2015/12/apache-authbasic-but-excluding-ip/","title":"Apache AuthBasic but excluding IP"},{"content":"When configuring RAID it\u0026rsquo;s quite important to have the same partition tables on every disk. I\u0026rsquo;v done this many times on msdos partition tables like this:\nsfdisk -d /dev/sda | sfdisk /dev/sdb but it\u0026rsquo;s not working any more on GPT partition tables. Hopefully it still can be done but with different toolstack 😄\nInstall gdisk:\napt-get install -y gdisk Then use sgdisk like this:\nsgdisk -R /dev/sd_dest /dev/sd_src sgdisk -G /dev/sd_dest First command will copy partition from /dev/sd_src to /dev/sd_dest. Second will randomize partition UUID\u0026rsquo;s - needed only if you want to use disks in same machine (this is my case).\n","href":"/2014/07/copy-gtp-partiotion-table-between-disks/","title":"Copy GTP partiotion table between disks"},{"content":"","href":"/tags/django/","title":"Django"},{"content":"There is need plugin for Django, named django-debug-toolbar but it needs some time to configure. So when I need simple way to debug SQL queries I use small hack. Add to your settings.py:\nLOGGING = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;handlers\u0026#39;: { \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, } }, \u0026#39;loggers\u0026#39;: { \u0026#39;django.db.backends\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;], \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, }, } } To get this working DEBUG option have to be set to True:\nDEBUG = True After this setup, when you run you app in development mode:\n./manage.py runserver you will see SQL queries in console output.\nSources https://docs.djangoproject.com/en/1.6/topics/logging/#examples\n","href":"/2014/05/quickly-setup-sql-query-logging-on-console-in-django/","title":"Quickly setup SQL query logging on console in Django"},{"content":"","href":"/tags/centos/","title":"CentOS"},{"content":"On Debian in default installation you have different configuration files for PHP in Apache, FPM, CLI, etc. But on CentOS you have only one php.ini for all of them. In case I have, I need to have different configuration file for scripts running in CLI mode (more memory, etc). I could run it like this:\nphp -c /etc/php-cli.ini script.php But this a little burdensome. So I do it like this:\ncat \u0026gt; /etc/profile.d/php-cli-ini.sh \u0026lt;\u0026lt;SCRIPT #!/bin/bash alias php=\u0026#34;php -c /etc/php-cli.ini\u0026#34; SCRIPT cp /etc/php.ini /etc/php-cli.ini Logout, login and now every user can run PHP scripts in CLI with different configuration - exactly what I need :)\n","href":"/2014/05/changing-default-php-ini-file-for-php-cli-on-centos/","title":"Changing default php.ini file for PHP-CLI on CentOS"},{"content":"Everybody knows passwd command but it\u0026rsquo;s useless when you need to change ex. root password from command line without waiting for input. In such case oneliner below could help:\necho \u0026#34;root:new_password\u0026#34; | chpasswd ","href":"/2014/05/command-to-change-root-password/","title":"Command to change root password"},{"content":"These are few steps to get Steam running on Ubuntu:\nwget -c media.steampowered.com/client/installer/steam.deb dpkg -i steam.deb apt-get install -f apt-get update Solutions for some issues Some time ago I needed 32 bit flash even on 64 bit system - I don\u0026rsquo;t need it currently but I\u0026rsquo;m living this as a tip.\napt-get install adobe-flashplugin:i386 After Ubuntu upgrade I was unable to run Steam anymore - It shouted on me with strange \u0026ldquo;networking problem\u0026rdquo;. I have to clean Steam configuration with:\nsteam --reset Sources http://linuxg.net/how-to-install-the-latest-steam-client-available-on-ubuntu-13-10-13-04-12-10-12-04-and-linux-mint-15-14-13/\nhttp://askubuntu.com/questions/353522/why-is-steam-not-able-to-connect-steam-network\n","href":"/2014/04/install-steam-on-debian-ubuntu/","title":"Install Steam on Debian/Ubuntu"},{"content":"","href":"/tags/fedora/","title":"Fedora"},{"content":"When I was trying to update packages on one host I\u0026rsquo;ve stuck with yum hung on update. I run strace and see:\nstrace -p 43734 Process 43734 attached - interrupt to quit futex(0x807c938, FUTEX_WAIT, 1, NULL \u0026lt;unfinished ...\u0026gt; Process 43734 detached It looks like yum database was corrupted, to repair this run:\nrm -f /var/lib/rpm/__db* rpm --rebuilddb yum clean all yum update Instead rm on db-files you could use gzip to have backup of these files.\n","href":"/2014/04/rebuild-yum-rpm-database/","title":"Rebuild yum/rpm database"},{"content":"","href":"/tags/red-hat/","title":"Red Hat"},{"content":"I\u0026rsquo;ve few Nagios checks that require root privileges but running nrpe as root user is not acceptable. I prefer to use sudo for only these few commands.\nRun visudo and coment out this line:\n#Defaults requiretty This change is crucial to get scripts working.\nThen add at the end of file:\n%nrpe ALL=(ALL) NOPASSWD: /usr/lib64/nagios/plugins/ I\u0026rsquo;ve used nrpe group, but you have to add exactly group that your nrpe process uses.\nNow you should be able to run checks as root - edit /etc/nagios/nrpe.cfg and add check like this:\ncommand[check_as_root]=/usr/bin/sudo /usr/lib64/nagios/plugins/check_with_root_privileges ","href":"/2014/03/nagios-run-checks-as-root-with-nrpe/","title":"Nagios - run checks as root with NRPE"},{"content":"","href":"/tags/seo/","title":"SEO"},{"content":"After reading some SEO stuff I wanted to add some meta tags to my WordPress blog. I found this site: codex.wordpress.org/Meta_Tags_in_WordPress.\nSo WordPress thinks that it\u0026rsquo;s not necessary to have this meta tags any more\u0026hellip; But I want it! 😃 Next funny thing is how they suggest to add meta tags: copy header.php - what about theme updates?\nI prefer to use functions.php file - just create it in your courrent theme directory with such content:\n\u0026lt;?php // this will remove WordPress version from header remove_action(\u0026#39;wp_head\u0026#39;, \u0026#39;wp_generator\u0026#39;); // handler function for adding custom tags function custom_header_meta() { ?\u0026gt;\u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;Tomasz Gągor\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;contact\u0026#34; content=\u0026#34;my@mail.com\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;\u0026lt;?php wp_title( \u0026#39;|\u0026#39;, true, \u0026#39;right\u0026#39; ); ?\u0026gt;\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;some keywords\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;verify-v1\u0026#34; content=\u0026#34;google webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;bing webmaster identification\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;fb:admins\u0026#34; content=\u0026#34;facebook identificatio\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;copyright\u0026#34; content=\u0026#34;Copyright (c)1997-2014 Tomasz Gągor. All Rights Reserved.\u0026#34; /\u0026gt; \u0026lt;?php } // running handler function add_action(\u0026#39;wp_head\u0026#39;, \u0026#39;custom_header_meta\u0026#39;); ","href":"/2014/03/wordpress-add-meta-tags-author-description-keywords-etc/","title":"WordPress - add meta tags: author, description, keywords, etc"},{"content":"","href":"/tags/mediawiki/","title":"MediaWiki"},{"content":"Let say you have MediaWiki installation but you lost admin credentials. If you have other account or if you could create one without any rights we\u0026rsquo;re in home 😉\nWe have few options to do this.\nReset admin password We have to connect to database and use this SQL:\nUPDATE `user` SET user_password = CONCAT( SUBSTRING(user_password, 1, 3), SUBSTRING(MD5(user_name), 1, 8), \u0026#39;:\u0026#39;, MD5(CONCAT(SUBSTRING(MD5(user_name), 1, 8), \u0026#39;-\u0026#39;, MD5(\u0026#39;new password\u0026#39;)))) WHERE user_name = \u0026#39;Admin\u0026#39;; Just replace Admin with your username and new password with your password.\nRaise another user rights If we don\u0026rsquo;t want to mess with admin account we could raise permissions for other user, ex. Tom:\nINSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;bureaucrat\u0026#39;); INSERT INTO user_groups (ug_user, ug_group) VALUES( SELECT user_id FROM user WHERE user_name = \u0026#39;Tom\u0026#39;, \u0026#39;sysop\u0026#39;); Sources http://www.mediawiki.org/wiki/Manual_talk:Resetting_passwords\nhttp://www.mediawiki.org/wiki/Manual_talk:AdminSettings.php\n","href":"/2014/03/mediawiki-recover-admin-rights/","title":"Mediawiki - recover admin rights"},{"content":"I need to check memory usage of memcached server so I used:\necho stats | nc 127.0.0.1 11211 STAT pid 2743 STAT uptime 263 STAT time 1395438951 STAT version 1.4.13 STAT pointer_size 64 STAT rusage_user 0.482926 STAT rusage_system 2.675593 STAT curr_items 8667 STAT total_items 10742 STAT bytes 23802513 STAT curr_connections 296 STAT total_connections 399 STAT connection_structures 297 STAT cmd_flush 0 STAT cmd_get 52578 STAT cmd_set 10792 STAT get_hits 28692 STAT get_misses 23886 STAT evictions 0 STAT bytes_read 35984361 STAT bytes_written 192647437 STAT limit_maxbytes 536870912 STAT threads 2 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT replication MASTER STAT repcached_qi_free 8189 STAT repcached_wdata 0 STAT repcached_wsize 1026048 END For me, bytes value was important but you could find more about all statistics here.\n","href":"/2014/03/checking-memcached-status/","title":"Checking memcached status"},{"content":"","href":"/tags/memcached/","title":"memcached"},{"content":"","href":"/tags/postfix/","title":"postfix"},{"content":"I have development server with postfix - I wanted to allow outbound traffic to one domain but cut off all the rest. I definitely do not want that test mail or any debug info goes to service users.\nI have to add something like that to /etc/postfix/transport:\nallowed.domain.com : * discard: Then run:\npostmap /etc/postfix/transport At end, add these to /etc/postfix/main.cf:\ntransport_maps = hash:/etc/postfix/transport Reload postfix:\npostfix reload Test if it works:\necho test | mail -s test whatever@whatever.com You should see in logs that message was dropped:\nMar 18 21:14:28 devmx1 postfix/cleanup[29968]: 1E77654391: message-id=20140318211428.1E77280521@domain.com\u0026gt; Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: from=\u0026lt;root@domain.com\u0026gt;, size=431, nrcpt=1 (queue active) Mar 18 21:14:28 devmx1 postfix/discard[29970]: 1E77654391: to=\u0026lt;whatever@whatever.com\u0026gt;, relay=none, delay=0.1, delays=0.09/0.01/0/0, dsn=2.0.0, status=sent (whatever.com) Mar 18 21:14:28 devmx1 postfix/qmgr[28282]: 1E77654391: removed ","href":"/2014/03/postfix-automatically-drop-outbound-mail/","title":"Postfix - automatically drop outbound mail"},{"content":"In recent Ansible update to 1.5 version there is really nice feature ssh pipelining. This option is serious alternative to accelerated mode.\nJust add to you config file (ex. ~/.ansible.cfg):\n[ssh_connection] pipelining=True Now run any playbook - you will see the difference 😄\nSource (and extended info about):\nhttp://blog.ansibleworks.com/2014/01/15/ssh-connection-upgrades-coming-in-ansible-1-5/\n","href":"/2014/03/ansible-ssh-pipelining/","title":"Ansible - ssh pipelining"},{"content":"","href":"/tags/devops/","title":"DevOps"},{"content":"To najlepszy przepis na chrusty jaki znam - wychodzą bardzo kruche i delikatne.\nSkładniki  4 zółtka, 4 łyżki wina białego lub czerwonego, 4 łyżki mąki.  Sposób przygotowania Wszystkie składniki wymieszać i wyrobić. Ciasto powinno być mniej wiecej takie jak na pierogi. Bić pałką/wałkiem, składać na pół i tak kilka razy przez ok 5 minut. Potem ciasto rozwałkować bardzo cieniutko i wykrawać chrusty, małe bo mocno rosną. Następnie wrzucać na rozgrzany olej/smalec.\n","href":"/2014/02/chrusty-faworki/","title":"Chrusty, faworki"},{"content":"","href":"/categories/gotowanie/","title":"Gotowanie"},{"content":"I had quite simple task - compare two lists of hosts and check if hosts from first one are also on the second one. I started with diff:\ndiff -u biglist.txt hosts_to_check.txt | grep -E \u0026#34;^\\+\u0026#34; It was fine but output needs some filtering to get what I want.\nI\u0026rsquo;ve found another example with grep:\ngrep -Fxv -f biglist.txt hosts_to_check.txt | sort -n This will search for all lines in hosts_to_check.txt which don\u0026rsquo;t match any line in biglist.txt. So after this I\u0026rsquo;ve got list of hosts that I have to check. That\u0026rsquo;s exactly what I need 😄\n","href":"/2014/02/comparing-two-lists-in-bash/","title":"Comparing two lists in bash"},{"content":"","href":"/tags/active-directory/","title":"Active Directory"},{"content":"After WSUS installing on Windows Server 2012 I discovered that it\u0026rsquo;s running on port 8530, different than on older version of Windows (it was using port 80 from beginning). But what\u0026rsquo;s more interesting it was running ONLY on IPv6 interface! Switching binding configuration in IIS doesn\u0026rsquo;t help.\nI could stand switching port - it\u0026rsquo;s nothing hard with GPO, but IPv6 only configuration was not acceptable.\nAfter googling for some time I found one command that solved my problems by switching WSUS to older behavior and run it on port 80 (on default website).\nJust run on elevated command line:\nC:\\Program Files\\Update Services\\Tools\\WSUSutil usecustomwebsite false After half a minute WSUS was working like a charm 😃\nSources http://social.technet.microsoft.com/Forums/windowsserver/en-US/88514e56-1179-4af7-9f5e-5339d3e750a5/how-to-change-wsus-2012-port-to-80?forum=winserverwsus\nhttp://community.spiceworks.com/topic/160971-how-do-you-change-the-port-number-for-your-wsus\n","href":"/2014/01/change-default-wsus-port-from-8530-to-80-on-windows-server-2012/","title":"Change default WSUS port from 8530 to 80 on Windows Server 2012"},{"content":"After reading some good opinions about MariaDB I wanted to give it a try. Upgrade looks quite straight forward but I found some issues a little tricky.\nInstallation Add repo and key:\ncat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main deb-src http://mirrors.supportex.net/mariadb/repo/5.5/debian wheezy main SRC (find more repositories here)\nNow install MariaDB:\nsudo apt-get update sudo apt-get install mariadb-server It could be better to install mariadb-server-5.5 and mariadb-client-5.5 package instead, because of this error.\nMariaDB repo pinning Some time after installation I have problem with newer packages from Debian repositories that upgraded my MariaDB installation back to MySQL - it\u0026rsquo;s described here, so I used pinning to resolve that.\ncat \u0026gt; /etc/apt/preferences.d/mariadb.pref \u0026lt;\u0026lt;PIN Package: * Pin: origin mirrors.supportex.net Pin-Priority: 1000 PIN Results Before migration to MariaDB, front page of my blog needs about 650 ms to generate. After switch, it was only about 550ms. So it\u0026rsquo;s about 15% - absolutely for free 😄\n","href":"/2014/01/debian-upgrade-mysql-to-mariadb/","title":"Debian - Upgrade MySQL to MariaDB"},{"content":"I was thinking about allowing access to my website using SPDY protocol for better performance and security (and for fun of course 😃 ). But SPDY have one disadvantage - you need SSL certificate signed by known authority that will verfiy in common browsers. So you can\u0026rsquo;t use self signed certificates because everyone will see a warning entering your site. Certs are quite expensive so I started searching for free one and to my surprise I found such!\nI found these two sites where you can generate freeware certificates for your website:\n https://www.startssl.com/ (I prefer this one because it better recognized) https://www.cacert.org  I wouldn\u0026rsquo;t trust these certification authorities enough to use it for: access my mail or other private data. But I\u0026rsquo;m fine with using it for my public websites (like my blog) to gain speed from SPDY.\nConfiguring cert Fetch the Root CA and Class 1 Intermediate Server CA certificates:\nwget http://www.startssl.com/certs/ca.pem wget http://www.startssl.com/certs/sub.class1.server.ca.pem Create a unified certificate from your certificate and the CA certificates:\ncat ssl.crt sub.class1.server.ca.pem ca.pem \u0026gt; /etc/nginx/conf/ssl-unified.crt Enable SPDY Configure your nginx server to use the new key and certificate (in the global settings or a server section):\nssl on; ssl_certificate /etc/nginx/conf/ssl-unified.crt; ssl_certificate_key /etc/nginx/conf/ssl.key; Then enable SPDY like that:\nserver { listen your_ip:80; listen your_id:443 default_server ssl spdy; # other stuff } Advertise SPDY protocol Now advertise SPDY with Alternate-Protocol header - add this clause in main location:\nadd_header Alternate-Protocol \u0026#34;443:npn-spdy/2\u0026#34;; Have fun with SPDY on your site 😄\n","href":"/2014/01/nginx-enabling-spdy-with-freeware-certificate/","title":"Nginx - enabling SPDY with freeware certificate"},{"content":"I\u0026rsquo;ve been using different code editors for different purposes. Gedit was fine for small scripts but not for bigger projects. It lacks intelligent code completion (function/class names, etc.). I was searching for convenient editor for Python, Perl, Ruby with support for frameworks like Django, Rails, etc. I know Sublime Text - but it\u0026rsquo;s paid. There is LimeText - open source clone, but it\u0026rsquo;s not ready to be used on daily basics.\nI found Brackets - open source editor designed by Adobe. I\u0026rsquo;m testing it right now.\nBrackets installation on Ubuntu sudo add-apt-repository ppa:webupd8team/brackets sudo apt-get update sudo apt-get install brackets Source http://www.webupd8.org/2013/11/install-brackets-in-ubuntu-via-ppa-open.html\nI was using Brackets for some time and while it\u0026rsquo;s really nice editor - it\u0026rsquo;s mostly designed for webmasters (writing web apps). But I also need to write in Ruby, Python, Perl and many other - then Github announced Atom editor. I switched to Atom - it\u0026rsquo;s similar to Brackets but covers my interests better 🙂\nAtom installation on Ubuntu Similarly to Brackets installation. Add ppa and install:\nsudo add-apt-repository ppa:webupd8team/atom sudo apt-get update sudo apt-get install -y atom Atom plugins I use Atom is great because of all plugins available - my favorite are:\n language-docker (Dockerfile syntax highlighting) language-terraform (Terraform files syntax highlighting) language-python autocomplete-python (with Kite) linter-foodcritic linter-python-pep8 linter-rubocop minimap tabs-to-spaces  Atom themes I use Default dark theme in Atom is really nice - but there are better themes. My favorite are:\n UI Theme: seti-ui Syntax theme: Monokai  ","href":"/2014/01/searching-for-better-code-editor/","title":"Searching for better code editor"},{"content":"","href":"/tags/spdy/","title":"SPDY"},{"content":"","href":"/tags/windows/","title":"Windows"},{"content":"","href":"/tags/wsus/","title":"WSUS"},{"content":"After connecting few computers with Windows 8.1 to domain we found that these computers are not recognized or recognized as Windows 6.3 (which is true) on WSUS 3.0 running on Windows Server 2008. The bad thing was that they can\u0026rsquo;t properly report to WSUS and get updates from it.\nI found that there are two updates that have to be installed (but they\u0026rsquo;re not working without additional steps):\n http://support.microsoft.com/kb/2720211 http://support.microsoft.com/kb/2734608  After installation of second update there are two additional steps that have to be performed to get WSUS working:\n Reindex the WSUS Database Use the Server Cleanup Wizard - this one is trivial, so I hope you get this right  Reindex the WSUS Database To do this perform these steps:\n Copy sript from this site to file named WsusDBMaintenance.sql Install sqlcmd from this site - search for file named like \u0026ldquo;SQLServer2005_SQLCMD\u0026rdquo; with proper architecture (x86/amd64/ia64) run: sqlcmd -S np:\\\\.\\pipe\\MSSQL$MICROSOFT##SSEE\\sql\\query -i C:\\path to script saved in first point\\WsusDBMaintenance.sql  Use WSUS Server Cleanup Wizard  Done. Your WSUS will not recognize 8.1 clients but will work with them and serve updates.\nSources http://social.technet.microsoft.com/Forums/en-US/559fe878-e2a2-4ec6-9d91-55ea1b67caef/manage-windows-81-windows-server-2012-r2-on-wsus-30?forum=winserverwsus\n","href":"/2014/01/manage-windows-8-1-and-windows-server-2012-r2-in-wsus-3-0/","title":"Manage Windows 8.1 and Windows Server 2012 R2 in WSUS 3.0"},{"content":"I love Shotwell for it\u0026rsquo;s simplicity and easy export to Piwigo. After Christmas I added new photos to my library but after that I made some modifications to them (red eye reduction, etc\u0026hellip;). Because Shotwell generate thumbnails only on import, all my modifications were not visible on preview.\nI\u0026rsquo;ve started searching how to regenerate thumbs and found this info. There were two issues with this method:\n this howto was for old version (with old paths) and only for 128px thumbs I definitely don\u0026rsquo;t want to regenerate thumbnails for 40k photos!  After some tweaking this will do work for thumbnails from last month (enough for me):\nsqlite3 ~/.local/share/shotwell/data/photo.db \\  \u0026#34;select id||\u0026#39; \u0026#39;||filename from PhotoTable where date(timestamp,\u0026#39;unixepoch\u0026#39;,\u0026#39;localtime\u0026#39;) \u0026gt; date(\u0026#39;now\u0026#39;,\u0026#39;start of month\u0026#39;,\u0026#39;-1 month\u0026#39;) order by timestamp desc\u0026#34; | while read id filename; do tf1=$(printf ~/.local/share/shotwell/thumbs/thumbs128/thumb%016x.jpg $id); tf2=$(printf ~/.local/share/shotwell/thumbs/thumbs360/thumb%016x.jpg $id); test -e \u0026#34;$tf\u0026#34; || { echo -n \u0026#34;Generating thumb for $filename\u0026#34;; convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 128x128 $tf1 convert \u0026#34;$filename\u0026#34; -auto-orient -thumbnail 360x360 $tf2 echo } done Remember to install imagemagick:\napt-get install imagemagick ","href":"/2014/01/regenerate-thumbnails-in-shotwell-for-last-month/","title":"Regenerate thumbnails in Shotwell 0.15 (for last month)"},{"content":"","href":"/tags/javascript/","title":"Javascript"},{"content":"Few days ago I\u0026rsquo;ve read a book ‘Even Faster Web Sites‘ about websites optimisation and I found one thing usefuluseful, not only on websites. There was a small tip about looploop unlooping. I want to quote them for later use.\nFirst - with switch statement var iterations = Math.ceil(values.length / 8); var startAt = values.length % 8; var i = 0; do { switch(startAt) { case 0: process(values[i++]); case 7: process(values[i++]); case 6: process(values[i++]); case 5: process(values[i++]); case 4: process(values[i++]); case 3: process(values[i++]); case 2: process(values[i++]); case 1: process(values[i++]); } startAt = 0; } while(--iterations \u0026gt; 0); Second - without switch var iterations = Math.floor(values.length / 8); var leftover = values.length % 8; var i = 0; if(leftover \u0026gt; 0) { do { process(values[i++]); } while(--leftover \u0026gt; 0); } do { process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); process(values[i++]); } while (--iterations \u0026gt; 0); I found second example more readable and I prefer it.\nThese examples after translation could be easily used in other scripting languages.\n","href":"/2014/01/loop-unlooping-in-javascript/","title":"Loop unlooping in Javascript"},{"content":"Some time ago I write article about tracking nicknames of users (from comments) on a WordPress blog with Piwik. This time I\u0026rsquo;m doing same but for Google Analytics.\nI\u0026rsquo;m using Google Analytics plugin for WordPress so I\u0026rsquo;ve edited googleanalytics.php file to add some additional code for user tracking:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _gaq = _gaq || []; _gaq.push([\u0026#39;_setAccount\u0026#39;, \u0026#39;UA-YOUR-UNIQ-NUMBER\u0026#39;]); _gaq.push([\u0026#39;_setCustomVar\u0026#39;, 1, \u0026#39;Nickname\u0026#39;, comment_author, 1]); _gaq.push([\u0026#39;_trackPageview\u0026#39;]); (function() { var ga = document.createElement(\u0026#39;script\u0026#39;); ga.type = \u0026#39;text/javascript\u0026#39;; ga.async = true; ga.src = (\u0026#39;https:\u0026#39; == document.location.protocol ? \u0026#39;https://ssl\u0026#39; : \u0026#39;http://www\u0026#39;) + \u0026#39;.google-analytics.com/ga.js\u0026#39;; var s = document.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; s.parentNode.insertBefore(ga, s); })(); \u0026lt;/script\u0026gt; Source https://developers.google.com/analytics/devguides/collection/gajs/gaTrackingCustomVariables?hl=pl\n","href":"/2014/01/tracking-users-by-nickname-on-wordpress-using-google-analytics/","title":"Tracking users by nickname on WordPress using Google Analytics"},{"content":"Some time ago I\u0026rsquo;ve show how to precompress js and css file with gzip to be available for Nginx\u0026rsquo;s mod_gzip. In default configuration Apache don\u0026rsquo;t have such module but similar functionality could be achieved with few custom rewirtes.\nBasically we will start with these rewrites to serve gzipped CSS/JS files if they exist and the client accepts gzip compression:\nRewriteEngine on RewriteCond %{HTTP:Accept-encoding} gzip RewriteCond %{REQUEST_FILENAME}\\.gz -s RewriteRule ^(.*)\\.(js|css)$ $1\\.$2\\.gz [QSA] Then we need to setup proper content types for such compressed files - I know how to do this in two ways:\n pure rewrites with mod_header - witch should serve correct content type and prevent mod_deflate to gzip files that are already gzipped RewriteRule \\.css\\.gz$ - [T=text/css,E=no-gzip:1,E=manualgzip:1] RewriteRule \\.js\\.gz$ - [T=text/javascript,E=no-gzip:1,E=manualgzip:1] \u0026lt;ifmodule mod_headers.c\u0026gt; # setup this header only if rewrites above were used Header set Content-Encoding \u0026#34;gzip\u0026#34; env=manualgzip \u0026lt;/ifmodule\u0026gt;  by using Files clause (we could add this globally in httpd.conf) \u0026lt;files *.css.gz\u0026gt; ForceType text/css Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt; \u0026lt;files *.js.gz\u0026gt; #ForceType text/javascript # lately this one is more popular ForceType application/javascript Header set Content-Encoding \u0026#34;gzip\u0026#34; \u0026lt;/files\u0026gt;   Both ways work fine. First one sets no-gzip variable to bypass second time compression. Second one rely on such option in my mod_deflate\u0026rsquo;s config:\nSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|bz2|sit|rar|gz)$ no-gzip dont-vary which won\u0026rsquo;t compress any gz file, and this is why I have to setup Content-Encoding to gzip manually.\nIn both cases you will end with javacript and CSS files served from earlier prepared precomressed versions, with proper content type without engaging mod_deflate regardless you use js/css or js.gz/css.gz extension. But I strongly suggest to use extensions without gz - you will be able to disable this mechanism without any change in website code.\nIf you don\u0026rsquo;t know how to prepare files just look here.\nP.S.\nI found another similar but BAD example - it\u0026rsquo;s using AddEncoding clause to add gzip content type to ALL gzip files - this will cause problems with other compressed files with gz extension ex. tar.gz. Don\u0026rsquo;t do this. My rules above are more selective.\nSources http://stackoverflow.com/questions/7947906/add-expiry-headers-using-apache-for-paths-which-dont-exist-in-the-filesystem http://stackoverflow.com/questions/9076752/how-to-force-apache-to-use-manually-pre-compressed-gz-file-of-css-and-js-files\n","href":"/2013/12/apache-precompressing-static-files-with-gzip/","title":"Apache - precompressing static files with gzip"},{"content":"","href":"/tags/mod_rewrite/","title":"mod_rewrite"},{"content":"","href":"/tags/android/","title":"Android"},{"content":"I\u0026rsquo;m happy owner of Galaxy Nexus 7 and lately I updated my tablet to Android 4.4 Kitkat. One of features I most expected was ability to block some permissions of some applications. Such setting was available in 4.4 version but was removed in latest 4.4.2 - Google didn\u0026rsquo;t explain it exactly why. I don\u0026rsquo;t like when for ex. game need: camera or GPS access - for what I asked?\nBut there is new app so called App Ops that unhides build-in interface allowing edit of application permissions. I strongly suggest to install it.\nRequirements You will need rooted device with Android 4.3 or 4.4 version.\nThis instruction could brick your device - use it on your own responsibility.\nInstall Xposed Read instructions here: http://forum.xda-developers.com/showthread.php?t=1574401 (because installation of this package have in history some bricked devices).\n download the Xposed Installer APK and install it launch the Xposed Installer and go to the \u0026ldquo;Framework\u0026rdquo; section, then click on \u0026ldquo;Install/Update\u0026rdquo; reboot your device  Install App Ops Read instructions here: http://forum.xda-developers.com/showthread.php?t=2564865\n download newest version from here, for now it will be AppOpsXposed-1.5 install it search for new App ops option in Settings under PERSONAL section  ","href":"/2013/12/android-xposed-appops-reclaim-control-over-installed-applications-permissions/","title":"Android: Xposed + AppOps - reclaim control over installed applications permissions"},{"content":"After the last NSA scandal I\u0026rsquo;ve found some time to read some texts about PFS and ECDSA keys lately. I always used RSA keys but wanted to give a try to ECDSA so I wanted to give it a try (test performance, etc). Here is how I\u0026rsquo;ve done it.\nFirstly find your favorite curve. A short tip about bit length and complexity could be found here. From it you will now that using 256 bit ECDSA key should be enough for next 10-20 years.\n$ openssl ecparam -list_curves secp112r1 : SECG/WTLS curve over a 112 bit prime field secp112r2 : SECG curve over a 112 bit prime field secp128r1 : SECG curve over a 128 bit prime field secp128r2 : SECG curve over a 128 bit prime field secp160k1 : SECG curve over a 160 bit prime field secp160r1 : SECG curve over a 160 bit prime field secp160r2 : SECG/WTLS curve over a 160 bit prime field secp192k1 : SECG curve over a 192 bit prime field secp224k1 : SECG curve over a 224 bit prime field secp224r1 : NIST/SECG curve over a 224 bit prime field secp256k1 : SECG curve over a 256 bit prime field secp384r1 : NIST/SECG curve over a 384 bit prime field secp521r1 : NIST/SECG curve over a 521 bit prime field prime192v1: NIST/X9.62/SECG curve over a 192 bit prime field prime192v2: X9.62 curve over a 192 bit prime field prime192v3: X9.62 curve over a 192 bit prime field prime239v1: X9.62 curve over a 239 bit prime field prime239v2: X9.62 curve over a 239 bit prime field prime239v3: X9.62 curve over a 239 bit prime field prime256v1: X9.62/SECG curve over a 256 bit prime field sect113r1 : SECG curve over a 113 bit binary field sect113r2 : SECG curve over a 113 bit binary field sect131r1 : SECG/WTLS curve over a 131 bit binary field sect131r2 : SECG curve over a 131 bit binary field sect163k1 : NIST/SECG/WTLS curve over a 163 bit binary field sect163r1 : SECG curve over a 163 bit binary field sect163r2 : NIST/SECG curve over a 163 bit binary field sect193r1 : SECG curve over a 193 bit binary field sect193r2 : SECG curve over a 193 bit binary field sect233k1 : NIST/SECG/WTLS curve over a 233 bit binary field sect233r1 : NIST/SECG/WTLS curve over a 233 bit binary field sect239k1 : SECG curve over a 239 bit binary field sect283k1 : NIST/SECG curve over a 283 bit binary field sect283r1 : NIST/SECG curve over a 283 bit binary field sect409k1 : NIST/SECG curve over a 409 bit binary field sect409r1 : NIST/SECG curve over a 409 bit binary field sect571k1 : NIST/SECG curve over a 571 bit binary field sect571r1 : NIST/SECG curve over a 571 bit binary field c2pnb163v1: X9.62 curve over a 163 bit binary field c2pnb163v2: X9.62 curve over a 163 bit binary field c2pnb163v3: X9.62 curve over a 163 bit binary field c2pnb176v1: X9.62 curve over a 176 bit binary field c2tnb191v1: X9.62 curve over a 191 bit binary field c2tnb191v2: X9.62 curve over a 191 bit binary field c2tnb191v3: X9.62 curve over a 191 bit binary field c2pnb208w1: X9.62 curve over a 208 bit binary field c2tnb239v1: X9.62 curve over a 239 bit binary field c2tnb239v2: X9.62 curve over a 239 bit binary field c2tnb239v3: X9.62 curve over a 239 bit binary field c2pnb272w1: X9.62 curve over a 272 bit binary field c2pnb304w1: X9.62 curve over a 304 bit binary field c2tnb359v1: X9.62 curve over a 359 bit binary field c2pnb368w1: X9.62 curve over a 368 bit binary field c2tnb431r1: X9.62 curve over a 431 bit binary field wap-wsg-idm-ecid-wtls1: WTLS curve over a 113 bit binary field wap-wsg-idm-ecid-wtls3: NIST/SECG/WTLS curve over a 163 bit binary field wap-wsg-idm-ecid-wtls4: SECG curve over a 113 bit binary field wap-wsg-idm-ecid-wtls5: X9.62 curve over a 163 bit binary field wap-wsg-idm-ecid-wtls6: SECG/WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls7: SECG/WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls8: WTLS curve over a 112 bit prime field wap-wsg-idm-ecid-wtls9: WTLS curve over a 160 bit prime field wap-wsg-idm-ecid-wtls10: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls11: NIST/SECG/WTLS curve over a 233 bit binary field wap-wsg-idm-ecid-wtls12: WTLS curvs over a 224 bit prime field Oakley-EC2N-3: IPSec/IKE/Oakley curve #3 over a 155 bit binary field. Not suitable for ECDSA. Questionable extension field! Oakley-EC2N-4: IPSec/IKE/Oakley curve #4 over a 185 bit binary field. Not suitable for ECDSA. Questionable extension field! Now generate new private key with chosen curve (prime256v1 looks fine, like: c2pnb272w1, sect283k1, sect283r1 or secp256k1, etc)\n$ openssl ecparam -out ec_key.pem -name prime256v1 -genkey And generate self-signed certificate that could be directly used:\n$ openssl req -new -key ec_key.pem -x509 -nodes -days 365 -out cert.pem You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [AU]:PL State or Province Name (full name) [Some-State]:example.pl Locality Name (eg, city) []:example.pl Organization Name (eg, company) [Internet Widgits Pty Ltd]:example.pl Organizational Unit Name (eg, section) []:example.pl Common Name (e.g. server FQDN or YOUR name) []:example.pl Email Address []:hostmaster@example.pl ","href":"/2013/12/generate-ecdsa-key-with-openssl/","title":"Generate ECDSA key with OpenSSL"},{"content":"Lately I tried to remove some streams from MKV file - I wanted: video, audio in my language and no subtitles. I achieved it with mkvtoolnix utils.\nFirstly I have to identify streams in file:\n$ mkvmerge -i input_file.mkv File \u0026#39;test.mkv\u0026#39;: container: Matroska Track ID 0: video (V_MPEG4/ISO/AVC) Track ID 1: audio (A_DTS) Track ID 2: audio (A_AC3) Track ID 3: audio (A_DTS) Track ID 4: audio (A_AC3) Track ID 5: subtitles (S_TEXT/UTF8) Track ID 6: subtitles (S_TEXT/UTF8) Chapters: 16 entries You could use more verbose tool mkvinfo for that purpose too.\nNow we know what to do next:\n$ mkvmerge -o out.mkv -d 0 --audio-tracks 2 --no-subtitles input_file.mkv mkvmerge v6.3.0 (\u0026#39;You can\u0026#39;t stop me!\u0026#39;) built on Jun 29 2013 11:48:33 \u0026#39;test.mkv\u0026#39;: Using the demultiplexer for the format \u0026#39;Matroska\u0026#39;. \u0026#39;test.mkv\u0026#39; track 0: Using the output module for the format \u0026#39;AVC/h.264\u0026#39;. \u0026#39;test.mkv\u0026#39; track 2: Using the output module for the format \u0026#39;AC3\u0026#39;. The file \u0026#39;out.mkv\u0026#39; has been opened for writing. Progress: 100% The cue entries (the index) are being written... Muxing took 3 minutes 10 seconds. This is the fastest way - no need for conversion of any stream.\nSource: http://bunin.livejournal.com/357913.html\n","href":"/2013/12/delete-audio-track-from-mkv-file/","title":"Delete audio track from mkv file"},{"content":"Some time ago I prepared a PC that was responsible for batch encoding of movies to formats suitable for web players (such as. Video.js, JW Player, Flowplayer, etc.)\nI used HandBrake for conversion to MP4 format (becase this soft was the fastest one) and ffmpeg (aka avconv in new version) for two pass encoding to WEBM.\nBelow are commands used by me for that conversion:\n MP4  HandBrakeCLI -e x264 -q 20.0 -a 1 -E faac -B 64 -6 mono -R 44.1 -D 0.0 -f mp4 --strict-anamorphic -m -x ref=1:weightp=1:subq=2:rc-lookahead=10:trellis=0:8x8dct=0 -O -i \u0026#34;input_file.avi\u0026#34; -o \u0026#34;output_file.mp4\u0026#34;  WEBM  avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -an -pass 1 -f webm /dev/null avconv -y -i \u0026#34;input_file.avi\u0026#34; -codec:v libvpx -b:v 600k -qmin 10 -qmax 42 -maxrate 500k -bufsize 1000k -threads 4 -codec:a libvorbis -b:a 96k -pass 2 -f webm \u0026#34;output_file.webm\u0026#34; Nginx configuration for MP4 I used configuration similar to that below for MP4 pseudostreaming and to protect direct urls to videos from linking on other sites (links will expire after sometime). There is also example usage of limit_rate clause that will slow down downloading of a file (it\u0026rsquo;s still two times bigger than video streaming speed so should be enough).\nlocation ~ \\.m(p4|4v)$ { ## This must match the URI part related to the MD5 hash and expiration time.  secure_link $arg_ticket,$arg_e; ## The MD5 hash is built from our secret token, the URI($path in PHP) and our expiration time.  secure_link_md5 somerandomtext$uri$arg_e; ## If the hash is incorrect then $secure_link is a null string.  if ($secure_link = \u0026#34;\u0026#34;) { return 403; } ## The current local time is greater than the specified expiration time.  if ($secure_link = \u0026#34;0\u0026#34;) { return 403; } ## If everything is ok $secure_link is 1.  mp4; mp4_buffer_size 10m; mp4_max_buffer_size 1024m; limit_rate 1024k; limit_rate_after 5m; } Sources http://nginx.org/en/docs/http/ngx_http_mp4_module.html\nhttp://wiki.nginx.org/HttpSecureLinkModule\n","href":"/2013/12/preparing-video-files-for-streaming-on-website-in-mp4-and-webm-format/","title":"Preparing video files for streaming on website in MP4 and WEBM format"},{"content":"SPDY is new protocol proposed by Google as an alternative for HTTP(S). Currently Chrome and Firefox browsers are using it as default if available on server. It is faster in most cases by few to several percent. The side effect of using mod_spdy is that it\u0026rsquo;s working well only with thread safe Apache\u0026rsquo;s modules. PHP module for Apache is not thread safe so we need to use PHP as CGI or FastCGI service. CGI is slow - so running mod_spdy for performance gain with CGI is simply pointless. FastCGI is better but it\u0026rsquo;s not possible to share APC cache in FastCGI mode (ex. using spawn-fcgi), so it\u0026rsquo;s poor too. Best for PHP is PHP-FPM which is FastCGI service with dynamic process manager and could use full advantages of APC. In such configuration I could switch from apache prefork to worker which should use less resources and be more predictable.\nInstallation On Squeeze we need to install dot.deb repository - instructions are here: http://www.dotdeb.org/instructions/\nThen we could install:\napt-get install apache2-mpm-worker php5-fpm libapache2-mod-fastcgi Now, mod_spdy - packages are available here: https://developers.google.com/speed/spdy/mod_spdy/ Choose your architecture.\nwget https://dl-ssl.google.com/dl/linux/direct/mod-spdy-beta_current_i386.deb dpkg -i mod-spdy-beta_current_i386.deb Installation of this package will add automatically a new apt repository for mod_spdy.\nIf you have Apache\u0026rsquo;s module for PHP still installed you should remove it (you won\u0026rsquo;t need in anymore):\napt-get purge libapache2-mod-php5 Configuring PHP-FPM First I\u0026rsquo;m changing php-fpm default pool configuration file - edit /etc/php5/fpm/pool.d/www.conf\n; I want it to listen on socket, not on port listen = /var/run/php5-fpm/site1.socket ;uncomment to set proper permission for socket listen.owner = www-data listen.group = www-data listen.mode = 0660 ;uncomment and change to - PHP leaks, so kill child after 100 requests pm.max_requests = 100 ; for proper chroot handling we will need also php_admin_value[doc_root] = /var/www/site1 php_admin_value[cgi.fix_pathinfo] = 0 Now restart php-fpm:\nservice php5-fpm restart Connecting Apache with PHP-FPM In VirtualHost paste this:\n\u0026lt;IfModule mod_fastcgi.c\u0026gt; Alias /php5.fcgi /var/www/site1/php5.fcgi FastCGIExternalServer /var/www/site1/php5.fcgi -socket /var/lib/apache2/fastcgi/site1.socket AddType application/x-httpd-fastphp5 .php Action application/x-httpd-fastphp5 /php5.fcgi \u0026lt;Directory \u0026#34;/var/www/site1/\u0026#34;\u0026gt; Order deny,allow Deny from all \u0026lt;Files \u0026#34;php5.fcgi\u0026#34;\u0026gt; Order allow,deny Allow from all \u0026lt;/Files\u0026gt; \u0026lt;/Directory\u0026gt; \u0026lt;/IfModule\u0026gt; Enable needed modules and restart Apache:\na2enmod actions a2enmod fastcgi service apache2 restart SSL SPDY requires encrypted connection so you need configured SSL (virtualhost running on port 443). Typical configuration for SSL looks similar to this:\n\u0026lt;virtualhost *:443\u0026gt; # some random stuff - exactly like in you NON SSL configuration 😄 SSLEngine on SSLCertificateFile /etc/ssl/certs/example.com.crt SSLCertificateKeyFile /etc/ssl/private/example.com.priv.key SSLCACertificateFile /etc/ssl/private/ca.crt\u0026lt;/virtualhost\u0026gt; Testing Should work now 😃\nSo, use Chromium, enter the site you just configured and then on second tab go to: chrome://net-internals/#spdy. You should see your site there if it\u0026rsquo;s running on SPDY.\nYou could also use plugins for Firefox or Chromium to test if site is running on SPDY.\nAdvertise SPDY on HTTP When you test if SPDY is working fine (and is faster in your configuration) you could advertise availability of SPDY protocol on your HTTP VirtualHost. Thanks to that when browser supports SPDY it will use it for faster access. To do this just add header in configuration:\nHeader set Alternate-Protocol \u0026#34;443:spdy/2\u0026#34; There are more options that could be used, if you need just check docs here.\n","href":"/2013/12/running-apache-with-mod_spdy-and-php-fpm/","title":"Running Apache with mod_spdy and PHP-FPM"},{"content":"","href":"/tags/mdadm/","title":"mdadm"},{"content":"","href":"/tags/raid/","title":"RAID"},{"content":"Yesterday I have problem with fglrx witch cause ugly system reset. After that, one of my drives was marked as failed in RAID5 array. Hotspare was automatically used to rebuild array. But this hotspare is the oldest and slowest drive I\u0026rsquo;ve got\u0026hellip;\nAfter rebuild I\u0026rsquo;ve tested failed drive and it was fine - no bad block, no any other issue - so I wanted it running back in array.\nWhat I do:\n I\u0026rsquo;ve added checked disk to array as spare  mdadm /dev/md0 -a /dev/sdb1  I\u0026rsquo;ve set this \u0026ldquo;slow\u0026rdquo; drive as failed (and rebuild started)  mdadm /dev/md0 -f /dev/sdf1  I removed this drive  echo 1 \u0026gt; /sys/block/sdf/device/delete  I run dmesg to see what was scsi host of this drive  dmesg [ 1302.433419] sd 8:0:0:0: [sdf] Synchronizing SCSI cache [ 1302.433468] sd 8:0:0:0: [sdf] Stopping disk  Now I could readd this drive  echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host8/scan  I run dmesg again to see if disk was detected  dmesg [ 1489.013270] scsi 8:0:0:0: Direct-Access ATA SEAGATE ST2000DM001 1AQ1 PQ: 0 ANSI: 5 [ 1489.013375] sd 8:0:0:0: [sdf] 3907029168 512-byte logical blocks: (2.00 TB/1.81 TiB) [ 1489.013397] sd 8:0:0:0: Attached scsi generic sg6 type 0 [ 1489.013445] sd 8:0:0:0: [sdf] Write Protect is off [ 1489.013447] sd 8:0:0:0: [sdf] Mode Sense: 00 3a 00 00 [ 1489.013466] sd 8:0:0:0: [sdf] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [ 1498.318159] sdf: sdf1 [ 1498.318391] sd 8:0:0:0: [sdf] Attached SCSI disk  Now I could re-add this drive to the array as spare  mdadm /dev/md0 -a /dev/sdf1 ","href":"/2013/12/re-adding-failed-drive-in-mdadm/","title":"Re-adding failed drive in mdadm"},{"content":"I was configuring GlusterFS on few servers using Ansible and have a need to update /etc/hosts with hostnames for easier configuration. I found this one working:\n- name: Update /etc/hosts lineinfile: dest=/etc/hosts regexp=\u0026#39;.*{{item}}$\u0026#39; line=\u0026#39;{{hostvars.{{item}}.ansible_default_ipv4.address}} {{item}}\u0026#39; state=present with_items: \u0026#39;{{groups.somegroup}}\u0026#39; Source: http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html\n","href":"/2013/12/ansible-dynamicaly-update-etc-hosts-files-on-target-servers/","title":"Ansible - Dynamicaly update /etc/hosts files on target servers"},{"content":"","href":"/tags/ghost/","title":"Ghost"},{"content":"I\u0026rsquo;ve started testing new Ghost blogging platform for a while on a virtual machine before I take decision about switching to it (or maybe won\u0026rsquo;t)\u0026hellip; After few days, I wanted to go forward with more testing and stuck on \u0026ldquo;e-mail and password\u0026rdquo; login prompt 😃\nI\u0026rsquo;ve started looking into files and found ghost_dir/content/data/ghost-dev.db SQLite database. It can be opened like that:\nsqlite3 content/data/ghost-dev.db Then you could see whats your mail (and other info):\nsqlite\u0026gt; select * from users Now password - after searching a little I found: ghost_dir/core/server/models/user.js file. There is a tip in it:\n// Hash the provided password with bcrypt return nodefn.call(bcrypt.hash, _user.password, null, null); So I used this site: http://bcrypthashgenerator.apphb.com/ to generate bcrypt hash and updated it in DB:\nsqlite\u0026gt; update users set password=\u0026#34;$2a$10$f29LDrB8S1JMfdF40Vmf1.h2OyhtlcefaMrFQVpHeX9XQ7Xiq17KC\u0026#34; where id = 1; sqlite\u0026gt; .quit Additionally as suggested by henshao: if the account has been locked, you can set status to active to unlock the account, like that:\nsqlite\u0026gt; update users set status = “active”; Now try to log with updated credentials.\nP.S. I strongly suggest to change password after successful login.\n","href":"/2013/11/reset-user-password-in-your-own-ghost-blog/","title":"Reset user password in your own Ghost Blog"},{"content":"It\u0026rsquo;s quite rare to have problems with XFS and inodes exhaustion. Mostly because XFS doesn\u0026rsquo;t have inode limit in a manner known from other filesystems - it\u0026rsquo;s using some percentage of whole filesystem as a limit and in most distributions it\u0026rsquo;s 25%. So it\u0026rsquo;s really huge amount of inodes. But some tools and distributions lowered limit ex. 5% or 10% and there you could have problems more often.\nYou could check what is you limit by issuing xfs_info with drive and searching for imaxpct value:\nroot@zombi:~# xfs_info /srv/backup/ metadane=/dev/mapper/slow-backup isize=256 agcount=17, agsize=2621440 blks = sectsz=512 attr=2 data = bsize=4096 blocks=44564480, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 log =internal bsize=4096 blocks=20480, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime=brak extsz=4096 blocks=0, rtextents=0 In this case I have 25% and it could be changed dynamically with xfs_growfs -m XX where XX is new percentage of volume capacity.\nIt\u0026rsquo;s also possible to change imaxpct on creation time by adding option -i maxpct=XX.\n","href":"/2013/11/inodes-on-xfs/","title":"Inodes on XFS"},{"content":"","href":"/tags/xfs/","title":"XFS"},{"content":"I\u0026rsquo;ve bought a NAS and customized it a little. But there was one thing which make my nights sleepless. NAS was seeking disks every 5~10 seconds - these was really irritating - especially when it was silent in room. I found that part of firmware was indexing or logging something so I wanted it dead! kill -9 was unsuccessful - process restarted after a while\u0026hellip;. wrrr\u0026hellip;\nI googled a little and found another signal I could use SIGSTOP, which will freeze process until I send SIGCONT to it - that was exactly what I need (because I normally use NFS/Samba and don\u0026rsquo;t need nothing more running on this device).\nkill -SIGSTOP `pgrep svcd` Because with this process paused Web GUI is not working, I need from time to time run it again:\nkill -SIGCONT `pgrep svcd` Sources http://major.io/2009/06/15/two-great-signals-sigstop-and-sigcont/\n","href":"/2013/11/kill-with-sigstop-and-sigcont/","title":"Kill with SIGSTOP and SIGCONT"},{"content":"I\u0026rsquo;ve just bought new toy - Iomega StorCenter ix2-200 Cloud Edition. I have to play with few options before I could start using it. First thing - Firmware upgrade.\nFirmware upgrade I\u0026rsquo;ve started searching for latest firmware for ix2-200 Cloud and found that I have to register on Lenovo site to get firmware\u0026hellip; I don\u0026rsquo;t like such sites where they force me to give all private data, but after few clicks on \u0026ldquo;Recommended articles\u0026rdquo; on that site I landed here:\nhttps://lenovo-eu-en.custhelp.com/app/answers/detail/a_id/26790\nSo it looks that I don\u0026rsquo;t need to register - point for me.\nSSH access I like to be root on my devices so I want SSH access with root privileges - nothing easier. Login to admin panel and then go to URL:\nhttp://[your-nas-ip]/diagnostics.html On this site you have to enable \u0026ldquo;remote access for technical support\u0026rdquo;, then you could login to your device via SSH using credentials:\n login: root password: soho+your_admin_password  That\u0026rsquo;s all I need to start 😃\nOther customization This device is running Debian Lenny for ARM but changed a little by producer. This HOWTO shows few tricks about installing custom software and changing default behavior. Based on: http://techmonks.net/installing-transmission-and-dnsmasq-on-a-nas/\n","href":"/2013/11/my-new-toy-iomega-storcenter-ix2-200-cloud-edition/","title":"My new toy - Iomega StorCenter ix2-200 Cloud Edition"},{"content":"After some configuration changes I\u0026rsquo;ve stuck with VBP not listening nor on HTTP, nor on SSH port. Last resort was to use CLI to reenable HTTP access. Connect with parameters:\n Baud rate: 9600 Parity: none Bits: 8 Stopbits: 1 Flow control: none  Then in login prompt you have to use login credentials (yes - they\u0026rsquo;re the same on every box (WTF?)):\n User: - root Pass: - @#$%^\u0026amp;*!()  Password is shift + 2345678190 - there is 1 before 9!\nAfter logging on, there are three commands that should reenable HTTP access:\n/etc/conf/bin/ep_mfg cfg_commit /etc/conf/bin/config_network Works for me!\nSources http://community.polycom.com/t5/Management-Security-and-Rich/VBP-ST-Factory-Reset-Problems/td-p/8974 http://community.polycom.com/t5/Management-Security-and-Rich/I-can-t-access-to-a-Polycom-VBP-5300-ST-by-HTTP/td-p/18698 http://blogs.scansource.com/polycom-vbp-e-series-reset-procedures/\n","href":"/2013/11/reenable-web-interface-on-polycom-vbp-5300-st-from-cli/","title":"Reenable web interface on Polycom VBP 5300 ST from CLI"},{"content":"I\u0026rsquo;ve crated this blog to get feedback from other IT guys about what I\u0026rsquo;m doing wrong (or not good enough). But this idea failed\u0026hellip;\nI have only few comments on my blog (and about thousand and a half spams per month) - so, no feedback in my national language at all.\nSwitching to English should make my audience bigger and I hope to have more attention thanks to that. This will be also a good practice of my English skill. I hope to find enough free time to translate some of older articles, but for now I\u0026rsquo;m thinking rather about changing engine of blog - to something more convenient. Maybe Octopress or Ghost.\nI hope you enjoy a little more.\n","href":"/2013/11/changing-language-of-articles-on-my-blog-to-english/","title":"Changing language of articles on my blog to English"},{"content":"","href":"/tags/gearman/","title":"Gearman"},{"content":"Niedawno zainteresowałem się usługą Gearman i jedynej rzeczy której mi brakowało to jakiegoś łatwego mechanizmu zarządzającego workerami. Ale jak zwykle okazało się że inni mieli już ten problem i odpowiednie narzędzie istnieje - mowa o GearmanManagerze.\nInstalacja GearmanManagera Aby zainstalować GeramanManagera na serwerze gdzie już mamy Gearmana trzeba wykonać kilka kroków (wcześniej powinniśmy też zainstalować moduł gearmana do php\u0026rsquo;a):\napt-get install git -y git clone https://github.com/brianlmoon/GearmanManager.git cd GearmanManager/install chmod +x install.sh ./install.sh Detecting linux distro as redhat- or debian-compatible Where is your php executable? (usually /usr/bin) /usr/bin Which PHP library to use, pecl/gearman or PEAR::Net_Gearman? 1) pecl 2) pear #? 1  Installing to /usr/local/share/gearman-manager Installing executable to /usr/local/bin/gearman-manager Installing configs to /etc/gearman-manager Installing init script to /etc/init.d/gearman-manager Install ok! Run /etc/init.d/gearman-manager to start and stop Worker scripts can be installed in /etc/gearman-manager/workers, configuration can be edited in /etc/gearman-manager/config.ini Mamy działającego GearmanManagera. Zalecam przyglądnięcie się plikowi config-advanced.ini bo jest tam kilka opcji, które warto dodatkowo ustawić.\nSprawdzenie działania Geramana i GearmanManagera Przykład workera można znaleźć tutaj http://brian.moonspot.net/GearmanManager. Po pobraniu go i zapisaniu w pliku /etc/gearman-manager/workers/fetch_url.php możemy ręcznie zakolejkować zadanie dla Geramana:\ngearman -f fetch_url -- http://google.pl/robots.txt Źródło:\nhttps://github.com/brianlmoon/GearmanManager\nhttp://brian.moonspot.net/GearmanManager\n","href":"/2013/11/gearmanmanager-wygodne-zarzadzanie-workerami/","title":"GearmanManager: wygodne zarządzanie workerami"},{"content":"W teorii nie powinno się blokować aktualizacji pakietów bo łatają dziury itd\u0026hellip;. Ale! Zdarzyły mi się ostatnio dwie sytuacje, które do tego mnie zmusiły:\n aktualizacja hudsona kończyła się błędem przy starcie usługi, aktualizacja domU Xen skończyła się problemem z kompatybilnością mechanizmu udev w systemie i jądrze (hypervisor miał starsze jądro niż spodziewało się DomU).  W takich sytuacjach bardzo przydaje się możliwość zablokowania aktualizacji jednej \u0026ldquo;psującej\u0026rdquo; paczki na pewien okres czasu by nie opóźniać innych aktualizacji a sobie dać czas na rozpracowanie problemu.\nWstrzymywanie aktualizacji pakietu Aktualizację wstrzymujemy o tak:\necho \u0026#34;paczka hold\u0026#34; | dpkg --set-selections Odblokowanie aktualizacji pakietu By ponownie zezwolić na aktualizację wystarczy:\necho \u0026#34;paczka install\u0026#34; | dpkg --set-selections Sprawdzenie listy wstrzymanych paczek dpkg --get-selections | grep hold Źródło:\nhttp://www.debianadmin.com/how-to-prevent-a-package-from-being-updated-in-debian.html\n","href":"/2013/11/debian-zablokowanie-aktualizacji-pakietu/","title":"Debian - zablokowanie aktualizacji pakietu"},{"content":"Ostatnio trafiłem na ciekawą usługę, która pozwala oddelegować długo trwające zadania z usługi webowej. Mowa o Gearman\u0026rsquo;ie. Usługa jest o tyle ciekawa że nie narzuca ani języka dla klienta (większość popularnych ma gotowe biblioteki), ani język dla skryptów w tej usłudze nie jest narzucany. Można tę usługę wykorzystać jako most pomiędzy PHP a np. Javą/Pythonem lub do zlecenia zadań z serwera na Linux\u0026rsquo;ie do wykonania na serwerze Windowsowym (bo np. narzędzia dostępne są tylko dla Windowsa). O innych zaletach można poczytać na stronce więc nie będę przynudzać.\nStandardowo zainstalowałem paczkę z repo Debiania i rozbiłem się przy kompilacji modułu z PECL\u0026rsquo;a - w repo była jakaś prehistoryczna wersja. Postanowiłem uruchomić aktualną wersje 1.0.6 z gałęzi testowej przekompilowując ją na Wheezym (by uniknąć zależności z wersji testowej).\nInstalacja gearman\u0026rsquo;a Dorzucamy źródła z testing - dzięki temu nie aktualizujemy systemu ale będziemy mogli pobrać świeże paczki źródłowe:\necho \u0026#34;deb-src http://ftp.pl.debian.org/debian jessie main non-free contrib\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list apt-get update Przygotowujemy katalog roboczy, pobieramy paczki i kompilujemy:\nmkdir gearman cd gearman apt-get build-dep gearman-job-server -y apt-get install bzr apt-get source gearman-job-server cd gearmand-1.0.6 ./debian/rules clean ./debian/rules binary cd .. dpkg -i gearman-job-server_1.0.6-2_i386.deb gearman-tools_1.0.6-2_i386.deb libgearman-dev_1.0.6-2_i386.deb libgearman7_1.0.6-2_i386.deb apt-get install -f -y Musiałem ręcznie doinstalować bazar (paczka bzr), bo w czasie kompilacji pojawiały się błędy z tym poleceniem - nie jestem pewien na ile jest potrzebne ale oczywiście możemy to posprzątać po skompilowaniu paczek.\nP.S. Jestem przekonany że zamiast \u0026ldquo;rules clean/binary\u0026rdquo; jest jakieś polecenie, którego powinno się użyć ale nie mogłem sobie go przypomnieć\u0026hellip;\nInstalacja modułu dla PHP'a  Ponieważ wcześniej zainstalowaliśmy aktualne biblioteki libgearman-dev to instalacja modułu dla PHP powinna być bardzo prosta:\npecl install gearman echo \u0026#34;extension=gearman.so\u0026#34; \u0026gt; /etc/php5/conf.d/gearman.ini P.S. W paczkach PHP 5.3 z dotdeb\u0026rsquo;a można znaleźć już skompilowany moduł dla gearman\u0026rsquo;a.\nNa razie tyle - muszę teraz poszukać jak w wygodny, zautomatyzowany sposób zarządzać skryptami zleconymi do gearman\u0026rsquo;a.\n","href":"/2013/10/instalacja-gearman-job-server-1-0-6-na-debianie-wheezy/","title":"Instalacja gearman-job-server 1.0.6 na Debianie Wheezy"},{"content":"Kolejna zabawna sytuacja - pewna aplikacja dotNET\u0026rsquo;owa działała dziwnie na 64-bitowym systemie, a tymczasem na 32-bitowej maszynie ta sama aplikacja działała bez problemów. Jedyna różnica to inne wersje klientów ODBC na tych systemach, które po kilku testach okazały się być przyczyną całego zła.\nPojawił się pomysł by odpalić aplikacje na 64 bitowym systemie ale w trybie 32 bit - poniżej krótkie HOWTO jak to osiągnąć:\n potrzebujemy narzędzia corflags.exe które pozwoli oznaczyć nam binarkę jako 32-bitową, do pobrania tutaj a instrukcja jej użycia tutaj. Instalujemy Windows SDK i zaznaczamy wyłącznie .NET Development Tools w kategorii Developer Tools / Windows Development Tools Odpalamy CMD i w nim CorFlags z lokalizacji: C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin (przynajmniej u mnie):  cd C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\ CorFlags.exe c:\\sciezka\\do\\pliku.exe /32BIT+ Tyle - aplikacja uruchomiła się bez problemu jako 32 bitowa i korzystała z 32 bitowego ODBC.\nŹródła http://stackoverflow.com/questions/10945664/run-anycpu-as-32-bit-on-64-bit-systems\nhttp://stackoverflow.com/questions/242304/where-should-i-download-corflags-exe-from\n","href":"/2013/10/uruchamiania-aplikacji-net-jako-32-bitowej-w-64-bitowym-systemie/","title":"Uruchamianie aplikacji .NET jako 32-bitowej w 64-bitowym systemie"},{"content":"Od jakiegoś czasu można kupić w NetArcie certyfikaty SSL, a niedawno zrobili na nie promocję - 15zł za pierwszy rok (za certyfikat na jedną stronkę). Tzw. tanie i dobre. Po wyrobieniu certyfikatu i zapisaniu z panelu klienta mam pliczki: stonka.crt i netart_rootca.crt, które wrzucamy do Apachego, powiedzmy tak:\nSSLCertificateFile /etc/ssl/certs/stonka.crt SSLCertificateKeyFile /etc/ssl/private/priv.key SSLCACertificateFile /etc/ssl/certs/netart_rootca.crt Certyfikat działa w Chromie ale nie weryfikuje się w Firefoxie i Internet Explorerze. FF wyświetla błąd: sec_error_unknown_issuer - co oznacza brak certyfikatu wystawcy gdzieś w łańcuchu certyfikatów. W FAQ zero jak chodzi o konfigurację certyfikatów na serwerze poza NetArt\u0026rsquo;em\u0026hellip;\nPrzeglądnąłem informacje certyfikatu rootca:\nopenssl x509 -in netart_rootca.crt -text -noout\u0026lt;/pre\u0026gt; Certificate: Data: Version: 3 (0x2) Serial Number: 46:53:b1:a6:1e:ba:2d:c7:a3:2e:f9:39:5a:4e:f8:8c Signature Algorithm: sha1WithRSAEncryption Issuer: C=PL, O=Unizeto Technologies S.A., OU=Certum Certification Authority, CN=Certum Global Services CA Validity Not Before: Jul 6 10:31:40 2012 GMT Not After : Jul 4 10:31:40 2022 GMT Subject: C=PL, O=NetArt Sp\\xC3\\xB3\\xC5\\x82ka Akcyjna S.K.A., OU=http://nazwa.pl, CN=nazwaSSL Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public Key: (2048 bit) Modulus (2048 bit): 00:cc:91:f5:f7:01:09:4f:75:c8:09:c7:14:8f:e4: 1a:99:78:20:99:40:59:6f:10:2f:ff:fe:d0:10:ff: 06:a3:39:3d:c4:f1:4b:07:cf:22:39:20:80:43:50: c1:af:b4:01:71:a0:a3:30:11:52:d3:d2:98:d9:c2: 69:f7:e3:00:d9:19:3f:3d:b3:3b:52:75:e3:d3:0c: ab:ff:57:01:3a:83:5c:f5:02:bb:28:fe:90:38:8e: a2:84:cf:61:48:e7:99:e0:72:24:b6:11:58:4a:18: 57:0d:34:18:5e:35:c8:b3:ac:04:5f:8d:38:2f:a2: cf:d2:dc:74:d8:41:02:ec:e0:db:0c:54:81:a4:7a: c5:34:d5:19:86:b6:1e:65:f7:3c:f6:b2:dd:3a:b5: b7:91:61:18:fd:81:2c:8a:68:d7:d6:a8:33:b7:47: b8:f9:48:ad:35:ee:11:93:f9:c2:a9:fa:94:8e:4f: bb:d1:1e:a7:64:74:b4:f9:0f:88:a7:11:a7:33:1a: c2:b1:14:0c:12:a8:6b:82:44:78:4e:d5:79:8f:5c: 60:29:47:4c:36:35:52:c7:ad:6c:c0:20:39:93:f1: c8:b3:3b:d9:c6:ec:dd:22:45:27:a2:50:12:07:f8: fe:38:79:24:89:b9:f7🇩🇪e0:c6:e9:64:e3:f4:0b: fa:c7 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 CRL Distribution Points: URI:http://crl.certum.pl/gsca.crl Authority Information Access: CA Issuers - URI:http://repository.certum.pl/gsca.cer X509v3 Authority Key Identifier: keyid:45:C5:B2:86:4E:CC:DD:29:97:E4:DD:14:C4:6E:AE:4D:B8:C1:77:F8 X509v3 Subject Key Identifier: 9D:CE:F0:5A:B4:CB:25:CF:36:A5:82:5D:8F:F7:7F:98:46:19:37:2E X509v3 Key Usage: critical Certificate Sign, CRL Sign X509v3 Certificate Policies: Policy: X509v3 Any Policy CPS: https://www.certum.pl/CPS Signature Algorithm: sha1WithRSAEncryption 53:01:c7:87:ad:ac:d7:52:32:1f:79:5d:87:f0:01:88:8e:99: 3f:07:d8:e4:bc:84:0a:8d:5f:d5:d5:62:c2:9b:79:33:46:f9: 8a:d9:b2:96:ed:35:8a:29:3b:5f:38:7a:6a:70:1d:8b:84:1a: a3:90:81:f7:2e:60:77:78:f0:d0:84:a3:e9:8a:3c:ef:8a:34: 6b:b1:9c:e8:e1:76:f4:87:1e:7b:3c:18:6f:98:70:2c:2a:8a: 22:f5:ba:96:52:7e:26:62:8b:96:03:32:22:f9:80:d7:f1:dd: 9e:c2:79:b4:17:0d:40:ff:50:6a:28:6f:e8:6f:11:8a:f9:b4: 65:2b:52:86:31:50:c7:4d:e6:f3:be🇩🇪6a:d1:89:90:27:61: 6c:1c:7d:90:1f:9a:ed:02:d4:01:22:5e:8b:0b:c9:99:34:f1: 1d:04:f4:d6:d0:71:7c:8f:0c:31:a3:2f:20:ad:35:c8:d3:b4: 0b:38:74:89:a5:d3:55:72:e9:af:b0:b8:9f:02:c9:85:69:01: d8:7e:00:44:25:91:2c:5e:5b:9f:ed:52:a8:bb:5d:94:20:f4: c4:82:35🇩🇪e5:d3:05:3c:14:d5:08:80:e4:74:47:e3:fa:f7: 8c:73:40:a8:2d:ea:1f:96:c8:e3:03:2c:62:08:cc:44:02:46: a5:81:c2:0a CA NetArtu nie jest domyślnie zainstalowane w żadnej przeglądarce więc nic dziwnego - ale są tam klucze Unizeto/Certum - dorzucę więc klucz CA (Chrome najwidoczniej sam potrafi to zrobić):\nwget http://repository.certum.pl/gsca.cer openssl x509 -inform der -in gsca.cer -out gsca.pem cat gsca.pem \u0026gt;\u0026gt; netart_rootca.crt Restart Apachego i przeglądarki już nie krzyczą. Mogliby się tylko wysilić na jakąś instrukcję albo udostępnienie od razu cabudle.crt z wszystkimi potrzebnymi certami.\n","href":"/2013/10/certyfikaty-nazwassl-na-wlasnym-serwerze/","title":"Certyfikaty nazwaSSL na własnym serwerze"},{"content":"Trafił mi się ostatnio ciekawy problem - otóż standardowo przed końcem roku poprawiałem filtry antyspamowe i optymalizowałem konfigurację Postfix\u0026rsquo;a. Chciałem zmienić domyślną wartość smtpd_delay_reject=yes na smtpd_delay_reject=no by odrzucać spamerów najwcześniej jak to możliwe. I ciekawe kuku, które sobie zrobiłem polegało na tym że sam nie mogłem wysyłać poczty po logowaniu SSL\u0026rsquo;em\u0026hellip;\nDostawałem przy tym bardzo wymowną odpowiedź:\nOct 8 16:30:39 tyr postfix/smtpd[21039]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP\u0026lt;/unknown\u0026gt; Więc wrzuciłem debug_peer_list = 67.x.x.x do main.cf by zobaczyć dokładniej o co biega:\nOct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt;\u0026gt;\u0026gt; START Client host RESTRICTIONS \u0026lt; \u0026lt;\u0026lt; Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=permit_sasl_authenticated status=0 Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject Oct 8 16:47:49 tyr postfix/smtpd[23899]: NOQUEUE: reject: CONNECT from unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown[67.x.x.x]\u0026gt;: Client host rejected: Access denied; proto=SMTP Oct 8 16:47:49 tyr postfix/smtpd[23899]: generic_checks: name=reject status=2 Oct 8 16:47:49 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 554 5.7.1 \u0026lt;unknown [67.x.x.x]\u0026gt;: Client host rejected: Access denied Oct 8 16:47:49 tyr postfix/smtpd[23899]: watchdog_pat: 0xb82c67f8 Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026lt; unknown[67.x.x.x]: QUIT Oct 8 16:47:53 tyr postfix/smtpd[23899]: \u0026gt; unknown[67.x.x.x]: 221 2.0.0 Bye\u0026lt;/unknown\u0026gt; generic_checks: name=permit_sasl_authenticated status=0 sugeruje że autoryzacja jest ok, a chwile później reject. Sprawdziłem konfigurację SASL\u0026rsquo;a (z dovecot\u0026rsquo;a) i zaczynałem bezskutecznie komentować kolejne linie w main.cf. Jaja polegały na tym że po ustawieniu smtpd_delay_reject=yes wszystko wracało do normy\u0026hellip; Ale nie chciałem tego tak zostawić.\nOlśniło mnie dopiero po chwili - przecież połączenia SSL SMTP odbywają się na inny port - zdefiniowany w master.cf - może coś tam bruździ. A tutaj od razu rzuciła mi się w oczy różnica w konfiguracji dla usługi submission i ssmtp:\nsubmission inet n - - - - smtpd ... -o smtpd_sender_restrictions=reject_sender_login_mismatch,permit_sasl_authenticated,reject ... smtps inet n - - - - smtpd ... -o smtpd_client_restrictions=permit_sasl_authenticated,reject ... Submission ustawiałem niedawno i widocznie trafiłem na lepszego FAQ\u0026rsquo;a 😃 Wygląda na to że sprawdzanie autoryzacji SASL w smtpd_client_restrictions odbywało się w tym przypadku zanim klient się autoryzował (albo było jakieś lekkie opóźnienie). Zamiana smtpd_client_restrictions na smtpd_sender_restrictions załatwiło sprawę. Przy okazji zauważyłem że po SSL\u0026rsquo;u można było spooflować innych użytkowników co również postanowiłem szybko naprawić. A wszystko dlatego że zachciało mi się \u0026ldquo;wczesnych optymalizacji\u0026rdquo; i chciałem w tych usługach pominąć część checków, które mam w main.cf.\nW minimalnej wersji konfiguracja w master.cf powinna wyglądać tak:\nsubmission inet n - - - - smtpd -o smtpd_tls_security_level=encrypt -o smtpd_tls_auth_only = yes -o smtpd_sasl_auth_enable=yes smtps inet n - - - - smtpd -o smtpd_tls_wrappermode=yes -o smtpd_sasl_auth_enable=yes P.S. Znalazłem potencjalnego winnego - w dokumentacji dovecot\u0026rsquo;a wykorzystano smtpd_client_restrictions: http://wiki2.dovecot.org/HowTo/PostfixAndDovecotSASL\n","href":"/2013/10/postfix-ciekawy-problem-z-smtpd_delay_reject-i-permit_sasl_authenticated/","title":"Postfix: ciekawy problem z smtpd_delay_reject i permit_sasl_authenticated"},{"content":"Pomimo że Python dużo częściej wykorzystywany jest w środowiskach UNIX\u0026rsquo;owcy/Linux\u0026rsquo;owych to znajdzie się kilka fajnych zastosowań dla tego języka na Windowsie. Możliwości na instalację jest kilka, a najprostsza to wykorzystanie instalatora ActiveState. Wersja ta ma w sobie wszystko co potrzebne:\n rozszerzenia dla API Windows menadżera pakietów PyPM dokumentację  Niestety jakiś czas temu zmieniły się zasady licencjonowania w ActiveState i aktualne wersje dla zastosowań produkcyjnych wymagają zakupu licencji (1000$/rok - aż chce się zacytować z Dnia Świra: czizys k\u0026hellip;wa\u0026hellip;). Wiem jak lepiej wydać taką kasę więc spróbuję uzyskać podobną funkcjonalność na tym co można pobrać za darmo z sieci.\n  Instalatory Python\u0026rsquo;a dla Windows można znaleźć tutaj: http://www.python.org/download/releases/\n  Teraz pakiety z obsługą API Windows (wybieramy stosownie do wcześniej pobranej wersji Pythona): http://sourceforge.net/projects/pywin32/files/pywin32/\n  I na koniec setuptools by móc doinstalować dodatkowe moduły. Wybieramy interesującą nas wersję tutaj: https://pypi.python.org/pypi/setuptools/\n  Obecnie jest to 1.1.6 - zgodnie z opisem z tej strony: https://pypi.python.org/pypi/setuptools/1.1.6#windows pobieramy ez_setup.py i uruchamiamy.\n  Na koniec odpalamy CLI i instalujemy inne przydatne nam paczki, np.:\neasy_install couchdb easy_install cx-oracle  P.S. I tutaj mały kruczek - instalacja cx-Oracle z pomocą easy_install uda się tylko na 32-bitowych Windowsach, na 64-bitowych konieczne jest zainstalowanie Visual Studio Express by możliwe było skompilowanie paczek\u0026hellip; (tak mnie też się w tej chwili już odechciewało\u0026hellip;)\nAle na szczęście w przypadku tej paczki da się inaczej, wystarczy pobrać już skompilowaną paczkę ze strony: http://cx-oracle.sourceforge.net dopasowaną do wybranej wcześniej wersji Pythona. P.S. 2. Można się obyć bez tej paczki i wykorzystać pyodbc razem z kontrolerem ODBC z klienta Oracle, ale pyodbc nie obsługuje wywołań procedur ze zmiennymi wiązanymi in/out lub out - a ja akurat tego potrzebowałem, jeśli to nie twój problem to pyodbc będzie prostsze 😄    ","href":"/2013/09/instalacja-pythona-na-windowsie/","title":"Instalacja Python’a na Windowsie"},{"content":"Ten one liner załatwia sprawę:\npython -c \u0026#39;import django; print \u0026#34;.\u0026#34;.join([str(s) for s in django.VERSION]);\u0026#39; ","href":"/2013/09/sprawdzanie-zainstalowanej-wersji-django/","title":"Sprawdzanie zainstalowanej wersji Django"},{"content":"","href":"/tags/spam/","title":"SPAM"},{"content":"I\u0026rsquo;ve just received a comment on my blog with text attached below. It looks like spam message template. I think it could be easily used for creation of banning rules, etc. Use it in a way that will make this dumbass spammer look even more stupid ;-)\n{ {I have|I’ve} been {surfing|browsing} online more than {three|3|2|4} hours today, yet I never found any interesting article like yours. {It’s|It is} pretty worth enough for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made good content as you did, the {internet|net|web} will be {much more|a lot more} useful than ever before.| I {couldn’t|could not} {resist|refrain from} commenting. {Very well|Perfectly|Well|Exceptionally well} written!| {I will|I’ll} {right away|immediately} {take hold of|grab|clutch|grasp|seize|snatch} your {rss|rss feed} as I {can not|can’t} {in finding|find|to find} your {email|e-mail} subscription {link|hyperlink} or {newsletter|e-newsletter} service. Do {you have|you’ve} any? {Please|Kindly} {allow|permit|let} me {realize|recognize|understand|recognise|know} {so that|in order that} I {may just|may|could} subscribe. Thanks.| {It is|It’s} {appropriate|perfect|the best} time to make some plans for the future and {it is|it’s} time to be happy. {I have|I’ve} read this post and if I could I {want to|wish to|desire to} suggest you {few|some} interesting things or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write next articles referring to this article. I {want to|wish to|desire to} read {more|even more} things about it!| {It is|It’s} {appropriate|perfect|the best} time to make {a few|some} plans for {the future|the longer term|the long run} and {it is|it’s} time to be happy. {I have|I’ve} {read|learn} this {post|submit|publish|put up} and if I {may just|may|could} I {want to|wish to|desire to} {suggest|recommend|counsel} you {few|some} {interesting|fascinating|attention-grabbing} {things|issues} or {advice|suggestions|tips}. {Perhaps|Maybe} you {could|can} write {next|subsequent} articles {relating to|referring to|regarding} this article. I {want to|wish to|desire to} {read|learn} {more|even more} {things|issues} {approximately|about} it!| {I have|I’ve} been {surfing|browsing} {online|on-line} {more than|greater than} {three|3} hours {these days|nowadays|today|lately|as of late}, {yet|but} I {never|by no means} {found|discovered} any {interesting|fascinating|attention-grabbing} article like yours. {It’s|It is} {lovely|pretty|beautiful} {worth|value|price} {enough|sufficient} for me. {In my opinion|Personally|In my view}, if all {webmasters|site owners|website owners|web owners} and bloggers made {just right|good|excellent} {content|content material} as {you did|you probably did}, the {internet|net|web} {will be|shall be|might be|will probably be|can be|will likely be} {much more|a lot more} {useful|helpful} than ever before.| Ahaa, its {nice|pleasant|good|fastidious} {discussion|conversation|dialogue} {regarding|concerning|about|on the topic of} this {article|post|piece of writing|paragraph} {here|at this place} at this {blog|weblog|webpage|website|web site}, I have read all that, so {now|at this time} me also commenting {here|at this place}.| I am sure this {article|post|piece of writing|paragraph} has touched all the internet {users|people|viewers|visitors}, its really really {nice|pleasant|good|fastidious} {article|post|piece of writing|paragraph} on building up new {blog|weblog|webpage|website|web site}.| Wow, this {article|post|piece of writing|paragraph} is {nice|pleasant|good|fastidious}, my {sister|younger sister} is analyzing {such|these|these kinds of} things, {so|thus|therefore} I am going to {tell|inform|let know|convey} her.| {Saved as a favorite|bookmarked!!}, {I really like|I like|I love} {your blog|your site|your web site|your website}!| Way cool! Some {very|extremely} valid points! I appreciate you {writing this|penning this} {article|post|write-up} {and the|and also the|plus the} rest of the {site is|website is} {also very|extremely|very|also really|really} good.| Hi, {I do believe|I do think} {this is an excellent|this is a great} {blog|website|web site|site}. I stumbledupon it ;) {I will|I am going to|I’m going to|I may} {come back|return|revisit} {once again|yet again} {since I|since i have} {bookmarked|book marked|book-marked|saved as a favorite} it. Money and freedom {is the best|is the greatest} way to change, may you be rich and continue to {help|guide} {other people|others}.| Woah! I’m really {loving|enjoying|digging} the template/theme of this {site|website|blog}. It’s simple, yet effective. A lot of times it’s {very hard|very difficult|challenging|tough|difficult|hard} to get that „perfect balance” between {superb usability|user friendliness|usability} and {visual appearance|visual appeal|appearance}. I must say {that you’ve|you have|you’ve} done a {awesome|amazing|very good|superb|fantastic|excellent|great} job with this. {In addition|Additionally|Also}, the blog loads {very|extremely|super} {fast|quick} for me on {Safari|Internet explorer|Chrome|Opera|Firefox}. {Superb|Exceptional|Outstanding|Excellent} Blog!| These are {really|actually|in fact|truly|genuinely} {great|enormous|impressive|wonderful|fantastic} ideas in {regarding|concerning|about|on the topic of} blogging. You have touched some {nice|pleasant|good|fastidious} {points|factors|things} here. Any way keep up wrinting.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I’ve {incorporated||added|included} you guys to {|my|our||my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey}! Someone in my {Myspace|Facebook} group shared this {site|website} with us so I came to {give it a look|look it over|take a look|check it out}. I’m definitely {enjoying|loving} the information. I’m {book-marking|bookmarking} and will be tweeting this to my followers! {Terrific|Wonderful|Great|Fantastic|Outstanding|Exceptional|Superb|Excellent} blog and {wonderful|terrific|brilliant|amazing|great|excellent|fantastic|outstanding|superb} {style and design|design and style|design}.| {I love|I really like|I enjoy|I like|Everyone loves} what you guys {are|are usually|tend to be} up too. {This sort of|This type of|Such|This kind of} clever work and {exposure|coverage|reporting}! Keep up the {superb|terrific|very good|great|good|awesome|fantastic|excellent|amazing|wonderful} works guys I’ve {incorporated|added|included} you guys to {|my|our|my personal|my own} blogroll.| {Howdy|Hi there|Hey there|Hi|Hello|Hey} would you mind {stating|sharing} which blog platform you’re {working with|using}? I’m {looking|planning|going} to start my own blog {in the near future|soon} but I’m having a {tough|difficult|hard} time {making a decision|selecting|choosing|deciding} between BlogEngine/Wordpress/B2evolution and Drupal. The reason I ask is because your {design and style|design|layout} seems different then most blogs and I’m looking for something {completely unique|unique}. P.S {My apologies|Apologies|Sorry} for {getting|being} off-topic but I had to ask!| {Howdy|Hi there|Hi|Hey there|Hello|Hey} would you mind letting me know which {webhost|hosting company|web host} you’re {utilizing|working with|using}? I’ve loaded your blog in 3 {completely different|different} {internet browsers|web browsers|browsers} and I must say this blog loads a lot {quicker|faster} then most. Can you {suggest|recommend} a good {internet hosting|web hosting|hosting} provider at a {honest|reasonable|fair} price? {Thanks a lot|Kudos|Cheers|Thank you|Many thanks|Thanks}, I appreciate it!| {I love|I really like|I like|Everyone loves} it {when people|when individuals|when folks|whenever people} {come together|get together} and share {opinions|thoughts|views|ideas}. Great {blog|website|site}, {keep it up|continue the good work|stick with it}!| Thank you for the {auspicious|good} writeup. It in fact was a amusement account it. Look advanced to {far|more} added agreeable from you! {By the way|However}, how {can|could} we communicate?| {Howdy|Hi there|Hey there|Hello|Hey} just wanted to give you a quick heads up. The {text|words} in your {content|post|article} seem to be running off the screen in {Ie|Internet explorer|Chrome|Firefox|Safari|Opera}. I’m not sure if this is a {format|formatting} issue or something to do with {web browser|internet browser|browser} compatibility but I {thought|figured} I’d post to let you know. The {style and design|design and style|layout|design} look great though! Hope you get the {problem|issue} {solved|resolved|fixed} soon. {Kudos|Cheers|Many thanks|Thanks}| This is a topic {that is|that’s|which is} {close to|near to} my heart… {Cheers|Many thanks|Best wishes|Take care|Thank you}! {Where|Exactly where} are your contact details though?| It’s very {easy|simple|trouble-free|straightforward|effortless} to find out any {topic|matter} on {net|web} as compared to {books|textbooks}, as I found this {article|post|piece of writing|paragraph} at this {website|web site|site|web page}.| Does your {site|website|blog} have a contact page? I’m having {a tough time|problems|trouble} locating it but, I’d like to {send|shoot} you an {e-mail|email}. I’ve got some {creative ideas|recommendations|suggestions|ideas} for your blog you might be interested in hearing. Either way, great {site|website|blog} and I look forward to seeing it {develop|improve|expand|grow} over time.| {Hola|Hey there|Hi|Hello|Greetings}! I’ve been {following|reading} your {site|web site|website|weblog|blog} for {a long time|a while|some time} now and finally got the {bravery|courage} to go ahead and give you a shout out from {New Caney|Kingwood|Huffman|Porter|Houston|Dallas|Austin|Lubbock|Humble|Atascocita} {Tx|Texas}! Just wanted to {tell you|mention|say} keep up the {fantastic|excellent|great|good} {job|work}!| Greetings from {Idaho|Carolina|Ohio|Colorado|Florida|Los angeles|California}! I’m {bored to tears|bored to death|bored} at work so I decided to {check out|browse} your {site|website|blog} on my iphone during lunch break. I {enjoy|really like|love} the {knowledge|info|information} you {present|provide} here and can’t wait to take a look when I get home. I’m {shocked|amazed|surprised} at how {quick|fast} your blog loaded on my {mobile|cell phone|phone} .. I’m not even using WIFI, just 3G .. {Anyhow|Anyways}, {awesome|amazing|very good|superb|good|wonderful|fantastic|excellent|great} {site|blog}!| Its {like you|such as you} {read|learn} my {mind|thoughts}! You {seem|appear} {to understand|to know|to grasp} {so much|a lot} {approximately|about} this, {like you|such as you} wrote the {book|e-book|guide|ebook|e book} in it or something. {I think|I feel|I believe} {that you|that you simply|that you just} {could|can} do with {some|a few} {%|p.c.|percent} to {force|pressure|drive|power} the message {house|home} {a bit|a little bit}, {however|but} {other than|instead of} that, {this is|that is} {great|wonderful|fantastic|magnificent|excellent} blog. {A great|An excellent|A fantastic} read. {I’ll|I will} {definitely|certainly} be back.| I visited {multiple|many|several|various} {websites|sites|web sites|web pages|blogs} {but|except|however} the audio {quality|feature} for audio songs {current|present|existing} at this {website|web site|site|web page} is {really|actually|in fact|truly|genuinely} {marvelous|wonderful|excellent|fabulous|superb}.| {Howdy|Hi there|Hi|Hello}, i read your blog {occasionally|from time to time} and i own a similar one and i was just {wondering|curious} if you get a lot of spam {comments|responses|feedback|remarks}? If so how do you {prevent|reduce|stop|protect against} it, any plugin or anything you can {advise|suggest|recommend}? I get so much lately it’s driving me {mad|insane|crazy} so any {assistance|help|support} is very much appreciated.| Greetings! {Very helpful|Very useful} advice {within this|in this particular} {article|post}! {It is the|It’s the} little changes {that make|which will make|that produce|that will make} {the biggest|the largest|the greatest|the most important|the most significant} changes. {Thanks a lot|Thanks|Many thanks} for sharing!| {I really|I truly|I seriously|I absolutely} love {your blog|your site|your website}.. {Very nice|Excellent|Pleasant|Great} colors \u0026amp; theme. Did you {create|develop|make|build} {this website|this site|this web site|this amazing site} yourself? Please reply back as I’m {looking to|trying to|planning to|wanting to|hoping to|attempting to} create {my own|my very own|my own personal} {blog|website|site} and {would like to|want to|would love to} {know|learn|find out} where you got this from or {what the|exactly what the|just what the} theme {is called|is named}. {Thanks|Many thanks|Thank you|Cheers|Appreciate it|Kudos}!| {Hi there|Hello there|Howdy}! This {post|article|blog post} {couldn’t|could not} be written {any better|much better}! {Reading through|Looking at|Going through|Looking through} this {post|article} reminds me of my previous roommate! He {always|constantly|continually} kept {talking about|preaching about} this. {I will|I’ll|I am going to|I most certainly will} {forward|send} {this article|this information|this post} to him. {Pretty sure|Fairly certain} {he will|he’ll|he’s going to} {have a good|have a very good|have a great} read. {Thank you for|Thanks for|Many thanks for|I appreciate you for} sharing!| {Wow|Whoa|Incredible|Amazing}! This blog looks {exactly|just} like my old one! It’s on a {completely|entirely|totally} different {topic|subject} but it has pretty much the same {layout|page layout} and design. {Excellent|Wonderful|Great|Outstanding|Superb} choice of colors!| {There is|There’s} {definately|certainly} {a lot to|a great deal to} {know about|learn about|find out about} this {subject|topic|issue}. {I like|I love|I really like} {all the|all of the} points {you made|you’ve made|you have made}.| {You made|You’ve made|You have made} some {decent|good|really good} points there. I {looked|checked} {on the internet|on the web|on the net} {for more info|for more information|to find out more|to learn more|for additional information} about the issue and found {most individuals|most people} will go along with your views on {this website|this site|this web site}.| {Hi|Hello|Hi there|What’s up}, I {log on to|check|read} your {new stuff|blogs|blog} {regularly|like every week|daily|on a regular basis}. Your {story-telling|writing|humoristic} style is {awesome|witty}, keep {doing what you’re doing|up the good work|it up}!| I {simply|just} {could not|couldn’t} {leave|depart|go away} your {site|web site|website} {prior to|before} suggesting that I {really|extremely|actually} {enjoyed|loved} {the standard|the usual} {information|info} {a person|an individual} {supply|provide} {for your|on your|in your|to your} {visitors|guests}? Is {going to|gonna} be {back|again} {frequently|regularly|incessantly|steadily|ceaselessly|often|continuously} {in order to|to} {check up on|check out|inspect|investigate cross-check} new posts| {I wanted|I needed|I want to|I need to} to thank you for this {great|excellent|fantastic|wonderful|good|very good} read!! I {definitely|certainly|absolutely} {enjoyed|loved} every {little bit of|bit of} it. {I have|I’ve got|I have got} you {bookmarked|book marked|book-marked|saved as a favorite} {to check out|to look at} new {stuff you|things you} post…| {Hi|Hello|Hi there|What’s up}, just wanted to {mention|say|tell you}, I {enjoyed|liked|loved} this {article|post|blog post}. It was {inspiring|funny|practical|helpful}. Keep on posting!| I {{leave|drop|{write|create}} a {comment|leave a response}|drop a {comment|leave a response}|{comment|leave a response}} {each time|when|whenever} I {appreciate|like|especially enjoy} a {post|article} on a {site|{blog|website}|site|website} or {I have|if I have} something to {add|contribute|valuable to contribute} {to the discussion|to the conversation}. {It is|Usually it is|Usually it’s|It’s} {a result of|triggered by|caused by} the {passion|fire|sincerness} {communicated|displayed} in the {post|article} I {read|looked at|browsed}. And {on|after} this {post|article} Kopiowanie wolumenów LVM z dd i netcat | timor’s site. I {{was|was actually} moved|{was|was actually} excited} enough to {drop|{leave|drop|{write|create}}|post} a {thought|{comment|{comment|leave a response}a response}} {:-P|:)|;)|;-)|: -)} I {do have|actually do have} {{some|a few} questions|a couple of questions|2 questions} for you {if you {don’t|do not|usually do not|tend not to} mind|if it’s {allright|okay}}. {Is it|Could it be} {just|only|simply} me or {do|does it {seem|appear|give the impression|look|look as if|look like} like} {some|a few} of {the|these} {comments|responses|remarks} {look|appear|come across} {like they are|as if they are|like} {coming from|written by|left by} brain dead {people|visitors|folks|individuals}? :-P And, if you are {posting|writing} {on|at} {other|additional} {sites|social sites|online sites|online social sites|places}, {I’d|I would} like to {follow|keep up with} {you|{anything|everything} {new|fresh} you have to post}. {Could|Would} you {list|make a list} {all|every one|the complete urls} of {your|all your} {social|communal|community|public|shared} {pages|sites} like your {twitter feed, Facebook page or linkedin profile|linkedin profile, Facebook page or twitter feed|Facebook page, twitter feed, or linkedin profile}?| {Hi there|Hello}, I enjoy reading {all of|through} your {article|post|article post}. I {like|wanted} to write a little comment to support you.| I {always|constantly|every time} spent my half an hour to read this {blog|weblog|webpage|website|web site}’s {articles|posts|articles or reviews|content} {everyday|daily|every day|all the time} along with a {cup|mug} of coffee.| I {always|for all time|all the time|constantly|every time} emailed this {blog|weblog|webpage|website|web site} post page to all my {friends|associates|contacts}, {because|since|as|for the reason that} if like to read it {then|after that|next|afterward} my {friends|links|contacts} will too.| My {coder|programmer|developer} is trying to {persuade|convince} me to move to .net from PHP. I have always disliked the idea because of the {expenses|costs}. But he’s tryiong none the less. I’ve been using {Movable-type|WordPress} on {a number of|a variety of|numerous|several|various} websites for about a year and am {nervous|anxious|worried|concerned} about switching to another platform. I have heard {fantastic|very good|excellent|great|good} things about blogengine.net. Is there a way I can {transfer|import} all my wordpress {content|posts} into it? {Any kind of|Any} help would be {really|greatly} appreciated!| {Hello|Hi|Hello there|Hi there|Howdy|Good day}! I could have sworn I’ve {been to|visited} {this blog|this web site|this website|this site|your blog} before but after {browsing through|going through|looking at} {some of the|a few of the|many of the} {posts|articles} I realized it’s new to me. {Anyways|Anyhow|Nonetheless|Regardless}, I’m {definitely|certainly} {happy|pleased|delighted} {I found|I discovered|I came across|I stumbled upon} it and I’ll be {bookmarking|book-marking} it and checking back {frequently|regularly|often}!| {Terrific|Great|Wonderful} {article|work}! {This is|That is} {the type of|the kind of} {information|info} {that are meant to|that are supposed to|that should} be shared {around the|across the} {web|internet|net}. {Disgrace|Shame} on {the {seek|search} engines|Google} for {now not|not|no longer} positioning this {post|submit|publish|put up} {upper|higher}! Come on over and {talk over with|discuss with|seek advice from|visit|consult with} my {site|web site|website} . {Thank you|Thanks} =)| Heya {i’m|i am} for the first time here. I {came across|found} this board and I find It {truly|really} useful \u0026amp; it helped me out {a lot|much}. I hope to give something back and {help|aid} others like you {helped|aided} me.| {Hi|Hello|Hi there|Hello there|Howdy|Greetings}, {I think|I believe|I do believe|I do think|There’s no doubt that} {your site|your website|your web site|your blog} {might be|may be|could be|could possibly be} having {browser|internet browser|web browser} compatibility {issues|problems}. {When I|Whenever I} {look at your|take a look at your} {website|web site|site|blog} in Safari, it looks fine {but when|however when|however, if|however, when} opening in {Internet Explorer|IE|I.E.}, {it has|it’s got} some overlapping issues. {I just|I simply|I merely} wanted to {give you a|provide you with a} quick heads up! {Other than that|Apart from that|Besides that|Aside from that}, {fantastic|wonderful|great|excellent} {blog|website|site}!| {A person|Someone|Somebody} {necessarily|essentially} {lend a hand|help|assist} to make {seriously|critically|significantly|severely} {articles|posts} {I would|I might|I’d} state. {This is|That is} the {first|very first} time I frequented your {web page|website page} and {to this point|so far|thus far|up to now}? I {amazed|surprised} with the {research|analysis} you made to {create|make} {this actual|this particular} {post|submit|publish|put up} {incredible|amazing|extraordinary}. {Great|Wonderful|Fantastic|Magnificent|Excellent} {task|process|activity|job}!| Heya {i’m|i am} for {the primary|the first} time here. I {came across|found} this board and I {in finding|find|to find} It {truly|really} {useful|helpful} \u0026amp; it helped me out {a lot|much}. {I am hoping|I hope|I’m hoping} {to give|to offer|to provide|to present} {something|one thing} {back|again} and {help|aid} others {like you|such as you} {helped|aided} me.| {Hello|Hi|Hello there|Hi there|Howdy|Good day|Hey there}! {I just|I simply} {would like to|want to|wish to} {give you a|offer you a} {huge|big} thumbs up {for the|for your} {great|excellent} {info|information} {you have|you’ve got|you have got} {here|right here} on this post. {I will be|I’ll be|I am} {coming back to|returning to} {your blog|your site|your website|your web site} for more soon.| I {always|all the time|every time} used to {read|study} {article|post|piece of writing|paragraph} in news papers but now as I am a user of {internet|web|net} {so|thus|therefore} from now I am using net for {articles|posts|articles or reviews|content}, thanks to web.| Your {way|method|means|mode} of {describing|explaining|telling} {everything|all|the whole thing} in this {article|post|piece of writing|paragraph} is {really|actually|in fact|truly|genuinely} {nice|pleasant|good|fastidious}, {all|every one} {can|be able to|be capable of} {easily|without difficulty|effortlessly|simply} {understand|know|be aware of} it, Thanks a lot.| {Hi|Hello} there, {I found|I discovered} your {blog|website|web site|site} {by means of|via|by the use of|by way of} Google {at the same time as|whilst|even as|while} {searching for|looking for} a {similar|comparable|related} {topic|matter|subject}, your {site|web site|website} {got here|came} up, it {looks|appears|seems|seems to be|appears to be like} {good|great}. {I have|I’ve} bookmarked it in my google bookmarks. {Hello|Hi} there, {simply|just} {turned into|became|was|become|changed into} {aware of|alert to} your {blog|weblog} {thru|through|via} Google, {and found|and located} that {it is|it’s} {really|truly} informative. {I’m|I am} {gonna|going to} {watch out|be careful} for brussels. {I will|I’ll} {appreciate|be grateful} {if you|should you|when you|in the event you|in case you|for those who|if you happen to} {continue|proceed} this {in future}. {A lot of|Lots of|Many|Numerous} {other folks|folks|other people|people} {will be|shall be|might be|will probably be|can be|will likely be} benefited {from your|out of your} writing. Cheers!| {I am|I’m} curious to find out what blog {system|platform} {you have been|you happen to be|you are|you’re} {working with|utilizing|using}? I’m {experiencing|having} some {minor|small} security {problems|issues} with my latest {site|website|blog} and {I would|I’d} like to find something more {safe|risk-free|safeguarded|secure}. Do you have any {solutions|suggestions|recommendations}?| {I am|I’m} {extremely|really} impressed with your writing skills {and also|as well as} with the layout on your {blog|weblog}. Is this a paid theme or did you {customize|modify} it yourself? {Either way|Anyway} keep up the {nice|excellent} quality writing, {it’s|it is} rare to see a {nice|great} blog like this one {these days|nowadays|today}.| {I am|I’m} {extremely|really} {inspired|impressed} {with your|together with your|along with your} writing {talents|skills|abilities} {and also|as {smartly|well|neatly} as} with the {layout|format|structure} {for your|on your|in your|to your} {blog|weblog}. {Is this|Is that this} a paid {subject|topic|subject matter|theme} or did you {customize|modify} it {yourself|your self}? {Either way|Anyway} {stay|keep} up the {nice|excellent} {quality|high quality} writing, {it’s|it is} {rare|uncommon} {to peer|to see|to look} a {nice|great} {blog|weblog} like this one {these days|nowadays|today}..| {Hi|Hello}, Neat post. {There is|There’s} {a problem|an issue} {with your|together with your|along with your} {site|web site|website} in {internet|web} explorer, {may|might|could|would} {check|test} this? IE {still|nonetheless} is the {marketplace|market} {leader|chief} and {a large|a good|a big|a huge} {part of|section of|component to|portion of|component of|element of} {other folks|folks|other people|people} will {leave out|omit|miss|pass over} your {great|wonderful|fantastic|magnificent|excellent} writing {due to|because of} this problem.| {I’m|I am} not sure where {you are|you’re} getting your {info|information}, but {good|great} topic. I needs to spend some time learning {more|much more} or understanding more. Thanks for {great|wonderful|fantastic|magnificent|excellent} {information|info} I was looking for this {information|info} for my mission.| {Hi|Hello}, i think that i saw you visited my {blog|weblog|website|web site|site} {so|thus} i came to “return the favor”.{I am|I’m} {trying to|attempting to} find things to {improve|enhance} my {website|site|web site}!I suppose its ok to use {some of|a few of} your ideas ","href":"/2013/09/spammer-screwed-up/","title":"Spammer screwed up"},{"content":"","href":"/tags/raspberry-pi/","title":"Raspberry Pi"},{"content":"Wyprzeć się nie mogę że gadżety działające na Linuksie po prostu mnie kręcą, więc tylko kwestią czasu było aż Pi zawita na moim biurku. Zakupiłem więc model B w drugiej wersji, obudowę z możliwością mocowania VESA, kabelek HDMI, ładowarka z mojej myszy pasowała idealnie (5.05V i 1A) i na początek karta SD klasa 10 4GB.\nSzukając różnych systemów (a może ROM\u0026rsquo;ów) natrafiłem na oficjalną stronę: http://www.raspberrypi.org/downloads\nNa początek wystarczy a sprawdzając na stronach projektów okazało się że wersje na tej stronie są całkiem aktualne.\nChociaż paczka NOOB wyglądała kusząco to byłą największa i ściągnęła mi się jak już działałem na OpenELEC\u0026rsquo;u. Na dobrą sprawę nie rozumiem czemu tę drogę opisano jako trudniejszą\u0026hellip;\nNagrywanie obrazu na kartę SD  Wpinamy kartę SD w czytnik Odpalamy:  dmesg i szukamy jaką literkę jej przyporządkowano w systemie\n Odpalamy na tym dysku:  fdisk -l /dev/sdh (w moim przypadku) i upewniamy się że rozmiar i tablica partycji odpowiadają naszej karcie (pomyłka literki to bankowa utrata danych i co najmniej jedna nieprzespana noc\u0026hellip;)\n Rozpakowujemy obraz:  unzip 2013-07-26-wheezy-raspbian.zip  Możemy wgrywać:  sudo dd bs=1M if=2013-07-26-wheezy-raspbian.img of=/dev/sdh status kopiowania możemy podglądać tak:\nkill -USR1 `pidof dd` (o ile mamy uruchomiony jeden proces dd w systemi)\n W przypadku niektórych obrazów może być potrzebna zmiana rozmiaru partycji (bo domyślnie przygotowane są dla mniejszych kart - polecam parted lub gparted do tego zadania  Można startować!\n","href":"/2013/09/raspberry-pi-pierwsze-kroki/","title":"Raspberry Pi: pierwsze kroki"},{"content":"Polubiłem Nginx\u0026rsquo;a i wykorzystuję go na coraz więcej sposobów. Kilka rzeczy udało mi się całkiem fajnie w nim skonfigurować i postanowiłem zebrać te przykłady by następnym razem gdy postanowię do nich sięgnąć nie musieć wertować konfigów po serwerach 😃\nSłowo wstępu Niektóre rewrite\u0026rsquo;y kończą się znakiem ? - czemu?\nOtóż Nginx próbuje automatycznie dodawać parametry na końcu przepisanego adresu. Jeśli jednak wykorzystamy zmienną $request_uri to ona sama w sobie zawiera już parametry zapytania (czyli to co w URI znajduje się po znaku ?) i właśnie dodanie pytajnika tuż za tą zmienną powoduje że argumenty nie są dublowane.\nMa to też zastosowanie gdy chcemy by rewrite kierował np. na główną stronę beż żadnych dodatkowych argumentów (zostaną one obcięte).\nWięcej na ten temat można znaleźć w dokumentacji Nginx.\nInna warta wspomnienia uwaga dotyczy drobnej optymalizacji, o której warto pamiętać na etapie tworzenia rewrite\u0026rsquo;ów (można znaleźć masę kiepskich przykładów w sieci): na początku najlepiej jest stworzyć coś co działa (i przy małym ruchu może to być wystarczające) a później optymalizować - moje przykłady starałem się zoptymalizować według zalecanych praktyk.\nDlatego zamiast pisać:\nrewrite ^(.*)$ $scheme://www.domain.com$1 permanent; lepiej napisać:\nrewrite ^ $scheme://www.domain.com$request_uri? permanent; (nie wykorzystujemy przechowywania wartości dopasowania - mniejsze zużycie pamięci i lżejsza interpretacja REGEXP\u0026rsquo;a).\nA jeszcze lepiej napisać:\nreturn 301 $scheme://www.domain.com$request_uri; (w ogóle nie wykorzystujemy REGEXP\u0026rsquo;ów praktycznie zerowy narzut na przetwarzanie) - dzięki za uwagę: lukasamd.\nPrzekierowanie starej domeny na nową server { listen 80; server_name old-domain.com www.old-domain.com; return 301 $scheme://www.new-domain.com$request_uri; # rewrite ^ $scheme://www.new-domain.com$request_uri? permanent;  # or  # rewrite ^ $scheme://www.new-domain.com? permanent; } Wykorzystanie return w tej sytuacji jest nieco bardziej optymalne gdyż nie angażuje w ogóle silnika REGEXP a w tej sytuacji jest wystarczające.\nPierwsza linia z rewrite i $request_uri spowoduje przepisywanie też parametrów wywołań do nowej lokalizacji co jest jak najbardziej sensowne gdy pomimo domeny nie zmieniła się zbytnio struktura strony.\nJeśli strona jednak się zmieniła to możemy zdecydować o przekierowaniu bez parametrów - po prostu na główną stronę - i to robi druga linia.\nW obu przypadkach parametr permanent nakazuje użycie kodu przekierowania HTTP 301 (Moved Permanently), co ułatwi zorientowanie się crawlerom że ta zmiana jest już na stałe.\nDodanie WWW na początku domeny server { listen 80; server_name domaim.com; return 301 $scheme://www.domain.com$request_uri; #rewrite ^ $scheme://www.domain.com$request_uri? permanent;  # or  #rewrite ^(.*)$ $scheme://www.domain.com$1 permanent; } Przykład zakomentowany jest według dokumentacji mniej optymalny ale również zadziała. Reszta jest prosta i samoopisująca się 😃\nA to jeszcze bardziej ogólna wersja dla wielu domen:\nserver { listen 195.117.254.80:80; server_name domain.pl domain.eu domain.com; return 301 $scheme://www.$http_host$request_uri; #rewrite ^ $scheme://www.$http_host$request_uri? permanent; } Ta wersja wykorzystuje zmienną $http_host do przekierowania na domenę z zapytania (zmienna ta zawiera też numer portu jeśli jest niestandardowy np. 8080, w przeciwieństwie do zmiennej $host, która zawiera tylko domenę).\nUsunięcie WWW z początku domeny server { listen 80; server_name www.domain.com; return 301 $scheme://domain.com$request_uri; #rewrite ^ $scheme://domain.com$request_uri? permanent; } Czasami może się przydać jeszcze inny kawałek, gdy strona działa na wielu domenach i chcemy przekierować wszystkie:\nserver { server_name www.domain.com _ ; # server_name www.domain1.com www.domain2.com www.domain3.eu www.domain.etc.com;  if ($host ~* www\\.(.*)) { set $pure_host $1; return 301 $scheme://$pure_host$request_uri; #rewrite ^ $scheme://$pure_host$request_uri? permanent;  #rewrite ^(.*)$ $scheme://$pure_host$1 permanent;  } } Choć to podejście nie jest zalecane (pomimo zwięzłości). Lepiej zdefiniować dwa bloki server z domenami www.* i domenami bez www na początku. Ale z drugiej strony to cholernie wygodne\u0026hellip; 😃\nPrzekierowanie \u0026ldquo;pozostałych\u0026rdquo; zapytań na domyślną domenę server { listen 80 default; server_name _; rewrite ^ $scheme://www.domena.com; #rewrite ^ $scheme://www.domena.com/search/$host; } To bardzo przydatny przykład - czyli domyślny vhost, który \u0026ldquo;przyjmie\u0026rdquo; wszystkie zapytania do domen nie zdefiniowanych w konfiguracji i przekieruje na naszą \u0026ldquo;główną stronę\u0026rdquo;.\nZakomentowany przykład jest nieco bardziej przekombinowany bo próbuje wykorzystać wyszukiwarkę na naszej stronie do wyszukania \u0026ldquo;czegoś\u0026rdquo; pomocnego - z tym przykładem należy uważać bo jeśli do serwera trafi dużo błędnych zapytań to może zostać przeciążony \u0026ldquo;bzdetnymi\u0026rdquo; wyszukiwaniami.\nPrzekierowanie pewnych podstron po zmianie struktury strony server { listen 80; server_name www.domain.com; location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /tag/something $scheme://new.domain.com permanent; rewrite /category/hobby /category/painting permanent; # etc ...  rewrite ^ /index.php last; } } Im starsza strona tym więcej zbiera się linków, których po prostu nie można usunąć, a które z racji wprowadzonych zmian nie mają prawa bytu w nowym układzie. Warto je przekierować w nowe miejsca, lub najbardziej odpowiadające/bliskie tym starym. Problemem może się wkrótce stać duża lista przekierowań, która zaciemni konfigurację.\nPowyższy sposób w dość optymalny sposób porządkuje takie przekierowania - najpierw sprawdza czy przypadkiem nie próbujemy pobrać istniejących plików, jeśli nie to wrzuca nas nas na listę przekierowań, a jeśli i tu nic nie znajdzie to zapytanie przekazywane jest do głównego skryptu strony.\nPrzekierowanie w zależności od wartości parametru w URI if ($args ~ producent=toyota){ rewrite ^ $scheme://toyota.domena.com$request_uri? permanent; } To rzadko stosowane przekierowanie a w dodatku mało czytelnie i ponoć mało wydajne\u0026hellip; Ale potrafi być bardzo przydatne gdy chcemy przepisać adres w zależności od wartości parametru np. gdy pewna podstrona doczeka się rozbudowy w zupełnie nowym serwisie lub gdy chcemy ładnie przekierować adresy ze starej strony na nową.\nBlokowanie dostępu do ukrytych plików location ~ /\\. { access_log off; log_not_found off; deny all; } Przyznaję - to nie rewrite\u0026hellip; Ale ta linijka jest równie przydatna - pozwala zablokować możliwość pobierania ukrytych plików (np. .htaccess\u0026rsquo;ów po konfiguracji z Apachego).\nWyłączenie logowania dla robots.txt i favicon.ico location = /favicon.ico { try_files /favicon.ico =204; access_log off; log_not_found off; } location = /robots.txt { try_files /robots.txt =204; access_log off; log_not_found off; } To też nie rewrite - ale bardzo fajnie obsługuje sytuację gdy mamy i gdy nie mamy powyższych dwóch pliczków. Po pierwsze wyłącza logowanie i serwuje je gdy są dostępne. Gdy nie istnieją to serwuje puste pliki (kod 204) dzięki czemu nie przeszkadzają nam 404-ki 😃\nBlokowanie dostępu do obrazków dla nieznanych referererów location ~* ^.+\\.(?:jpg|png|css|gif|jpeg|js|swf)$ { # definiujemy poprawnych refererow  valid_referers none blocked *.domain.com domain.com; if ($invalid_referer) { return 444; } expires max; break; } Zabezpieczenie warte tyle co nic bo banalne do ominięcia - ale jeśli zdarzy się że ktoś postanowi wykorzystać grafikę z naszej stronki np. w aukcji na allegro czy własnym sklepie to tą prostą sztuczką możemy go przyciąć i przeważnie jest to wystarczające.\nMuszę też zaznaczyć szczególne znaczenie wartości kodu błędu 444 w Nginx\u0026rsquo;ie - powoduje on zerwanie połączenia bez wysyłania jakiejkolwiek odpowiedzi. Jeśli nie chcemy być tak okrutni to możemy użyć innego kodu, np.: 403 albo 402 😃\nPrzekierowanie ciekawskich w \u0026ldquo;ciemną dupę\u0026rdquo; location ~* ^/(wp-)?admin(istrator)?/? { rewrite ^ http://whatismyipaddress.com/ip/$remote_addr redirect; } Ten prosty redirect odwodzi wielu amatorów zbyt głębokiego penetrowania naszej strony\u0026hellip; A pozostałych na pewno rozbawi 😃\nInne przykłady konfiguracji na mojej stronie:\n Nginx - hide server version and name in Server header and error pages Nginx - kompresowanie plików dla gzip_static Nginx - konfiguracja pod WordPress’a Nginx - ustawienie domyślnego vhosta Nginx - mój domyślny config  Źródła http://www.engineyard.com/blog/2011/useful-rewrites-for-nginx/\nhttp://wiki.nginx.org/HttpRewriteModule\n","href":"/2013/09/nginx-przydatne-rewritey-i-rozne-sztuczki/","title":"Nginx - przydatne rewrite’y i różne sztuczki"},{"content":"Chcąc pobawić się tor\u0026rsquo;em postanowiłem udostępnić jedna z moich stron z wykorzystaniem mechanizmu hidden service. Nie spodobał mi się jedynie sposób generowania nazw, które były mało opisowe - ale najwidoczniej nie mnie jednemu, bo szybko namierzyłem Shallot, który generuje kolejne nazwy aż trafi na pasującą do zadanego regexp\u0026rsquo;a.\n","href":"/2013/09/tor-generowanie-milszej-nazwy-dla-hidden-service/","title":"tor: generowanie milszej nazwy dla hidden service"},{"content":"Ostatnio zbyt dużo grzebię przy \u0026ldquo;windach\u0026rdquo; - ale cóż, czasem trzeba. Ostatnio ustawiałem DFS\u0026rsquo;a z replikacją dla dwóch sporych zasobów i jedna z rzeczy, o którą się rozbiłem to brak jakiegokolwiek podglądu tej synchronizacji z GUI. Ale znalazłem jedno polecenie, które działa w shellu (choć to się chyba batch tutaj nazywa) od Windows Server 2008 R2:\ndfsrdiag ReplicationState /member:nazwaservera Polecenie co prawda nie podaje postępu procentowego ale można zobaczyć \u0026ldquo;czy coś jeszcze się synchronizuje\u0026rdquo; i czy nie ma żadnych błędów. Jeżeli to polecenie to za mało to można spróbować bardziej gadatliwej wersji:\ndfsrdiag ReplicationState /member:nazwaservera /all ","href":"/2013/09/dfs-sprawdzanie-statusu-replikacji/","title":"DFS - sprawdzanie statusu replikacji"},{"content":"Ostatnio aktualizowałem swojego Nexusa do 4.3 i było miło tylko mi roota i recovery wystrzeliło\u0026hellip; No ale żaden problem - chwila googlania, kilka poleceń i mam recovery i roota. Dzisiaj wrzuciłem niedużą aktualizację, która łata kilka bugów i znów po root\u0026rsquo;cie ;/\nZapisze sobie więc instrukcję by kolejnym razem już nie googlać 😃\nP.S. Wiem co robię ryzykując uceglenie swojego urządzenia - u mnie ta instrukcja działa ale nie mogę tego zagwarantować każdemu - dlatego jeśli już się zdecydujesz to ROBISZ TO NA WŁASNĄ ODPOWIEDZIALNOŚĆ!\nBędzie potrzebne Android SDK lub przynajmniej mała paczka z fastboot i adb (więcej info o instalacji w linkach na końcu).\n Na początek sprawdzamy czy nie ma nowego recovery, preferuję CWM\u0026rsquo;a więc zerkamy tutaj Pobieram wersję touch (bo na tak dużym ekranie całkiem komfortowo się ją obsługuje): recovery-clockwork-touch-6.0.3.6-grouper.img Następna ważna paczka to SuperSU - sprawdzamy jaka jest najnowsza wersja: www.chainfire.eu, a gdy już to wiemy to googlamy za nią na xda developers lub download.chainfire.eu Aktualna paczka to: UPDATE-SuperSU-v1.55.zip Zaktualizowaną paczkę SuperSU pobieramy/wrzucamy na urządzenie. Wyłączamy tablet Włączamy tablet przytrzymując równocześnie przyciski Volume down + Power do czasu aż urządzenie zacznie startować - czekamy aż zobaczymy Bootmanagera (rozgrzebany robot 😃 ) Przechodzimy do katalogu z fastboot i otwieramy tam Command Line (np. przytrzymując Shift na folderze i wybierając opcję Otwórz tutaj okno wiersza poleceń - u mnie:  cd D:\\Nexus 7\\4.3\\adt-bundle-windows-x86_64-20130729\\sdk\\platform-tools  Wpisujemy polecenie (uwzględniają wersję recovery, którą wgrywamy):  fastboot flash recovery recovery-clockwork-touch-6.0.3.6-grouper.img Jeżeli wszystko pójdzie pomyślnie to powinno wypisać coś w rodzaju:\nsending \u0026#39;recovery\u0026#39; (6740 KB)... OKAY [ 0.826s] writing \u0026#39;recovery\u0026#39;... OKAY [ 0.486s] finished. total time: 1.312s Jeżeli zobaczymy: \u0026lt; waiting for device \u0026gt; tzn. że urządzenie nie jest gotowe i można wcisnąć Ctrl+C, może nawet trzeba będzie poszukać na to rozwiązania\u0026hellip; \n Korzystając z klawiszy Volume down/up wybieramy Recovery Mode i wybór potwierdzamy przyciskiem Power Po załadowaniu CWM\u0026rsquo;a wybieramy kolejno:  install zip choose zip from sdcard (lądujemy po tym w /sdcard) i w moim przypadku idę dalej do 0/Download/ wybieramy - UPDATE-SuperSU-v1.55.zip potwierdzamy wybór Yes - Install UPDATE-SuperSU-v1.55.zip po instalacji wybieramy +++++Go Back+++++ następnie reboot system now gdy pojawi się ROM may flash stock recovery on boot. Fix? THIS CAN NOT BE UNDONE. - wybranie YES nadpisze stock\u0026rsquo;owe recovery (właściwie po to instaluję CWM\u0026rsquo;a), ale można też wybrać NO a CWM przepadnie    No i brawo - mamy 4.3 build JWR66Y i root\u0026rsquo;a z CWM recovery 😃\nPo restarcie sprawdzamy czy mamy roota odpalając choćby SuperSU.\nŹródła http://www.info-pc.info/2013/07/how-to-root-nexus-7-android-43-jwr66v.html\nhttp://forum.xda-developers.com/showthread.php?t=2377511\nhttp://forum.xda-developers.com/showthread.php?t=1538053\n","href":"/2013/09/rootowanie-androida-4-3-na-google-nexus-7-po-aktualizacji-do-jwr66y/","title":"Root’owanie Androida 4.3 na Google Nexus 7 po aktualizacji do JWR66Y"},{"content":"Niedawno chciałem skopiować maszynę wirtualną z jednego hypervisora na innego. Były to 3 wolumeny LVM o rozmiarach od 50 do 100GB. Dawno temu zrobiłem sobie skrypty do backupu - jeden kompresuje wolumeny LVM - a drugi pozwala odtworzyć z dekompresja na drugim serwerze. Tyle że przy tak dużej maszynce będzie to trwało masakrycznie długo - fajnie byłoby móc równocześnie kopiować i odtwarzać (live)\u0026hellip;\nI wtedy przypomniało mi się narzędzie netcat - zrobiłem snapshoty wolumenów i mogłem zaczynać. W najbardziej podstawowej wersji potrzebowałem tylko tyle:\n na źródle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | nc -l -p 8888  na docelowym: nc 192.168.1.10 8888 | dd of=/dev/vgsas/vm1-sys   Lub wariacje z kompresją:\n na źródle: dd if=/dev/vgsas/vm1-sys | pv --size 50G | pigz -2 | nc -l -p 8888  na docelowym: nc 192.168.1.10 8888 | pigz -d | dd of=/dev/vgsas/vm1-sys   No dobra - pv nie jest najbardziej podstawowe\u0026hellip; Ale umożliwia podgląd postępu wysyłania/obierania (zależy, z której strony go wrzucić) co przy tak długim procesie jest niezmiernie przydatne.\nDo kompresji zalecałbym pigz (czyli Parallel GZIP) z ratio dostosowanym do przepustowości sieci - po gigabicie się nie opłacało nawet na ośmiordzeniowcu.\n","href":"/2013/09/kopiowanie-wolumenow-lvm-z-dd-i-netcat/","title":"Kopiowanie wolumenów LVM z dd i netcat"},{"content":"","href":"/tags/gpo/","title":"GPO"},{"content":"Raz na jakiś czas trzeba coś niestandardowego wrzucić do instalacji w Active Directory a że nie wszystkie aplikacje mają dostępne paczki MSI to trzeba się nieco natrudzić.\nPoniżej wrzucam skrypt, który instaluje GIMP\u0026rsquo;a 2.8 z domyślnego instalatora (wersja InnoSetup) przy okazji odinstalowując wcześniejsze wersje zainstalowane ręcznie.\nZapisujemy poniższy kod jako np. gimp-install.cmd\n@echo off REM Installs GIMP cls echo ---------------------------------------------------- echo . echo . echo . Installing/Updating GIMP - Please Wait echo . echo . echo ---------------------------------------------------- REM Test if actual IF exist \u0026#34;%ProgramFiles%\\GIMP\\bin\\gimp-2.8.exe\u0026#34; GOTO SkipInstall REM Exit the application taskkill.exe /F /FI \u0026#34;IMAGENAME eq gimp-2.8.exe\u0026#34; \u0026gt;nul REM Uninstall existing GIMP version, delete folder if exist \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; \u0026#34;%ProgramFiles%\\GIMP 2\\uninst\\unins000.exe\u0026#34; /VERYSILENT :: Wait for 20 seconds ping -n 40 127.0.0.1 \u0026gt; NUL if exist \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; rd \u0026#34;%ProgramFiles%\\GIMP 2\\\u0026#34; /Q /S REM Install new version \u0026#34;\\\\serwerplikow.local\\Instalki\\GIMP\\gimp-2.8.4-setup.exe\u0026#34; /VERYSILENT /NORESTART /DIR=\u0026#34;%PROGRAMFILES%\\GIMP 2.8\u0026#34; REM Skip installation if acctuall :SkipInstall REM Return exit code to SCCM exit /B %EXIT_CODE% Tworzymy nową regułkę GPO i zmierzamy do: Computer Configuration\\Policies\\Windows Settings\\Scripts\\Startup\nW nowym okienku wybieramy Show Files\u0026hellip;\nWklejamy plik skryptu do tego folderu i teraz możemy dodać go w tym samym oknie (Add\u0026hellip;) - dzięki wrzuceniu skryptu w tym miejscu będzie się on automatycznie replikować na inne kontrolery. Skrypt będzie co prawda uruchamiany przy każdym starcie komputera ale pierwszy warunek będzie sprawdzać czy aplikacja jest zainstalowana więc nie spowolni to znacznie startu.\nSkrypt znalazłem gdzieś na sieci ale nie mogę namierzyć źródła.\n","href":"/2013/08/gpo-instalacja-gimpa-2-8/","title":"GPO: Instalacja GIMP’a 2.8"},{"content":"Narzędzia xz-utils dostępne w nowszych systemach korzystają z mocniejszych algorytmów kompresji (jakaś odmiana LZMA, coś w stylu 7zip\u0026rsquo;a) przy zachowaniu kompatybilności składni poleceń z gzip\u0026rsquo;em/bzip\u0026rsquo;em - da się je zatem łatwo zintegrować w obecnych systemach. Ja chciałem wykorzystać xz do kompresji logów, które bywają przydatne ale przez większość czasy tylko zajmują miejsce :simple_smile:\nW /etc/logrotate.conf dopisujemy:\ncompresscmd /usr/bin/xz uncompresscmd /usr/bin/unxz compressext .xz compressoptions -9T2 compressoptions można nie ustawiać bo domyślnie ma wartość -9 (czyli kompresuj na maxa), mój dodatek (czyli -T2) użyje dwóch wątków procesora gdy już ten mechanizm zostanie zimplementowany (bo na razie nie jest) :simple_smile:\n","href":"/2013/07/logrotate-kompresja-logow-xz/","title":"logrotate: kompresja logów xz"},{"content":"Na jednym z urządzeń miałem problem z odtworzeniem plików (głównie MKV) z dźwiękiem zakodowanym w DTS. Pomijając że np. na tablecie 6-cio kanały DTS jest mi \u0026ldquo;niezbędny inaczej\u0026rdquo; to konwertując go do AAC stereo plik jest po prostu sporo mniejszy. Oczywiście nie zamierzam transkodować ścieżki video i na moje potrzeby mogłem sobie odpuścić zmianę częstotliwości próbkowania.\nNajprościej wykorzystać pakiet ffmpeg (po nowemu avconv) lub mencoder (choć ten miewał niegdyś problem z poprawnym zapisywaniem wynikowych plików mkv, więc potrzebny jest dodatkowo mkvmerge z pakietu mkvtoolnix). mencoder transkoduje szybciej wykorzystując więcej rdzeniu CPU, ale później potrzebny był drugi przebieg z mkvmerge. ffmpeg jechał na jednym procku nawet z opcją threads ale za to wszystko mogę zrobić jednym poleceniem.\nPaczki instalujemy tak:\napt-get install libav-tools libavcodec-extra MKV DTS do MKV AAC stereo avconv -i input.mkv -c copy -c:a libvo_aacenc -b:a 128k -ac 2 -threads auto output-stereo-aac.mkv Gdybyśmy chcieli zakodować wszystkie kanały z DTS do AAC to wystarczy pominąć parametr -ac 2.\nMKV DTS do MKV AC3 (wszystkie kanały) avconv -i input.mkv -c copy -c:a libvo_aacenc -b:a 128k -threads auto output-ac3.mkv MKV DTS do MKV AC3 z mencoder\u0026rsquo;em AVI=`mktemp video.XXXXXX.avi` mencoder \u0026#34;input.mkv\u0026#34; -o $AVI -oac lavc -lavcopts acodec=ac3:abitrate=448 -ovc copy mkvmerge $AVI -o output.AC3.mkv rm $AVI ","href":"/2013/07/bezstratna-konwersja-mkv-z-dts-do-ac3-lub-aac/","title":"Bezstratna konwersja MKV z DTS do AC3 lub AAC"},{"content":"Raz na jakiś czas gdy grzebię przy maciorach muszę \u0026ldquo;odkryć\u0026rdquo; nowy volumen FC (lub rzadziej SCSI), który właśnie utworzyłem a restart serwera nie wchodzi w rachubę (zresztą na części systemów nic on nie da).\nBy to zrobić są dwie możliwości:\nRęczne wydanie poleceń odkrywających volumeny (na jajkach od 2.6.x) Sprawdzamy jakie mamy karty:\nls /sys/class/fc_host/ (wypisze się coś w stylu: host1, host2)\nWydajemy do wybranej przez nas karty żądanie wykonania LIP (to się chyba tłumaczy jako loopback initialization) co skutkuje przeskanowaniem szyny FC:\necho 1 \u0026gt;/sys/class/fc_host/host1/issue_lip Czekamy 15~30 sekund aby zadziałało polecenie.\nRządamy przeskanowania dostępnych volumenów SCSI/FC:\necho - - - \u0026gt;/sys/class/scsi_host/host1/scan (myślniki w echo oznaczają sprawdzenie wszystkich kanałów, targetów i lun\u0026rsquo;ów - jeżeli mamy bardzo dużo volumenów to można tutaj nieco optymalizować, ale to nie był mój problem)\nOdpalamy np. dmesg aby zobaczyć jakie nowe volumeny się pojawiły.\nŹródło: http://misterd77.blogspot.com/2007/12/how-to-scan-scsi-bus-with-26-kernel.html\nKorzystamy ze skryptu rescan-scsi-bus.sh Skrypt ten automatycznie robi to co potrzebujemy, skanując wszystkie karty FC pod kątem nowych volumenów.\nwget http://rescan-scsi-bus.sh/ -O rescan-scsi-bus.sh chmod +x rescan-scsi-bus.sh ./rescan-scsi-bus.sh I tyle!\nŹródło: http://rescan-scsi-bus.sh/\n","href":"/2013/07/dodawanie-urzadzen-scsifc-bez-restartu-serwera/","title":"Dodawanie urządzeń SCSI/FC bez restartu serwera"},{"content":"","href":"/tags/fc/","title":"FC"},{"content":"","href":"/tags/scsi/","title":"SCSI"},{"content":"","href":"/tags/bittorrent/","title":"BitTorrent"},{"content":"","href":"/tags/btsync/","title":"btsync"},{"content":"Bardzo spodobała mi się nowa aplikacja do zdalnej synchronizacji folderów z wykorzystaniem P2P. Ja wykorzystałem ją do automatycznych backupów archiwum zdjęć - zebrało mi się tego już prawie 130GB! Każde narzędzie, które chce np. je kompresować i raz na czas robić FULL backup jest skazane na porażkę - a fotek przybywa.\nNa początek pobieramy interesującą nas wersję:\nwget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_i386.tar.gz wget http://btsync.s3-website-us-east-1.amazonaws.com/btsync_x64.tar.gz Oraz paczkę ze skryptami dla Debiana:\nwget http://www.yeasoft.com/downloads/various/btsync-linux-deploy.tar.gz Ja wyciągam z niej skrypt /etc/init.d/btsync i konfigurację /etc/*\nTeraz user z ograniczonymi uprawnieniami, na którym będzie działać nasza usługa (można wykorzystać swojego usera aby móc synchronizować swoje foldery):\nuseradd -d /var/lib/btsync -m -s /bin/sh btsync Odpalamy z domyślną konfiguracją żeby tylko działało - dokonfiguruje później:\ncd /etc/btsync mv default.conf default.btsync.btsync.conf chown root:btsync default.btsync.btsync.conf chmod 640 default.btsync.btsync.conf service btsync start Powinno działać więc sprawdźmy:\nps axu | grep bts netstat -nlp Tadam!\n","href":"/2013/07/debian-instalacja-bittorrent-sync-btsync/","title":"Debian - Instalacja Bittorrent Sync (btsync)"},{"content":"Jest kilka powodów dla których tworzenie patchy jest przydatne - jeśli tu jesteś to pewnie masz jakiś własny\u0026hellip;\nTworzenie patch\u0026rsquo;a diff -crB old new \u0026gt; from-old-to-new.patch W powyższym poleceniu założyłem że old i new to katalogi z wieloma podkatalogami i plikami - stąd opcja -r. -c dodaje kilka linijek \u0026ldquo;kontekstu\u0026rdquo; przez co łatwiej rozeznać się w patch\u0026rsquo;u. Opcja -B ignoruje puste linie, których patchowanie mnie nie interesuje.\nPatchowanie Na początek zawsze warto wywołać polecenie z opcją -dry-run by zobaczyć czy patch wykona się poprawnie:\npatch --dry-run -p1 -i from-old-to-new.patch Opcja -p1 zakłada że uruchamiamy patcha z folderu \u0026ldquo;projektu\u0026rdquo;, który chcemy patchować. Opcję -i można z powodzeniem zastąpić przekierowaniem \u0026lt; .\nJeżeli nie widzimy żadnych komunikatów \u0026ldquo;FAILED\u0026rdquo; to możemy uruchomić patcha:\npatch -p1 -i from-old-to-new.patch ","href":"/2013/04/tworzenie-patchy-z-poleceniami-diff-i-patch/","title":"Tworzenie patch’y z poleceniami diff i patch"},{"content":"Miałem ostatnio zabawną sytuację gdy kilka serwerów z zainstalowanym NTPD miało rozjazdy rzędu kilkunastu sekund. Wyszło na to że moje serwery synchronizowały się z różnymi zewnętrznymi serwerami NTP pomiędzy, którymi były rozjazdy i te rozjazdy synchronizowały się na moich serwerach. Jeden \u0026ldquo;z moich\u0026rdquo; ustanowiłem głównym a wszystkie inne przekierowałem na niego (komentując wszystkie inne serwery NTP w konfiguracji). Wymusiłem synchronizację:\nntp -q Sprawdziłem jak duży jest offset i jitter (powinny być bardzo małe):\nntpq -p i tak rzeczywiście było - problem z głowy.\nP.S. Jeżeli nie potrzebujemy/chcemy/etc instalować serwera NTP to możemy zamiast tego użyć polecenia ntpdate w cronie (np. co godzinę):\nntpdate moj.serwer.ntp ","href":"/2013/03/rozsynchronizowane-serwery-ntp/","title":"Rozsynchronizowane serwery NTP"},{"content":"","href":"/tags/fortigate/","title":"Fortigate"},{"content":"Do niedawna na moim telefonie VPN\u0026rsquo;ami były: PPTP lub L2TP - oba niespecjalnie mi się podobały. Ale od wersji 4-tej pojawiły się dwa nowe tryby: IPSec Xauth PSK i IPSec Xauth RSA. W pierwszym autoryzacja wykorzystuje login i hasło, w drugim certyfikaty.\nTryb IPSec Xauth PSK jest bardzo wygodny bo łatwo można połączyć go z zewnętrznymi mechanizmami uwierzytelniającymi np. LDAP, Active Directory, itp.\nPokażę jak skonfigurować swojego Fortigate\u0026rsquo;a by umożliwić połączenie z telefonów i tabletów na Androidzie 4.x do \u0026ldquo;Intranetu\u0026rdquo;. Większość konfiguracji można przeprowadzić tylko w trybie CLI - zakładam że wiesz jak to zrobić. To co wygodniej można zrobić w trybie WWW to głównie tworzenie reguł dostępu na zaporze.\nP.S. Teoretycznie powinno to też zadziałać na IPhone\u0026rsquo;ach/IPad\u0026rsquo;ach itp., przy czym udało mi się to zestawić na 4GS ale na 5-ce już nie - nie miałem tych aparatów na tyle długo by dokładniej to zbadać.\nNa początek konfigurujemy fazę pierwszą - ja lubię interface mode ale oczywiście będzie to działać również w trybie policy (trzeba będzie nieco polecenia zmienić).\nconfig vpn ipsec phase1-interface edit \u0026#34;vpn-android\u0026#34; set type dynamic set interface \u0026#34;port1\u0026#34; set dhgrp 2 set peertype one set xauthtype auto set mode aggressive set mode-cfg enable set proposal aes128-sha1 aes128-md5 3des-sha1 set negotiate-timeout 15 set peerid \u0026#34;jakis-mobilny-identyfikator\u0026#34; set authusrgrp \u0026#34;grupa-ludzikow\u0026#34; set psksecret haslo set unity-support enable set ipv4-start-ip 172.16.0.5 set ipv4-end-ip 172.16.0.100 set ipv4-netmastk 255.255.255.0 set ipv4-dns-server1 172.16.0.1 set ipv4-dns-server2 8.8.8.8 next end Jedna ciekawostka - opcja unity-support domyślnie jest ustawiana na enable i proponuję ją tak zostawić - dzięki temu ta sama konfiguracja będzie działać na urządzeniach z iOS\u0026rsquo;e i OS X, wystarczy skonfigurować VPN jako Cisco 😃\nNo to faza druga:\nconfig vpn ipsec phase2-interface edit \u0026#34;mobile-vpn-p2\u0026#34; set keepalive enable set pfs disable set phase1name \u0026#34;vpn-android\u0026#34; set proposal aes128-sha1 aes128-md5 3des-sha1 set keylifeseconds 3600 next end Kilka ustawień na interfejsie:\nconfig system interface edit \u0026#34;vpn-android\u0026#34; set ip 172.16.0.1 255.255.255.255 set allowaccess ping set type tunnel set remote-ip 139.x.x.10 set interface \u0026#34;port1\u0026#34; next end Włączymy sobie serwer DNS na interfejsie VPN:\nconfig system dns-server edit \u0026#34;vpn-android\u0026#34; next end No to teraz trzeba utworzyć co najmniej jedną regułę na zaporze zezwalającą na dostęp z interfejsu vpn-android (z adresów tej sieci) do jakiejś sieci intranetowej (np. wewnętrznego serwera WWW), na odpowiednich portach.\nPonadto jeśli chcemy móc korzystać z cache DNS\u0026rsquo;a na interfejsie vpn-android to musimy umożliwić do niego dostęp tworząc regułę z interfejsu vpn-android (z adresów przydzielanych przez mode-cfg) do vpn-android (przynajmniej do DNS\u0026rsquo;a - 172.16.0.1).\nJeżeli chcemy by urządzenia mobilne mogły łączyć się do zewnętrznych usług przez tunel VPN (co w niektórych przypadkach może być bardzo cenne) to musimy też utworzyć regułę z interfejsu vpn-android do któregoś z interfejsów WAN\u0026rsquo;owskich.A tak należy to wyklikać na telefonie\nhttp://kb.fortinet.com/kb/viewContent.do?externalId=FD31619\u0026amp;sliceId=1\n","href":"/2013/03/fortigate-vpn-ipsec-psk-xauth-z-androida-4-x/","title":"Fortigate - VPN IPSec PSK XAuth z Android’a 4.x"},{"content":"","href":"/tags/fortios/","title":"FortiOS"},{"content":"","href":"/tags/vpn/","title":"VPN"},{"content":"On Debian you have to install nginx-extras package (because it have built in headers_more module). Then you need two options (best in global configuration /etc/nginx/nginx.conf file, http part):\nserver_tokens off; more_set_headers \u0026#39;Server: BadAss\u0026#39;; And it\u0026rsquo;s good to setup non standard error pages on every site (500 and 404 at minimum):\nerror_page 403 404 http://mysite.com/areyoulost; error_page 502 503 504 /500.html; ","href":"/2013/01/nginx-hide-server-version-and-name-in-server-header-and-error-pages/","title":"Nginx - hide server version and name in Server header and error pages"},{"content":"W PHP 5.3 pojawiła się nowa zmienna: max_input_vars, która limituje ilość pól możliwych do przesłania przez formularz, obcinając nadmiarowe. Pozwala to zapobiec atakom DoS na tablice hashujące (przynajmniej w tym jednym miejscu). Domyślna wartość tej zmiennej to 1000 i kreatywnym programistom udaje się tą wartość bez problemu osiągnąć 😃\nWarte odnotowania jest to że mając suhosin\u0026rsquo;a trzeba pamiętać o jeszcze dwóch innych zmiennych:\nmax_input_vars = 3000 suhosin.post.max_vars = 3000 suhosin.request.max_vars = 3000 Zmienne można zmienić od razu w /etc/php5/apache2/php.ini (choć te dla suhosin\u0026rsquo;a lepiej wrzucić do /etc/php5/conf.d/suhosin.ini). A jeszcze lepszym pomysłem jest ustawienie tych zmiennych bezpośrednio dla danego vhost\u0026rsquo;a, w Apache\u0026rsquo;m np. tak:\nphp_value max_input_vars 3000 php_value suhosin.post.max_vars 3000 php_value suhosin.request.max_vars 3000 ","href":"/2013/01/php-max_input_vars/","title":"PHP - max_input_vars"},{"content":"","href":"/tags/badblocks/","title":"badblocks"},{"content":"Dyski się zużywają i w końcu wcześniej czy później pojawiają się na nich bad sectory. Jeden z moich dysków ciut się posypał a że służy wyłącznie do backupów to mogę z tym żyć. Ale z drugiej strony jeżeli już będę musiał sięgnąć do backupów to chcę mieć pewność że coś odzyskam, dlatego postanowiłem zrobić kilka testów. Nawet jeśli nie naprawi to sektorów to przynajmniej zostaną zaznaczone jako uszkodzone i realokowane.\nNa początek zacząłem od próby puszczenia badblocks w trybie nie destruktywnym na całym dysku:\nbadblocks -b 4096 -nsv /dev/sdf Poszukiwanie wadliwych bloków w trybie z niedestruktywnym zapisem Od bloku 0 do 488378645 Poszukiwanie wadliwych bloków (odczyt i niedestruktywny zapis) Testowanie wzorcem losowym: ^Ctowe w 1.28%, minęło 10:47 (błędów: 0/0/0) Interrupted at block 6262912 Otrzymano przerwanie, sprzątam Zaczekałem 10 minut i na podstawie bieżących statystyk oszacowałem że test zajmie ponad 13 godzin\u0026hellip; Jak widać przerwałem - sprawdźmy więc co powiedzą testy smart\u0026rsquo;a. Co prawda niczego one nie naprawią ale zidentyfikują, na którym sektorze zaczynają się problemy. Będę mógł wtedy puścić badblocks już od tego miejsca oszczędzając nieco czasu\u0026hellip; Więc:\nsmartctl -t long /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34;. Drive command \u0026#34;Execute SMART Extended self-test routine immediately in off-line mode\u0026#34; successful. Testing has begun. Please wait 255 minutes for test to complete. Test will complete after Sat Nov 24 03:59:21 2012 Use smartctl -X to abort test. 255 minut to trochę ponad 4 godziny - przynajmniej skończy się do rana. No to jeszcze:\nshutdown -h 4:15 I zerkniemy jutro co znajdzie test 😃\nDzień drugi Zerkamy\u0026hellip; i:\nsmartctl -l selftest /dev/sdf smartctl 5.41 2011-06-09 r3365 [x86_64-linux-3.2.0-34-generic] (local build) Copyright (C) 2002-11 by Bruce Allen, http://smartmontools.sourceforge.net === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Extended offline Completed: read failure 90% 15196 164529056 # 2 Short offline Aborted by host 10% 15192 - Widać tutaj wyniki dwóch testów - przy czym interesuje nas ten dłuższy z numerkiem 1. Niestety mam jakiś bad sektor i mam pierwszy adres jego wystąpienia - szkoda że to gdzieś w pierwszych 10% dysku - miałem nadzieję że gdzieś dalej (byłoby mniej do sprawdzenia). Ale jeśli badsectory zaczynają się tak wcześnie to puszczę badblocks od początku - niech przejedzie cały dysk - albo się coś naprawi, albo padnie. Ale przynajmniej sprawa się wyjaśni 😉\nŹródła http://smartmontools.sourceforge.net/badblockhowto.html\nhttp://sourceforge.net/apps/trac/smartmontools/wiki/SamsungF4EGBadBlocks\nman badblocks\n","href":"/2012/12/linux-naprawianie-bad-sectorow/","title":"Linux - naprawianie bad sectorów"},{"content":"","href":"/tags/smartctl/","title":"smartctl"},{"content":"","href":"/tags/piwik/","title":"Piwik"},{"content":"Korzystam z instancji Piwik\u0026rsquo;a do monitorowania odwiedzin na stronie i postanowiłem pokombinować czy da się w ten sposób monitorować wejścia konkretnych osób na bazie wpisanego w polu komentarza loginu/ksywki. Jak zacząłem grzebać to przy okazji zmieniłem też sposób ładowania skryptów Piwika na asynchroniczny.\nA leci to mniej więcej tak:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var i,x,y,ARRcookies=document.cookie.split(\u0026#34;;\u0026#34;); var comment_author = \u0026#34;\u0026#34;; for (i=0;i\u0026lt;ARRcookies.length;i++) { x=ARRcookies[i].substr(0,ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)); y=ARRcookies[i].substr(ARRcookies[i].indexOf(\u0026#34;=\u0026#34;)+1); x=x.replace(/^\\s+|\\s+$/g,\u0026#34;\u0026#34;); if (x.indexOf(\u0026#34;comment_author\u0026#34;) != -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_email\u0026#34;) == -1 \u0026amp;\u0026amp; x.indexOf(\u0026#34;comment_author_url\u0026#34;) == -1) { comment_author = unescape(y); } } var _paq = _paq || []; (function(){ var u=((\u0026#34;https:\u0026#34; == document.location.protocol) ? \u0026#34;https://url.instancji.piwika.pl/\u0026#34; : \u0026#34;http://url.instancji.piwika.pl/\u0026#34;); _paq.push([\u0026#39;setSiteId\u0026#39;, 1]); _paq.push([\u0026#39;setTrackerUrl\u0026#39;, u+\u0026#39;piwik.php\u0026#39;]); _paq.push([\u0026#39;setCustomVariable\u0026#39;,\u0026#39;1\u0026#39;,\u0026#39;Author\u0026#39;, comment_author]); _paq.push([\u0026#39;trackPageView\u0026#39;]); _paq.push([\u0026#39;enableLinkTracking\u0026#39;]); var d=document, g=d.createElement(\u0026#39;script\u0026#39;), s=d.getElementsByTagName(\u0026#39;script\u0026#39;)[0]; g.type=\u0026#39;text/javascript\u0026#39;; g.defer=true; g.async=true; g.src=u+\u0026#39;piwik.js\u0026#39;; s.parentNode.insertBefore(g,s); })(); \u0026lt;/script\u0026gt; Źródło http://piwik.org/docs/javascript-tracking/#toc-asynchronous-tracking http://codex.wordpress.org/WordPress_Cookies#Commenters\n","href":"/2012/12/piwik-sledzenie-asynchroniczne-logowanie-ksywy-komentujacego-w-wordpressie/","title":"Piwik: śledzenie asynchroniczne + logowanie ksywy komentującego w WordPress’ie"},{"content":"Dawno, dawno temu\u0026hellip; Za górami, za lasami\u0026hellip; czytałem sobie tekst Lemat\u0026rsquo;a o dokuczaniu spamerom i pomyślałem że sam też tak mogę i nawet chcę więc popełniłem skrypcik, który dla losowych słów generował maile. Skrypcik działał z dwa lata na mojej poprzedniej stronie i nie raz zdarzyło się tam jakiejś mendzie zapętlić. Jakoś nie miałem czasu od razu, a później zapomniałem wrzucić go na nową stronie i tak zostało - na pewien czas.\nNiedawno przeglądając logi zauważyłem że jakieś spam-boty jednak tęsknią za tą podstroną i postanowiłem ją wskrzesić.\nPomysł jest prosty i polega na generowaniu na pewnej podstronie dużej ilości \u0026ldquo;z dupy\u0026rdquo; maili. Oczywiście w robots.txt dajemy Disallow dla tej ścieżki i jeśli bot jest kulturalny to nie będzie tam zaglądał. Crawlery spamerów przeważnie kulturalne nie są więc tam wpadną i się zapętlą zapychając sobie bazę śmieciami - taka baza traci wartość dla klientów spamera, więc przeważnie skutkuje to dodaniem naszej strony na spamerską \u0026ldquo;black listę - nie crawlować\u0026rdquo;. Dla mnie cool 😃 (w rzadszych przypadkach spamer może chcieć się zemścić\u0026hellip;).\nJeżeli ktoś będzie zainteresowany to mogę udostępnić skrypcik (ma postać szablonu strony dla WordPress\u0026rsquo;a) - choć zachęcam do samodzielnego napisania własnego - zawsze trudniej będzie spamerom z automatu go przeskoczyć (jak np. antyspam.pl). Należy przy tym zwrócić uwagę na kilka rzeczy:\n nie zrób sobie kuku - nie generuj fake emaili dla własnych, istniejących domen (bo może się okazać że któryś spamer jednak spróbuje wysłać te setki tysięcy błędnych maili i zrobić Ci DDOS\u0026rsquo;a zabijając serwer błędnie nadanymi mailami), nie rób innym kuku - nie generuj fake maili w cudzych domenach (no chyba że w domenach Canadian Pharmacy), jeżeli jednak nadal upierasz się przy istniejącej domenie (np. by śledzić statystyki zapytań\u0026hellip;) to ustaw rekord MX na hosta z IP 127.0.0.1 - jak dobrze pójdzie spamer sam sobie zrobi DOS\u0026rsquo;a (spodziewałbym się jednak w takiej sytuacji odwetu) :simple_smile:  Źródło szablonu Paczkę z szablonem można pobrać tutaj.\nPaczka zawiera plik havefun_template.php, który należy umieścić w katalogu szablonu WordPress\u0026rsquo;a. Ja wykorzystałem w generatorze wp_post z WordPress\u0026rsquo;a - zmodyfikowania zapytania dla innej bazy nie powinno być zbyt trudne. Samo zapytanie nieco zoptymalizowałem przez co zwraca mniej losowe wyniki (zwraca losowy blok n wierszy z tabeli a nie całkiem losowe elementy) co dla potrzeb tego skryptu jest całkiem OK. Jeżeli pomysł z bazą się nie podoba (bo np. może generować zbyt duże obciążenie) to proponuję wyszukać sobie jakiś generator Lorem ipsum\u0026hellip;\nMiłej zabawy.\nBawiąc się ostatnio nowym szablonem Twenty Thirteen przygotowałem nową wersję pliku havefun_template.php dla tego szablonu.\n","href":"/2012/12/jak-dokuczac-spamerom/","title":"Jak dokuczać spamerom"},{"content":"Ruski serwer WWW ma przydatną funkcję serwowania wersji plików skompresowanych gzip\u0026rsquo;em - przez co możemy plik skompresować raz i będzie on serwowany klientom obsługującym kompresję HTTP ale już bez każdorazowego kompresowania go. Jest to bardzo przydatne na stronach z dużym ruchem gdzie można w ten sposób zaoszczędzić takty CPU na właściwą obsługę połączeń a nie kompresję. Drugie miejsce gdzie może to być przydatne to VPS\u0026rsquo;y i \u0026ldquo;cienkie\u0026rdquo; serwery, które na kompresji przy większym obciążeniu spędzają zbyt dużo czasu i daje się to odczuć w działaniu strony. A że obecnie standardem jest ładowanie przez stronki np. jQuery, paru pluginów do niego, jQueryUI, masy CSS\u0026rsquo;ów, itd - to na prawdę jest co kompresować 😃\nDodatkowa zaleta tego rozwiązania jest taka, że o ile w przypadku kompresji online raczej kompresujemy na niskich poziomach by nieco zmniejszyć dane a równocześnie nie zabijać CPU - to w przypadku kompresji pod ten moduł możemy raz na jakiś czas \u0026ldquo;przejechać\u0026rdquo; pliki maksymalną kompresją by były jeszcze mniejsze (zaoszczędzi to nieco pasma i równocześnie przyspieszy ładowanie strony bo jest mniej do pobrania).\nPrekompresję wybranych typów plików można przeprowadzić np. tak:\nfind /var/www -iname *.js -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; find /var/www -iname *.css -print0 |xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;gzip -c9 \u0026#34;{}\u0026#34; \u0026gt; \u0026#34;{}.gz\u0026#34; \u0026amp;\u0026amp; touch -r \u0026#34;{}\u0026#34; \u0026#34;{}.gz\u0026#34;\u0026#39; gzip skompresuje a touch zadba by timestampy obu plików były takie same (co jest zalecaną praktyką przy wykorzystaniu tego modułu). Dwa powyższe polecenia można odpalić praktycznie w ciemno na obecnie serwowanej stronie i nie powinny spowodować zbyt dużego chaosu\u0026hellip; Trzeba pamiętać jedynie by zostały uruchomione z użytkownika, który ma dostęp do zapisu w danym vhoście i by serwer WWW miał dostęp do tak utworzonego pliku.\nZ moich obserwacji wynika że gdy timestamp pliku bez gz jest nowszy niż wersji skompresowanej to serwowana jest ta świeższa wersja ale wtedy już z ewentualną kompresją online.\nMocniejsze kompresowanie plików PNG Może nie do końca w temacie tego posta ale bardzo często gdy stosuję gzip_static_module to optymalizuję też PNG\u0026rsquo;i. Pliki PNG są kompresowane w sposób bezstratny ale przeważnie programy graficzne nie zapisują ich w najmniejszej możliwej postaci bo trwałoby to zbyt długo. Można użyć do tego celu narzędzi takich jak: optipng lub pngcrush. A poniżej odpowiedni one-liner by skompresować wszystkie PNG\u0026rsquo;i w serwisach WWW:\nfind /var/www -iname *.png -print0 |xargs -0 optipng -quiet -preserve -o7 Uwaga! Wywołanie tego skryptu może potrwać bardzo długo i z tego powodu raczej nie nadaje się do umieszczenia w cronie. Jednak ręczne puszczenie po dużych zmianach w serwisie lub z filtrem w find by kompresowane były tylko nowsze pliki będzie OK.\nOptymalizowanie plików JPG/JPEG Co prawda pliki JPG/JPEG kompresują się stratnie ale możne je nieco zoptymalizować np. usuwając niepotrzebne dane z nagłówków i zamieniając na progresywne. Możemy do tego wykorzystać takie polecenie:\nfind /var/www -iname \u0026#39;*.jpg\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;jpegtran -optimize -progressive -copy none -outfile \u0026#34;{}\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; ","href":"/2012/12/nginx-kompresowanie-plikow-dla-gzip_static/","title":"Nginx - kompresowanie plików dla gzip_static"},{"content":"Gdy już się dorobi systemu Active Directory wygodnie jest wykorzystać jego bazę użytkowników do autoryzacji w różnych miejscach, np. do pewnych \u0026ldquo;tajnych i tajniejszych\u0026rdquo; stron w Apache. Najprościej można to zrobić z wykorzystaniem LDAP.\nWarto sprawdzić czy i jak możemy dostać się do kontrolerów. Gdy już mamy wszystkie potrzebne parametry konfigurujemy Apachego - na początek aktywujemy moduły:\na2enmod ldap a2enmod authnz_ldap Teraz możemy edytujemy globalny plik konfiguracyjny mod_ldap\u0026rsquo;a by ustawić nieco cache\u0026rsquo;y (bardzo przydatne). Wartości można dostosować do potrzeb ale przykładowe powinny wystarczyć na początku:\nLDAPSharedCacheSize 500000 LDAPCacheEntries 1024 LDAPCacheTTL 600 LDAPOpCacheEntries 1024 LDAPOpCacheTTL 600 \u0026lt;Location /ldap-status\u0026gt; SetHandler ldap-status Order deny,allow Deny from all # Allow from 127.0.0.1 ::1 Allow from 192.168.1.15/32 Satisfy all \u0026lt;/Location\u0026gt; Warto zwrócić uwagę na drugą część - można aktywować podgląd statusu mod_ldap\u0026rsquo;a co bywa przydatne na etapie testów (np. by zobaczyć kto aktualnie jest zalogowany) - ja umożliwiłem dostęp ze swojego zdalnego komputera. Teraz restart Apachego aby wszystko się załadowało:\ninvoke-rc.d apache2 restart Środowisko mamy gotowe, możemy skonfigurować vhosta. Poniższe opcje wrzucamy np. do klauzuli \u0026lt;Directory\u0026gt; lub \u0026lt;Location\u0026gt;:\nAuthtype basic Authname \u0026#34;Zaloguj sie kontem domenowym\u0026#34; AuthBasicProvider ldap AuthzLDAPAuthoritative Off AuthUserFile /dev/null AuthLDAPUrl \u0026#34;ldap://server1.nazwadomeny.local:389 10.0.0.32:389/DC=nazwadomeny,DC=local?sAMAccountName?sub?(objectClass=person)\u0026#34; NONE AuthLDAPBindDN \u0026#34;CN=kontodomenowe,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; AuthLDAPBindPassword \u0026#34;haslo konta domenowego\u0026#34; Require valid-user # Require ldap-dn ou=Jakies przykladowe OU Najważniejsze parametry to:\n AuthLDAPUrl - tutaj podajemy parametry do połączenia LDAP, w pierwszej części możemy podać kilka serwerów by mieć backup, później korzeń od którego będzie rozpoczynane przeszukiwanie drzewa (tutaj cała domena), na końcu filtry, AuthLDAPBindDN - konto którym autoryzujemy się do kontrolera by móc przeszukiwać na nim obiekty (podane jako ścieżka LDAP), AuthLDAPBindPassword - hasło do powyższego konta, Require ldap-dn - można nie tylko sprawdzać czy użytkownik jest prawidłowy ale również czy jest w pewnym OU, grupie itd\u0026hellip;  Teoretycznie wszystko jest poprawnie ale za cholerę nie chciało działać dopiero dodanie do /etc/ldap/ldap.conf opcji: REFERRALS off sprawiło że autoryzacja działa poprawnie - oneliner by to uzyskać:\necho \u0026#34;REFERRALS off\u0026#34; \u0026gt;\u0026gt; /etc/ldap/ldap.conf Ja mam Apachego 2.2 i jest to jedyna opcja by uzyskać ten efekt. W wersji od 2.4 jest dodatkowa opcja LDAPReferrals, która pozwala na zmianę tego zachowania wprost z konfiguracji Apachego.\n","href":"/2012/12/apache-mod_authnz_ldap-z-active-directory/","title":"Apache: mod_authnz_ldap z Active Directory"},{"content":"","href":"/tags/ldap/","title":"LDAP"},{"content":"Chciałem wysłać z Python\u0026rsquo;a maila z krzakami tab by ładnie się wyświetlały i okazało się to całkiem nietrywialne.\nNa szczęście googiel podpowiedział mi doskonałego gotowca, którego zamierzam zapisać by mi nie zginął:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import smtplib from email.mime.text import MIMEText from email.Header import Header from email.Utils import parseaddr, formataddr def send_email(sender, recipient, subject, body): \u0026#34;\u0026#34;\u0026#34;Send an email. All arguments should be Unicode strings (plain ASCII works as well). Only the real name part of sender and recipient addresses may contain non-ASCII characters. The email will be properly MIME encoded and delivered though SMTP to localhost port 25. This is easy to change if you want something different. The charset of the email will be the first one out of US-ASCII, ISO-8859-1 and UTF-8 that can represent all the characters occurring in the email. \u0026#34;\u0026#34;\u0026#34; # Header class is smart enough to try US-ASCII, then the charset we # provide, then fall back to UTF-8. header_charset = \u0026#39;ISO-8859-2\u0026#39; # We must choose the body charset manually for body_charset in \u0026#39;US-ASCII\u0026#39;, \u0026#39;UTF-8\u0026#39;, \u0026#39;ISO-8859-2\u0026#39;: try: body.encode(body_charset) except UnicodeError: pass else: break # Split real name (which is optional) and email address parts sender_name, sender_addr = parseaddr(sender) recipient_name, recipient_addr = parseaddr(recipient) # We must always pass Unicode strings to Header, otherwise it will # use RFC 2047 encoding even on plain ASCII strings. sender_name = str(Header(unicode(sender_name), header_charset)) recipient_name = str(Header(unicode(recipient_name), header_charset)) # Make sure email addresses do not contain non-ASCII characters sender_addr = sender_addr.encode(\u0026#39;ascii\u0026#39;) recipient_addr = recipient_addr.encode(\u0026#39;ascii\u0026#39;) # Create the message (\u0026#39;plain\u0026#39; stands for Content-Type: text/plain) msg = MIMEText(body.encode(body_charset), \u0026#39;plain\u0026#39;, body_charset) msg[\u0026#39;From\u0026#39;] = formataddr((sender_name, sender_addr)) msg[\u0026#39;To\u0026#39;] = formataddr((recipient_name, recipient_addr)) msg[\u0026#39;Subject\u0026#39;] = Header(unicode(subject), header_charset) # Send the message via SMTP to localhost:25 smtp = smtplib.SMTP(\u0026#34;localhost\u0026#34;) smtp.sendmail(sender, recipient, msg.as_string()) smtp.quit() Wykorzystanie:\nsend_email( u\u0026#34;Gąska \u0026lt;gaska@test.pl\u0026gt;\u0026#34;, u\u0026#34;Tchórz \u0026lt;tchorz@test2.pl\u0026gt;\u0026#34;, u\u0026#34;Grzegrzółkę testując..\u0026#34;, u\u0026#34;ąśłóŧ itd...\u0026#34; ) Źródło http://mg.pov.lt/blog/unicode-emails-in-python.html\n","href":"/2012/12/python-wysylanie-maili-w-unicode/","title":"Python - wysyłanie maili w unicode"},{"content":"Można lubieć AD, można go nie lubieć\u0026hellip; Ale jak już się ma to warto czasem zintegrować go z tym\u0026hellip; i tamtym\u0026hellip; Od strony Linuksa najwygodniej można to osiągnąć przez LDAP. A żeby to dobrze zrobić trzeba najpierw przetestować czy aby wszystko działa jak byśmy sobie tego życzyli. I tutaj bardzo przydatne jest narzędzie ldapsearch.\nDo odpytywania LDAP\u0026rsquo;a potrzebujemy jeden pakiecik, który zawiera kilka narzędzi do jego obsługi:\napt-get install ldap-utils Teraz możemy próbować przeszukiwać katalog np. tak:\nldapsearch -L -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 389 -W Polecenie to odpyta kontroler o adresie kontroler.nazwadomeny.local (oczywiście możemy użyć też adresu IP) o wszystkie elementy w domenie. Port 389 na kontrolerze domeny musi być otwarty na zaporze - można też wykorzystać 3268 (o ile jest otwarty).\nParametr -b określa początkową gałąź wyszukiwania - możemy tu dodać konkretne OU itd.. by zmniejszyć liczbę elementów.\n-D to użytkownik na którego się logujemy by uzyskać dostęp do katalogu.\n-W zapyta nas o hasło dla tego użytkownika.\nOczywiście możemy nie potrzebować wszystkich obiektów z katalogu a tylko np. loginy kont użytkowników (ale już nie kont maszyn) - do tego celu możemy użyć filtrów, np. tak:\nldapsearch -x -b \u0026#34;DC=nazwadomeny,DC=local\u0026#34; -D \u0026#34;CN=jakies_konto_w_ad,OU=System Accounts,DC=nazwadomeny,DC=local\u0026#34; -h kontroler.nazwadomeny.local -p 3268 -w \u0026#39;haslo czystym tekstem\u0026#39; \u0026#39;(\u0026amp;(objectClass=person)(!(objectClass=computer)))\u0026#39; sAMAccountName No i dostajemy listę loginów żywych użytkowników. Użycie opcji -w może wydawać się nieco kontrowersyjne ale z drugiej strony jest bardzo wygodne gdy chcemy dane wyjściowe wykorzystać w skrypcie. Przykładowo możemy wynik tego polecenie puścić przez awk by otrzymać same loginy i dodatkowo wszystkie małymi/dużymi literami:\npoprzednie_polecenie | awk \u0026#39;/sAMAccountName/ {print tolower($2);}\u0026#39; W podobny sposób możemy wyciągnąć wszystkie emaile osób z pewnej grupy itd, itp\u0026hellip;.\nTutaj można znaleźć podstawowe info o regułach tworzenia filtrów: http://www.ldapexplorer.com/en/manual/109010000-ldap-filter-syntax.htm\n","href":"/2012/12/ldapsearch-w-active-directory/","title":"ldapsearch w Active Directory"},{"content":"Składniki  1 paczka bezsmakowego kleiku ryżowego (190~170 g), 2 jajka, 1 kostka miękkiego masła (250 g), 1 płaska łyżeczka proszku do pieczenia, pół szklanki cukru, cukier waniliowy (16 g) (niekoniecznie), 8 czubatych łyżek wiórków kokosowych, dżem do wypełnienia ciastek (opcjonalnie - ja lubię je również bez dżemu).  Sposób przygotowania Wszystkie składniki połączyć razem i wyrobić (najlepiej się ugniata ręką). Formować kulki wielkości małego orzecha włoskiego, układać na blaszce wyłożonej papierem do pieczenia w niewielkich odległościach (trochę urosną). Jeżeli planujemy opcję z dżemem to w każdej kulce należy zrobić wgłębienie - ja lekko przyciskałem palcem.\nPiec w temperaturze 180ºC przez 15~20 minut, aż zrobią się złociste. Po ostudzeniu wgłębienia napełnić dżemem.\nŹródło (moja wersja jest nieco przerobiona): http://www.mojewypieki.com/przepis/ciasteczka-z-kleiku-ryzowego\n","href":"/2012/11/ciastka-z-kleiku-ryzowego/","title":"Ciastka z kleiku ryżowego"},{"content":"Składniki  2 szklanki miąższu dyni (startego lub zmiksowanego), 2 szklanki mąki, 0,75 szklanki cukru, 3 jajka, 5 łyżeczek cynamonu, 16 g cukru wanilinowego, 0,75 szklanki oleju, łyżeczka sody oczyszczonej, garść orzechów włoskich.  Sposób przygotowania Ubijamy białka z cukrem. Do ubitej piany dodajemy żółtka i kolejne składniki: mąkę, olej, cukier wanilinowy, sodę i cynamon. Całość miksujemy jeszcze przez chwilę, a następnie dodajemy posiekany lub starty i odsączony (można np. mocno ścisnąć w dłoniach) miąższ dyni oraz drobno pokrojone orzechy włoskie i jeszcze chwilę mieszamy.\nMasę przekładamy do wysmarowanej masłem i oprószonej mąką formy - na dużej blaszce ciasto wyjdzie dość cienkie, więc można piec np. w foremce keksowej. Blaszkę z ciastem wkładamy do nagrzanego do 180°C piekarnika i w tej temperaturze pieczemy przez ok. 40-50 minut. Można podawać posypane cukrem pudrem lub cynamonem.\n","href":"/2012/11/ciasto-z-dynia/","title":"Ciasto z dynią"},{"content":"CouchDB databases on version 0.11.x swell very fast. They should be compacted daily for best performance and space usage. Here is my script that could be run in cron and will compact all databases:\n#!/bin/bash IP=\u0026#34;10.0.0.121\u0026#34; DBS=`curl -sS -X GET http://$IP:5984/_all_dbs | sed -r \u0026#34;s/([,\\\u0026#34;[])|(\\])+/ /g\u0026#34;` for d in $DBS; do curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://$IP:5984/$d/_compact done More informations about compacting could be found here (also for version 1.2.x).\n","href":"/2012/11/automatically-compact-couchdb-databases-in-0-11-x/","title":"Automatically compact CouchDB databases in version 0.11.x"},{"content":"","href":"/tags/couchdb/","title":"CouchDB"},{"content":"","href":"/tags/ext4/","title":"ext4"},{"content":"","href":"/tags/lvm/","title":"LVM"},{"content":"Po zakupie nowych dysków zamierzam utworzyć zdegradowaną macierz RAID5 z dwóch dysków (na trzecim na razie znajdują się dane), potem utworzyć wolumen LVM, sformatować go, przekopiować dane z pojedynczego dysku na macierz i dołączyć trzeci dysk do macierzy odbudowując parzystość. Zadanie będzie o tyle ciekawe że dysk ma 4KB sektory i trzeb będzie dbać o wyrównanie zasobu do rozmiaru sektora, a w przypadku LVM\u0026rsquo;a wyrównanie do chunk\u0026rsquo;a z macierzy.\nPrawidłowe wyrównanie partycji Kupując nowy dysk (o pojemności od 500GB w górę), mamy spore szanse że trafimy na sztukę, która wykorzystuje 4KB sektory do alokacji danych. Ponieważ statystyczny rozmiar przechowywanych plików rośnie i nawet proste zdjęcie ma powyżej 1MB to wykorzystanie bloków o tym rozmiarze większym niż 512 bajtów jest jak najbardziej uzasadnione - zresztą większość systemów plików i tak wykorzystuje bloki 4~8KB. Jest tylko jedno ALE: jeżeli nie uwzględnimy tego podczas partycjonowania dysku to sektory 4KB systemu plików zamiast znajdować się w równo w odpowiadających im fizycznych 4KB sektorach dysku - mogą zachodzić na 2 sektory fizyczne - w takiej sytuacji każde odwołanie to takiego sektora w systemie plików wymaga odczytania/zapisanie dwóch sektorów fizycznych. Co prawda w dyskach stosuje się mechanizmy, które powinny zoptymalizować takie sytuacje ale jak potwierdzają benchmarki źle wyrównane partycja mogą znacznie obniżyć wydajność dysku. A jeszcze zabawniej jest jeśli kupimy dysk SSD bo w nich bardzo często fizyczne bloki są 128~512KB i żeby było zabawniej to bardzo często dyski SSD deklarują (choćby przez SMART\u0026rsquo;a) że mają bloki 512B - SIC!\nJeśli mamy fart i nasz dysk raportuje rozmiar fizycznego sektora to możemy to sprawdzić tak:\ncat /sys/block/sda/queue/physical_block_size U mnie to polecenie zwróciło 4096 czyli 4KB co jest zgodne z deklaracją producenta.\nDobre podejście do tematu wyrównania partycji zaproponował Teo Tso czyli wybranie takich parametrów głowic/sektorów na ścieżce by fdisk automatycznie wyrównywał partycje do oczekiwanej przez nas wielkości bloku. Teo proponował użycie 224 głowic i 56 sektorów - co da wyrównanie do 128KB dla wszystkich partycji z wyjątkiem pierwszej (pierwsza będzie wyrównana do 4KB o ile nie wymusimy startu z 256 sektora). Jeżeli mamy dysk z blokami 4KB lub pierwszą partycję zamierzamy wykorzystać jako np. /boot (gdzie wydajność nie ma aż takiego dużego znaczenia) to jest to ok. Ale jeśli kompatybilność z DOS\u0026rsquo;em mamy w poważaniu to możemy w fdisku utworzyć pierwszą partycję wyrównaną do 128KB lub 1MB.\nPrzeważnie wolałem cfdiska od fdiska (bo po co się męczyć z topornym interfejsem) ale nie udało mi się skubańca zmusić by tworzył pierwszą partycję w sposób nie kompatybilny z DOS\u0026rsquo;em. fdisk pomimo toporności po podaniu liczby głowic i sektorów w ścieżce podpowiedział mi poprawne wyrównanie partycji (a gdybyśmy posiadali starszą wersję, która nie jest tak sprytna to przynajmniej możemy podać ręcznie od którego sektora ma zaczynać się partycja).\nWyrównanie do 128KB fdisk -u -H 224 -S 56 /dev/sdX Opcja -u zmienia domyślną jednostkę na sektory (mamy wtedy mniejsze liczby, które łatwiej się przelicza). Dla wyrównania do 128KB pierwsza partycja powinna się zaczynać na 256 sektorze. Do wyrównania do 4KB wystarczy zacząć na 56 sektorze (wystarczające przy części nowszych dysków twardych).\nWyrównanie do 1MB Jeśli jednak dysponujemy dyskiem SSD z ciężko powiedzieć jak dużym blokiem to lepiej wykorzystać wyrównanie do 1MB - zmarnujemy trochę więcej miejsca (tych parę MB jakoś przeżyjemy) ale w tym rozmiarze na pewno zmieści się każdy sektor (a może nawet Erase Block, który obecnie przeważnie ma 512KB choć zdarzają się sztuki z 4MB). Można to osiągnąć z ustawieniem 64 głowic i 32 sektorów na ścieżkę, robimy to tak:\nfdisk -u -H 64 -S 32 /dev/sdX Pierwsza partycja powinna się zaczynać na 2048 sektorze dla wyrównania do 1MB lub 8192 sektorze jeśli chcemy zacząć od 4MB.\nPo odpaleniu fdiska z tymi parametrami wybieramy opcję n i lecimy dalej zgodnie z podpowiedziami, np.:\nfdisk -u -H 224 -S 56 /dev/sdc Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;m\u0026lt;/strong\u0026gt; Polecenie a zmiana flagi rozruchu b modyfikacja etykiety dysku BSD c zmiana flagi kompatybilności z DOS-em d usunięcie partycji l wypisanie znanych typów partycji m wyświetlenie tego menu \u0026lt;strong\u0026gt;n dodanie nowej partycji\u0026lt;/strong\u0026gt; o utworzenie nowej, pustej DOS-owej tablicy partycji p wypisanie tablicy partycji q zakończenie bez zapisywania zmian s utworzenie nowej, pustej etykiety dysku Suna t zmiana ID systemu partycji u zmiana jednostek wyświetlania/wprowadzania v weryfikacja tablicy partycji w zapis tablicy partycji na dysk i zakończenie x dodatkowe funkcje (tylko dla ekspertów) Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;n\u0026lt;/strong\u0026gt; Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt;): Using default response p Numer partycji (1-4, domyślnie \u0026lt;strong\u0026gt;1\u0026lt;/strong\u0026gt;): Przyjęto wartość domyślną 1 Pierwszy sektor (2048-15240575, domyślnie \u0026lt;strong\u0026gt;2048\u0026lt;/strong\u0026gt;): Przyjęto wartość domyślną 2048 Ostatni sektor, +sektorów lub +rozmiar{K,M,G} (2048-15240575, domyślnie 15240575): \u0026lt;strong\u0026gt;+100M\u0026lt;/strong\u0026gt; Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;p\u0026lt;/strong\u0026gt; Dysk /dev/sdc: 7803 MB, bajtów: 7803174912 głowic: 224, sektorów/ścieżkę: 56, cylindrów: 1214, w sumie sektorów: 15240576 Jednostka = sektorów, czyli 1 * 512 = 512 bajtów Rozmiar sektora (logiczny/fizyczny) w bajtach: 512 / 512 Rozmiar we/wy (minimalny/optymalny) w bajtach: 512 / 512 Identyfikator dysku: 0xc3072e18 Urządzenie Rozruch Początek Koniec Bloków ID System /dev/sdc1 2048 206847 102400 83 Linux Polecenie (m wyświetla pomoc): \u0026lt;strong\u0026gt;w\u0026lt;/strong\u0026gt; Tablica partycji została zmodyfikowana! Wywoływanie ioctl() w celu ponownego odczytu tablicy partycji. UWAGA: ponowny odczyt tablicy partycji zakończył się błędem 16: Urządzenie lub zasoby zajęte. Jądro nadal używa starej tablicy. Nowa tablica będzie używana po następnym restarcie systemu albo po uruchomieniu partprobe(8) lub kpartx(8) Synchronizacja dysków. P.S. Po co w ogóle partycjonować - można użyć całych urządzeń Co prawda w przypadku gdy zamierzam całe dyski poświęcić na RAID\u0026rsquo;a mógłbym ich nie partycjonować tylko użyć całych urządzeń i zadbać tylko by mdadm poprawnie się wyrównał (co zresztą robi automatycznie do 4KB), ale obawiałem się jak względem takich dysków zachowają się inne systemy które mam zainstalowane na komputerze - nie chciałbym aby przypadkiem uznały że to jakiś uszkodzony dysk po czym zainicjowały tablicę partycji\u0026hellip; Może to nieuzasadniona fobia ale wiem że względem partycji RAID Autodetect nic takiego mi się nie przytrafi 😃\nTworzenie macierzy RAID Jak już utworzymy pożądane partycje na pierwszym dysku to musimy je powielić na wszystkich pozostałych dyskach, które zamierzamy włączyć do macierzy - najprościej zrobić to sfdisk\u0026rsquo;iem:\nsfdisk -d /dev/sdX | sfdisk /dev/sdY Przy czym dysk sdX to źródłowy dysk z gotowymi partycjami, a dysk sdY to każdy na którym chcemy odtworzyć partycje. Można by zrobić na to ładną pętelkę jeśli mamy więcej tych dysków ale celowo tego nie zrobię bo jak znam życie to kiedyś ktoś to skopiuje ze znakiem nowego wiersza i wrzuci do konsoli\u0026hellip; 😀\nTeraz tworzę zdegradowaną (z dwóch dysków) macierz RAID5\u0026hellip; Ale na dobrą sprawę bezpieczniej byłoby utworzyć macierz RAID1 z dwóch dysków (o ile miejsca byłoby wystarczająco na przeniesienie danych) a po przeniesieniu danych na macierz dodanie trzeciego dysku i powiększenie macierzy z reshapingiem do RAID5 - postanowiłem pominąć takie rozwiązanie bo nie bałem się utraty danych, miałem dokładną kopię na innej macierzy 😃\nWięc tworzymy macierz:\nmdadm --create /dev/md0 --level=5 --raid-devices=2 --chunk=512k /dev/sdX1 /dev/sdY1 Na dobrą sprawę z powyższych opcji tylko chunk nadaje się do \u0026ldquo;dopasowania\u0026rdquo; - ja mam zamiar utworzyć macierz z dość dużych zasobów (3x2TB) i przechowywać na niej raczej duże pliki więc 512KB wydaje mi się dobrą wartością. Gdybyśmy jednak potrzebowali większej liczby I/O dla małych pliczków to mniejsza wartość może być lepsza. Warto zrobić kilka benchmarków dla różnych wartości chunk\u0026rsquo;a i dopasować do przewidywanego przez nas obciążenia.\nAby macierz była widoczne już w czasie startu systemu należy dodatkowo wykonać polecenie:\nmdadm --detail --scan \u0026gt;\u0026gt; /etc/mdadm/mdadm.conf Warto też w pliku /etc/mdadm/mdadm.conf odkomentować i wpisać jakieś sensowne wartości dla HOMEHOST, MAILADDR.\nI teraz możemy odbudować obraz initrd:\nupdate-initramfs -u Optymalizacja macierzy RAID5 Sam proces tworzenia macierzy może zająć kilka/kilkanaście godzin - dlatego warto mu pomóc kilkoma zmianami.\nspeed_limit_xxx Na pierwszy rzut - domyślne wartości dla minimalnej i maksymalnej prędkości budowania/regenerowania/odtwarzania macierzy RAID5, można je sprawdzić:\ncat /proc/sys/dev/raid/speed_limit_max 200000 cat /proc/sys/dev/raid/speed_limit_min 1000 Domyślnie jest to minimum 1MB/s i maksimum 200MB/s. Podbijając minimalną prędkość do 10~50MB/s można kosztem większego obciążenia systemu zmusić macierz by odbudowywała się szybciej. Osobiście uważam tą optymalizację za mało znaczącą bo cały mechanizm dość elastycznie reaguje na obciążenie systemu i jeśli nic nie robimy to prędkości odbudowy są dość wysokie. Ale można to zrobić tak:\necho 50000 \u0026gt; /proc/sys/dev/raid/speed_limit_min lub tak:\nsysctl -w dev.raid.speed_limit_min=50000 stripe_cache_size Ta optymalizacja pomogła mi dużo - ok. 30% wzrost wydajności macierzy (również w czasie odbudowy parzystości). Możemy sprawdzić wartość tego parametru, np. tak:\ncat /sys/block/md0/md/stripe_cache_size Domyślnie jest to wartość 128 - bida z nędzą, u mnie ustawienie na 32768 (maksymalna wartość tego parametru) dało największy wzrost wydajności - ale już 8192 znacznie poprawiło wydajność.\nfor i in 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;Testowanie $i\u0026#34; echo $i \u0026gt; /sys/block/md0/md/stripe_cache_size sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=/dev/zero of=file bs=1M count=10000 done Wyłączenie NCQ dla wszystkich dysków w macierzy Wydało mi się to nieco kontrowersyjne bo NCQ powinno pomagać przy losowych odczytach/zapisach - ale zapuściłem test bonnie++ i okazało się że z włączonym NCQ czasy dostępu dla niektórych obciążeń rosną nawet dziesięciokrotnie! Warto więc sprawdzić tą opcję pod przewidywanym przez nas scenariuszem obciążenia.\nNCQ dla poszczególnych dysków można wyłączyć np. tak:\nfor d in sdb sdc sdd do echo 1 \u0026gt; /sys/block/$d/device/queue_depth done Wyłączenie cache dyskowych Wyłączenie wbudowanej w dyski pamięci cache akurat nie zwiększa wydajności ale w przypadku awarii zasilania (lub innej gwałtownej awarii systemu) zwiększa szanse macierzy na przeżycie takiego incydentu. Obecnie większość systemów plików korzysta z opóźnienia zapisu by bardziej optymalnie zapisać dane na dysku - dlatego gdy zapisujemy dany to najpierw trafiają one do cache systemowego. Dopiero po sync\u0026rsquo;u są przesyłane do cache dyskowego skąd dopiero po pewnym czasie trafiają na dysk. Wyłączenie pamięci cache na dyskach \u0026ldquo;usuwa\u0026rdquo; nam to drugie opóźnienie.\nhdparm -W0 /dev/sd* Zmiana parametrów odczytu z wyprzedzeniem Bardzo ważnym parametrem mającym wpływ na wydajność macierzy jest odpowiednie ustawienie odczytu z wyprzedzeniem. Obecnie ustawioną wartość możemy sprawdzić tak (wartość wyrażona jest w 512 bajtowych sektorach):\nblockdev --getra /dev/md0 U mnie domyślnie było 4096 (a na innym starszym systemie tylko 1536) to może być zbyt mało dla konfiguracji RAID. Większe wartości można ustawić np. tak:\nblockdev --setra 65536 /dev/md0 A tak można wykonać sprawdzanie, która wartość będzie dla nas najbardziej optymalna:\ndd if=/dev/zero of=file bs=1M count=10000 for i in 1536 4096 8192 16384 32768 65536 131072 262144 524288; do echo \u0026#34;Testowanie $i\u0026#34; blockdev --setra $i /dev/md0 sync echo 3 \u0026gt; /proc/sys/vm/drop_caches dd if=file of=/dev/null bs=1M done Tworzenie zasobu LVM Mając już macierze tworzę na niej volumen LVM - najpierw przygotowanie zasobu:\npvcreate /dev/md0 Jeżeli w /etc/lvm/lvm.conf mamy ustawione opcje:\nmd_component_detection = 1 md_chunk_alignment = 1 data_alignment_detection = 1 To LVM powinien automatycznie wykryć rozmiar chunk\u0026rsquo;a z RAID\u0026rsquo;a i dostosować swoje metadane, oraz początek danych tak by wszystko było prawidłowo wyrównane względem macierzy.\nJeśli nie mamy szczęścia (bardzo stare jajko/LVM) to będziemy musieli użyć opcji -metadatasize i/lub -dataalignment:\npvcreate --metadatasize 500k /dev/md0 Teraz ciekawostka - LVM potrzebuje na 192KB na dane nagłówkowe każdego wolumenu i każdy utworzony później zasób byłby o te 192KB przesunięty, więc \u0026hellip; trafia nasze wyrównanie do 128KB. Dlatego zmuszamy LVM\u0026rsquo;a by zaalokował nieco więcej - tutaj 256KB. OK - ale w poleceniu jest 250 - WTF? I to aby było zabawnie jest to jak najbardziej prawidłowa wartość - nie wiem jaka w tym logika, ale by metadane zajmowały 256KB podajemy 250k, by zajmowały 512KB podajemy 500k itd\u0026hellip;\nInaczej jest z opcją -dataalignment, tutaj najbardziej optymalnie należy podać rozmiar chunk\u0026rsquo;a*ilość aktywnych dysków (dla RAID5 odejmujemy jeden) - albo minimalnie rozmiar chunka, np.:\npvcreate --dataalignment 512K /dev/md0 Poprawność możemy sprawdzić poleceniem:\npvs /dev/md0 -o+pe_start PV VG Fmt Attr PSize PFree 1st PE /dev/md0 vgraid lvm2 a- 1,82t 488,89g 512,00k U mnie 1st PE zaczyna się na 512KB, więc jest OK.\nTeraz tworzymy grupę:\nvgcreate vgraid /dev/md0 Polecenie vgcreate posiada parametr -s który pozwala określić do wielokrotności jakiej wartości będzie zaokrąglana wielkość wolumenu - wartość ta powinna być wielokrotnością chunk\u0026rsquo;a z macierzy. Domyślnie ma ona wartość 4MB więc wszystko będzie ładnie wyrównane.\nI możemy zacząć tworzyć volumeny logiczne:\nlvcreate -L1T -nsrv vgraid Formatowanie To teraz formatowanie - tutaj też czasem trzeba się wysilić by utworzony przez nas filesystem działał możliwie optymalnie na macierzy i bloku o odpowiednim rozmiarze. W przypadku filesystemu na macierzy są dwa ważne parametry: stride i stripe-width. Stride powinien odpowiadać rozmiarowi podanemu jako chunk podczas tworzenia macierzy ale wyrażonego w blokach systemu plików (domyślnie 4KB). Stripe-width powinno być ustawione na: stride * N, gdzie N to ilość aktywnych dysków w macierzy (dla RAID5 jest to ilość dysków minus 1) - przykładowo dla bloku 4KB i chunk\u0026rsquo;a 512KB, stride powinien wynosić 128 (512/4). Z kolei stripe-width dla 3 dysków to 128*(3-1)=256. Prawidłowe dobranie tych parametrów może dać wzrost wydajności rzędu 40% (według moich testów). Teoretycznie na nowszych systemach tworzone systemy plików automatycznie powinny wykryć najbardziej optymalne wyrównanie - możemy więc spróbować puścić format bez tych parametrów i później skontrolować ich wartości poleceniem:\ntune2fs -l /dev/vgraid/srv Na początek przykład dla systemu plików dla małych i średnich plików:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -m0 /dev/vgraid/srv Jeden dodatkowy parametr to resize - pozwala on zmienić domyślne ustawienie maksymalnego rozmiaru do którego możemy powiększyć dany filesystem - domyślnie jest to wartość 1000 razy większa od początkowej wielkości - ciut przekozaczone. Może nie oszczędzi to dużo miejsca na dysku (kilkadziesiąt/kilkaset MB) ale na pewno skróci czas fsck\u0026rsquo;a.\nKolejny dodatek to -m0 które wyłącza alokację 5% przestrzeni dyskowej dla root\u0026rsquo;a i usług systemowych - po prostu tutaj tego nie potrzebuję a 5% z 1TB to 50GB marnującego się miejsca!\nJeśli wiemy że będziemy przechowywać na danym zasobie tylko stosunkowo duże pliki to można użyć takich opcji:\nmkfs.ext4 -E stride=128,stripe-width=256,resize=4T -T largefile -m0 /dev/vgraid/srv Opcja -T to wykorzystanie szablonów opcji dla tworzenia systemów plików, które można edytować i dodawać w pliku: /etc/mke2fs.conf. Szablon largefile wykorzystuje mniejszą liczbę inodów dla nowego filesystemu, jeśli zamierzamy przechowywać głównie duże pliki to zmniejszy to czas formatowania i późniejszych fsck\u0026rsquo;ów na tym systemie plików.\nTestowanie wydajności Zalecałbym testowanie wydajności na kolejnych etapach przygotowania dysków i powtarzać te same benchmarki po każdej zmianie, tak więc testujemy:\n Na początek wszystkie dyski, z których zamierzamy zbudować RAID\u0026rsquo;a by wykluczyć ewentualne \u0026ldquo;padaki\u0026rdquo;/uszkodzone kable, itp. Np. hdparm/dd na czystym dysku i dodatkowo iozone/bonnie++ na filesystemie by sprawdzić czy nie skopaliśmy wyrównania partycji. Po zbudowaniu RAID\u0026rsquo;a powtarzamy testy - na całym urządzeniu (dd) i po sformatowaniu (iozone/bonnie++) by upewnić się że RAID jest prawidłowo wyrównany (przy czym przy RAID5 możemy się spodziewać wyników przy zapisie niższych niż przy odczycie - jest to OK). Jest to też dobry moment na sprawdzenie kilku optymalizacji dla macierzy: stripe_cache_size, wyłączenie NCQ, ustawienia odczytu z wyprzedzeniem, wyłączenie cache dysków - po każdej z tych zmian ponawiamy benchmarki by upewnić się że uzyskaliśmy poprawę/lub nie. Po przygotowaniu LVM\u0026rsquo;a - ponawiamy benchmarki na volumenie by upewnić się że nie skopaliśmy wyrównania partycji/chunk\u0026rsquo;a/itd\u0026hellip; Dopieszczamy opcje formatowania filesystemu i jego montowania (np. noatime, commit, data, itd) - i znów posiłkujemy się benchmarkami by potwierdzić że pniemy się z wydajnością w górę.  Jeśli myślicie że to dużo to zalecałbym powtórzenie części benchmarków 2~3 krotnie by upewnić się że wyniki nie odbiegają znacznie od siebie. Dodatkowo powinniśmy czyścić przed każdym banchmarkiem cache dyskowy aby mieć pewność że lepsze wyniki nie są skutkiem wczytania danych do pamięci. Z tego samego powodu jeśli uruchamiamy benchmarki to ilość zapisywanych/odczytywanych danych powinna być minimum 1,5~2 razy większa niż pamieć RAM zainstalowana w systemie by na pewnie wszystkie dane nie zmieściły się w cache\u0026rsquo;u. Oczywiście można to zlać ale później nie ma się co dziwić że system działa wolno - a na produkcyjnej maszynie dużo trudniej zaorać całą konfiguracją i utworzyć od początku z prawidłowym wyrównaniem.\nZalecane jest wykorzystanie IOZone lub Bonnie++ ponieważ testują one nie tylko prosty sekwencyjny odczyt, ale również tworzenie/kasowanie plików o różnych rozmiarach i w różnej ilości - to pozwala lepiej sprawdzić opóźnienia występujące przy przewidywanych przez nas obciążeniach oraz upewnić się że cała zabawa z wyrównywaniem zasobów miała sens. Oczywiście to tylko zalecenia 😉\nhdparm W przypadku macierzy wykorzystanie prostego hdparm\u0026rsquo;a do testów:\nhdparm -tT /dev/md0 nie zwróci realnych i sensownych wyników. Pomiar jest zbyt krótki by uzyskać sensowne wyniki - lepiej wykorzystać dd z dużą ilością danych do odczytu.\ndd Narzędzie jakże prymitywne a tak przydatne. Możemy nim zmierzyć sekwencyjny odczyt/zapis z/do macierzy i uzyskać bardziej realne wyniki niż hdparm\u0026rsquo;em. Wystarczy wymusić operację na ok. dwukrotnie większej ilości danych niż ilość pamięci RAM. Dodatkowo czyścimy cache\u0026rsquo;e przed i po mierząc całościowy czas, np. tak:\nsync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) Jest to szczególnie przydatne np. przy dopasowywaniu optymalnej dla nas wartości parametru stripe_cache_size, wystarczy przygotować odpowiednią pętlę:\nfor i in 128 256 512 1024 2048 4096 8192 16384 32768; do echo \u0026#34;stripe_cache_size $i\u0026#34; echo $i \u0026gt; /sys/block/md0/md/stripe_cache_size sync; echo 3 \u0026gt; /proc/sys/vm/drop_caches; time (dd if=/dev/zero of=/mnt/test/test.img bs=1024K count=10240 \u0026amp;\u0026amp; sync) done Parametr bs określa bufor wykorzystywany przy operacjach odczytu/zapisu - można go dostosować do rozmiaru chunk\u0026rsquo;a/stripe-width/itp.. count określa jak dużo takich buforów odczytać/zapisać - w powyższym przypadku jest to 10 tys. jednomegabajtowych buforów więc łącznie 10GB.\nbonnie++ Bonnie++ wymaga nieco przygotowania ale wyniki w moim przypadku bardzo odpowiadały rzeczywistym. Co pokrótce trzeba zrobić:\nmkdir /srv/test chown -R guest:guest /srv/test bonnie++ -d /srv/test -s 16g -m nazwa_maszyny -f -u guest Możemy dodać opcję -b by po każdej operacji wykonywany był sync - to byłoby coś podobnego do systemów bazodanowych lub pocztowych. Jeżeli chcemy zasymulować standardowe operacje na plikach to nie potrzebujemy tej opcji.\niozone W najprostszym wykonaniu:\niozone -a Wykona to serię pomiarów na różnych rozmiarach plików, ilości powtórzeń itd. Najbardziej interesująca opcją w konfiguracji RAID jest \u0026ldquo;Stride read\u0026rdquo;.\niozone -S 8192 -t 1 Parametr -S przekazuje rozmiar pamięci cache procesora - wykorzystywany do alokacji pamięci blokami itp (sprawdzałem czy to cokolwiek pomoże.. ale nie widziałem dużej różnicy).\nParametr -t 1 to benchmark przepustowości dysku a parametr cyfrowy określa liczbę równoczesnych wątków, które będą odczytywać/zapisywać - można w ten sposób zasymulować np. równoczesny streaming dla wielu źródeł, itp.\niozone -S 8192 -a -s 40960 Tutaj parametr -s wskazuje na jakim rozmiarze pliku w KB ma być prowadzony benchmark - tutaj 40MB.\nPodsumowanie Ogólnie zadowolony jestem że udało mi się to wszystko zebrać w jednym poście bo dotychczas miałem to zapisane w wielu różnych zakładkach i spory problem gdy potrzebowałem \u0026ldquo;właśnie tego jednego polecenia\u0026rdquo;. Ale niezadowolony jestem z tego że nie udało mi się ustalić całości tego postępowania dla dysków o sektorach/erase block\u0026rsquo;ach większych niż 4 KB. Chodzi mi szczególnie o brak jasnego wyjaśnienia czy RAID5 jest prawidłowo wyrównywany bo według jednych tak właśnie jest, a według innych tak nie jest. Stąd niektórzy zalecają by stosować format metadanych dla macierzy w wersji 0.9 lub 1.0 zamiast 1.2 ale nie ma jasnych źródeł tego rozumowania. Mam nadzieję że kiedyś uda mi się to jednoznacznie rozsądzić - na pewno zaktualizuję wtedy tego posta.\nŹródełka na których oparłem to HOWTO http://serverfault.com/questions/390294/mdadm-raid5-too-slow-when-writing\nhttp://serverfault.com/questions/250707/why-does-mdadm-write-unusably-slow-when-mounted-synchronously\nhttp://serverfault.com/questions/416321/mdadm-raid-5-failed-with-2-drives-while-rebuilding\nhttp://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html\nhttp://askubuntu.com/questions/20852/making-stripe-cache-size-permanent\nhttp://h3x.no/2011/07/09/tuning-ubuntu-mdadm-raid56\nhttps://raid.wiki.kernel.org/index.php/Performance\nhttp://www.mjmwired.net/kernel/Documentation/md.txt\nhttp://wiki.hetzner.de/index.php/Partition_Alignment/en\nhttp://www.fhgfs.com/wiki/wikka.php?wakka=PartitionAlignment\nO tym że LVM na RAID5 sam się wyrównuje:\nhttp://marc.info/?l=linux-raid\u0026amp;m=126267824425009\u0026amp;w=2\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\nMDADM metadata format 1.2 wyrownuje sie do 4KiB:\nhttp://www.redhat.com/archives/linux-lvm/2009-September/msg00092.html\nKalkulator stride\u0026rsquo;a\nhttp://busybox.net/~aldot/mkfs_stride.html\nFormat raid\u0026rsquo;a:\nhttps://raid.wiki.kernel.org/index.php/RAID_superblock_formats\nWyrównanie do 4kb - choć wydaje mi się że gościu robi to na czuja i ledwie mu się udało:\nhttp://blog.bigsmoke.us/2010/05/13/aligning-partitions-with-raid-and-lvm-on-drives-with-4-kb-sectors\n","href":"/2012/11/lvm-na-raid5-i-dysku-z-sektorami-4kb/","title":"LVM na RAID5 i dysku z sektorami 4KB"},{"content":"Po każdej aktualizacji Ubuntu mam trochę zabawy by pozbierać do kupy skaner i drukarkę z mojego urządzenia wielofunkcyjnego Brother DCP-130C. Wybrałem je bo był to jedyny producent, który deklarował wsparcie dla Linux\u0026rsquo;a\u0026hellip; choć z perspektywy czasu nie jestem pewien czy otrzymałem to czego się spodziewałem\u0026hellip; Co prawda zamieszczają instrukcje i aktualizują drivery ale jeszcze ani raz nie zdarzyło mi się by po aktualizacji systemu postępowanie według tych instrukcji zadziałało bez dodatkowej pomocy.. Olać!\nInstrukcja jest dla mojej drukarki ale powinna zadziałać również dla innych modeli Brother\u0026rsquo;a - w razie potrzeby można się posiłkować instrukcjami producenta.\nUruchomienie drukarki Pobieramy drivery dla drukarki z poniższej strony:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_prn.html\nInstalujemy zależności:\napt-get install ia32-libs lib32stdc++ Instalujemy drivery (w moim przypadku dla DCP-130C):\ndpkg -i --force-all dcp130clpr-1.0.1-1.i386.deb dpkg -i --force-all dcp135ccupswrapper-1.0.1-1.i386.deb Teraz w przeglądarce wchodzimy na stronę konfiguracji CUPS\u0026rsquo;a: http://localhost:631/printers - powinna tam widnieć nasza drukarka z URI usb://Brother/ (np. usb://Brother/DCP-130C?serial=BRO00000000). Jeśli URI jest inne to modyfikujemy drukarkę by wybrać odpowiednie urządzenie i sterownik. Gdyby jednak drukarka nie została automatycznie dodana to wchodzimy na http://localhost:631/admin i klikamy Dodawanie drukarki - podajemy dane autoryzacyjne dla root\u0026rsquo;a, wybieramy urządzenie i sterownik.\nPo dodaniu drukarki można wejść w jej ustawienia (u mnie http://localhost:631/printers/DCP-130C) i w menu Administracja wybrać opcję: Ustaw domyślne opcje by określić domyślne parametry wydruków.\nZ dodawaniem drukarki nie miałem dużych problemów, ciekawiej jest ze skanerem\u0026hellip;\nUruchomienie skanera Z poniższej strony pobieramy drivery dla naszego modelu drukarki:\nhttp://welcome.solutions.brother.com/bsc/public_s/id/linux/en/download_scn.html\nJa pobrałem deb\u0026rsquo;a dla wersji 64-bitowej: brscan2-0.2.5-1.amd64.deb.\nInstalujemy (instrukcja producenta tutaj):\napt-get install sane-utils dpkg -i --force-all brscan2-0.2.5-1.amd64.deb Teoretycznie powinno to wystarczyć by root mógł korzystać ze skanera, więc popracujmy by użytkownicy też mogli. Edytujemy jako root plik /lib/udev/rules.d/40-libsane.rules i odszukujemy linię \u0026ldquo;# The following rule will disable\u0026rdquo; (u mnie gdzieś w okolicy 1180 linii), przed nią wklejamy tekst:\n# Brother scanners ATTRS{idVendor}==\u0026#34;04f9\u0026#34;, ENV{libsane_matched}=\u0026#34;yes\u0026#34; Zapisujemy plik. Teraz musimy dodać zainteresowanych użytkowników do grupy scanner, robimy to np. tak:\ngpasswd -a roman scanner I powtarzamy dla innych userów.\nI na koniec wisienka na torcie - na 64 bitowym systemie postępowanie według instrukcji zwyczajnie nie działa bo potrzebne biblioteki instalują się w złym miejscu\u0026hellip; sic!\nPliki trafiają do /usr/lib64 zamiast do /usr/lib - aby wszystko działało jak trzeba musimy przekopiować (albo chociaż podlinkować) z /usr/lib64 do /usr/lib - co dokładnie należy skopiować można znaleźć tutaj.\n","href":"/2012/11/instalacja-drukarki-i-skanera-brother-dcp-130c-na-ubuntu-12-04/","title":"Instalacja drukarki i skanera Brother DCP-130C na Ubuntu 12.04"},{"content":"","href":"/tags/mail/","title":"mail"},{"content":"","href":"/tags/poczta/","title":"poczta"},{"content":"Czasami potrzebny jest nam serwer pocztowy, który przyśle informacje dla root\u0026rsquo;a (np. monity smartd, mdadm, sypnięte crony itp) ale równocześnie nie chcemy stawiać pełnego serwera typu postfix/exim. Warto w tym celu wykorzystać zestaw heirloom-mailx + ssmtp. hairloom-mailx jest prostym shellowym klientem SMTP - przy okazji linkuje polecenie mail (przydatne w skryptach). ssmtp pełni funkcję serwera SMTP ale nie działa jako demon - proces uruchamia się gdy jest potrzebny i znika po wysłaniu maili. Dodatkowo ssmtp może zostać skonfigurowany by wysyłać maile nie tylko przez relay\u0026rsquo;a ale również autoryzując się na zewnętrznym serwerze pocztowym, np. gmail\u0026rsquo;u czy gdziekolwiek indziej.\nInstalacja apt-get install ssmtp heirloom-mailx Konfiguracja z relay\u0026rsquo;em Edytujemy plik /etc/ssmtp/ssmtp.conf - w konfiguracji z relay\u0026rsquo;em wystarczą te linijki:\nroot=postmaster@domena.pl mailhub=domena.pl hostname=serwerek.domena.pl Warto zwrócić uwagę że dobrze by było aby domena serwerek.domena.pl istniała i wskazywała na nasz serwer - dzięki temu poczta nie odpadnie na prostych filtrach antyspamowych.\nKonfiguracja pod gmail\u0026rsquo;a W ssmtp.conf potrzebujemy:\nroot=postmaster@domena.pl mailhub=smtp.gmail.com:587 hostname=serwerek.domena.pl AuthUser=twojekonto@gmail.com AuthPass=twoje_haslo_do_gmaila UseTLS=YES UseSTARTTLS=YES AuthMethod=LOGIN Test Warto teraz sprawdzić czy poczta wychodzi jak powinna:\necho test | mail -s \u0026#34;testowa wiadomosc\u0026#34; postmaster@domena.pl ","href":"/2012/11/prosty-mta-z-heirloom-mailx-i-ssmtp/","title":"Prosty MTA z heirloom-mailx i ssmtp"},{"content":"","href":"/tags/nautilus/","title":"Nautilus"},{"content":"Lubię mieć porządek w folderach i jedna rzecz, która nie daje mi spokoju w systemach plików ext to widoczność folderu lost+found - niby można go skasować i powinien się odtworzyć (choć podobno odtworzenie w czasie fsck\u0026rsquo;a może spowodować utratę danych - trochę to dziwne i nie znalazłem źródła no ale powiedzmy że nie chcę go usuwać). Chciałem go ukryć (choćby w Nautilusie) by mnie nie drażnił. Oczywiście opcja z \u0026ldquo;.\u0026rdquo; na początku odpada, ale na szczęście Nautilus wykorzystuje pewien hack, który umożliwia ukrycie dowolnego pliku/folderu.\nTworzymy w głównym katalogu danego punktu montowania, plik .hidden i wpisujemy do niego nazwy plików, które chcemy ukryć - w moim przypadku lost+found:\ncd /root/katalogu echo \u0026#34;lost+found\u0026#34; \u0026gt;\u0026gt; .hidden Po odświeżeniu Nautilus nie pokazuje już katalogu lost+found - dla mnie bomba.\n","href":"/2012/10/nautilus-ukrywanie-lostfound/","title":"Nautilus - ukrywanie lost+found"},{"content":"","href":"/tags/autoit/","title":"AutoIt"},{"content":"Składniki  1 kostka masła, 1 żółtko, 6 łyżek cukru pudru, 1 szklanka mleka, 10 dkg wiórek kokosowych, 1 kieliszek wódki (ale z dwoma jest ostrzejsze), 3 łyżki kakao, 37 dkg herbatników kakaowych (doskonale spisują się dwie paczki maślanych herbatników kakaowych).  Masa kakaowa Herbatniki dokładnie rozwałkować, dodać kakao i wymieszać. Zagotować pół szklanki mleka i gorącym zalać herbatniki. Całość wymieszać na jednolitą masę. W drugim pojemniku rozmiksować pół kostki masła i 3 łyżki cukru pudru - po chwili dodać żółtko i wódkę. Do tej masy stopniowo dodajemy herbatniki.\nMasa kokosowa Zagotować pół szklanki mleka i gorącym zalać wiórki kokosowe, wymieszać by równomiernie namokły. Rozmiksować pół kostki masła z 3 łyżkami cukru pudru. Stopniowo dodawać wiórki i nadal miksować.\nSkładamy całość Wałkujemy masę kakaową na papierze do pieczenie lub folii (papier jest lepszy) - staramy się uzyskać kształt zbliżony do prostokąta (ułatwi to zwijanie rolady). Tak przygotowaną ciemną masę pokrywamy równomierną warstwą masy kokosowej. Później delikatnie zaczynamy zwijać z jednej strony jak makowiec - na koniec staramy się dość dokładnie docisnąć papier by rolada nie \u0026ldquo;rozdęła\u0026rdquo; się przed stwardnieniem. Taki pakunek wrzucamy do lodówki na minimum 2~3 godziny.\nP.S. Chciałem wrzucić fotkę ale zbyt szybko zniknęła - może innym razem 😄\n","href":"/2012/10/rolada-kokosowo-czekoladowa-na-zimno/","title":"Rolada kokosowo-czekoladowa na zimno"},{"content":"Zdarzyło mi się kilka usług działających na serwerach Windows, które zwyczajnie sypały się i to tak brzydko że systemowe ustawienia by restartowały się po padzie nie wystarczały. Usługa działała np. 3 godziny by potem się położyć, albo nigdy nie wstawała po starcie systemu - po prostu jakaś masakra. Zgłoszenia do producenta często wyglądały tak że w jego testowym środowisku błędu nie udaje się powtórzyć (i nic dziwnego bo nie sądzę by z ich konfiguracji korzystali rzeczywiści użytkownicy). Oczywiście można zaczekać aż ukaże się magiczna łata usuwająca błąd ale do tego czasu to na nas wszyscy będą \u0026ldquo;wieszać psy\u0026rdquo; za niedostępność usługi.\nW najprostszym podejściu można napisać bat\u0026rsquo;cha, który będzie restartował usługę (kładł i startował, sprawdzał czy działa i tylko przy braku działania uruchamiał, itp) - taki skrypcik odpalamy co 5 minut i powinno to załatwić sprawę. Załatwi to najprostsze przypadki - ale nie będzie całkiem elastyczne.\nSzukałem programu, który działałby jako usługa i \u0026ldquo;pilnował\u0026rdquo; innych usług systemowych i\u0026hellip; można coś takiego kupić za 200$ (sic!). Postanowiłem poszukać na forum AutoIt\u0026rsquo;a - basciopodobnego języka skryptowego, który wielokrotnie pomagał mi zautomatyzować pewne zadania administracyjne - trafiłem na tego posta: http://www.autoitscript.com/forum/topic/80201-service-udf-v2-run-your-exe-as-service/\nZ tego adresu pobieramy źródła - zmieniamy nazwę pliczku ServiceExample.au3 na np. ServiceKeeper.au3 i edytujemy (szukamy linijki region -\u0026gt; insert your running code here:\n#region --\u0026gt; insert your running code here  Local $status = _Service_QueryStatus(\u0026#34;shittyservice\u0026#34;) If $status[1] \u0026lt;\u0026gt; $SERVICE_RUNNING Then If $bDebug Then logprint(\u0026#34;shittyservice service not running!\u0026#34;) If $status[1] == $SERVICE_PAUSED Then If _Service_Resume(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service resumed successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service resume failded!\u0026#34;) If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) EndIf EndIf EndIf If $status[1] == $SERVICE_STOPPED Then If _Service_Start(\u0026#34;shittyservice\u0026#34;) == 1 Then If $bDebug Then logprint(\u0026#34;shittyservice service started successfully\u0026#34;) Else If $bDebug Then logprint(\u0026#34;shittyservice service start failded!\u0026#34;) EndIf EndIf EndIf Sleep(60000) # jak czesto sprawdzac san uslug - czas w milisekundach #endregion --\u0026gt; insert your running code here Powyższy przykład sprawdza czy pewna \u0026ldquo;shittyusługa\u0026rdquo; działa, a jeśli nie to podejmuje różne próby jej uruchomienia. Przykład najprostszy z możliwych ale bardzo łatwo można zamiast sprawdzenia jedne usługi powielić ten kod dla kilku usług i sprawdzać je w pętli (w wolnej chwili może wrzucę taki kod).\nSkrypcik dostosowujemy do swoich potrzeb, kompilujemy i wrzucamy na serwer potrzebujący \u0026ldquo;rozrusznika\u0026rdquo;. Usługę trzeba zainstalować przez odpalenie skryptu z parametrem -i, w naszym przypadku wyglądało by to tak:\nServiceKeeper.exe -i Usługę można odinstalować przez uruchomienie z parametrem -u.\nDobrze ustawić tą usługę by startowała z opóźnieniem. Nie wydaje mi się sensowe sprawdzanie stanu usług częściej niż raz na minutę.\nZdarzało mi się że ta usługa sypała się przy jej zatrzymywaniu (jest o tym wzmianka na forum), ale to mnie akurat mało boli bo innych problemów nie miałem.\n","href":"/2012/10/utrzymanie-przy-zyciu-sypiacych-sie-uslug-na-serwerach-windows/","title":"Utrzymanie przy życiu sypiących się usług na serwerach Windows"},{"content":"","href":"/tags/open-source/","title":"Open Source"},{"content":"Jeśli szukamy statystyk dla strony internetowej i ze względu na jej zawartość (np. sklep, coś ze zwiększonym naciskiem na poufność etc..) nie potrafimy zaufaj wujkowi Googlowi to warto przyglądnąć się Piwikowi.\nJest to system statystyk aspirujący do bycia Open Source\u0026rsquo;ową alternatywą dla Google Analytics. Aspirujący (a nie będący) z tego względu że Google przechodząc na domyślny HTTPS (SPDY) dla zalogowanych użytkowników uniemożliwił śledzenie stron z których pochodzą odwiedziny (tzw. refferals) - tym prostym sposobem tylko GA jest w stanie dostarczyć pełnych informacji o wszystkich użytkownikach.\nNiemniej Piwik nadal może być bardzo atrakcyjnym narzędziem choćby dlatego że:\n statystyki zbieramy \u0026ldquo;lokalnie\u0026rdquo;, czyli gdzieś na naszym serwerze - mamy nad nimi władzę (z wszystkimi konsekwencjami), do Piwik\u0026rsquo;a możemy łatwo stworzyć własne rozszerzenia, które przedstawią to co chcemy w sposób jakiego oczekujemy, statystyki możemy zbierać przez dodanie odwołania do skryptu w kodzie strony lub okresowo importując logi z serwera (jak wspomniany przeze mnie kiedyś awstats) lub na oba wspomniane sposoby - stąd Piwik świetnie sprawdza się w hostingach (możemy pokazać klientowi ile miał wejść, zrobić statystyki wykorzystania pasma itp), oprawa graficzna jest świeższa niż w AWStats 😃 ma nieduże wymagania, głównie: PHP + MySQL.  Instrukcję instalacji i więcej informacji można znaleźć tutaj: http://pl.piwik.org/dokumentacja/instalacja-piwika/\n","href":"/2012/09/piwik-alternatywa-dla-google-analytics/","title":"Piwik - alternatywa dla Google Analytics"},{"content":"Co prawda adresy URL pozwalają na stosowanie zarówno dużych jak i małych liter ale różne systemy mogą je różnie obsługiwać i może się trafić sytuacja, w której nie zechcemy by np. duże litery w ogóle pojawiały się w adresach URL. Doskonały przykład to mój niedawny wpis: Apache: ograniczenie dostępu dla zalogowanych użytkowników z mod_rewrite i mod_auth_basic.\nZachodzi tam sytuacja, w której katalog użytkownika jest jego loginem małymi literami (bądź dużymi - jak kto woli), a użytkownik wpisując login może użyć zarówno małych jak i dużych liter i tutaj zaczyna się jazda. Można użyć modyfikatora NC (no case), ale to wpłynie tylko na porównania - przepisanie ścieżki na nazwę podaną przez usera (z dużymi i małymi literami) przekieruje do katalogu, którego nie ma (bo jest katalog tylko małymi/dużymi).\nI wtedy przyda się taka sztuczka:\nRewriteEngine On RewriteMap lc int:tolower RewriteCond %{REQUEST_URI} [A-Z] RewriteRule (.*) ${lc:$1} [R=301,L] Definiujemy mapę korzystając z wbudowanego w moduł słownika tolower, a następnie jeśli w URL\u0026rsquo;u występują (w tym przypadku) duże litery to przekierowujemy na URL\u0026rsquo;a z małymi.\n","href":"/2012/09/mod_rewrite-wymuszenie-malych-liter-w-adresie-url/","title":"mod_rewrite - wymuszenie małych liter w adresie URL"},{"content":"Obecnie dostępna jest już beta 2 Debiana Wheezy i utrzymywane są aktualizacje bezpieczeństwa więc powolutku można na testowych maszynach sprawdzać co i jak się zmieniło.\nPonieważ do finalnej wersji pewnie sporo się jeszcze zmieni to postaram się z czasem aktualizować ten post by zawierał bieżące informacje.\nW razie wątpliwości patrz tutaj: http://wiki.debian.org/DebianTesting\n Robimy backup Aktualizujemy źródła wskazywały na paczki gałęzi testing (poniższe polecenie nadpisze Twoje obecne repozytoria):  cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy main non-free contrib deb http://security.debian.org/ wheezy/updates main contrib non-free deb-src http://security.debian.org/ wheezy/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ wheezy-updates main non-free contrib SRC Teraz odświeżamy repozytoria:\nsudo apt-get update Proponuję pobrać też pliki by podczas aktualizacji wszystkie leżały w cache\u0026rsquo;u - na wypadek gdyby nagle padło łącze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz zaktualizujemy kluczowe paczki:\nsudo apt-get install apt dpkg I resztę systemu:\nsudo apt-get dist-upgrade To teraz pozostało sprawdzić co poszło nie tak\u0026hellip; Powodzenia 😉\n","href":"/2012/09/aktualizacja-debian-squeeze-do-wheezy/","title":"Aktualizacja Debian Squeeze do Wheezy"},{"content":"","href":"/tags/wheezy/","title":"Wheezy"},{"content":"Dziś TIP z przeciwnego obozu - oprócz linuksowych systemów administruję również paroma serwerami windowsowymi i tutaj również (a czasem nawet bardziej) uda mi się znaleźć coś wartego zapamiętania.\nJedna z najbardziej charakterystycznych rzeczy na komputerach przyłączonych do domeny Windows to wyświetlanie \u0026ldquo;różnych dziwnych rzeczy\u0026rdquo; przy starcie systemu. Zarówno na Windowsie 2000 jak i na XP\u0026rsquo;ku na małym okienku przewijają informacje o aktualizacji polityk, instalacji oprogramowania itp\u0026hellip;\nZachowanie to zmieniło się na Vistach i 7-kach, które są nieco mniej rozmowne i wyświetlają jedynie komunikat typu \u0026ldquo;Trwa uruchamianie systemu\u0026hellip;\u0026rdquo; i tyla\u0026hellip; Załóżmy że wrzucimy do instalacji kilka paczek i jeszcze zmienimy kilka polityk i przez to komputer na tym napisie zatrzyma się na 5~10 minut - co zrobi użyszkodnik po 3 minutach? Dojdzie do wniosku że \u0026ldquo;coś się zwiesiło\u0026rdquo; i zresetuje komputer, a że akurat było nieco operacji dyskowych to mamy praktycznie pewną rozwałkę tego systemu.\nJednym z rozwiązań jest zwiększenie \u0026ldquo;gadatliwości\u0026rdquo; tego etapu uruchamiania systemu - jest to możliwe o ile posiadamy kontroler domeny na minimum Windows Server 2008. Tworzymy GPO w którym ustawiamy: Computer Config -\u0026gt; Admin Templates -\u0026gt; System -\u0026gt; Verbose vs normal status messages na Enabled.\nWłączenie tej opcji spowoduje zwiększenie liczby wypisywanych komunikatów, nazw instalowanych programów i operacji wykonywanych przez system w fazie uruchamiania - większość użyszkodników widząc że zmieniają się statusy (coś instaluje itp) zwiększa \u0026ldquo;okienko czasowe\u0026rdquo; do naciśnięcia resetu do ok 15 minut 😉\n","href":"/2012/09/gpo-windows-7-postep-przetwarzania-polityk-przy-starcie-systemu/","title":"GPO: Windows 7 - postęp przetwarzania polityk przy starcie systemu"},{"content":"Gdy administruje się dużymi stronami internetowymi raz na czas np. po większych zmianach w konfiguracji zachodzi potrzeba sprawdzenia czy na stronie nie ma stron prowadzących donikąd. O ile w małych serwisach można samemu szybko przeklikać się przez stronkę to dla starych rozrośniętych serwisów nie jest to takie proste.\nJest kilka narzędzi których można użyć do testowania linków na stronach - każde z nich ma swoje zalety i wady, postaram się je przybliżyć.\nwget Wget\u0026rsquo;a najprawdopodobniej już masz i możesz zaczynać:\nwget -o /tmp/wget.log -nv -r -p http://example.com W pliku /tmp/wget.log możemy znaleźć komunikaty błędów i na dobrą sprawę tyle. Ciężko to przetworzyć ale jeśli nasz serwis ma mechanizm do np. mailowego powiadomienia w momencie wystąpienia krytycznego błędu to wget\u0026rsquo;em najszybciej można takie strony wyłapać.\nlinkchecker Uruchamia się go tak:\nlinkchecker -t3 --no-warnings http://example.com Program jakoś nie przypadł mi do gustu - działał cholernie wolno i to na całkiem małej stronie.\nlinklint Z trzech programów ten ma najdziwniejszą składnię - ale da się tego nauczyć, a możliwości ma chyba najwięcej.\nlinklint -error -warn -xref -forward -out report.txt -net -http -host example.com /@ Wystarczająco szybki i generuje przejrzyste raporty.przejrzyste raporty.\n","href":"/2012/09/sprawdzanie-nieaktywnych-linkow-na-stronie/","title":"Sprawdzanie nieaktywnych linków na stronie"},{"content":"Jeśli posiadasz napęd taśmowy LTO do archiwizacji/backupu danych to wiesz że bardzo ważne jest opisywanie taśm szczególnie gdy trzeba coś odzyskać. Jeżeli masz szczęście to posiadasz nie pojedynczy napęd a autoloader obsługujący wiele taśm i wiesz że najlepiej gdy taśmy są opisane kodami paskowymi które autoloader potrafi rozpoznać. Dzięki temu nie ma potrzeby odczytywania nr. seryjnego z taśmy tylko szybciutko skanerem kodów. Tasiemki można kupować już z kodami ale tańsze są te bez kodów\u0026hellip; i tu pytanie - czy da się tanio zdobyć etykiety?\nKtoś poświęcił trochę czasu i przygotował webowe narzędzie do generowania kodów paskowych dla napędów LTO - plik generowany jest do postaci pdf\u0026rsquo;a który można później wydrukować na papierze samoprzylepnym, pomachać chwilę nożyczkami i TA DAM! Mamy etykiety 😉\nOczywiście zabawa ma sens jeśli potrzebujemy tylko kilku szt. taśm tygodniowo - przy większej ilości lepiej wydać kasę i niech ktoś inny powycina etykiety za nas.\n","href":"/2012/09/generator-kodow-paskowych-dla-napedow-tasmowych-lto/","title":"Generator kodów paskowych dla napędów taśmowych LTO"},{"content":"","href":"/tags/lto/","title":"LTO"},{"content":"Szkoda że polecenia do obsługi NFS\u0026rsquo;a nie zaczynają się od nfs* - łatwiej byłoby mi je zapamiętać. A jednym z takich, zapominanych najczęściej jest listowanie zasobów, szczególnie przydatne gdy korzysta się z NFS\u0026rsquo;a na jakimś NAS\u0026rsquo;ie (którego magiczny soft nie pokazuje gdzie i co eksportuje):\nshowmount -e 192.168.1.10 Export list for 192.168.1.10: /mnt/pools/A/A0/Music * /mnt/pools/A/A0/Movies * /mnt/pools/A/A0/Backups * /mnt/pools/A/A0/Pictures * /mnt/pools/A/A0/Documents * Teraz możemy zamontować zasób:\nmount 192.168.1.10:/mnt/pools/A/A0/Music /mnt/music ","href":"/2012/09/listowanie-zasobow-nfs/","title":"Listowanie zasobów NFS"},{"content":"","href":"/tags/nfs/","title":"NFS"},{"content":"Niedawno trafiłem na ciekawy problem w mod_rewrite - by przekierowywać użytkowników logujących się jednym z modułów mod_auth_basic do dedykowanych im katalogów, równocześnie blokując dostęp do katalogów innych użytkowników. Nie brzmi to jakoś strasznie ale problem okazał się być całkiem nietrywialnym. Teoretyczne rozwiązanie sprowadzało się do wyszukania loginu użytkownika ze ścieżki URI i porównania z nazwą użytkownika ze zmiennej %{REMOTE_USER} - jeśli wartości się różnią to Forbidden. Ale szybko okazało się że w RewriteCond zmienne z dopasowań można podstawiać tylko w pierwszym parametrze i że o ile można RewriteCond\u0026rsquo;y połączyć wyrażeniami logicznymi typu AND/OR to nie ma możliwości porównania czy dopasowania z kolejnych RewriteCond\u0026rsquo;ów są identyczne. Po kilku dniach szperania w dokumentacji i różnych tutorialach udało mi się trafić na jedną wartościową wskazówkę ale tej stronki już nie ma, więc opiszę problem dla potomnych.\nZałożenia są takie:\n mamy vhost\u0026rsquo;a który udostępnia wszystkie foldery użytkowników, każdy użytkownik posiada folder o nazwie dokładnie takiej samej jak jego login, użytkownik po zalogowaniu ma być przekierowany do swojego folderu i przy próbie przejścia do folderów innych użytkowników albo nawracamy go do jego folderu/albo dajemy forbidden.  Konfiguracja vhost\u0026rsquo;a Poniżej podstawowa konfiguracja vhost\u0026rsquo;a:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Rewrite\u0026rsquo;y I najciekawsza część czyli rewrite\u0026rsquo;y. Zaczynamy od przekierowania użytkownika do jego folderu:\nRewriteRule ^$ /%{REMOTE_USER} [R,L] Powyższy rewrite sprawdza czy próbujemy wejść do głównego katalogu, jeśli tak to przekierowujemy do katalogu użytkownika.\nTo teraz magia, której długo szukałem 😃\nRewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] Już wyjaśniam co to robi - zacznę od przypomnienia że pomimo takiego zapisu w konfiguracji reguły są przetwarzane nieco inaczej: Apache najpierw sprawdza czy dany URI pasuje do wyrażenia w RewriteRule, a dopiero gdy tak jest sprawdzane są warunki w RewriteCond. Czyli RewriteRule dopasowuje pierwszą część URI aż do znaku ukośnika / i zapamiętuje w zmiennej $1. Dopiero teraz RewriteCond dopasowuje i zapamiętuje login użytkownika w zmiennej %1 (tak rule zapamiętuje w zmiennych z $, cond w zmiennych z %). Teraz gdy mamy już zapamiętane loginy z URI i zmiennej to możemy je zapisać obok siebie w kolejnym RewriteCond oddzielając znakiem który w loginie wystąpić nie powinien (np. dwukropkiem) - $1:%1. Teraz dopasowujemy pierwszą część \u0026ldquo;sklejki\u0026rdquo;, czyli ^([^:]+): i zaraz potem wymagamy by pojawiła się ta sama wartość przez wsteczną referencję \\1$ - to porównywane jest z pierwszym parametrem cond\u0026rsquo;a. To dopasowanie jest prawdziwe gdy użytkownik loguje się prawidłowo, więc negujemy je stawiając ! na początku regexp\u0026rsquo;a, by każde błędne logowanie powodowało wywołanie RewriteRule, czyli Forbidden.\nPogmatwane? Więc teraz na przykładzie:\n błędne logowanie (bo prostsze):\nlogin: roman\nuri: zbyszek/\npo dopasowaniu w $1 mamy zbyszek, a w %1 mamy roman, więc $1:%1 to zbyszek:roman, ostatni cond sprawdza czy zbyszek:roman różni się od zbyszek:zbyszek - a skoro tak to blokujemy dostęp, dobre logowanie:\nlogin: roman\nuri: roman/\npo dopasowaniu w $1 i %1 mamy roman i sprawdzamy czy $1:%1 jest zgodne z roman:roman, a jest więc po negacji nie blokujemy dostępu. Skoro dostępu nie blokujemy to roman może dostać się do swoich plików.  Finalna konfiguracja Zostało przedstawienie całościowo konfiguracji:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName files.example.com ServerAlias www.files.example.com DocumentRoot /var/www/files ErrorLog ${APACHE_LOG_DIR}/error.log LogLevel warn CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;Directory /var/www/files\u0026gt; AllowOverride none AuthType basic AuthName \u0026#34;Zaloguj sie\u0026#34; AuthUserFile /etc/apache2/passwd Require valid-user RewriteEngine on RewriteRule ^$ /%{REMOTE_USER} [R,L] RewriteCond %{REMOTE_USER} ^(.+) RewriteCond %1:$1 !^([^:]+):\\1$ RewriteRule ^([^/]+)/ - [F,L] \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Duże i małe litery wpisywane w loginie przez userów Zerknij tutaj.\n","href":"/2012/09/apache-ograniczenie-dostepu-dla-zalogowanych-uzytkownikow-z-mod_rewrite-i-mod_auth_basic/","title":"Apache: ograniczenie dostępu dla zalogowanych użytkowników z mod_rewrite i mod_auth_basic"},{"content":"","href":"/tags/mod_auth_basic/","title":"mod_auth_basic"},{"content":"Wybór dobrego X-terminala to w życiu admina prawie jak wybór żony\u0026hellip; spędza się wspólnie dużo czasu i miło gdy estetycznie wygląda, robi to co chcemy, itd\u0026hellip; 😉\nNie lubię gnome-terminal'a bo domyślnie binduje F10 co wnerwia mnie w midnight commanderze, stąd szukałem i szukałem i jak dotychczas najbardziej podpasował mi unicode-rxvt. Można uruchamiać go po prostu jako urxvt lub uruchomić demona urxvtd po zalogowaniu i potem odpalać tylko klienta urxvtc. Druga metoda skutkuje natychmiastowym startem terminala, wiec gdy podbinduję go sobie pod F12 mam terminal zawsze pod ręką w mniej niż sekundę. Dodatkowo fajnie wygląda z półprzezroczystością i nie ma żadnych dodatkowych menu/gadżetów.\nSkróty klawiaturowe Wygodne skróty klawiaturowe to kolejny atut tego terminala, najczęściej korzystam z:\n shift+dół - otwarcie nowej karty, shift+lewo/shift+prawo - przejście na kartę w lewo/prawo, ctrl+lewo/ctrl+prawo - przeniesienie karty w lewo/prawo, ctrl+d - zamknij kartę.  Więcej ciekawych informacji o tym terminalu można znaleźć tutaj.\nKonfig dla urxvt cat \u0026gt; ~/.Xdefaults \u0026lt;\u0026lt;SRC visualBell: False Xft.antialias: true Xft.hinting: true Xft.hintstyle: 0 Xft.dpi: 75 urxvt*termName: rxvt-unicode urxvt*geometry: 110x35 urxvt*background: rgba:2000/2000/2000/dddd urxvt*foreground: white urxvt*depth: 32 urxvt*fading: 40 urxvt*shading: 40 # fajny font ale w nowszych Ubuntu dziwnie zachowuje sie kursor ;-( #urxvt*font: xft:Terminus:pixelsize=14 #urxvt*font: xft:Monospace:pixelsize=12 urxvt*font: xft:Ubuntu Mono:pixelsize=14 urxvt*scrollBar: false urxvt*saveLines: 30000 urxvt*tintColor: gray urxvt*perl-ext-common: default,matcher,selection-autotransform,tabbed,selection-pastebin # urlLauncher otwiera wpisaną przeglądarkę # po kliknięciu środkowym klawiszem myszy #urxvt*urlLauncher: firefox urxvt*urlLauncher: chromium-browser # to już nie jest config dla urxvt # ale przeważnie też go dorzucam xterm*geometry: 110x35 xterm*background: black xterm*foreground: grey xterm*fading: 90 xterm*shading: 50 xterm*inheritPixmap: true xterm*font: -misc-fixed-medium-r-*-*-12-*-*-*-*--iso10646-1 xterm*scrollBar: false xterm*saveLines: 30000 xterm*tintColor: gray SRC P.S. Jeśli urxvt wydaje Ci się zbyt spartański to zerknij na terminator\u0026rsquo;a.\n","href":"/2012/09/unicode-rxvt-moje-ustawienia/","title":"unicode-rxvt - moje ustawienia"},{"content":"","href":"/tags/urxvt/","title":"urxvt"},{"content":"","href":"/tags/fsck/","title":"fsck"},{"content":"Jeżeli chcemy by po ponownym uruchomieniu w czasie startu zostały sprawdzone wszystkie dyski narzędziem to można to osiągnąć na dwa sposoby. Zawsze gdy tego potrzebuję zastanawiam się tylko jaki plik trzeba było utworzyć\u0026hellip; forcefsck, fsck, fsckforce\u0026hellip; więc notuję 😉\nUtworzenie pliku /forcefsck Pierwsza metoda polega na utworzeniu pliku forcefsck w głównym katalog, robimy to poniższym poleceniem:\nsudo touch /forcefsck Polecenie shutdown Druga metoda wykorzystuje parametr -F polecenia shutdown ale nie działa na wszystkich dystrybucjach (w takiej sytuacji patrz pierwsza metoda):\nsudo shutdown -rF now ","href":"/2012/09/wymuszenie-fsck-po-restarcie/","title":"Wymuszenie fsck po restarcie"},{"content":"Znajomi co jakiś czas pytają mnie: jak nazywa się ta aplikacja, którą masz na telefonie do\u0026hellip;? Z jakiego programu do poczty korzystasz na tel\u0026hellip;? itd\u0026hellip;\nPytacie - więc macie 😃\nDGT GTD Bardzo przydatna lista TODO. Stworzona z myślą o metodzie Getting Things Done i bardzo ułatwia pamiętanie u różnych zadaniach. Posiada też bardzo wygodny widżet, na którym możemy podglądnąć nasze najbliższe zadania.\nOpera Mini Wbudowana przeglądarka jest niezła, ale Opera Mini kompresuje mocno strony wykorzystując pośredniczące serwery proxy co znacznie obniża koszty transmisji danych. Od niedawna posiada tez prymitywny ale niezgorszy czytnik RSS\u0026rsquo;ów.\nWiFinder Prosty i funkcjonalny skaner sieci WiFi - wolę go zamiast domyślnego narzędzie do wyszukiwania i podłączania sieci WiFi.\nWiFi Analyzer Bardziej zaawansowany skaner, pokazujący moc sygnały do poszczególnych AP, jakość danego kanału. Szpanerska aplikacja przy kolegach adminach 😉\nK-9 Mail Klient pocztowy - dość zaawansowany funkcjonalnie ale z bardzo prostym interfejsem (choć początkowa konfiguracja bywa nieco \u0026ldquo;tricky\u0026rdquo;).\nAdFree Android Na root\u0026rsquo;owanym telefonie trzeba to mieć - aplikacja blokuje reklamy w większości popularnych aplikacji.\nAdvanced Task Killer Bardzo przydatny programik - szczególnie na słabszych telefonach (jak mój). Zabija aplikacje działające w tle, zwalniając pamięć i odciążając procesor. Dzięki temu nawet cienki telefon działa całkiem żwawo. Dostajemy również widżet, który za jednym dotknięciem zabija wszystko 😉\nTransportoid Rozkład jazdy komunikacji miejskiej dla wielu polskich miast. Można sprawdzić połączenia danej linii, odjazdy z danego przystanku, wyszukać połączenia. Jesteśmy automatycznie informowani o zmianach rozkładu, który można z poziomu aplikacji pobrać (bez żadnych rejestracji).\n","href":"/2012/09/moje-ulubione-aplikacje-na-androida/","title":"Moje ulubione aplikacje na Android’a"},{"content":"Jedną z rzeczy, które podobają mi się w maszynach wirtualnych Xen jest możliwość zrobienia backupu całego obrazu i szybkie odzyskanie już w trakcie ciężkiej awarii. Gdy dodatkowo korzysta się z LVM\u0026rsquo;a to można na chwilę wyłączyć DomU, utworzyć snapshot jego dysków, uruchomić DomU i w trakcie działania robić spójny backup ze snapshot\u0026rsquo;a. Dzięki takiemu mechanizmowi serwer jest niedostępny przez kilkanaście sekund, a backup spójny jakby został wykonany przy całkowicie wyłączonej maszynie. Taki backup sprowadza się do kilku poleceń które można oskryptować np.:\nlvcreate -L1000M -s -n volumendomu-snap /dev/vg/volumendomu dd if=/dev/vg/volumendomu | gzip -9 \u0026gt; backup.img.gz Problem pojawia się przy próbie montowania takiego snapshot\u0026rsquo;a by uzyskać dostęp do plików gdy na volumenie LVM zostanie utworzona partycja i dopiero ona formatowana (domyślnie przy ext3/4). Czyli potrzebujemy zamontować partycję z volumenu LVM ale ta nie jest wprost widoczna (nie ma urządzenia np. /dev/vg/volumendomu1).\nTen sam problem pojawia się przy dostępie do partycji \u0026ldquo;zaszytych\u0026rdquo; w obrazie zrzuconym narzędziem dd z całego dysku, np.:\ndd if=/dev/sda of=/mnt/backups/somewhere.img W obu przypadkach w obrazie/wolumenie jest zaszyta partycja i przy próbie montowania dostaniemy tylko monit o nieznanym typie systemu plików.\nBy wylistować partycje wewnątrz obrazu lub wolumenu najwygodniej posłużymć się narzędziem parted :\nsudo parted -s /mnt/backups/somewhere.img \u0026#34;unit B print\u0026#34; Model: (file) Dysk /mnt/backups/somewhere.img: 500105740288B Rozmiar sektora (logiczny/fizyczny): 512B/512B Tablica partycji: msdos Numer Początek Koniec Rozmiar Typ System plików Flaga 1 1048576B 500105740287B 500104691712B primary ntfs Parted może działać albo w trybie interaktywnym albo razem z parametrem -s podajemy na końcu skrypt z poleceniami które mają zostać podane. Powyższe wywołanie zmienia jednostki z kilo/megabajtów na bajty (dokładnie tego potrzebujemy jako offset - nic nie będziemy musieli przeliczać).\nTeraz możemy próbować zamontować daną partycję korzystając z parametru offset w mount, np. tak:\nsudo mount -o loop,ro,offset=1048576 -t ntfs /mnt/backups/somewhere.img /mnt/test Jeśli posiadamy stosunkowo aktualne wersje jajka i pakietu util-linux to powyższa sztuczka powinna się udać. Jeśli takowych nie posiadamy to możemy mieć problemy przy próbie montowania kolejnych partycji. Wtedy może być potrzebne rozpakowania pojedynczej partycji z obrazu poleceniem dd.\n","href":"/2012/09/montowanie-partycji-z-obrazu-dysku/","title":"Montowanie partycji z obrazu dysku"},{"content":"Kiedyś poproszono mnie o przeszukanie wszystkich plików php na serwerze webowym po kątem wywołania pewnej funkcji. Oczywiste wydało mi się użycie rekurencyjnie grep\u0026rsquo;a, więc:\ngrep -R \u0026#34;JAKAS_FUNKCJA\u0026#34; /var/www/*.php Ale szybko okazało się że grep dopasowuje maskę *.php również do katalogów, więc nie przeszukiwał katalogów które nie kończyły się na .php ehhh\u0026hellip;..\nDrugie podejście okazało się trafniejsze - najpierw poleceniem find wyszukuję wszystkie pliki php, a dopiero później grepuję (wypisując nazwę pliku i numer linii):\nfind /var/www/ -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;JAKAS_FUNKCJA\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; Przykładowo wynik:\nfind /var/www -iname \u0026#39;*.php\u0026#39; -print0 | xargs -0 -I\u0026#39;{}\u0026#39; sh -c \u0026#39;grep -iHn \u0026#34;eval(\u0026#34; \u0026#34;{}\u0026#34;\u0026#39; /var/www/*****/wp-admin/press-this.php:208: var my_src = eval( /var/www/*****/wp-admin/press-this.php:219: var my_src = eval( /var/www/*****/wp-admin/press-this.php:402: eval(data); /var/www/*****/wp-admin/includes/class-pclzip.php:4063:// eval(\u0026#39;$v_result = \u0026#39;.$p_options[PCLZIP_CB_PRE_EXTRACT].\u0026#39;(PCLZIP_CB_PRE_EXTRACT, $v_local_header);\u0026#39;); /var/www/*****/wp-includes/class-phpmailer.php:1916: //TODO using /e (equivalent to eval()) is probably not a good idea /var/www/*****/wp-includes/class-json.php:22: * Javascript, and can be directly eval()\u0026#39;ed with no further parsing /var/www/*****/wp-includes/functions.php:190: if ( doubleval($bytes) \u0026gt;= $mag ) ","href":"/2012/08/przeszukiwanie-plikow-danego-typu-pod-katem-tekstu/","title":"Przeszukiwanie plików danego typu pod kątem tekstu"},{"content":"To raczej nie jest podstawowy konfig i próżno szukać go na stronie WordPress\u0026rsquo;a, więc odradzam tę zabawę jeśli nie zna się zbyt dobrze nginx\u0026rsquo;a.\nPonieważ serwerek, na którym działa stronka to sprzęcik z Atomem 330 i mocy na CPU zbyt wiele nie ma to popularne pluginy (np. W3 Total Cache) potencjalnie zwiększające wydajność tak na prawdę zmulały stronkę jeszcze bardziej. Pluginów sprawdziłem kilka i każdorazowo efekt był podobny - stronka działała wolniej niż bez ich pomocy.\nDruga sprawa to zwiększony ruch - w takiej konfiguracji już przy kilku osobach równocześnie przeglądających blog, serwerek zwyczajnie nie radził sobie z dynamicznym generowaniem strony.\nPomysł na rozwiązanie problemu z wydajnością polegał na odpaleniu cache\u0026rsquo;ującego reverse proxy przed właściwą stroną, z krótkim okresem ważności cache\u0026rsquo;u (max kilka sekund) tak by przy dużym obciążeniu strony serwować głównie z cache\u0026rsquo;u (tylko co pewien czas ktoś będzie miał niefart i będzie musiał zaczekać na wygenerowanie strony), przy czym komentarze i panel administracyjny działają z pominięciam cache\u0026rsquo;owania (czyli każdorazowo trafiają przez proxy do aplikacji).\nWażne jednak by osoba wysyłająca komentarz mogła wynik swojego działania zobaczyć od razu na stronie. Ponieważ komentarze wysyłane są metodą HTTP POST to w momencie odebrania takiego połączenia będzie ustawiane ciasteczko dezaktywujące cache dla danego połączenia na kilka sekund (do momentu jego wygaśnięcia).\nPoniżej plik konfiguracyjny, który należy zapisać np. w: /etc/nginx/sites-available/wordpress\n# na początek ustawiamy lokalizację dla cache\u0026#39;u proxy_cache_path /var/cache/nginx/wordpress levels=1:2 keys_zone=WORDPRESS:10m inactive=24h max_size=100m; # nie chcę stronki z www na początku więc cały ruch przekierowują # na stronkę bez www server { listen 10.0.1.2:80; server_name www.example.com; rewrite ^ http://example.com$request_uri? permanent; } # tutaj ma miejsce magia - główny host obsługujący stronkę # to tak na prawdę cache\u0026#39;ujące proxy serwujące okresowo # generowane pliki server { listen 10.0.1.2:80 default; access_log /var/log/nginx/wordpress.access.log; server_name example.com; # ten rewrite przerzuca do panelu admina nawet jeśli  # na końcu nie wpiszemy ukośnika  # bez niego też to działa ale przekierowanie jest przetwarzane  # przez skrypt stronki i działa wolniej  rewrite ^/wp-admin$ /wp-admin/ last; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s  # (moja stronka nie obsługuje zbyt dużego ruchu i rzadko  # jest modyfikowana - np. przez komentarze - więc 60s jest OK,  # na bardziej aktywnych stronkach można się pokusić o ustawienie  # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to  # cache zapewni obsługę nawet kilku tys. zapytań na sekundę  proxy_cache_valid 200 60s; # informacje dla backendu na jakiego host się wbijamy  # i z jakiego \u0026#34;prawdziwego\u0026#34; IP  proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # możemy ukryć niektóre nagłówki np. by nie podpowiadać  # z jakich pluginów korzystamy w WordPressie  proxy_hide_header X-Powered-By; # kilka ustawień timeout\u0026#39;ów  proxy_connect_timeout 60; proxy_read_timeout 120; proxy_send_timeout 120; # Ważne - poniższa opcja ustawia w jaki sposób generowane są  # nazwy plików w cache\u0026#39;u,  # dodając np. kolejne zmienne możemy zróżnicować cache dla  # pewnych grup użytkowników  proxy_cache_key \u0026#34;$scheme$request_method$host$request_uri\u0026#34;; # domyślna lokalizacja  location / { # ustawiamy domyślną wartość zmiennej  set $no_cache \u0026#34;\u0026#34;; # If non GET/HEAD, don\u0026#39;t cache \u0026amp; mark user as uncacheable for 1 second via cookie  # jeśli metoda inna niż GET/HEAD to oznacz usera przez cookie jako niecachowanego  # na czas 60s (ustawiany poniżej)  if ($request_method !~ ^(GET|HEAD)$) { set $no_cache \u0026#34;1\u0026#34;; } # jeśli zalogowany to nie cache\u0026#39;uj  if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache \u0026#34;1\u0026#34;; } # jeżeli któryś z wcześniejszych warunków jest spełniony  # to ustawiamy cookie, które poinformuje nas by nie cachować  # kolejnych zapytań  # (z powodu \u0026#34;dziwnego\u0026#34; zachowania if w nginx\u0026#39;ie ustawienie  # tego bezpośrednio we wcześniejszych warunkach nie działa)  if ($no_cache = \u0026#34;1\u0026#34;) { add_header Set-Cookie \u0026#34;_mcnc=1; Max-Age=61; Path=/\u0026#34;; add_header X-Microcachable \u0026#34;0\u0026#34;; } # jeśli cookie jest ustawione to pomijamy cache i serwujemy  # świeżą treść  if ($http_cookie ~* \u0026#34;_mcnc\u0026#34;) { set $no_cache \u0026#34;1\u0026#34;; } # dwie poniższe opcje zapewniają pominięcia cache\u0026#39;owania  # w przypadku gdy wystąpi któryś z wcześniejszych warunków  proxy_no_cache $no_cache; proxy_cache_bypass $no_cache; # Serwujemy cache jeśli strona jest obecnie odświeżana  # lub wystąpi błąd  proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; # pliki większe niż 1M nie będą cache\u0026#39;owane  proxy_max_temp_file_size 1M; # cache\u0026#39;ujemy tylko odpowiedzi 200 i przez 60s  # (moja stronka nie obsługuje zbyt dużego ruchu i rzadko  # jest modyfikowana - np. przez komentarze - więc 60s jest OK,  # na bardziej aktywnych stronkach można się pokusić o ustawienie  # 1~3s przez co stronka jest praktycznie dynamiczna ale mimo to  # cache zapewni obsługę nawet kilku tys. zapytań na sekundę  proxy_cache_valid 200 60s; # zmieniamy domyślny klucz cache\u0026#39;owania tak by uwzględniał  # naszą zmienną  proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; # wskazujemy konkretną lokalizację cache\u0026#39;u  proxy_cache WORDPRESS; # podajemy lokalizację backendu (nie widziałem sensu  # by udostępniać go na zewnętrznym adresie)  proxy_pass http://127.0.0.1:81; # można ustawić dodatkowo cache\u0026#39;owanie strony w przeglądarce  # (całkiem niezależnie od tego co będzie w cache\u0026#39;u na serwerze)  expires 60s; } # dla panelu administracyjnego ustawiamy proxy bez cache\u0026#39;u  location ~* wp\\-(admin|login) { # dostęp do panelu administracyjnego dodatkowo chronimy  # hasłem - po co?  # bo w tym katalogu są różne fajne skrypty, w których już  # nie raz znaleziono dziury  auth_basic \u0026#34;Go Away\u0026#34;; auth_basic_user_file htpasswd; # proxy bez cache\u0026#39;u  proxy_pass http://127.0.0.1:81; } # statykę cache\u0026#39;ujemy mocniej niż treści dynamiczne  # a czemu nie puszczam jej bezpośrednio? bo wyplute przez  # backend zostaną skompresowane i w takiej postaci zachowają  # się w cache\u0026#39;u - gdybym serwował je bezpośrednio to nginx  # kompresowałby np. css\u0026#39;y/js\u0026#39;y przy każdym dostępie do nich  location ~* \\.(jpg|png|gif|jpeg|css|js|mp3|wav|swf|mov|doc|pdf|xls|ppt|docx|pptx|xlsx)$ { # cache\u0026#39;ujemy statykę przez 2 godziny  proxy_cache_valid 200 120m; # dodatkowo ustawiamy długie cache\u0026#39;owanie w przeglądarkach  expires 864000; # puszczamy wszystko w proxy + cache  proxy_pass http://127.0.0.1:81; proxy_cache WORDPRESS; # wyłączam logowanie dostępu do statyki nawet w przypadku błędów  # to mało istotne  log_not_found off; access_log off; } # jeszcze inaczej ustawiam cache dla RSS\u0026#39;ów  location ~* \\/[^\\/]+\\/(feed|\\.xml)\\/? { # cache\u0026#39;ujemy RSS\u0026#39;y przez 45 minut  if ($http_cookie ~* \u0026#34;comment_author_|wordpress_(?!test_cookie)|wp-postpass_\u0026#34; ) { set $no_cache 1; } proxy_cache_key \u0026#34;$scheme://$host$request_uri $no_cache\u0026#34;; proxy_cache_valid 200 45m; proxy_cache MYSITE; proxy_pass http://127.0.0.1:81; } } # to teraz konfiguracja serwera serwującego treści dynamiczne server { # nasłuchujemy lokalnie bo z zewnątrz strona dostępna  # jest przez proxy  listen 127.0.0.1:81; error_log /var/log/nginx/mysite.error.log; root /var/www/wordpress; index index.php; # logujemy prawdziwe IP dzięki odpowiednim nagłówkom  # przesyłanym przez proxy  set_real_ip_from 127.0.0.0/24; real_ip_header X-Real-IP; # tutaj praktycznie klasyka - z tym że zamiast na końcu  # wskazywać index.php robię najpierw kilka rewrite\u0026#39;ów  # np. dla przeniesionych stron, itp  location / { try_files $uri $uri/ @rewrites; } location @rewrites { rewrite /main http://example.com/about/? permanent; rewrite /projekty http://example.com/category/projects/? permanent; rewrite /tag/gd http://example.com? permanent; rewrite /category/hobby http://roman.com? permanent; rewrite ^ /index.php last; } # na bardziej obleganych stronach limitowanie wyszukiwania może # pomóc, ale to nie mój przypadek # location /search { limit_req zone=mysitesearch burst=3 nodelay; rewrite ^ /index.php; }  location ~* \\.(?:ico|css|js|gif|jpe?g|png)$ { # cache\u0026#39;owanie atrybutów statycznych plików  open_file_cache max=1000 inactive=120s; open_file_cache_valid 45s; open_file_cache_min_uses 2; open_file_cache_errors off; # maksymalne cache\u0026#39;owanie statyki w przeglądarkach  expires max; } # no i na końcu obsługa skryptów php  location ~* \\.php$ { # albo plik istnieje i go serwujemy albo dajemy Forbidden  try_files $uri =403; # kilka standardowych ustawień  include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # blokujemy możliwość wykonywania skryptów z katalogu upload  # (nawet jeśli komuś uda się je wepchnąć)  if ($uri !~ \u0026#34;^/wp-content/uploads/\u0026#34;) { fastcgi_pass php-fastcgi; } # informacje dla proxy jak długo może cache\u0026#39;ować  add_header Cache-Control \u0026#34;max-age:60, public\u0026#34;; expires 60s; } # blokuję dostęp do plików zaczynających się od kropki  location ~ /\\. { access_log off; log_not_found off; deny all; } # wyłączam logowanie do nieistotnych plików  location = /robots.txt { allow all; access_log off; log_not_found off; } location = /favicon.ico { access_log off; log_not_found off; } } Konfiguracja potrzebuje jednego folderu na cache, do którego dostęp do zapisu ma nginx (użytkownik na którym działa proces):\nmkdir -p /var/cache/nginx/wordpress chown -R www-data:www-data /var/cache/nginx/wordpress A żeby zaczął działać trzeba go \u0026ldquo;włączyć\u0026rdquo; i przeładować nginx\u0026rsquo;a:\ncd /etc/nginx/sites-available/ ln -s wordpress /etc/nginx/sites-enabled/wordpress service nginx reload Jedyna rzecz, której brakuje w tym configu to konfiguracja backend\u0026rsquo;u do PHP\u0026rsquo;a (u mnie nazwana php-fastcgi) - może kiedyś zrobię HOWTO o konfiguracji NGINX+PHP, ale na tą chwilę zakładam że sobie poradzisz 😃\nBenchmark Sprawdźmy jak to wygląda teraz:\nab -n 1000 -c 10 https://gagor.pl/ This is ApacheBench, Version 2.3 \u0026lt; $Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking gagor.pl (be patient) Completed 100 requests Completed 200 requests Completed 300 requests Completed 400 requests Completed 500 requests Completed 600 requests Completed 700 requests Completed 800 requests Completed 900 requests Completed 1000 requests Finished 1000 requests Server Software: nginx Server Hostname: gagor.pl Server Port: 80 Document Path: / Document Length: 36263 bytes Concurrency Level: 10 Time taken for tests: 8.716 seconds Complete requests: 1000 Failed requests: 22 (Connect: 0, Receive: 0, Length: 22, Exceptions: 0) Write errors: 0 Total transferred: 36601094 bytes HTML transferred: 36263088 bytes Requests per second: 114.73 [#/sec] (mean) Time per request: 87.159 [ms] (mean) Time per request: 8.716 [ms] (mean, across all concurrent requests) Transfer rate: 4100.91 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 1 16 7.1 15 55 Processing: 17 67 97.0 53 789 Waiting: 5 28 55.6 19 456 Total: 37 83 98.0 68 803 Percentage of the requests served within a certain time (ms) 50% 68 66% 74 75% 78 80% 80 90% 87 95% 95 98% 665 99% 755 100% 803 (longest request) Dla mnie bomba 😃\nPomysł na taki rodzaj cache\u0026rsquo;owania zaczerpnąłem stąd.\n","href":"/2012/06/nginx-konfiguracja-pod-wordpressa/","title":"Nginx - konfiguracja pod WordPress’a"},{"content":"","href":"/tags/reverse-proxy/","title":"reverse proxy"},{"content":"Pisałem już HOWTO o konfiguracji Xen\u0026rsquo;a ale nie opisałem jak się bawić wirtualkami gdy Xen\u0026rsquo;a już mamy. To nadrabiam.\nTworzenie i usuwanie maszyn wirtualnych Do tworzenia/niszczenia DomU wykorzystuję pakiet xen-tools dostarczający m.in. dwa narzędzia:\n xen-create - dla którego przygotowałem dość skomplikowaną konfigurację przy okazji wcześniejszego posta: Instalacja i konfiguracja DomU. Przykład użycia: xen-create --hostname example-domu --ip 10.0.0.77 \\ --gateway 10.0.0.1 --broadcast 10.0.0.255 --netmask 255.255.255.0 \\ --bridge br10 --vcpus 2 --memory=2G  xen-delete-image - narzędzie do kasowania wirtualnych maszyn. Maszyna musi być wyłączona aby można było ją usunąć. Narzędzie to kasuje plik konfiguracyjny maszyny wirtualnej oraz przydzielone jej volumeny lvm. Przykład użycia: xen-delete-image nazwamaszyny   Zarządzanie maszynami wirtualnymi Do uruchamiania, wyłączanie, resetowania (i ogólnie zarządzania) maszynami wirtualnymi służy tylko jedno polecenie: xm z różnymi parametrami:\n xm list- listuje uruchomione w danej chwili wirtualne maszyny, wyświetlając przydzieloną im ilość pamięci, procesorów, stan (uruchomiona, zawieszona), czas działania (uptime). Dla objaśnienia Domain-0 (zwane też Dom-O) to hyperwisor czyli fizyczna maszyna na której uruchomione są wirtualki. xm top - polecenie wyświetla dokładne dane chwilowego zużycia zasobów dla różnych wirtualnych maszyn i Dom-0. xm create nazwapliku.cfg - uruchamia maszynę wirtualną zgodnie z instrukcjami zawartymi w pliku konfiguracyjnym (przydzielone dyski, pamięć, etc). xm shutdown nazwamaszyny - wysyła sygnał wyłączenia maszyny wirtualnej i wraca do wiersza poleceń. Dokładnie to polecenie wysyła sygnał ACPI równoważny przyciśnięciu przycisku Power na obudowie komputera - system operacyjny wykrywa to zdarzenie i zaczyna się wyłączać. Jest to zalecana instrukcja do wyłączania wirtualek. Należy pamiętać, że po wykonaniu tego polecenia jeszcze przez kilka/kilkanaście sekund maszyna działa - do puki nie skończy się wyłączać. xm shutdown -w nazwamaszyny - działa jak powyższe polecenie, ale dodatkowo czeka aż maszyna wirtualna zostanie wyłączona a przydzielone jej zasoby zwolnione. Gdy to polecenie skończy się wykonywać mamy pewność, że maszyna jest już wyłączona. xm destroy nazwamaszyny - polecenie do twardego resetu maszyny wirtualnej. Najpierw odbierany jest czas procesora dla maszyny, potem zwalniana pamięć i zarezerwowane uchwyty. Wykorzystując to polecenie może dojść do utraty danych lub uszkodzenia OS‘u na wirtualnej maszynie. xm reboot nazwamaszyny - restartuje maszynę wirtualną w bezpieczny sposób (czyli wysyła sygnał ACPI do wyłączenia i startuje DomU). Gdy zmodyfikujemy plik konfiguracyjny danego DomU nie wystarczy wywołać xm reboot - przeważnie potrzeba położyć maszynę i ponownie ją uruchomić, np. tak:  xm shutdown -w maszyna \u0026amp;\u0026amp; xm create maszyna.cfg  xm pause nazwamaszyny - pauzuje wirtualną maszynę, zamrażając ją w obecnym stanie razem z pamięcią itd. xm unpause nazwamaszyny - uruchamia zapauzowaną wcześniej maszynę wirtualną. Działa odwrotnie do polecenia powyżej. xm console nazwamaszyny - polecenie działa jak „podpięcie monitora” do fizycznej maszyny, na pierwszy terminal. Bardzo przydatne zaraz po utworzeniu wirtualki jak również w różnych sytuacjach kryzysowych 😉  Jest jeszcze kilka innych poleceń np. dodających na gorąco urządzenia blokowe ale ich działanie mocno zależy od wersji Xen\u0026rsquo;a i jajka.\n","href":"/2012/06/xen-podstawowe-polecenia/","title":"Xen - Podstawowe polecenia"},{"content":"Ustawienie domyślnego vhosta w nginx\u0026rsquo;ie jest ładnie opisane w dokumentacji i początkowo wydawało się dobrze działać ale gdy wykorzystałem tą konfigurację na serwerze z wieloma adresami IP i nasłuchiwaniem na porcie 80 (bez podania IP) to zachowywało się to dość dziwnie (przeważnie nie ładowało tej strony którą chciałem). Od teraz tworzę konfigurację domyślnego vhosta dla każdego z dostępnych adresów IP. Powiem szczerze że nie miałem czasu na głębsze zbadanie tego zachowania i wykorzystałem rozwiązanie, które działało w każdym przypadku czyli po jednym konfigu na IP + przekierowanie na ogólną stronę.\nserver { listen 10.0.0.100:80 default_server; server_name _; server_name_in_redirect off; rewrite ^ http://www.gagor.pl permanent; } ","href":"/2012/06/nginx-ustawienie-domyslnego-vhosta/","title":"Nginx - ustawienie domyślnego vhosta"},{"content":"VLAN\u0026rsquo;y są prostą metodą na separację sieci. Gdy mamy wiele sieci może zajść potrzeba by poszczególne DomU miały dostęp do różnych VLAN\u0026rsquo;ów (czasem nawet wielu równocześnie). Jeżeli serwer z Dom0 posiada minimum giga-bitową kartę sieciową (a najlepiej kilka) to powinniśmy być w stanie z godziwą jakością udostępnić systemom DomU różne VLAN\u0026rsquo;y z interfejsów gospodarza.\nZaprezentowane poniżej skrypty zapożyczyłem z tej strony: http://renial.net/weblog/2007/02/27/xen-vlan\nNa początek musimy zainstalować pakiet vlan:\napt-get install vlan Później w pliku /etc/xen/xend-config.sxp ustawiamy taka skrypt dla konfiguracji sieci:\n(network-script network-multi-vlan) Tworzymy plik /etc/xen/scripts/network-multi-vlan i wpisujemy w nim (no dobra - komentarze można pominąć):\n#!/bin/sh #=================================================================== # Xen vlan bridge start/stop script. # Xend calls a network script when it starts. # The script name to use is defined in /etc/xen/xend-config.sxp # in the network-script field. # # This script creates multiple bridges to segregate individual # domUs to separate VLANs. Customize to fit your needs. # # Usage: # # network-multi-vlan (start|stop|status) # #=================================================================== dir=$(dirname \u0026#34;$0\u0026#34;) # Poniższa linijka pozwala udostępnić dany interfejs sieciowy # w całości jako domyślny bridge - jeśli chcemy się ograniczyć # wyłącznie do bridge\u0026#39;y na VLAN\u0026#39;ach to możemy tą linijkę # zakomentować (jak ja) \u0026#34;$dir/network-bridge\u0026#34; \u0026#34;$@\u0026#34; vifnum=0 netdev=eth0 # No i teraz odpalamy kolejne VLAN\u0026#39;y na poszczególnych interfejsach. # Brak parametru netdev jest równoznaczny wybraniu netdev=eth0 # Parametr VLAN jest... hm... samoopisujący \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=10 netdev=eth1 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=11 \u0026#34;$dir/network-bridge-vlan\u0026#34; \u0026#34;$@\u0026#34; vlan=23 netdev=eth2 Pobieramy skrypt network-bridge-vlan do lokalizacji: /etc/xen/scripts/network-bridge-vlan.\ncd /tmp wget https://gagor.pl/wp-content/uploads/2012/06/network-bridge-vlan.gz gunzip network-bridge-vlan.gz mv network-bridge-vlan /etc/xen/scripts/network-bridge-vlan Źródła  http://wiki.xensource.com/xenwiki/XenDom0VLANstoDomUVirtualNICs - ogólne schematy konfiguracji VLAN\u0026rsquo;ów na Xen\u0026rsquo;ie, http://renial.net/weblog/2007/02/27/xen-vlan - skrypty do konfiguracji VLAN\u0026rsquo;ów pochodzą z tej strony.  ","href":"/2012/06/xen-konfiguracja-interfejsu-sieciowego-dom0-jako-brdigea-dla-roznych-vlanow/","title":"Xen - Konfiguracja interfejsu sieciowego Dom0 jako brdige’a dla VLAN’ów"},{"content":"Ostatnio trafiłem na ciekawy problem, który wielokrotnie rozwiązywałem w nginx\u0026rsquo;ie ale tym razem musiałem zrobić to w Apache. Pewna stronka działa sobie na HTTPS\u0026rsquo;ie i chciałem by wszystkie powiązane z nią pliki były serwowane z jej adresu szyfrowanym połączeniem by nie pojawiały się w przeglądarce monity że \u0026ldquo;część ruchu nie jest szyfrowana\u0026rdquo;. Tyle że część potrzebnych plików była już obecnie serwowana na innym serwerze (w innej domenie) poprzez HTTP.\nMogłem albo skopiować te pliki i wykombinować jakiś mechanizm synchronizujący albo wykorzystać proxy + cache. Drugie rozwiązanie wydało mi się prostsze i ładniejsze 😃\nNa początek włączamy w Apache\u0026rsquo;m odpowiednie moduły:\na2enmod proxy a2enmod proxy_http Teraz przykładowa konfiguracja vhosta:\n\u0026lt;VirtualHost *:80\u0026gt; #podstawowa konfiguracja vhosta ServerAdmin webmaster@example.com ServerName www.example.com ServerAlias example.com # wyłącza działanie Apache\u0026#39;go jako przekazującego proxy (forwarding proxy) ProxyRequests Off # nie chciałem by błędy HTTP z backendowego serwera były przekazywane # zamiast nich będą błędy Apache\u0026#39;go ProxyErrorOverride On # przykład prostego reverse proxy - wystarczą dwa poniższe polecenia # ProxyPass proxuje ruch do danego serwera pod wskazanym URL\u0026#39;em ProxyPass /stats/ http://google-anal.com/ # ProxyPassReverse modyfikuje nagłówki odpowiedzi ze zdalnego serwera # tak by odpowiedź wyglądała na wysłaną z lokalnego serwera ProxyPassReverse /stats/ http://google-anal.com/ # ciekawszy przykład proxowania z dodatkowymi ustawieniami cachowania # najpierw konfiguracja cache\u0026#39;u dyskowego \u0026lt;IfModule mod_disk_cache.c\u0026gt; # CacheRoot to wymagany parametr - ścieżka w której znajduje się cache CacheRoot /var/cache/apache2/mod_disk_cache/example # ponieważ przewidywałem że cache\u0026#39;owanych będzie kilka GB małych plików # to by listowanie ich w cache\u0026#39;u było efektywne warto wykorzystać wielo # poziomowe zagłębienie katalogów - wtedy na każdym poziomie, w danym # katalogu będzie stosunkowo mało plików, indeksy mniejsze, listowanie # szybsze CacheDirLevels 5 CacheDirLength 2 # teraz czas na konfiguracje cache\u0026#39;u \u0026lt;IfModule mod_cache.c\u0026gt; # pozwolę sobie na zignorowanie nagłówków Expires/Cache-Control # z aplikacji sam lepiej wiem że te pliki nie zmieniają się # zbyt często CacheIgnoreCacheControl On # pliki stracą ważność w cache\u0026#39;u po tygodniu CacheDefaultExpire 604800 # ponieważ zasoby nie różnią się dla różnych zalogowanych użytkowników # zignoruję cookies\u0026#39;y CacheIgnoreHeaders Set-Cookie # nie chcę by proxy weryfikowało czy pojawiła się nowa wersja obrazka # bo raczej rzadko pojawiają się zmiany CacheIgnoreNoLastMod On # cache uruchamiany dla dwóch \u0026#34;subkatalogów\u0026#34; # http://example.com/images oraz http://example.com/files CacheEnable disk /images CacheEnable disk /files \u0026lt;/IfModule\u0026gt; \u0026lt;/IfModule\u0026gt; # włączamy mod_rewrite - będzie za chwilkę potrzebny RewriteEngine on # poprzednio do uruchomienia proxy wykorzystałem opcję ProxyPass, # ale często potrzebujemy bardziej zaawansowanego przekierowania # i wtedy warto wykorzystać mod_rewrite do modyfikacji URL\u0026#39;i w locie # koniecznie z flagą [P] RewriteRule ^/images/(.+)/(.+) http://10.0.0.100:8080/example-images/get.php?id=$1\u0026amp;width=$2 [P] \u0026lt;Location /images\u0026gt; # jak powyżej - modyfikacja zwrotnych nagłówków ProxyPassReverse /example-images/ # kilka nagłówków z backendu ukrywam by nie były przekazywane dalej Header unset Server Header unset Expires Header unset ETag # akurat nagłówek Via można wyłączyć w konfiguracji modułu proxy # ale ponieważ bywa przydatny przy debugowaniu to w niektórych miejscach # wolę gdy jest ustawiony - a w innych nie Header unset Via # ręczne ustawienie nagłówka Cache-Control i zezwolenie # na cachowanie przez inne proxy lub przeglądarki Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # i drugie przekierowanie RewriteRule ^/files/(.+)/(.+) http://10.0.0.100:8080/example-files/$1/$2 [P] \u0026lt;Location /files\u0026gt; ProxyPassReverse /example-files/ Header unset Server Header unset Via Header unset ETag Header unset Expires Header set Cache-Control \u0026#34;max-age=604800, public\u0026#34; \u0026lt;/Location\u0026gt; # standardowa konfiguracja DocumentRoot /var/www/example \u0026lt;Directory /var/www/example\u0026gt; Options -Indexes FollowSymLinks Includes AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; LogLevel warn CustomLog /var/log/apache2/example_access.log combined \u0026lt;/VirtualHost\u0026gt; ","href":"/2012/06/apache-reverse-proxy-z-cacheowaniem/","title":"Apache - reverse proxy z cache’owaniem"},{"content":"Gdy tworzymy pierwszą aplikację webową, która umożliwia upload plików przeważnie lądują one lokalnie w pewniej lokalizacji. Gdy druga aplikacja potrzebuje dostępu do tych plików wystarczy podać ścieżkę. Problemy zaczynają się gdy aplikacji jest kilka i rozmieszczonych na kilku serwerach. Można korzystać z sieciowych systemów plików ale to często nie jest zbyt wygodne - ciężko odpowiednio ustawić uprawnienia by pewne aplikacje miały dostęp do zapisu plików a inne nie, trzeba skonfigurować dany katalog w kilku miejscach w konfiguracji serwera WWW aby serwować pliki itp\u0026hellip;\nJeżeli w utrzymaniu swoich aplikacji dociera się do takiego momentu to kolejne pomysły zakładają wykorzystanie bazy danych do przechowywania plików. Ale chwila googlania i już wiemy że bazy typu SQL średnio radzą sobie z przetwarzaniem tak dużych rekordów jak pliki. Kolejny etap to sprawdzenie co mogą nam zaoferować bazy NoSQL.\nW tym miejscu przeczytałem wiele różnych artykułów i ostatecznie zastanawiałem się czy wybrać MongoDB czy CouchDB. Oba projekty są bardzo podobne a główną ich zaletą jest łatwość wykorzystania w istniejących aplikacjach webowych. Tutaj szczególnie CouchDB wypada bardzo dobrze bo zarządzanie i dostęp do bazy odbywa się standardowym protokołem HTTP - polecenia wydaje się POST\u0026rsquo;ami, a dane pobiera GET\u0026rsquo;ami. Dzięki takiej budowie łatwo można schować CouchDB za reverse proxy i serwować np. pliki uploadowane przez użytkowników wprost z bazy - bez dorabiania dodatkowych interfejsów. Bardzo łatwo też obsługuje się bazę z poziomu aplikacji AJAX. Więc w moim przypadku wypadło na CouchDB.\nInstalacja Instalacja na Debianie jest banalna i sprowadza się do:\napt-get install -y couchdb TADAM! Mamy działające CouchDB. Domyślnie baza nasłuchuje na adresie 127.0.0.1 i porcie 5984.\nDodatkowo Couch posiada webowy interfejs (zwany Futon\u0026rsquo;em) zarządzający dostępny na tym samym porcie pod adresem, np. http://127.0.0.1:5984/_utils/\nOptymalizacja NODELAY W moim przypadku CouchDB służy do przechowywania zarówno bardzo wielu małych plików, jak i kilku całkiem sporych. W przypadku bardzo małych plików CouchDB w wersji 0.11 czeka z wysłaniem odpowiedzi od razu bo być może uda się \u0026ldquo;dorzucić coś jeszcze\u0026rdquo;. Takie zachowanie może powodować opóźnienia przy pobieraniu małych plików, warto więc zmienić w pliku /etc/couchdb/local.ini taką opcję:\n[httpd] socket_options = [{nodelay, true}] Ustawienie opcji TCP NODELAY wyłączy opóźnienie przy wysyłaniu małych plików.\nIdentyfikatory Warto zastanowić się nad długością i schematem identyfikatorów dla rekordów. Domyślnie mają one 32 bajty co przy małej liczbie elementów w bazie jest mocną przesadą. Rozmiar identyfikatorów znacznie wpływa na rozmiar bazy i jej wydajność - dlatego czasem warto opracować własny schemat generowanych identyfikatorów. Przykładowo jeśli identyfikator będzie generowany z cyfr oraz dużych i małych liter to dla 3 znaków możemy wygenerować identyfikatory dla ponad 260 tys. elementów, dla 4 znaków już ponad 14 milionów co powinno wystarczyć dla średniej wielkości bazy.\nUstawienia bezpieczeństwa Trochę mnie zaskoczyło podejście do bezpieczeństwa w bazie CouchDB - domyślnie po instalacji baza działa w trybie Admin Party, czyli każda osoba która wejdzie do zarządzania bez logowania ma uprawnienia administratora 😄\nMnie ten stan rzeczy nie bardzo odpowiadał więc:\n odpalamy interfejs zarządzający - Futon i w prawym dolnym rogu szukamy tekstu: \u0026ldquo;Welcome to Admin Party! Everyone is admin. Fix this\u0026rdquo; - klikamy na \u0026ldquo;Fix this\u0026rdquo;, w okienku które się pojawi podajemy login i hasło administratora.\n  Po ustawieniu konta administratora dalsze musimy zadbać o ustawienie odpowiednich uprawnień dla każdej nowo tworzonej bazy, czyli bazy tworzą się dostępem dla wszystkich ale możemy ograniczyć np. zapis/odczyt dla pewnych grup użytkowników.\nZabezpieczenie bazy użytkowników Po ustawieniu pierwszego użytkownika kolejna rzecz, o którą powinniśmy zadbać do dostępna dla każdego do odczytu baza użytkowników. Najlepiej gdy tylko administratorzy będą mieli do niej dostęp. By to osiągnąć:\n w Futonie wchodzimy do bazy _users i klikamy przycisk \u0026ldquo;Security\u0026rdquo; na górze strony, w okienku które się pojawi w polu Readers -\u0026gt; Roles (pole na samym dole) wpisujemy [\u0026quot;admin\u0026quot;]  Od teraz tylko administratorzy mają dostęp do bazy _users.\nUstawienie bazy jako tylko do odczytu To co chciałem osiągnąć korzystając z CouchDB to jakieś repozytorium, do którego wrzucać mogą wybrańcy a czytać wszyscy (np. taki CDN dla stronki itp) ale otrzymanie takiego rezultatu jest nieco\u0026hellip; nieintuicyjne. By to osiągnąć:\n w bazie którą chcemy ustawić jako tylko do odczytu wchodzimy w Security i w polu Admin Roles (drugie z góry) wpisujemy [\u0026quot;admin\u0026quot;] - to zablokuje dostęp do panelu Security i możliwości modyfikowania design dokumentów, nadal możliwe jest jednak dodawanie, modyfikowani i kasowanie dokumentów,\n by zablokować dostęp do zapisu w CouchDB trzeba wykorzystać funkcję validate_doc_update która będzie wywoływana przy każdej próbie dostępu do pojedynczego dokumentu, by z niej skorzystać tworzymy nowy pusty dokument, zmieniamy pole _id dokumentu na _design/auth dodajemy pole nazwane language z wartością javascript dodajemy kolejne pole nazwane validate_doc_update z wartością:  function(newDoc, oldDoc, userCtx) { if (userCtx.roles.indexOf(\u0026#39;_admin\u0026#39;) !== -1) { return; } else { throw({forbidden: \u0026#39;Nie jesteś administratorem!\u0026#39;}); } }  zapisujemy dokument klikając na \u0026ldquo;Save Document\u0026rdquo;\n  ","href":"/2012/06/couchdb-instalacja-i-wstepna-konfiguracja/","title":"CouchDB - Instalacja i wstępna konfiguracja"},{"content":"Używam LVM\u0026rsquo;a zarówno na desktopie jak i wielu serwerach bo bardzo podoba mi się możliwość powiększenia akurat tej partycji, na której brakuje miejsca. O ile pamiętam jak powiększyć partycję XFS (xfs_growfs /punkt/montowania) to zawsze mam problem jak to zrobić na EXT3/4, więc notuję.\nPowiększenie wolumenu LVM (np. o 10 gigabajtów):\nlvextend -L+10G /dev/vggroup/vol Zwiększenie rozmiaru systemu plików do nowego rozmiaru wolumenu:\nresize2fs /dev/vggroup/vol Powyższe polecenie można wykonać na zamontowanym zasobie - online.\n","href":"/2012/06/dynamiczna-zmiana-rozmiaru-partycji-ext4-na-lvmie/","title":"Dynamiczna zmiana rozmiaru partycji EXT4 na LVM’ie"},{"content":"W tym poście nie rozpiszę się zbytnio - wrzucam tylko config od którego zaczynam konfigurację nginx\u0026rsquo;a.\nuser www-data; worker_processes 4; pid /var/run/nginx.pid; events { worker_connections 1024; ## zaakceptuj tak dużo połączeń jak to możliwe  multi_accept on; ## epoll jest preferowany na jajkach od 2.6  ## http://www.kegel.com/c10k.html#nb.epoll  use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## opcje TCP  sendfile on; tcp_nopush on; tcp_nodelay on; ## maksymalny rozmiar zapytnia  client_max_body_size 10m; ## timeout\u0026#39;y  client_body_timeout 60; client_header_timeout 60; keepalive_timeout 10; send_timeout 60; ## kompresja  gzip on; gzip_static on; gzip_vary on; gzip_disable \u0026#34;msie6\u0026#34;; gzip_comp_level 1; gzip_proxied any; gzip_buffers 16 8k; gzip_min_length 50; gzip_types text/plain text/css application/json application/x-javascript application/javascript text/javascript application/atom+xml application/xml application/xml+rss text/xml image/x-icon text/x-js application/xhtml+xml; ## bezpieczeństwo  ## security by obscurity - ukrywamy wersję nginx\u0026#39;a  server_tokens off; ignore_invalid_headers on; ## resetuj zbyt długie połączenia - powinno pomóc na slowlorisa  reset_timedout_connection on; ## włączenie ochrony przed clickjackingiem - uruchamiam to per vhost  ## https://developer.mozilla.org/en/The_X-FRAME-OPTIONS_response_header  #add_header X-Frame-Options SAMEORIGIN;  ## potrzebne zeby zbudowac mape ponizej  map_hash_bucket_size 256; ## blacklist\u0026#39;a botów i referer\u0026#39;ów  include blacklist.conf; ##  # jeśli połączenie na HTTPS\u0026#39;a to ustawiamy zmienną by mogło być obsłużone  ##  map $scheme $server_https { default off; https on; } include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } No i jeszcze zawartość blacklist.conf - na początek wystarczy a można rozszerzyć pod siebie:\n## blokowanie agentów map $http_user_agent $bad_bot { default 0; libwww-perl 1; ~(?i)(httrack|htmlparser|libwww) 1; } ## blokowanie refererów map $http_referer $bad_referer { default 0; ~(?i)(babes|click|diamond|forsale|girl|jewelry|love|nudit|organic|poker|porn|poweroversoftware|sex|teen|webcam|zippo|casino|replica) 1; } ","href":"/2012/06/nginx-moj-domyslny-config/","title":"Nginx - mój domyślny config"},{"content":"Jeśli po aktualizacji firmware na swoim firewall\u0026rsquo;u do wersji MR3 natrafisz na komunikat Warning: SQL Logging is not enabled przy dostępie do logów to prawdopodobnie musisz zmienić źródło logów dla interfejsu gui. Poniżej polecenie CLI i możliwe opcje:\nconfig log gui set logdevice {memory | disk | fortianalyzer} end Ja potrzebowałem ustawić tę opcję na fortianalyzer by uzyskać dostęp do moich logów.\n","href":"/2012/04/fortigate-warning-sql-logging-is-not-enabled/","title":"Fortigate: Warning: SQL Logging is not enabled"},{"content":"Chciałem zaimportować mój certyfikat self-signed do Nokii E72 by nie krzyczała przy sprawdzaniu poczty. Potrzebowałem certyfikatu w formacie DER, a miałem w PEM - chwilę szukałem jak dokonać konwersji, więc ku pamięci zapisuję kilka gotowych poleceń:\nKonwersja certyfikatu z PEM na DER openssl x509 -in in.crt -inform PEM -out out.crt -outform DER Konwersja certyfikatu z DER na PEM openssl x509 -in in.crt -inform DER -out out.crt -outform DER Konwersja klucza z formatu PEM na DER openssl rsa -in in.crt -inform PEM -out out.crt -outform DER Konwersja klucza z formatu DER na PEM openssl rsa -in in.crt -inform DER -out out.crt -outform PEM Po konwersji certyfikat w formacie DER wystarczy wrzucić na kartę i otworzyć z menadżera plików, zainstalować.\n","href":"/2012/04/konwersja-formatu-certyfikatu-dla-telefonow-nokia/","title":"Konwersja formatu certyfikatu dla telefonów Nokia"},{"content":"Czasami odpalam klona jakiegoś systemu by później po drobnych zmianach uczynić go osobnym bytem. Jednym z kroków po odtworzeniu systemu jest wygenerowanie nowego zestawu kluczy dla serwera OpenSSH (by mój klient ssh nie siał warning\u0026rsquo;ami). Można to wykonać tak:\nNajpierw kasujemy obecne klucze:\nrm /etc/ssh/ssh_host_* Teraz generujemy nowe:\ndpkg-reconfigure openssh-server I na koniec restartujemy usługę by załadować nowy zestaw kluczy (nie powinno to zerwać obecnej sesji, ale dla pewności lepiej zadanie odpalić w screen\u0026rsquo;ie):\nservice ssh restart Jeżeli zmienialiśmy klucze dla obecnego hosta to konieczne może być usunięcie nieaktualnego wpisu z ~/.ssh/known_hosts.\n","href":"/2012/02/ponowne-wygenerowanie-kluczy-serwera-openssh/","title":"Ponowne wygenerowanie kluczy serwera OpenSSH"},{"content":"Aby wybrane systemy DomU startowały automatycznie po restarcie hypervisora należy podlinkować ich pliki konfiguracyjne w katalogu /etc/xen/auto po uprzednim jego utworzeniu. Przykładowo:\nmkdir /etc/xen/auto ln -s /etc/xen/example.cfg /etc/xen/auto/example.cfg Od teraz DomU example będzie startować automatycznie.\n","href":"/2012/02/xen-ustawienie-autostartu-domu/","title":"Xen - ustawienie autostartu DomU"},{"content":"Jeżeli zdecydowaliśmy się na systemu DomU w obrazach to możemy korzystać z live migration. By uruchomić jej obsługę, trzeba w pliku /etc/xen/xend-config.sxp odkomentować odpowiednie linie i ustawić adres IP:\n(xend-relocation-server yes) (xend-relocation-port 8002) (xend-relocation-address \u0026#39;10.0.10.91\u0026#39;) Wykonywanie migracji xm migrate --live nazwa-domu nazwa.lub.ip.zdalnego.hosta ","href":"/2012/02/xen-wlaczenie-live-migration/","title":"Xen - Włączenie Live Migration"},{"content":"Ostatnio pisałem o konfiguracji Dom0 - dzisiaj napiszę o uruchamianiu DomU.\nDo instalacji DomU wykorzystuję skrypty z pakietu xen-tools, można go zainstalować poleceniem:\napt-get install xen-tools Oczywiście aby wszystko działało fajnie musimy ustawić kilka domyślnych opcji, robimy to edytując plik /etc/xen-tools/xen-tools.conf. Lecimy po kolei:\n# Virtual machine disks are created as logical volumes in # volume group \u0026#39;universe\u0026#39; (hint: LVM storage is much faster # than file) lvm = universe Osobiście korzystam z LVM\u0026rsquo;a który zgodnie z hint\u0026rsquo;em jest znacznie szybszy od plików obrazów. Daje do tego bardzo wygodne możliwość w zarządzaniu rozmiarami wolumenów przydzielonych DomU, możliwość tworzenia snapshotów (np. na potrzeby backupu). Postawienie LVM\u0026rsquo;a w skrócie wygląda tak:\napt-get install lvm2 pvcreate /dev/sdb1 # to tylko przyklad - samodzielnie ustal partycje :) vgcreate universe /dev/sdb1 vgchange -a y universe Pozostałe opcje i bardziej skomplikowane przypadki tworzenia LVM\u0026rsquo;a można znaleźć tutaj: http://tldp.org/HOWTO/LVM-HOWTO/commontask.html\nUstalamy domyślne parametry dla tworzonych DomU - oczywiście zawsze możemy je nadpisać poprzez podanie innej wartości w linii poleceń, np. -memory 4Gb:\nsize = 8Gb # Disk image size. memory = 512Mb # Memory size swap = 1Gb # Swap size xfs_options = noatime,nodiratime Domyślnie nie ma konfiguracji dla systemu plików ext4, wystarczy dodać w odpowiednich miejscach:\nfs = ext4 ext4_options = noatime,nodiratime,errors=remount-ro Ustawienia sieci:\n# Default gateway and netmask for new VMs gateway = 10.0.10.1 netmask = 255.255.255.0 broadcast = 192.168.3.255 Warto ustawić domyślne ustawienia sieci - choć ja osobiście wolę zawsze podawać te parametry ręcznie.\n# When creating an image, interactively setup root password passwd = 1 Dzięki tej opcji na koniec tworzenia DomU zostaniemy poproszeni o podanie hasła dla root\u0026rsquo;a.\n# This is most useful on 64 bit host machines, for other systems it # doesn\u0026#39;t need to be used. # arch = amd64 Domyślna architektura dla tworzonych DomU - można przedefiniować z linii poleceń.\n# Uncomment if you wish newly created images to boot once they\u0026#39;ve been # created. # boot = 0 Ja nie lubię gdy DomU uruchamiają się automatycznie po przygotowaniu bo często ręcznie modyfikuję konfigurację i dopiero uruchamiam wirtualkę - stąd 0.\n# Let xen-create-image use pygrub, so that the grub from the VM is used, # which means you no longer need to store kernels outside the VMs. # Keeps things very flexible. pygrub=1 role = pygrub, myconfig Ta opcja powoduje że DomU będzie posiadać własny kernel, który może być aktualizowany niezależnie od kernela hypervisora. Włączenie tej opcji wymaga dodatkowej roli (pygrub) która zainstaluje wymagane pakiety, etc. Niestety roli tej nie ma w standardowej instalacji więc zamieszczam poniżej plik znaleziony w sieci. Edytujemy plik /etc/xen-tools/role.d/pygrub:\n#!/bin/sh # # Configure the new image to be suitable for booting via pygrub # # Wejn # -- # http://wejn.org/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Update APT lists. # chroot ${prefix} /usr/bin/apt-get update # # Install the packages # set -e installDebianPackage ${prefix} perl installDebianPackage ${prefix} libklibc installDebianPackage ${prefix} klibc-utils installDebianPackage ${prefix} initramfs-tools installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #chroot ${prefix} /usr/bin/dpkg -l | grep linux-image-xen-amd64 #if [ $? -ne 0 ]; then # installDebianPackage ${prefix} linux-image-2.6-xen-686 #else # installDebianPackage ${prefix} linux-image-2.6-xen-amd64 #fi # Force initrd if none exists echo ${prefix}/boot/initrd* | grep -q 2\\\\.6 if [ $? -ne 0 ]; then chroot ${prefix} update-initramfs -c -k `ls -1 ${prefix}/lib/modules/ | head -n 1` fi # Generate grub menu.lst LNZ=`basename \\`ls -1 ${prefix}/boot/vmlinuz*|tail -n 1\\`` RD=`basename \\`ls -1 ${prefix}/boot/initrd*|tail -n 1\\`` mkdir -p ${prefix}/boot/grub cat - \u0026lt; ${prefix}/boot/grub/menu.lst default 0 timeout 1 title Debian root (hd0,0) kernel /boot/$LNZ root=/dev/xvda2 ro initrd /boot/$RD EOF Jeżeli na DomU mamy zamiar główny system plików sformatować jako XFS to pygrub nie będzie potrafił się z niego zbootować - dobrze za to działa z ext3 i ext4.\nRola myconfig do wstępnej konfiguracji systemu DomU Rola myconfig to mój własny zestaw skryptów, który konfiguruje DomU w taki sposób jak lubię - bym nie musiał każdorazowo tracić czasu na ustawienie nowego systemu pod siebie. Zawartość poniżej:\n#!/bin/sh # # Configure DomU to my needs # # Tomasz Gagor # -- # https://gagor.pl/ # prefix=$1 # # Source our common functions - this will let us install a Debian package. # if [ -e /usr/lib/xen-tools/common.sh ]; then . /usr/lib/xen-tools/common.sh else echo \u0026#34;Installation problem\u0026#34; fi # # Moje ulubione zrodla w sources.list # echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://security.debian.org/ squeeze/updates main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list echo \u0026#34;deb-src http://packages.dotdeb.org stable all\u0026#34; \u0026gt; ${prefix}/etc/apt/sources.list # Update APT lists. chroot ${prefix} /usr/bin/apt-get update # # Zainstaluj ulubione paczki # set -e installDebianPackage ${prefix} ssh installDebianPackage ${prefix} vim installDebianPackage ${prefix} mc installDebianPackage ${prefix} bash-completion installDebianPackage ${prefix} ethtool installDebianPackage ${prefix} less installDebianPackage ${prefix} screen installDebianPackage ${prefix} postfix installDebianPackage ${prefix} apticron # # Usun paczki ktorych nie potrzebuje lub nie lubie # # PPP removeDebianPackage ${prefix} pppconfig removeDebianPackage ${prefix} pppoeconf removeDebianPackage ${prefix} pppoe removeDebianPackage ${prefix} ppp removeDebianPackage ${prefix} libpcap0.7 # edytory removeDebianPackage ${prefix} nano removeDebianPackage ${prefix} ed removeDebianPackage ${prefix} nvi # inne #removeDebianPackage ${prefix} tasksel tasksel-data #removeDebianPackage ${prefix} pciutils #removeDebianPackage ${prefix} fdutils #removeDebianPackage ${prefix} cpio # skypt automatyczne czyszczacy cache apta # zmniejsza rozmiar backupow echo \u0026#39;#!/bin/bash\u0026#39; \u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh echo \u0026#39;apt-get clean\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/cron.daily/apt-get-clean.sh chmod +x ${prefix}/etc/cron.daily/apt-get-clean.sh # ethtool - zwieksza wydajnosc wirtualizowanych urzadzen sieciowych echo \u0026#34;post-up ethtool -K eth0 tx off\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/network/interfaces # konfiguracja vima tak jak lubie chroot ${prefix} /usr/sbin/update-alternatives --set editor /usr/bin/vim.basic cat \u0026lt; ${prefix}/etc/vim/vimrc.local syntax on set background=dark if has(\u0026#34;autocmd\u0026#34;) filetype plugin indent on endif set showmatch VIMRC # konfiguracja bash-autocompletion cat \u0026lt;\u0026gt; ${prefix}/etc/bash.bashrc # enable bash completion in interactive shells if [ -f /etc/bash_completion ]; then . /etc/bash_completion fi BASHCOMP echo \u0026#39;source /etc/bash.bashrc\u0026#39; \u0026gt;\u0026gt; ${prefix}/etc/profile # instalacja i konfiguracja postfixa # poniżej wpisz własny alias dla hostmastera echo \u0026#34;root: hostmaster@mojadomena.pl\u0026#34; \u0026gt;\u0026gt; ${prefix}/etc/aliases chroot ${prefix} /usr/bin/newaliases chroot ${prefix} /usr/sbin/postconf -e \u0026#34;myhostname = `cat ${prefix}/etc/hostname`\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#34;mydestination = `cat ${prefix}/etc/hostname`, `cat ${prefix}/etc/hostname`.in.veracomp.pl, localhost.localdomain, localhost\u0026#34; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;relayhost = mail.example.pl\u0026#39; chroot ${prefix} /usr/sbin/postconf -e \u0026#39;myorigin = /etc/mailname\u0026#39; # dostosuj domenę pod siebie echo \u0026#34;`cat ${prefix}/etc/hostname`.internal.example.pl\u0026#34; \u0026gt; ${prefix}/etc/mailname # konfiguracja locales chroot ${prefix} /usr/sbin/locale-gen # konfiguracja timezone echo \u0026#39;Europe/Warsaw\u0026#39; \u0026gt; ${prefix}/etc/timezone chroot ${prefix} dpkg-reconfigure -f noninteractive tzdata # konfiguracja mc i authorized_keys ssh takie jak na dom0 cp -r /root/.mc/ ${prefix}/root/ cp -r /root/.ssh/ ${prefix}/root/ ","href":"/2012/02/xen-na-squeeze-instalowanie-i-konfiguracja-hostow-gosci-domu/","title":"Xen na Squeeze - Instalowanie i konfiguracja hostów gości (DomU)"},{"content":"Bardzo udany przepis - gdzieś musiałem go udokumentować by nie zginął :)\nSkładniki  1 duża kaczka (2-2,5 kg) 4 ząbki czosnku 1 kg jabłek (w zależności od kaczki mogą wystarczyć 3 większe sztuki) sok z cytryny do skropienia jabłek 200 g suszonej żurawiny pół butelki czerwonego wytrawnego wina rozmaryn 4 łyżki miodu cynamon kardamon sól pieprz  Sposób wykonania Kaczkę nacieramy solą, pieprzem, rozmarynem oraz roztartym czosnkiem i pozostawiamy na min. 3-4 godziny w lodówce (można też przygotować ją wieczorem by była gotowa na następny dzień).\nJabłka obieramy, usuwamy z nich nasiona i kroimy w średniej wielkości kostkę. Do jabłek dodajemy żurawinę, skrapiamy całość sokiem z cytryny, po czym dodajemy cynamon, kardamon i miód.\nPo wyjęciu z lodówki wypełniamy wnętrze kaczki wcześniej przygotowanym farszem i zaszywamy. Do rozgrzanego piekarnika (170-190 stopni) wstawiamy kaczkę umieszczoną w naczyniu pod przykryciem. Po około 1,5 godziny wyjmujemy kaczkę z piekarnika i z obu stron nacinamy skórę pomiędzy udami a korpusem - umożliwi to wypłynięcie zebranego pod skórą tłuszczu. Co 30 minut należy kaczkę podlewać powstałym sosem.\nCzas pieczenia kaczki zależy od jej wagi. Ważąca 1,5-2 kg sztukę należy piec od 90 minut do 2 godzin. Na godzinę przed końcem pieczenia wlewamy czerwone wino (ok. pół butelki). Na ostatnie 20 minut zdejmujemy pokrywę i przełączamy piekarnik na zapiekanie od góry by skórka mogła się zrumienić.\nWynik Całkiem puściło moje kiepskie szycie ale smaku to nie popsuło :)\nŹródło: Ugotowani\n","href":"/2012/02/kaczka-pieczona-z-zurawina-i-jablkami/","title":"Kaczka pieczona z żurawiną i jabłkami"},{"content":"Po co mi to? Wiele razy miałem do czynienia z serwerami na których działało kilka/kilkanaście usług równocześnie, np. Apache (kilka stronek, webmail, phpmyadmin, itp), Postfix/Exim (poczta i żeby było fajnie to na kontach systemowych), Samba (jakieś zasoby dla pracowników), MySQL (baza dla stronek), PostgreSQL (bo jedna stronka potrzebowała), itd\u0026hellip;. Przy takiej konfiguracji pomijane są kwestie separacji usług zewnętrznych/wewnętrznych - no ale firma/instytucja mała nie ma sensu kasy na 3 kolejne serwery wydawać skoro działa\u0026hellip;.\nTaka konfiguracja nie jest zbyt bezpieczna i ma wiele wad:\n ciężko backupować tak duży system w postaci obrazów, a odzyskiwanie z backupów całości będzie trwać wieki, słaba separacja usług ułatwia ataki na zasoby wewnętrzne (np. dziura w stronie może pozwolić na dostęp do plików z Samby), trudno rozgraniczyć zasoby sprzętowe (procesor, pamięć) konkretnym usługom, trudno jest migrować usługi na inne serwery.  Wirtualizacja pozwala na zminimalizowanie tych problemów:\n backupować można pojedyncze usługi (działające na różnych maszynach wirtualnych) np. z pomocą snapshotów z minimalnym czasem przerwy w działaniu, oddzielenie usług w osobnych maszynach wirtualnych pozwala \u0026ldquo;zamknąć\u0026rdquo; napastnika w przypadku kompromitacji którejś z usług (choć zdarzały się błędy w implementacji maszyn wirtualnych pozwalające \u0026ldquo;wyskoczyć\u0026rdquo; z wirtualki), możemy przydzielać pamięć i konkretne rdzenie procesora danym maszynom wirtualnym, live migration pozwala w locie przenieść maszynę wirtualną na inny serwer.  Xen jest szczególnie dobrym wyborem jeżeli zamierzamy wirtualizować systemy Linux\u0026rsquo;owe. Można je uruchamiać w trybie para-wirtualizacji, która w mniejszym stopniu niż pełna wirtualizacja obciąża CPU.\nW trybie pełnej wirtualizacji można zainstalować praktycznie dowolny system (również Windows) choć jest to tryb mniej wydajny.\nUwaga Post ten jest próbą zebrania w jednym miejscu różnych moich doświadczeń z Xen\u0026rsquo;em ale nie miałem czasu na testową instalację według poniższego tekstu, więc mogłem gdzieś coś zamieszać. Dlatego proszę o komentowanie jeśli coś nie zadziała.\nKonfiguracja hypervisor\u0026rsquo;a (Dom0) Jeżeli zastanawiasz się czy stawiać Xen\u0026rsquo;a na 32 czy 64-bitowym systemie to wybierz 64-bit. Na systemie 64-bitowym można uruchamiać systemy gości 32 i 64-bitowe (w przeciwieństwie do hypervisora 32-bitowego) i nie będzie problemów z obsługą dużej ilości pamięci.\nZaczynamy od instalacji hypervisor\u0026rsquo;a, zmodyfikowanej do działania z Xen wersji jądra i pakietu podstawowych narzędzi. Wszystko co potrzebne jest w jednym metapakiecie (xen-linux-system ale gdyby nie chciał iść to lepiej wskazać konkretną wersję):\napt-get install xen-linux-system-2.6-xen-amd64 By mieć dostęp do pełnej wirtualizacji (np. by uruchamiać systemy Windows) należy doinstalować jeszcze jedną paczkę (jeśli nie potrzebujemy to zawsze można doinstalować później):\napt-get install xen-qemu-dm-4.0 Squeeze domyślnie korzysta z GRUB\u0026rsquo;a 2 i niezbyt przeze mnie lubianej metody wykrywania systemów, która po zainstalowaniu jajka dla Xen\u0026rsquo;a nadal będzie domyślnie startować podstawowy kernel. By to zmienić proponuję przenieść wykrywanie domyślnego systemu na pozycję późniejszą niż Xen\u0026rsquo;a - zmieniamy nazwę pliku:\nmv /etc/grub.d/10_linux /etc/grub.d/21_linux Ponadto jeżeli zamierzamy instalować DomU na LVM\u0026rsquo;ie to na pewno zainteresuje nas wyłączenie opcji OS prober by DomU nie były wykrywane jako zainstalowane lokalnie systemy (wyłączy to też wykrywanie Windowsa jeśli jest zainstalowany na tej samej maszynie).\nBy wyłączyć OS prober\u0026rsquo;a otwórz plik /etc/default/grub i dodaj na końcu:\n# Wylaczenie OS prober\u0026#39;a by nie wykrywac maszyn wirtualnych na # logicznych wolumenach LVM i nie pokazywac ich w menu grub\u0026#39;a GRUB_DISABLE_OS_PROBER=true Jeżeli chcesz dodać jakieś dodatkowe parametry przy bootowaniu Xen\u0026rsquo;a dodaj poniższe zmienne w /etc/default/grub (przypisane wartości nie są prawidłowe i jeśli nic nie potrzebujesz dodawać to pomiń ten krok):\n# parametry dla wszystkich opcji startowych Xen\u0026#39;a (rowniez recovery) GRUB_CMDLINE_XEN=\u0026#34;something\u0026#34; # dodatkowe parametry dla opcji non-recovery GRUB_CMDLINE_XEN_DEFAULT=\u0026#34;something else\u0026#34; Po modyfikacji opcji gruba musimy wykonać aktualizację poleceniem:\nupdate-grub Domyślnym zachowaniem dom0 Xen\u0026rsquo;a przy wyłączaniu bądź restartowaniu jest próba zahibernowania wszystkich działających domU. Gdy nie przewidzimy takiej sytuacji i nie mamy wystarczającej ilości wolnego miejsca w /var często proces ten kończy się błędami. Zdarzają się też problemy z prawidłową obsługą hibernacji po stronie systemów działających w domU, wtedy po uruchomieniu dom0 i próbie załadowania zapisanych stanów maszyn kończymy z wiszącymi wirtualkami.\nNa dobrą sprawę równie dobrym zachowaniem byłoby wyłączenie wszystkich domU i zaczekanie aż to nastąpi a po starcie dom0 uruchomienie ich od zera. Może cały proces będzie trwać chwilę dłużej ale mamy za to większą pewność że przejdzie bez niespodzianek. By ustawić takie zachowanie Xen\u0026rsquo;a edytujemy /etc/default/xendomains:\nXENDOMAINS_RESTORE=false XENDOMAINS_SAVE= Aby systemy domU miały dostęp do sieci należy w pliku /etc/xen/xend-config.sxp upewnić się że network-script ustawione jest na network-bridge.\n(network-script \u0026#39;network-bridge antispoof=yes\u0026#39;) Przy takim ustawieniu systemu domU będą korzystać z głównego interfejsu dom0 aby uzyskać dostęp do sieci. Dla wielu będzie to niezbyt optymalne rozwiązanie ale na początek wystarczy (niecierpliwi mogą zerknąć do dokumentacji Xen\u0026rsquo;a aby sprawdzić inne opcje XenNetworking).\nDodatkowa opcja antispoof=yes aktywuje firewall, który uniemożliwi systemom domU przypisywanie sobie adresów innych systemów domU. By to działało w konfiguracji maszyny wirtualnej w definicji interfejsu (vif) należy przypisać adres IP danemu systemowi domU.\nNa moim systemie nie wszystkie skrypty dla konfiguracji sieci miały atrybut do wykonywania co skutkowało błędami przy uruchamianiu domU (np. \u0026ldquo;missing vif-script, or network-script, in the Xen configuration file\u0026rdquo;). By temu zapobiec ustawmy wszystkim skryptom odpowiednie uprawnienia:\nchmod +x /etc/xen/scripts/* W pliku /etc/xen/xend-config.sxp można ponadto ustawić jak dużo pamięci i procesorów rezerwujemy dla systemu dom0. Ilość pamięci dla dom0 można ograniczyć dodając opcję dom0_mem dla kernela w zmiennej GRUB_CMDLINE_XEN w konfiguracji gruba.\nNiektórzy zalecają wyłączenie dom0-balloning (czyli zajmowania przez dom0 całej pozostałej pamięci po uruchomieniu systemów domU), a zamiast tego przydzielenie minimalnej ilości pamięci, np. tak:\n(dom0-min-mem 1024) (enable-dom0-ballooning no) Ja osobiście wolę pozostawić balloning włączony - po co blokować ram skoro nikt inny z niego nie skorzysta, a tak to przynajmniej jest więcej na bufory dyskowe 😉\nUstawienie konsoli By mieć dostęp do wyjścia z GRUB\u0026rsquo;a, Xen hypervisor\u0026rsquo;a, kernela i getty poprzez VGA i konsolę należy dokonać zmian w pliku /etc/default/grub jak poniżej:\nGRUB_SERIAL_COMMAND=\u0026#34;serial --unit=0 --speed=9600 --word=8 --parity=no --stop=1\u0026#34; GRUB_TERMINAL=\u0026#34;console serial\u0026#34; GRUB_TIMEOUT=5 GRUB_CMDLINE_XEN=\u0026#34;com1=9600,8n1 console=com1,vga\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;console=tty0 console=hvc0\u0026#34; A następnie w /etc/inittab aktualizujemy linijki:\n1:2345:respawn:/sbin/getty 38400 hvc0 2:23:respawn:/sbin/getty 38400 tty1 # NO getty on ttyS0! W ten sposób tty1 pokazuje wyjście VGA, a hvc0 kosolę. Standardowo po modyfikacji opcji gruba musimy wykonać aktualizację poleceniem:\nupdate-grub Jeśli nic nie skopaliśmy to po restarcie powinniśmy otrzymać hypervisora gotowego do uruchamiania DomU.\nŹródła  http://wiki.debian.org/Xen  ","href":"/2012/02/xen-na-squeeze-instalacja-i-konfiguracja-hypervisora/","title":"Xen na Squeeze - instalacja i konfiguracja hypervisor’a"},{"content":"Jakiś czas temu korzystałem z preload\u0026rsquo;a który sam uczył się jakie aplikacje odpalam i te programy ładował już podczas startu - przeważnie nieco spowalnia to start systemu ale gdy już się załaduje to programy, które uruchamiam jako pierwsze startują \u0026ldquo;z kopa\u0026rdquo;. Od jakiegoś czasu popularniejszy jest instalowany domyślnie w Ubuntu ureadahead - pełni on podobną funkcję jak preload.\nMożna zmusić ureadahead do ponownego wygenerowania nowej listy programów wczytywanych przy starcie do cache a oto jak zrobić:\n Należy skasować pliki z rozszerzeniem pack w /var/lib/ureadahead/:  sudo rm /var/lib/ureadahead/*.pack Można ustawić autmatyczne logowanie. Restartujemy system. Szybko logujemy się do systemu i uruchomiamy aplikacje, które chcemy aby szybciej startowały. Gdy wszystko się już załaduje sprawdzamy czy załadowały się wszystkie programy, na których nam zależało, np.:  sudo ureadahead --dump | grep firefox 6.Jeżeli nie załadowały się wszystkie programy to w pliku /etc/init/ureadahead.conf zwiększamy wartość w linii:\npre-stop exec sleep 45 na np. 90 i wracamy do punktu 1.\n","href":"/2012/01/wstepne-ladowanie-programow-przy-starcie-z-ureadahead/","title":"Wstępne ładowanie programów przy starcie z ureadahead"},{"content":"Aby umożliwić odwiedzającym nasze strony cachowanie obrazków (tak by nie musieli pobierać ich każdorazowo bo przecież nie zmieniają się aż tak często) konieczne jest ustawienie nagłówków: Cache-Control, Expires dla odpowiednich typów plików. W Apachem jest do tego dedykowany moduł - mod_expires. W Debianie dostarczany jest on bez domyślnej globalnej konfiguracji - a ja lubię gdy cacheuje mi się większość statyki. Zawsze można dostosować czas cachowania pod siebie względem określonego typu pliku, np. dla Java Scriptów ustawić na 1 dzień gdy często się zmieniają. Można też w samej aplikacji zmieniać ścieżkę do pliku by wymusić odświeżenie (lub podawać ścieżkę z jakimś losowym identyfikatorem wycinanym przez mod_rewrite).\nCache-Control, Expires Najpierw tworzymy globalny config (oczywiście można sobie robić też taki per host - jak kto woli):\ncat \u0026gt; /etc/apache2/mods-available/expires.conf \u0026lt; \u0026lt;CONF \u0026lt;IfModule mod_expires.c\u0026gt; # włączamy moduł ExpiresActive on # cacheowanie typowych obrazów przez 3 miesiące ExpiresByType image/jpg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/gif \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/jpeg \u0026#34;access plus 3 months\u0026#34; ExpiresByType image/png \u0026#34;access plus 3 months\u0026#34; # cacheowanie CSS i JavaScript przez 2-1 miesiąca ExpiresByType text/css \u0026#34;access plus 2 month\u0026#34; ExpiresByType text/javascript \u0026#34;access plus 1 day\u0026#34; ExpiresByType application/javascript \u0026#34;access plus 1 day\u0026#34; # domyślna wartość cachowania ExpiresDefault \u0026#34;access plus 3 months\u0026#34; CONF Dokładniejsze wyjaśnienie składni można znaleźć tutaj. Teraz aktywujemy moduł i restartujemy Apache\u0026rsquo;go (nie jestem pewien czy sam reload by wystarczył):\na2enmod expires service apache2 restart I tyle - jeśli sprawdzimy nagłówki (Firebug w Firefoxie albo Narzędzia programistyczne w Chromie) dla odpowiednich typów plików to powinny one zawierać przykładowo takie wartości:\nCache-Control: max-age=7776000 Expires: Thu, 19 Apr 2012 15:48:13 GMT FileETag Ustawiając powyższą konfigurację warto wyłączyć generowanie nagłówka ETag. Co prawda w dawnym zamyśle miał to być mechanizm pozwalający zbliżony do cachowania a dający 100% pewności że nowy plik zostanie pobrany a aktualne w kopie w cache już nie. Polega na obliczeniu hasha na podstawie \u0026ldquo;pewnych\u0026rdquo; danych o pliku (np. INode, data modyfikacji, rozmiar) - jeżeli posiadasz aktualny hash to nie musisz pobierać danych.\nPomysł na początku nie był zły\u0026hellip; ale obecne strony często udostępniają nawet kilkadziesiąt (i więcej) niedużych plików. By pobrać potrzebne do wygenerowania ETag\u0026rsquo;a dane trzeba wywołać operację stat na pliku co przy wielu równoczesnych odwołaniach potrafi mocno obciążyć dyski.\nNa serwerach ze średnim i dużym obciążeniem warto wyłączyć generowanie nagłówka w ten sposób:\necho \u0026#34;FileETag None\u0026#34; \u0026gt;\u0026gt; /etc/apache2/httpd.conf service apache2 reload ","href":"/2012/01/apache-mod_expires-konfiguracja/","title":"Apache mod_expires konfiguracja"},{"content":"Objaw przeważnie jest taki: łączysz się po ssh podając klucz/hasło i czekasz nawet i 10 sekund aż pojawi się prompt. Po połączeniu wszystkie polecenia działają z normalną szybkością. Brzmi znajomo? 😉\nTaki objaw przeważnie jest skutkiem problemów z działaniem DNS\u0026rsquo;a po stronie klienta lub serwera. Warto sprawdzić poleceniami host/dig/nslookup po obu stronach jak dużo czasu potrzeba na rozwiązanie nazw. Najlepiej rozwiązać problem z DNS\u0026rsquo;em ustawiając szybkie serwery ale gdy nie mamy takiej możliwości to po stronie serwera można ustawić w /etc/sshd_config opcję:\nUseDNS no I restart ssh:\nservice ssh restart Spowoduje to wyłączenie znacznej części zapytań DNS po stronie serwera (w tym sprawdzanie reverse DNS\u0026rsquo;a dla hosta klienta w momencie łączenia). Na kilku serwerach z kiepskim DNS\u0026rsquo;em opcja ta \u0026ldquo;daje niezłego kopa\u0026rdquo;.\n","href":"/2012/01/dlugie-oczekiwanie-na-nawiazanie-polaczenia-ssh/","title":"Długie oczekiwanie na nawiązanie połączenia ssh"},{"content":"Zawsze gdy potrzebuję zesniffować coś na żywo na Fortigate\u0026rsquo;ach muszę przeszukać Knowledge Base by przypomnieć sobie wszystkie polecenia do tego potrzebne. Tym razem robię notatki 😃\nSniffowanie diagnose debug enable diagnose debug flow filter addr ip address clear clear filter daddr dest ip address dport destination port negate inverse filter port port proto protocol number saddr source ip address sport source port vd index of virtual domain # np. diagnose debug flow filter saddr 10.10.80.3 diagnose debug flow filter daddr 8.8.8.8 diagnose debug flow filter dport 53 # wyświetl wyniki na konsoli diagnose debug flow show console enable # opcjonalne: wyświetla nazwy funkcji np. odwołania do routingu, itp diagnose debug flow show function-name enable # uruchomienie sniffowania - warto podać na końcu jakaś wartość # by sniffowanie zakończyło się po takiej liczbie pakietów # w przeciwnym wypadku wyniki będą się wypisywać na konsoli # aż uda nam się na oślep wyłączyć sniffowanie diagnose debug flow trace start 100 # zresetowanie filtrowania flow diagnose debug reset # wyłączenie sniffowania diagnose debug disable Diagnostyka tuneli IP-Sec # tutaj jest dużo prościej, najpierw włączamy debuga diagnose debug enable # a potem diagnose debug application ike 2 # lub dla bardzo, bardzo szczegółowych logów diagnose debug application ike -1 Niestety nie ma tutaj możliwości filtrowania (albo jeszcze o tym nie wiem), więc jeśli mamy dużo aktywnych tuneli to najlepiej zbierać wypisywane komunikaty do pliku i dopiero przeglądać.\n","href":"/2012/01/sniffowanie-w-fortios/","title":"Sniffowanie w FortiOS"},{"content":"Co jakiś czas powtarza się sytuacja, gdy muszę zaktualizować jakiś serwerek z Lennym do Squeeze\u0026rsquo;a i za każdym razem muszę googlać za odpowiednimi źródłami, które paczki najpierw, etc\u0026hellip; Więc sobie zebrałem wszystko w poniższym poście.\nW razie wątpliwości patrz tutaj: http://www.debian.org/releases/squeeze/releasenotes\n  Zrób backup konfiguracji.\n  Trzeba zaktualizować źródła by wskazywały na squeeze\u0026rsquo;a (poniższe polecenie nadpisze Twoje obecne repozytoria):\n  cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;SRC deb http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze main non-free contrib deb http://security.debian.org/ squeeze/updates main contrib non-free deb-src http://security.debian.org/ squeeze/updates main contrib non-free deb http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb-src http://ftp.pl.debian.org/debian/ squeeze-updates main non-free contrib deb http://backports.debian.org/debian-backports squeeze-backports main contrib non-free SRC Teraz trzeba odświeżyć repozytoria:\nsudo apt-get update Proponuję pobrać też pliki by podczas aktualizacji wszystkie leżały w cache\u0026rsquo;u - na wypadek gdyby nagle padło łącze itp\u0026hellip;\nsudo apt-get dist-upgrade -d Teraz można zaktualizować kluczowe paczki:\nsudo apt-get install apt dpkg I aktualizacja całego systemu:\nsudo apt-get dist-upgrade I można brać się do łatania bugów w starej konfiguracji\u0026hellip; 😉\n","href":"/2012/01/upgrade-debian-lenny-do-squeeze/","title":"Upgrade Debian Lenny do Squeeze"},{"content":"Jeżeli jesteś właścicielem Skody Fabii I to wcześniej czy później Twój BAT-Mobil zgłosi któreś z ostrzeżeń serwisowych:\n OIL - pojawia się przeważnie co 10 tys. km i wtedy gdy jeszcze nie trzeba wymieniać oleju w silniku 😄 service INSP - nie wiem jak często się pojawia a oznacza mniej więcej \u0026ldquo;czas wesprzeć finansowo lokalny autoryzowany serwis\u0026rdquo;.  Rozwiązanie znalazłem tutaj - cytuję by mi nie zginęło ;-)\n Kasowanie ostrzeżenia OIL:\n Przy wyłączonym silniku wciskamy i przytrzymujemy przycisk kasowania przebiegu dziennego/pokrętło ustawiania godziny, od teraz nazywany po prostu \u0026lsquo;przyciskiem\u0026rsquo;. Cały czas przytrzymując \u0026lsquo;przycisk\u0026rsquo;, włączamy zapłon. Powinniśmy zobaczyć napis OIL, puszczamy \u0026lsquo;przycisk\u0026rsquo;. Przekręcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, aż zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Możemy wyłączyć zapłon.  Kasowanie ostrzeżenia service INSP:\n Wykonujemy czynności z punktów 1-3 podanych powyżej. Wciskamy \u0026lsquo;przycisk\u0026rsquo;, powinniśmy zobaczyć napis INSP, puszczamy przycisk. Przekręcamy \u0026lsquo;przycisk\u0026rsquo; w prawo, aż zobaczymy \u0026lsquo;- - - -\u0026rsquo;. Możemy wyłączyć zapłon.  U mnie zadziałało, mam nadzieję, że się komuś przyda.\n ","href":"/2012/01/skoda-fabia-kasownie-ostrzezen-oil-i-service-insp/","title":"Skoda Fabia - Kasownie ostrzeżeń OIL i service INSP"},{"content":"Uruchamiamy SciTE i klikamy menu Options -\u0026gt; Open User Options File, wpisujemy dane:\n# domyślne korzystanie z fontów o stałej szerokości font.base=$(font.monospace) font.small=$(font.monospace) font.comment=$(font.monospace) font.text=$(font.monospace) font.text.comment=$(font.monospace) font.embedded.base=$(font.monospace) font.embedded.comment=$(font.monospace) font.vbs=$(font.monospace) # numerowanie wierszy line.margin.visible=1 line.margin.width=3+ # ikonki toolbara z tematu systemowego toolbar.usestockicons=1 # zaznaczanie blokowe rectangular.selection.modifier=8 # ustawienia wgłębień kodu indent.size=4 use.tabs=1 indent.automatic=1 Więcej opcji tutaj: http://www.scintilla.org/SciTEDoc.html\nSam pewnie jeszcze nie raz zaktualizuję ten wpis 😄\n","href":"/2012/01/moj-domyslny-config-dla-scite/","title":"Mój domyślny config dla SciTE"},{"content":"","href":"/tags/scite/","title":"SciTE"},{"content":"","href":"/tags/empathy/","title":"Empathy"},{"content":"W Gajim\u0026rsquo;ie od dawna brakowało mi wygodnego przeszukiwania po liście kontaktów (takiego jakie ma się pojawić już niebawem w wersji 0.15 - z półtora roku już na to czekam\u0026hellip;). W między czasie znalazłem chwilę by pobawić się Empathy - brzydki, nie ma przeglądarki usług XMPP, ale wyszukiwanie na rosterze jest dokładnie takie jakiego szukałem (w miarę wpisywania znaków zawęża listę kontaktów by pasowały do wpisywanego wzorca).\nTyle że skróty klawiaturowe w tym programie zwyczajnie mnie rozwalają - przez lata przyzwyczaiłem się że okienka czatu można zamknąć Escape\u0026rsquo;m - a tutaj nawet nie ma opcji, która pozwalała by na taką zmianę zachowania.\nRozwiązaniem jest edycja z root\u0026rsquo;a pliku /usr/share/empathy/empathy-chat-window.ui i zamiana linii:\n\u0026lt;accelerator key=\u0026#34;W\u0026#34; modifiers=\u0026lt;wbr\u0026gt;\u0026#34;GDK_CONTROL_\u0026lt;/wbr\u0026gt;\u0026lt;wbr\u0026gt;MASK\u0026#34;/\u0026gt;\u0026lt;/wbr\u0026gt; na:\n\u0026lt;accelerator key=\u0026#34;Escape\u0026#34; /\u0026gt; Ta da! Restart komunikatora i da się z nim żyć.\nŹródło: https://bugs.launchpad.net/ubuntu/+source/empathy/+bug/486508\n","href":"/2011/12/empathy-zamykanie-okienka-chatu-przyciskiem-escape/","title":"Empathy - zamykanie okienka chatu przyciskiem Escape"},{"content":"Można znaleźć wiele tutoriali jakimi narzędziami wykonywać backupy. W większości przypadków absolutnie wystarczający okaże się flexbackup. Bardziej wymagający wykorzystają BackupPC, Baculę lub Amandę. Narzędzia te pozwalają wykonać kopie pełne, różnicowe, przyrostowe - kompresując je dla zaoszczędzenia miejsca.\nWszystko fajnie - ale problemy pojawiają się przy dostępie do tych danych. Żeby odzyskać plik zmodyfikowany dzisiaj trzeba rozpakować najpierw kopię pełną, potem różnicową, przyrostową by wreszcie wyciągnąć plik z wczoraj\u0026hellip; hmm ten też jest skopany. No to lecimy jeszcze raz\u0026hellip;\nDodatkowo jeżeli danych jest kilkadziesiąt GB to cały proces wielokrotnej dekompresji może trwać nawet kilka godzin. Zacząłem szukać alternatywnej metody backupowania i trafiłem na ciekawy artykuł na podstawie, którego opracowałem własny skrypt do backupu: http://www.mikerubel.org/computers/rsync_snapshots/\n#!/bin/bash  # zdalny serwer z którego chcemy wykonać backup RHOST=\u0026#34;server.test.pl\u0026#34; # zasoby rsync ze zdalnego serwera które zostaną zsynchronizowane BACKUPDIRS=(\u0026#34;backup\u0026#34; \u0026#34;home\u0026#34; \u0026#34;mails\u0026#34;) # lokalny katalog do którego trafi backup DSTDIR=\u0026#34;/srv/backup/server\u0026#34; # maksymalny czas przechowywania backupów wyrażony w dniach MAXAGE=33 # dodatkowe opcje dla rsynca (--verbose można zastąpić --quiet # łatwiej zauważyć wtedy błędy), można włączyć kompresje # w przypadku synchronizacji ze zdalnej lokalizacji OPTIONS=\u0026#34;--del --verbose\u0026#34; # plik z hasłem dla rsync\u0026#39;a - zabezpieczenie marne ale w izolowanej # sieci dopuszczalne PASS=\u0026#34;/root/.rsync-passwd\u0026#34; DATE=`date +\u0026#39;%Y.%m.%d\u0026#39;` YESTERDAY=`ls -1 $DSTDIR | sort | tail -n1` LINKDEST=$DSTDIR/$YESTERDAY # prosty mechanizm lock\u0026#39;a by uniemożliwić wielokrotne uruchomienie # skryptu, np. w sytuacji gdy nie zdąży wykonać się pełny backup # przed kolejnym wywołaniem if [ -f \u0026#34;/tmp/server_sync\u0026#34; -o -f \u0026#34;/tmp/server_sync_block\u0026#34; ]; then echo \u0026#34;Another sync is still running!\u0026#34; exit 1 fi touch /tmp/server_sync # wykonujemy kolejno kopie for DIR in ${BACKUPDIRS[@]}; do mkdir -p $DSTDIR/$DATE/$DIR # sprawdzenie czy mamy wcześniejszy backup, # jak mamy to robimy snapshot if [ -d \u0026#34;$DSTDIR/$YESTERDAY/$DIR\u0026#34; ]; then rsync -a --link-dest=$LINKDEST/$DIR \\  backup@$RHOST::archive/$DIR/ \\  $DSTDIR/$DATE/$DIR/ \\  --password-file=$PASS $OPTIONS else # jak nie mamy to robimy nowy rsync -a backup@$RHOST::archive/$DIR/ \\  $DSTDIR/$DATE/$DIR/ \\  --password-file=$PASS $OPTIONS fi # sprawdzenie czy synchronizacja się udała # jeśli się nie udała to możemy chcieć skasować niedokończony # backup by kolejny nie musiał być \u0026#34;prawie pełnym\u0026#34; # plik /tmp/server_sync_block trzeba skasować ręcznie if [ $? -ne 0 -a $? -ne 24 ]; then echo \u0026#34;Something was wrong becase rsync return $?\u0026#34; touch /tmp/server_sync_block exit 2 fi done # zwalnianie lock\u0026#39;a rm -f /tmp/server_sync # kasowanie najstarszych backupów find $DSTDIR -maxdepth 1 -type d -mtime +$MAXAGE \\  -print0 | xargs -0 -r rm -r ","href":"/2011/12/automatyczne-backupy-w-stylu-snapshot-z-rsynciem/","title":"Automatyczne backupy w stylu snapshot z rsync’iem"},{"content":"Zdarzyło mi się bawić sprzętowymi Firewallami firmy Fortigate - chcąc sprawdzić działanie pewnych funkcji potrzebowałem uruchomić dwa/trzy pudełka na osobnych łączach. Pomysł polegał na próbie zmuszenia pudełek do współpracy z modemem iPlus na USB.\nDrugim fajnym zastosowaniem tego triku jest możliwość wykorzystania iPlusa jako \u0026ldquo;zapasowego łącza\u0026rdquo; w przypadku awarii głównego.\nDzięki pomocy inżyniera Fortigate szybko udało mi się zebrać potrzebne do działania parametry, które należy uruchomić poprzez command line (telnet/ssh).\nJeżeli chcemy uruchomić modem dla konkretnego vdom\u0026rsquo;u to należy go najpierw wybrać, np:\nconfig vdom edit root Jeśli nie korzystamy z vdom\u0026rsquo;ów to krok ten możemy pominąć. Dalej:\nconfig system modem set status enable set pin-init at+cpin=xxxx set auto-dial enable set idle-timer 6 set redial 2 set phone1 *99# set username1 ppp set passwd1 ppp set extra-init1 at+cgdcont=1,\\\u0026#34;IP\\\u0026#34;,\\\u0026#34;\u0026lt;a href=\u0026#34;http://www.plusgsm.pl%5C\u0026#34;\u0026gt;www.plusgsm.pl\\\u0026lt;/a\u0026gt;\u0026#34; end Przy czym w opcji pin-init zamiast xxxx należy podać PIN karty SIM.\nNa koniec potrzebny jest restart urządzenia:\nexecute reboot Po restarcie będziemy mieć do dyspozycji nowy interfejs (modem). Automatycznie zostanie ustawione dns proxy.\nTestowałem to na modemach Huawei E156G i Huawei E173 i różnych urządzeniach FortiGate z firmware w wersji MR2.\n","href":"/2011/12/konfiguracja-modemu-usb-iplus-na-urzadzeniach-fortigate/","title":"Konfiguracja modemu USB iPlus na urządzeniach FortiGate"},{"content":"Wcześniej czy później zawsze pojawia się potrzeba zoptymalizowania naszej bazy MySQL. Przedstawię kilka zmian w konfiguracji, które powinny zwiększyć wydajność w większości przypadków.\nMyISAM - key_buffer_size Najprostszą optymalizacją baz/tabel z mechanizmem MyISAM jest odpowiednie dobranie bufora na cache dla kluczy i indeksów (dane nigdy nie są cachowane). Poniższe zapytanie pozwala oszacować zalecany rozmiar cache\u0026rsquo;u:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.4999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_key_buffer_size FROM (SELECT LEAST(POWER(2,32),KBS1) KBS FROM (SELECT SUM(index_length) KBS1 FROM information_schema.tables WHERE engine=\u0026#39;MyISAM\u0026#39; AND table_schema NOT IN (\u0026#39;information_schema\u0026#39;,\u0026#39;mysql\u0026#39;)) AA ) A, (SELECT 2 PowerOf1024) B; Wynik określa zalecany rozmiar bufora (parametr key_buffer_size w pliku /etc/mysql/my.cnf) dla bieżącego stanu bazy - warto ciut dodać na zapas. Na systemach 32 bitowych parametr key_buffer_size może przyjmować maksymalnie 4GB, na 64 bitowych maksymalnie 8GB.\n[mysqld] key_buffer_size=xxxM InnoDB - innodb_buffer_pool_size W przypadku InnoDB cachowane mogą być zarówno dane jak i klucze/indeksy a rozmiar bufora określa parametr innodb_buffer_pool_size. Zalecaną minimalną wartość tego parametru możemy oszacować zapytaniem:\nSELECT CONCAT(ROUND(KBS/POWER(1024, IF(PowerOf1024\u0026lt;0,0,IF(PowerOf1024\u0026gt;3,0,PowerOf1024)))+0.49999), SUBSTR(\u0026#39; KMG\u0026#39;,IF(PowerOf1024\u0026lt;0,0, IF(PowerOf1024\u0026gt;3,0,PowerOf1024))+1,1)) recommended_innodb_buffer_pool_size FROM (SELECT SUM(data_length+index_length) KBS FROM information_schema.tables WHERE engine=\u0026#39;InnoDB\u0026#39;) A, (SELECT 2 PowerOf1024) B; W przypadku InnoDB po zmianie wartości innodb_buffer_pool_size konieczne jest również ustawienie innodb_log_file_size na 25% wartości innodb_buffer_pool_size lub 2047M (należy wybrać mniejszą z wartości). By wygenerować pliki log o nowych rozmiarach musimy postępować według poniższej instrukcji:\n Dopisujemy w pliku my.cnf parametry: [mysqld] innodb_buffer_pool_size=xxxM innodb_log_file_size=25% xxxM lub 2047M  Wyłączamy bazę: invoke-rc.d mysql stop  Kasujemy obecnie pliki log: rm /var/lib/mysql/ib_logfile[01]  Uruchamiamy bazę: invoke-rc.d mysql start  Po starcie bazy zostaną wygenerowane nowe pliki log o nowych rozmiarach.  InnoDB - kompaktowanie plików W domyślnej konfiguracji MySQL dla baz InnoDB (na Debianie na bank, przypuszczam że na innych distro jest podobnie) wszystkie tabele, indeksy, metadane tabel i inne dane dotyczące table InnoDB przechowywane są w jednym pliku: /var/lib/mysql/ibdata1\nNie jest to optymalne ustawienie szczególnie gdy mamy dużo baz i o znacznych rozmiarach - wykonywanie wielu równoczesnych operacji na jednym gigantycznym pliku potrafi mocno przymulić.\nPróba kompaktowania/optymalizowania tabel InnoDB nie powoduje zmniejszenia tego pliku - bo gdy próbujemy optymalizować tabele InnoDB powoduje to:\n ułożenie danych i indeksów wewnątrz pliku ibdata1 w sposób ciągły, wzrost rozmiaru pliku ibdata1 ponieważ powyższe dane dopisywane są na jego końcu.  Niewiele osób spodziewa się takiego rezultatu. Można zmniejszyć rozmiar tego pliku wyłączając dane tabel i ich indeksy do osobnych plików ale proces ten wymaga pełnego backupu bazy i jej odtworzenia, wiąże się więc z chwilowym (a w przypadku dużych baz - dłuższym) przestojem.\n Robimy pełny backup bazy, np. poleceniem:  mysqldump --all-databases --single-transaction -uroot -p \u0026gt; my-dump.sql (dump\u0026rsquo;a można dodatkowo skompresować gzipem dodając pipe\u0026rsquo;a - przyspieszy to odzyskiwanie przez zmniejszenie ilości danych potrzebnych do odczytania z dysku)\n Kasujemy wszystkie bazy z wyjątkiem schematu mysql (de facto powinno wystarczyć skasowanie i odzyskanie tylko baz korzystających z tabel InnoDB)\n  Wyłączamy usługę:\n  invoke-rc.d mysql stop Dodajemy w pliku /etc/mysql/my.cnf parametry:  [mysqld] innodb_file_per_table innodb_log_file_size=25% xG innodb_buffer_pool_size=xG  pierwsza opcja powoduje właściwe rozdzielenie tabel InnoDB do różnych plików (dodanie tej opcji na serwerze na którym znajdują się bazy InnoDB spowoduje ich uszkodzenie - odradzam), drugą i trzecią linijkę konfigurujemy według wcześniejszej instrukcji o innodb_buffer_pool_size, możemy też na czas przywracania danych dodać opcję bulk_insert_buffer_size=256M, skróci to czas potrzebny na przywrócenie bazy.  Kasujemy pliki: ibdata1, ib_logfile0 and ib_logfile1:  rm /var/lib/mysql/ibdata1 rm /var/lib/mysql/ib_logfile[01] Uruchamiamy serwer MySQL:  invoke-rc.d mysql start Przywracamy wszystkie bazy z dump\u0026rsquo;a:  cat my-dump.sql | mysql -uroot -p Plik ibdata1 urośnie ale od tej pory będzie zawierać wyłącznie metadane tabel a poszczególne tabele i indeksy będą przechowywane w osobnych plikach, np. tabela.ibd. Teraz optymalizowanie tabel InnoDB będzie powodować zmniejszenie rozmiarów plików *.ibd a plik ibdata1 nie będzie tak obciążony.\nZmiana metody flush\u0026rsquo;a W niektórych przypadkach ustawienie opcji innodb_flush_method na wartość O_DIRECT może poprawić wydajność, choć w innych wydajność może się pogorszyć (na forach sugerowano występowanie problemu na dedykowanych macierzach). Można bezpiecznie włączyć tę opcję i wykonać kilka bechmarków:\n[mysqld] innodb_flush_method=O_DIRECT Cache wyników zapytań Sprawdźmy najpierw czy cachowanie jest włączone (u mnie było to domyślne ustawienie), wydajemy zapytanie:\nSHOW VARIABLES LIKE \u0026#39;query_cache_type\u0026#39;; Możliwe są 3 ustawienia:\n ON (query_cache_type = 1) - cachowanie wszystkich zapytań, OFF (query_cache_type = 0) - cachowanie na żądanie, DEMAND (query_cache_type = 2) - cachowanie wyłączone.  W przypadku opcji DEMAND cachowanie jest włączane jeżeli po SELECT\u0026rsquo;cie dodamy SQL_CACHE. Mi osobiście najbardziej odpowiada opcja z cachowaniem wszystkiego.\nNastępnie należy ustawić w pliku my.cnf poniższe zmienne według potrzeb:\nquery_cache_limit = 2M query_cache_size = 32M Pierwsza opcja ustala maksymalny rozmiar pojedynczego zapytanie, które będzie cachowane - zapytania większe nie będą cachowane. Druga opcja ustala rozmiar całego bufora na cache (przeważnie więcej działa lepiej).\nMyISAM - unikanie repair with keycache Gdy nasza baza urośnie i będziemy mieć w niej tabele o rozmiarze przekraczającym 2GB to da się zauważyć że pewne operacja jak np. zakładanie indeksu, optymalizacja, naprawa - trwają cholernie długo. Można to szczególnie odczuć właśnie w momencie przekraczania rozmiaru 2GB i tak utworzenie indeksu na tabeli o rozmiarze 1,9GB trwa powiedzmy kilkanaście/kilkadziesiąt minut, a ta sama operacja na bazie o rozmiarze 2,1GB może zająć nawet kilka godzin.\nPrzyczynę łatwo namierzyć obserwując wynik polecenia:\nSHOW PROCESSLIST; w trakcie operacji na \u0026ldquo;małej\u0026rdquo; i \u0026ldquo;dużej\u0026rdquo; tabeli. \u0026ldquo;Mała\u0026rdquo; zatrzymuje się na dłużej na operacji Repair By Sorting, a \u0026ldquo;duża\u0026rdquo; kona godzinami na Repair With Keycache. Właśnie różnica w działaniu obu mechanizmów sortowania daje w kość:\n repair by sorting - wykorzystuje do sortowania wiele plików tymczasowych i wymaga sporo wolnego miejsca w katalogu ustawionym w opcji tmpdir (domyślnie ustawionej na /tmp) - jeżeli miejsca będzie za mało to wybierany będzie mechanizm \u0026ldquo;repair with keycache\u0026rdquo;, repair with keycache - wykorzystuje do sortowania bardzo mały bufor (u mnie 8MB), jest ok 10~20 krotnie wolniejszy niż \u0026ldquo;repair by sorting\u0026rdquo; a do tego tworzy mniej optymalne indeksy.  O tym który z mechanizmów zostanie wybrany decyduje opcja myisam_max_sort_file_size - zmienna ta ma domyślnie wartość 2GB i właśnie dlatego problemy pojawiają się po przekroczeniu tego rozmiaru. Proponuję ustawić ją sporo powyżej rozmiaru największych tablic - oczywiście jeśli miejsce w temp\u0026rsquo;ie pozwoli na to, np:\nmyisam_max_sort_file_size=8GB Przy takim ustawieniu warto mieć w /tmp minimum drugie tyle wolnego miejsca.\nŹródło http://dba.stackexchange.com/questions/3163/mysql-5-1-innodb-configuration-24gb-ram-bi-xeon-high-load\n","href":"/2011/12/mysql-proste-metody-optymalizacji/","title":"MySQL - Proste metody optymalizacji"},{"content":"","href":"/tags/rsync/","title":"rsync"},{"content":"","href":"/tags/snapshot/","title":"snapshot"},{"content":"","href":"/tags/dovecot/","title":"dovecot"},{"content":"Domyślna konfiguracja fail2ban\u0026rsquo;a (na Debianie) nie zawiera reguł pozwalających na blokowanie prób włamań na skrzynki POP/IMAP dla dovecota (no chyba że korzystamy z saslauthd). Można szybko utworzyć własny zestaw filtrów co przedstawię poniżej.\nTworzymy plik: /etc/fail2ban/filter.d/dovecot.conf\n[Definition] failregex = (?: pop3-login|imap-login): .*(?:Authentication failure|Aborted login \\(auth failed|Aborted login \\(tried to use disabled|Disconnected \\(auth failed|Aborted login \\(\\d+ authentication attempts).*rip=(?P\u0026lt;host\u0026gt;\\S*),.* ignoreregex = Później dopisujemy na końcu pliku: /etc/fail2ban/jail.conf\n[dovecot] enabled = true filter = dovecot port = pop3,pop3s,imap,imaps logpath = /var/log/mail.log maxretry = 20 # te dwa poniżej wedle uznania - ja mam dobrze ustawione default\u0026#39;y #findtime = 1200 #bantime = 1200 Zostało zrestartować fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart Tip na bazie dokumentacji: http://wiki.dovecot.org/HowTo/Fail2Ban\n","href":"/2011/11/fail2ban-regulki-dla-dovecota/","title":"fail2ban - regułki dla dovecot’a"},{"content":"Gdy już ustawimy reverse proxy przed Apache szybko można zauważyć że w logach zamiast adresów IP zdalnych użytkowników pojawia się tylko jeden adres: adres naszego proxy. Również z poziomu php\u0026rsquo;a jako adres klienta widać IP naszego proxy.\nBy poradzić sobie z tym problemem trzeba na serwerze reverse proxy ustawić przekazywanie informacji o oryginalnym adresie IP klienta w nagłówku X-Forwarded-For. W przypadku gdy reverse proxy działa na nginx\u0026rsquo;e wystarczy dodać taki wpis:\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; Teraz w trzeba zainstalować moduł mod_rpaf dla Apachego, który to zajmie się interpretacją nagłówka i podmianą IP proxy prawdziwym IP. Na Debianie wystarczy wpisać:\napt-get install libapache2-mod-rpaf Po instalacji należy w pliku /etc/apache2/mods-available/rpaf.conf w opcji RPAFproxy_ips dopisać adresy IP serwerów proxy, np (oczywiście wpisz swoje adresy):\nRPAFproxy_ips 127.0.0.1 10.24.0.5 Ważne by były to zaufane adresy IP - bo w tym miejscu pozwalamy by z tych lokalizacji możliwe było nadpisanie adresu IP np. w logach. Jeżeli pozwolimy na modyfikację adresów IP z zewnątrz to atakujący może wykorzystać to by nadpisać swój prawdziwy adres fałszywym.\nPozostało uruchomić moduł i zrestartować Apachego aby go załadował:\na2enmod rpaf invoke-rc.d apache2 restart Teraz zarówno w logach Apache\u0026rsquo;go jak i skryptach PHP\u0026rsquo;a będzie przekazywane rzeczywiste IP użytkownika.\n","href":"/2011/11/x-forwarded-for-mod_rpaf-logowanie-rzeczywistych-adresow-ip-na-apache-za-reverse-proxy/","title":"X-Forwarded-For + mod_rpaf - logowanie rzeczywistych adresów IP na Apache za reverse proxy"},{"content":"","href":"/tags/apc/","title":"APC"},{"content":"","href":"/tags/eaccelerator/","title":"eAccelerator"},{"content":"Przez pewien czas korzystałem z eAcceleratora do przyspieszenia działania stron pisanych w PHP\u0026rsquo;ie ale czasem bywał niestabilny. Aktualizacje pojawiały się rzadko a od czasu do czasu miewałem problemy ze stabilnością tej wtyczki na kilku bardziej skomplikowanych aplikacjach. Zdarzało się że pomimo zmiany kodu w skrypcie php, eAccelerator serwował wciąż stary plik - konieczny był restart Apache\u0026rsquo;go by wszystko działało jak trzeba.\nZacząłem szukać alternatywy i trafiłem na dwa moduły:\n APC (czyli Alternative PHP Cache), który ma być nawet domyślnie wbudowany w PHP od wersji 5.4, XCache - projekt rozwijany przez jednego z programistów lighttpd.  Byłem ciekaw wydajności poszczególnych rozwiązań względem siebie, więc przygotowałem małe środowisko testowe składające się z 4 maszyn wirtualnych (działających pod kontrolą VirtualBox\u0026rsquo;a):\n Apache 2.2 + PHP 5.3 Apache 2.2 + PHP 5.3 + eAccelerator 0.9.6.1 (instrukcja instalacji tutaj) Apache 2.2 + PHP 5.3 + APC 3.1.3p1 (pod Debianem paczka php-apc) Apache 2.2 + PHP 5.3 + XCache 1.3.0 (pod Debianem paczka php5-xcache)  Najpierw przygotowałem pierwszą maszynę wraz z konfiguracją MySQL\u0026rsquo;a i domyślną instalacją WordPress\u0026rsquo;a 3.2 by test był w miarę miarodajny. Kolejne maszynki to klony tej pierwszej, plus zainstalowane i domyślnie skonfigurowane kolejne rozszerzenia. Każdemu z rozszerzeń przyznałem 32 MB pamięci na cache.\nMetodyka testu Do automatyzacji testu posłużył mi skrypt w bash\u0026rsquo;u uruchamiający ab dla 1000 zapytań z kolejno rosnącą liczbą równoległych połączeń. Pozwoli to na porównanie wydajności optymalizatorów przy mniejszym i większym obciążeniu. Uruchomienie testu dla czystej instalacji bez dodatku pokaże jak duży przyrost wydajności daje się uzyskać.\nJako systemy testowe posłużyły mi wirtualki uruchomione pod kontrolą VirtualBOX\u0026rsquo;a z zainstalowanym aktualnym Debianem Squeeze. Przydzieliłem im po 1 GB RAM\u0026rsquo;u i 2 rdzenie CPU. Wirtualki te raczej nie są highend\u0026rsquo;em ale do ogólnego porównania optymalizatorów będą w zupełności wystarczające.\nWyniki Już na pierwszy rzut oka widać że opłaca się zainstalować dowolny optymalizator bo ich wydajność jest zbliżona a w stosunku do czystej instalacji pozwalają obsłużyć prawie czterokrotnie większy ruch. Przy czym system bez opcode cacher\u0026rsquo;a nie pokonał granicy 70 zapytań na sekundę - zaczął swapować i nie ukończył kolejnych testów.\nPoniżej wykres przedstawiający ilość obsługiwanych żądań w zależności od ilości równoczesnych połączeń: I jeszcze jeden wykres, na którym porównałem wydajność poszczególnych optymalizatorów względem czystego PHP\u0026rsquo;a Wychodzi na to że przez większość testu eAccelerator był najszybszy, gdzieniegdzie przeplatając się z APC. XCache nieznacznie ale na całej długości poniżej dwóch wcześniejszych. Całościowe różnice pomiędzy optymalizatorami przeważnie nie przekraczały 3 zapytań/sekundę - więc różnice pomiędzy nimi są rzędu 1~2%. Można na tej podstawie wywnioskować że wydajność jest tak zbliżona iż nie powinna być jedynym kryterium wyboru optymalizatora dla naszego systemu.\nPoniżej postaram się zebrać subiektywne oceny poszczególnych rozwiązań by dostarczyć dodatkowych argumentów.\neAccelerator eAccelerator był najszybszy w teście ale miewałem z nim problemy (kilka razy ale\u0026hellip;) stąd nie jest moim faworytem.\nZalety:\n zdecydowanie najszybszy, jest stosunkowo aktywnie rozwijany, dość stabilny, wbudowany encoder i dekoder skryptów (do dystrybucji kodu bez źródeł w postaci skompilowanej).  Wady:\n brak paczek w repozytoriach Debiana - ręczna kompilacja nie jest ciężka ale gdy trzeba go utrzymać na 30 serwerach to przestaje być zabawnie, pomimo że pojawiają się nowe wersje to ostatnio miałem problemy z pobraniem ich ze strony projektu - szukanie paczek \u0026ldquo;gdzieś\u0026rdquo; na sieci nie wydaje mi się bezpieczne, miałem problem z pewną dużą aplikacją, nie działała stabilnie pod eAcceleratorem, eAccelerator powstał na bazie kodu Turck MMCache (ten nie jest już rozwijany) -istnieją pewne wątpliwości licencyjne co do jego kodu\u0026hellip;  PHP APC APC pod względem wydajności nieznacznie ustępuje eAcceleratorowi. Ciekawą funkcją udostępnianą przez APC jest obsługa RFC1867 (File Upload Progress hook handler). Jest też pewna potwierdzona plotka mówiąca o włączeniu kodu APC do PHP\u0026rsquo;a 6. Teoretycznie jeżeli przesiądziemy się już teraz na APC to później powinno pójść łatwiej\u0026hellip;\nZalety:\n dość szybki, aktywnie rozwijany, bardzo stabilny, dostępna paczka w repozytoriach Debiana (i z tego co wiem na wielu innych systemach też przeważnie wystarczają domyślne repozytoria) obsługa RFC1867 (upload progress), zostanie włączony do PHP od wersji 5.4, APC udostępnia API umożliwiające tworzenie własnych obiektów w pamięci cache współdzielonych pomiędzy zapytaniami np. by nie pobierać za każdym razem właściwości profilu z bazy, listy użytkowników, itp (coś w stylu memcached). dostępny jest ze skryptem apc.php, który pozwala zarządzać obiektami w cache, czyścić itp.  Wady:\n podobno bywa problematyczny w konfiguracji z fast-cgi (choć u mnie działa), przy mocno zapchanym cache\u0026rsquo;u czyszczenie go potrafiło się zwiesić.  XCache Ostatni projekt rozwijany jest przez jednego z programistów lighttpd. W chwili obecnej wydaje się być dość dojrzałym i wystarczająco stabilnym do produkcyjnego użycia. Choć gdy próbowałem z niego korzystać jakiś rok/dwa temu to miałem sporo losowych padów - niezależnych od obciążenia serwera.\nZalety:\n stabilny, aktywnie rozwijany z kilkoma gałęziami (stable/unstable/devel) - możemy wybrać czy potrzebujemy funkcji czy stabilności.  Wady:\n nieznacznie, ale jednak najniższa wydajność, miałem z nim mało styczności a szybko zraziłem się do kiepskiej stabilności - obecnie wydaje się że nie stanowi to problemu.  Podsumowanie i mój wybór Do testu celowo wybrałem WordPress\u0026rsquo;a jako dość duży i wystarczająco skomplikowany projekt - jeśli on będzie działać stabilnie to większość mniejszych też powinna\u0026hellip; Ku mojemu zaskoczeniu żaden z optymalizatorów nie sypnął błędami. Dziwiło mnie to bo jeszcze jakiś czas temu eAccelerator czasami losowo mi się sypał - działał przez tydzień i nagle zgon w sobotę po południu\u0026hellip; Później próbowałem XCache i było podobnie\u0026hellip; tylko gorzej bo problemy występowały częściej. APC testowałem jako ostatnie ale w wykorzystywanych przeze mnie aplikacjach zachowywał się bardzo stabilnie i przewidywalni. Jedyny problem z wieszaniem się podczas czyszczenia/usuwania elementu z cache\u0026rsquo;u można obejść stosunkowo szybkim restartem Apachego - skuteczne i efekt ten sam 😃 Na jednym z serwerów testuję APC w trybie fast-cgi od około dwóch miesięcy i jak na razie nie mogę narzekać (może w wolnej chwili uzupełnię to zestawienie o testy w trybie fast-cgi).\nObecnie w większości administrowanych przeze mnie serwerów z PHP\u0026rsquo;em standardowo instaluję APC. Wybór jest dla mnie tym bardziej oczywisty że paczka ta jest dostępna w standardowych repozytoriach (łatwość aktualizacji itd) - nie ma zatem potrzeby jak w przypadku eAcceleratora instalowania wielu paczek z zależnościami by móc skompilować 1 moduł.\nDodatkową zaletą jest fakt że APC ma być standardowo wbudowany w kolejne wersje PHP\u0026rsquo;a - jeżeli w rozwijanych aplikacjach już teraz zwróci się uwagę na integrację z tym rozwiązaniem to w przyszłości migracja nie powinna przysporzyć problemów.\nJeżeli się wahasz - wybierz APC. Jeżeli w Twoim środowisku okaże się niestabilne zawsze możesz spróbować dwóch pozostałych rozwiązań.\nPrzydatne linki (część potwierdza moje obserwacje, są też testy z drupalem):\nhttp://php.net/manual/en/book.apc.php http://xcache.lighttpd.net/ http://eaccelerator.net/ (w chwili pisanie strona nie działała 😃)\nhttp://www.ducea.com/2006/10/30/php-accelerators/ http://2bits.com/articles/benchmarking-drupal-with-php-op-code-caches-apc-eaccelerator-and-xcache-compared.html http://2bits.com/articles/benchmarking-apc-vs-eaccelerator-using-drupal.html http://hostingfu.com/article/increasing-php-application-performance-xcache\n","href":"/2011/11/porownanie-optymalizatorow-php-eaccelerator-php-apc-xcache/","title":"Porównanie optymalizatorów PHP - eAccelerator, PHP APC, XCache"},{"content":"","href":"/tags/xcache/","title":"XCache"},{"content":"","href":"/tags/sles/","title":"SLES"},{"content":"Administrowałem do tej pory głównie darmowymi distro, ale gdzieś tam ukradkiem wkradło się kilka \u0026ldquo;siusiaków\u0026rdquo; (aka SUSE Linux Enterprise Server). Żyłem w utopijnym przekonaniu że skoro się za nie płaci to powinno się z nimi łatwiej współpracować\u0026hellip; w przypadku instalacji aktualizacji (a w szczególności SP) nie było to aż takie proste.\nPrzywykłem w darmowych dystrybucjach że gdy pojawiała się nowszą \u0026ldquo;większa wersja\u0026rdquo; to po prostu można było jednym poleceniem zaktualizować wszystkie pakiety. Źródła aktualizowały się automatycznie (lub prawie automatycznie) - później trzeba było połatać ewentualne zmiany w plikach konfiguracyjnych. W SUSE jest ciut inaczej\u0026hellip; 😉\nInstalacja SP1 na SLES\u0026rsquo;ie 11 Instrukcja jest dla SLES\u0026rsquo;a jedenastki (o ile pamiętam dziesiątkę aktualizowało się inaczej) i Service Pack\u0026rsquo;a 1 - ale powinna zadziałać również w przypadku każdego kolejnego SP. Zaczynamy!\nZ root\u0026rsquo;a uruchamiamy polecenia:\nzypper ref -s zypper up -t patch zypper up -t patch (Nie pomyliłem się - drugie polecenie należy uruchomić dwa razy - SIC!)\nPierwsze polecenie odświeży informacje o dostępnych usługach i repozytoriach.\nDrugie polecenie zainstaluje aktualizacje dla programów zarządzających paczkami w systemi, a kolejne wywołanie powinno zainstalować pozostałe dostępne aktualizacje. Podwójne wywołanie zypper up -t patch znajduje się w oficjalnej instrukcji - w nieoficjalnej znalezionej w sieci proponowano by uruchamiać to polecenie do puki nie będzie miało już nic więcej do zaktualizowania.\nPo wydaniu powyższych poleceń, w systemie (a dokładnie w plikach /etc/products.d/*.prod) powinny pojawić się informacje o dostępnych pakietach narzędzi migracyjnych. By je wylistować należy wydać polecenie:\ngrep \u0026#39;\u0026lt;product\u0026gt;\u0026#39; /etc/products.d/*.prod U mnie dało to taki wynik:\n\u0026lt;product\u0026gt;sle-sdk-SP1-migration\u0026lt;/product\u0026gt; \u0026lt;product\u0026gt;SUSE_SLES-SP1-migration\u0026lt;/product\u0026gt; Jeżeli u Ciebie to polecenie nic nie zwróciło tzn. że nie ma dostępnych aktualizacji lub że zbyt mało razy uruchomiono zypper up -t patch 😃\nPowyższe \u0026ldquo;produkty\u0026rdquo; - należy zainstalować poleceniem:\nzypper in -t product sle-sdk-SP1-migration SUSE_SLES-SP1-migration Aby zaktualizować system musimy mieć dostęp do podstawowego repozytorium z nowszymi wersjami pakietów - uzyskamy go rejestrując się:\nsuse_register -d 2 -L /root/.suse_register.log Jeżeli nie zapomnieliśmy o przedłużeniu licencji i rejestracja przebiegła pomyślnie to możemy odświeżyć zawartość nowych repozytoriów i usług:\nzypper ref -s Teraz wylistujmy dostępne repozytoria, poleceniem:\nzypper lr BARDZO WAŻNE: musimy wyłączyć stare repozytoria (dla systemu bez SP) i włączyć nowe repozytoria dla systemu z SP1 - jeżeli tylko włączymy nowe repozytoria to po kolejnej aktualizacji systemu mogą zainstalować się paczki w starszych wersjach rozwalając system! Sprawdzałem osobiście i rzeczywiście tak jest 😃\nWłączanie/wyłączanie repozytoriów umożliwiają polecenia:\nzypper mr -disable repozytorium_do_wylaczenia zypper mr -enable repozytorium_do_wlaczenia Dopiero teraz system jest gotowy do aktualizacji, którą przeprowadzamy poleceniem (jeżeli instalujemy zdalnie warto odpalić je spod screen‘a):\nzypper dup Zypper zapyta czy chcemy usunąć zainstalowane wcześniej \u0026ldquo;produkty migracyjne\u0026rdquo; i zaktualizować pozostałe pakiety - należy potwierdzić (oczywiście jeśli jesteśmy absolutnie pewni i mamy backup 😃 ).\nZNÓW WAŻNE: po zakończonej aktualizacji należy ponownie zarejestrować siusiaka aby usunąć repozytoria z aktualizacjami dla czystej wersji 11 i zastąpić je repami dla wersji z SP 1:\nsuse_register -d 2 -L /root/.suse_register.log Teraz możemy zrestartować system by przeładowało się jajko i wszystkie usługi - jeżeli wszystko poszło po naszej myśli to powita nas SLES 11 SP 1.\nPrawda że proste a cały proces wręcz oczywisty?\n","href":"/2011/10/sles-11-instalacja-service-packa/","title":"SLES 11 - instalacja Service Pack’a"},{"content":"Bardzo często konfigurując usługi dostępne publicznie poświęca się sporo czasu na maksymalne zwiększenie bezpieczeństwa przez \u0026ldquo;dopieszczanie\u0026rdquo; konfiguracji (certyfikaty z mocnym szyfrowaniem, ochronę pewnych stron hasłem, dostęp do SSH tylko kluczami, itd.) ale całkowicie pomija się przygotowanie systemu aktywnie monitorującego błędne próby autoryzacji. Oczywiście nie można umniejszać wagi pierwszego z wymienionych etapów ale zdecydowanie nie powinno pomijać się też tego drugiego. Przecież każdy admin chciałby wiedzieć gdy ktoś próbuje włamać się na jego serwer (FTP, HTTP, SSH, itp.) - tylko ilu z Nas zadaje sobie trud by uruchomić taki system?\nWielu z nas uważa że przygotowanie takiego mechanizmu jest zbyt pracochłonne, trudne, nie teraz, nie mam czasu, itd\u0026hellip;. Zapominamy przy tym że gotowy system zostanie oddany w ręce użyszkodników a Ci na pewno będą z niego korzystać wbrew wszelkim zasadom bezpieczeństwa 😃\nNa przykład taki Roman z loginem romek ustawi hasło do poczty abc123 - wirusy i boty zamiast próbować łamać hasło jednego usera bardzo często próbują wbić się na popularne loginy wykorzystując proste hasła - zaskakujące jak często się im to udaje. A później Roman ma pretensje do admina że jego znajomi dostali po 10000 maili z wirusami\u0026hellip;\nTymczasem w wielu przypadkach wystarczy fail2ban w praktycznie podstawowej konfiguracji by ochronić typowe usługi przed:\n atakami brute force jak w powyższym przykładnie (SSH, Apache, vsftpd, proftpd, Postfix, SASL Auth, etc), atakiem pocztowego bot netu (blokowanie hostów generujących dużą ilość błędów), złymi crawler\u0026rsquo;ami czy narzędziami skanującymi strony WWW, ataki flood na Bind DNS.  Narzędzie napisane jest w Perl\u0026rsquo;u i działa jako demon zapisując dane z syslog\u0026rsquo;a w małych paczkach i okresowo uruchamiając testy sprawdzające - wpływa to korzystnie na wydajność (bo nie musi analizować całodniowych logów jak prymitywniejsze narzędzia). Modułowa konfiguracja pozwala łatwo dodać filtry (wykrywające niezdefiniowane w standardzie zdarzenia) lub akcje (np. zamiast wycinać spam bota przez iptables można go wrzucić do naszego prywatnego RBL\u0026rsquo;a). W innym poście podałem przykład dodania reguł dla dovecot\u0026rsquo;a.\nPomimo iż narzędzie wydaje się zbytnio nie obciążać systemu to raczej nie odważyłbym się go zainstalować na mocno obciążonym serwerze, gdzie generują się miliony linii logów dziennie. W takiej sytuacji potrzebne jest inne rozwiązanie.\nInstalacja W moim przypadku na Debianie leci to typowo:\napt-get install fail2ban Aplikację można też dość łatwo zainstalować ze źródeł - zalezności nie ma zbyt dużo.\nKonfiguracja Gdy już zainstalujemy fail2ban\u0026rsquo;a otwieramy główny plik konfiguracyjny /etc/fail2ban/jail.conf. Idąc od początku warto zainteresować się opcjami:\n# poniżej należy umieścić listę adresów IP, sieci CIDR, z których # chcemy ignorować zagrożenia ignoreip = 127.0.0.1 10.3.4.0/24 # domyślny czas BAN\u0026#39;a - można nadpisać w konfiguracji danej usługi bantime = 36000 # domyślna ilość wykrytych akcji, po których zostanie dojdzie # do BAN\u0026#39;a maxretry = 3 # adres osoby, która ma być informowana o nałożeniu/zdjęciu BAN\u0026#39;a destemail = moj@mail.pl # domyślna akcja, predefiniowane są 3 możliwości: # action_ - tylko BAN # action_mw - BAN i powiadomienie mailem z danymi WHOIS # action_mwl - jak wyżej plus linie z loga action = %(action_mwl)s Dalej w konfigu znajdują się przygotowane definicje poszczególnych usług i ich konfiguracja, np. SSH:\n[ssh] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 5 bantime = 3600 [ssh-ddos] enabled = true port = ssh filter = sshd-ddos logpath = /var/log/auth.log maxretry = 6 Jak wspominałem wcześniej w danym bloku można wpisać inne niż domyślne wartości dla opcji bantime i maxretry. Opcja filter wskazuje na nazwę pliku z definicją filtru, np: /etc/fail2ban/filter.d/sshd.conf. W tej lokalizacji możemy dodawać też własne filtry, trzeba tylko znać wyrażenia regularne. W opcji port możemy zmienić port, na którym działa usługa (by banowanie działało), korzystając przy tym z nazw dostępnych w pliku /etc/services lub bezpośrednio wpisując numer portu.\nDomyślnie nie jest włączona ochrona żadnej z usług. By ją włączyć należy zmienić w stosownym bloku:\nenabled = false na:\nenabled = true i przeładować fail2ban\u0026rsquo;a:\ninvoke-rc.d fail2ban restart I na początek wystarczy. W kilku banalnych krokach uruchomiliśmy system monitorujący logi i blokujący próby włamań.\nOczywiście włączając ochronę danej usługi dobrze zapoznać się z regułami zapisanymi w danym filtrze - by nie być zaskoczonym gdy sypnie mailami 😉\nWyłączenie powiadamiania o starcie fail2ban\u0026rsquo;a Jeżeli włączymy powiadamianie mailem o banach jako gratis fail2ban będzie powiadamiać nas mailem o właczeniu i wyłączeniu ochrony dla każdej z usług - na wypadek gdyby ktoś bez naszej wiedzy zechciał go wyłączyć. Dla wielu może to być irytujące zachowanie.\nBy wyłączyć powiadomienia trzeba w pliku akcji - u mnie w: /etc/fail2ban/action.d/mail-whois-lines.conf wyczyścić opcje actionstart i actionstop by wyglądały jak poniżej:\nactionstop = actioncheck = ","href":"/2011/10/ochrona-uslug-przed-atakami-brute-force-z-fail2banem/","title":"Ochrona usług przed atakami brute force z fail2ban’em"},{"content":"","href":"/tags/pflogsumm/","title":"pflogsumm"},{"content":"Jeżeli administrujesz nawet niedużym serwerem pocztowym na pewno masz świadomość, że nie jesteś w stanie monitorować logów na bieżąco. Ciężko jest wyłapać np. problem w komunikacji z pewną domeną. Ciężko też oszacować skalę ruchu na serwerze zarówno pod kątem ilości jak i wolumenu maili. Trudno wybrać domeny, dla których warto by zrezygnować z greylistingu, itd, itp\u0026hellip;\nNa szczęście dostępne jest narzędzie pflogsumm, które wygeneruje nam dość wyczerpujące statystyki z logów postfix\u0026rsquo;a. Bardzo przydatne przy codziennym przeglądzie \u0026ldquo;stanu zdrowia\u0026rdquo; serwera pocztowego.\nPrzykładowy wycinek statystyk z pewnego małego serwerka prezentuje się tak:\nPostfix log summaries for Jul 4 Grand Totals ------------ messages 1158 received 1261 delivered 0 forwarded 5 deferred (50 deferrals) 2 bounced 392 rejected (23%) 0 reject warnings 0 held 0 discarded (0%) 164898k bytes received 242985k bytes delivered 201 senders 77 sending hosts/domains 354 recipients 51 recipient hosts/domains Per-Hour Traffic Summary time received delivered deferred bounced rejected -------------------------------------------------------------------- 0000-0100 26 28 2 0 7 0100-0200 14 18 3 0 10 0200-0300 4 4 1 0 8 0300-0400 6 6 1 0 8 0400-0500 4 4 0 0 8 0500-0600 2 2 1 0 9 0600-0700 8 8 1 0 9 0700-0800 16 18 1 0 10 0800-0900 58 60 1 0 8 0900-1000 104 110 5 0 17 1000-1100 132 152 2 0 18 1100-1200 106 106 1 0 31 1200-1300 64 70 2 0 9 1300-1400 112 132 2 0 14 1400-1500 98 106 1 0 78 1500-1600 86 88 2 0 32 1600-1700 56 56 3 0 23 1700-1800 58 77 5 2 19 1800-1900 36 36 3 0 16 1900-2000 26 26 2 0 24 2000-2100 48 50 3 0 9 2100-2200 32 42 2 0 10 2200-2300 34 34 3 0 10 2300-2400 28 28 3 0 5 ... Host/Domain Summary: Message Delivery sent cnt bytes defers avg dly max dly host/domain -------- ------- ------- ------- ------- ----------- 132 5688k 0 1.7 s 11.0 s gmail.com 104 2633k 0 4.6 s 2.8 m wp.pl 68 1525k 0 1.3 s 9.4 s interia.pl 42 744k 21 1.1 s 83.6 h o2.pl 30 89891 0 0.7 s 2.6 s op.pl 29 6677k 1 16.1 s 7.4 m poczta.onet.pl 26 540k 0 1.9 s 6.7 s poczta.fm ... Host/Domain Summary: Messages Received msg cnt bytes host/domain -------- ------- ----------- 50 4142k gmail.com 46 491259 facebookmail.com 38 1446k wp.pl 22 13520k interia.pl 14 675k o2.pl 10 105377 poczta.fm 10 57713 hotmail.com ... i dużo więcej... Instalacja na Debianie:\napt-get install pflogsumm Testowo polecenie można uruchomić w następujący sposób:\nsudo pflogsumm -i -d yesterday /var/log/mail.log /var/log/mail.log.1 W moim przypadku, logi przewijam codziennie ok 2:00 w nocy, dlatego podaję dwie ścieżki do plików log (bieżącego i wczorajszego) by mi te dwie godzinki nie umknęły 😉\nPowyższe polecenie wypisze na standardowe wyjście statystyki w postaci ładnie sformatowanych tekstowych tabel. Warto przyjrzeć się innym parametrom polecenia - można dzięki nim zrezygnować ze statystyk, które nas nie interesują, bądź zmienić domyślną kolejność.\nTeraz warto uruchomić okresowe raportowanie. Edytujemy crona:\nsudo crontab -e Na generowanie statystyk warto wybrać godzinę mniejszego obciążenia serwera (@daily oznacza północ), bo proces ich przygotowania dość mocno obciąży CPU. Wpisujemy polecenie wraz z interesującymi nas parametrami:\n@daily /usr/sbin/pflogsumm -i --problems_first --no_bounce_detail \\  --no_deferral_detail -d yesterday \\  /var/log/mail.log /var/log/mail.log.1 | \\  mail -e -s \u0026#34;Statystyki poczty na `uname -n`\u0026#34; postmaster Kolejnego dnia powinniśmy otrzymać nasze statystyki.\n","href":"/2011/09/pflogsum-statystyki-poczty-dla-postfixa/","title":"pflogsumm - statystyki poczty dla postfix’a"},{"content":"Miałem ostatnio dziwną przygodę: pewien serwer do backupu gdzie ląduje dużo małych plików i dodatkowo tworzonych jest sporo hardlinków zaliczył pada. Co prawda starałem się go grzecznie położyć z pomocą Magic SysRq ale ponieważ nie wiedziałem co było przyczyną awarii fsck wydawał się wskazany.\nPodczas próby uruchomienia fsck.ext4 na systemie plików o rozmiarze ok 14TB z kilkuset milionami plików po kilkudziesięciu sekundach otrzymywałem komunikat:\nBłąd podczas przydzielania struktury icount: Memory allocation failed\nGooglając dowiedziałem się że problem jest znany i występuje podczas sprawdzania bardzo dużych systemów plików z dużą ilością hardlinków. To akurat idealnie mój przypadek\u0026hellip; Przeważnie zdarza się to na systemach 32 bitowych gdy fsck próbuje zaalokować powyżej 2GB pamięci. Jest to górny limit możliwej do wykorzystania przez pojedynczy proces pamięci dla architektury 32 bitowej - nie można go przeskoczyć nawet stosując PAE. Ale mój system jest 64 bitowy, 8GB RAM\u0026rsquo;u, 8GB swap\u0026rsquo;a - nie jest dobrze jeśli brakuje pamięci ;-/\nZgodnie z sugestiami Teo Tso by zmniejszyć zapotrzebowanie fsck na pamięć można zmusić go by informacje o inodach i właściwościach katalogów przechowywał w tymczasowym katalogu. By wskazać katalog tymczasowy należy utworzyć plik: /etc/e2fsck.conf z zawartością:\n[scratch_files] directory = /var/cache/e2fsck Katalog trzeba utworzyć ręcznie:\nmkdir /var/cache/e2fsck Warto zadbać o kilka/kilkanaście gigabajtów wolnego miejsca w tym katalogu - a najlepiej na czas sprawdzania podmontować jakiś zasób z fizycznie innego dysku niż sprawdzany. Ja wykorzystałem w tym celu kilku gigabajtowego pen drive\u0026rsquo;a.\nTeraz można odpalić fsck\u0026rsquo;a (odpalając zdalnie warto zrobić to w screen\u0026rsquo;ie):\nfsck.ext4 -f -C0 /dev/md1 Parametr -f wymusi sprawdzenie, a -C0 wyświetli pasek postępu co przy długim sprawdzeniu da nam jakieś wyobrażenie postępu. Tak uruchomiony fsck działa zdecydowanie wolniej niż przy domyślnych ustawieniach ale przynajmniej powinien się wykonać.\n# fsck.ext4 -fD -C0 /dev/md1 e2fsck 1.41.12 (17-May-2010) Przebieg 1: Sprawdzanie i-węzłów, bloków i rozmiarów /dev/md1: |=========== | 19.4% Opis podobnego problemu ze wsparciem Teo Tso: http://www.linux-archive.org/ext3-users/103464-2gb-memory-limit-running-fsck-6tb-device.html\n","href":"/2011/09/fsck-ext4-blad-podczas-przydzielania-struktury-icount-memory-allocation-failed/","title":"fsck.ext4 - Błąd podczas przydzielania struktury icount: Memory allocation failed"},{"content":"","href":"/tags/magic-sysrq/","title":"Magic SysRq"},{"content":"Pomimo iż Linux uchodzi za stabilne środowisko to raz na jakiś czas trafi się ciężka zwiecha - z powodu przeciążenia, awarii sprzętu\u0026hellip; nieistotne\u0026hellip;\nZałóżmy że licho wzięło za cel główny serwer plików lub bazę danych dla wielu, wielu stron internetowych. Dostać się po ssh nie możemy bo lecą timeout\u0026rsquo;y, a siedząc bezpośrednio przy klawiaturze konsola nie reaguje. Mimo to coś ostro daje po dyskach, więc ewentualny twardy reset to na bank utrata części plików\u0026hellip; jeśli system po nim w ogóle wstanie\u0026hellip; 😑\nJeśli powyższa historyjka wygląda znajomo to zdecydowanie warto czytać dalej.\nW wielu dystrybucjach kernel standardowo jest skompilowany z opcją CONFIG_MAGIC_SYSRQ - opcja opisana jest jako szczególnie przydatna dla developerów jądra ale i nam może się przysłużyć jako ostatnia deska ratunku przed twardym resetem.\nAby wywołać funkcję trzeba przytrzymać na klawiaturze: Lewy ALT + PrintScrn/SysRq i przycisk określający funkcję. Kilka z możliwych funkcji to:\n r - przełącz klawiaturę z trybu RAW do XLATE (w wolnym tłumaczeniu: odzyskaj obsługę klawiatury od X\u0026rsquo;ów), e - wyślij SIGTERM do wszystkich procesów z wyjątkiem init\u0026rsquo;a, i - wyślij SIGKILL do wszystkich procesów z wyjątkiem init\u0026rsquo;a, s - wywołaj sync dla wszystkich zamontowanych zasobów (czyli zapisz wszystkie niezapisane dotąd transakcje dyskowe), u - przemontuj wszystkie zamontowane zasoby to trybu tylko do odczytu, b - natychmiast uruchom ponownie system, bez odmontowania partycji i bez synchronizacji dysków.  To tylko część funkcji, ale wykonanie ich w kolejności w jakiej zostały tu wypisane (czyli reisub) powinno skutkować STOSUNKOWO bezpiecznym resetem, po którym nawet nie powinno być potrzebne skanowanie dysków fsck\u0026rsquo;iem. Każda z funkcji potrzebuje kilka/kilkanaście sekund na wykonanie (szczególnie warto zaczekać by po sync\u0026rsquo;u przestała migać kontrolka aktywności dysków twardych) - nie warto się spieszyć. Po ostatnim wywołaniu system powinien się zrestartować - zdarzyło mi się tylko kilka razy by procedura ta zawiodła.\nMetodę można też wykorzystać dla serwerów zdalnych, które np. z jakiegoś powodu nie chcą się zrestartować w standardowy sposób. Można to zrobić wysyłając poszczególne polecenia do /proc/sysrq-trigger, np. tak:\necho b \u0026gt; /proc/sysrq-trigger Na zdalnych maszynach i VPS\u0026rsquo;ach wygodniejsze może być skonfigurowanie demona sysrqd.\nDla zainteresowanych, więcej funkcji Magic SysRq (głównie diagnostycznych) i dokładniejszy opis można znaleźć na stronie wiki.\n","href":"/2011/09/magic-sysrq-bezpieczny-reset-linuxa/","title":"Magic SysRq - bezpieczny reset Linux’a"},{"content":"","href":"/tags/approx/","title":"approx"},{"content":"Wielu administratorów gdy zaczyna swoją przygodę zarządza jedną/dwoma maszynami\u0026hellip; Po pewnym czasie jest ich już kilka\u0026hellip; W którymś momencie dostrzega się zalety wirtualizacji i na kilku maszynach fizycznych działa kilkanaście czy kilkadziesiąt maszyn wirtualnych. W takiej sytuacji pobieranie aktualizacji dla wszystkich maszyn potrafi mocno zabić łącze.\nI w tym momencie zaczynamy się zastanawiać czy może nie warto byłoby zrobić własnego mirror\u0026rsquo;a paczek dla naszego ulubionego distro\u0026hellip; do prywatnego użytku\u0026hellip; synchronizowanego w nocy by nikomu nie przeszkadzać\u0026hellip; i dostępnego nawet gdy będziemy offline\u0026hellip; Zaczynamy liczyć miejsce i okazuje się że repozytorium Debiana dla architektury i386 to prawie 60GB (sic!), no ale mamy kilka maszynek z arch amd64 i tutaj też prawie 60GB - auć. W tym miejscu wielu dochodzi do wniosku że to jeszcze nie pora na własnego mirror\u0026rsquo;a 😃\nRozwiązaniem tego dylematu jest wykorzystanie approx\u0026rsquo;a, który działa jako cachujące proxy dla repozytoriów Debiania i Ubuntu. Pośredniczy on w pobieraniu plików zapisując kopię każdego w lokalnym cache\u0026rsquo;u. Czyli pierwsze pobranie zasysa paczkę z sieci, a kolejne odwołania do approx\u0026rsquo;a serwują już tę pobraną kopię. Ponadto przyjemne jest zachowanie approx\u0026rsquo;a, który samodzielnie utrzymuje \u0026ldquo;czystość\u0026rdquo; cachu usuwając starsze paczki i aktualizując nowe. Druga wielka zaleta tej aplikacji to lokalne składowanie tylko tych paczek które pobieramy - bo mało kto potrzebuje całego repozytorium - drastycznie zmniejsza to rozmiar takiego mirror\u0026rsquo;a. U mnie mirror ma raptem kilkaset megabajtów.\nInstalacja i konfiguracja Ponieważ paczka jest w domyślnych repozytoriach wystarczy:\napt-get install approx i po chwili mamy działające proxy.\nDomyślnie pliki składowane są w: /var/cache/approx - warto zadbać o odrobinę wolnego miejsca w tej lokalizacji.\nNależy jeszcze skonfigurować odpowiednie repozytoria w pliku konfiguracyjnym: /etc/approx/approx.conf - u mnie wygląda on tak:\ndebian http://ftp.pl.debian.org/debian security http://security.debian.org/debian-security volatile http://volatile.debian.org/debian-volatile backports http://backports.debian.org/debian-backports # The following are the default parameter values, so there is # no need to uncomment them unless you want a different value. # See approx.conf(5) for details. $interface any $port 8080 $max_wait 30 #$max_rate unlimited #$user approx #$group approx #$syslog daemon #$pdiffs true #$verbose false #$debug true Po instalacji approx działa na porcie 8080 o czym trzeba pamiętać przy podawaniu URL\u0026rsquo;i w sources.list - mi to odpowiadało bo na porcie 80-tym mam odpalony serwer WWW pod jakieś śmieci\u0026hellip; Ale Wam może odpowiadać uruchomienie approx\u0026rsquo;a na 80-tce.\nTeraz restartujemy approx\u0026rsquo;a by przeładować konfigurację:\ninvoke-rc.d approx restart Zostało skonfigurować plik /etc/apt/sources.list tak by wskazywał na nasze proxy. Należy to zrobić na każdej maszynce w naszej sieci by cała zabawa miała sens.\nW przypadku Lenny\u0026rsquo;ego powinien on wyglądać mniej więcej tak:\ndeb http://approx.costam.pl:8080/debian lenny main non-free contrib deb-src http://approx.costam.pl:8080/debian lenny main non-free contrib deb http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb-src http://approx.costam.pl:8080/security lenny/updates main contrib non-free deb http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb-src http://approx.costam.pl:8080/volatile lenny/volatile main contrib non-free deb http://approx.costam.pl:8080/backports lenny-backports-sloppy main W przypadku Squeeze\u0026rsquo;a tak:\ndeb http://approx.costam.pl:8080/debian squeeze main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze main non-free contrib deb http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb-src http://approx.costam.pl:8080/security squeeze/updates main contrib non-free deb http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb-src http://approx.costam.pl:8080/debian squeeze-updates main non-free contrib deb http://approx.costam.pl:8080/backports squeeze-backports main No i na koniec sprawdzamy czy wszystko działa:\napt-get update Możemy spróbować coś zainstalować i zobaczyć jak się spisze proxy.\n","href":"/2011/09/approx-cachujace-proxy-dla-repozytoriow-debiana/","title":"approx - cachujące proxy dla repozytoriów Debiana"},{"content":"","href":"/tags/apt/","title":"apt"},{"content":"","href":"/tags/backports/","title":"backports"},{"content":"","href":"/tags/proxy/","title":"proxy"},{"content":"","href":"/tags/cache/","title":"cache"},{"content":"","href":"/tags/proc/","title":"proc"},{"content":"Linux bardzo agresywnie wykorzystuje wolną pamięć RAM do buforowania danych odczytywanych z dysków (inode\u0026rsquo;ów, plików, itd\u0026hellip;). Ma to niebagatelny wpływ na zwiększenie szybkości uruchamiania programów które już raz zostały uruchomione. Jednak nie zawsze jest to pożądane zachowanie, np. testując szybkość uruchomienia/wykonywania tworzonej przez nas aplikacji - buforowanie zmienia czas ładowania aplikacji przy kolejnych uruchomieniach. Dobrze byłoby móc wymusić zwolnienie buforów by każdy start programu miał porównywalne \u0026ldquo;warunki startowe\u0026rdquo;.\nNa szczęście można to zrobić w prosty sposób:\nsync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches Polecenie to zwolni niewykorzystywany cache pliku stronicowania, katalogów i inodów. Wcześniejsze uruchomienie sync pozwala zwolnić większą ilość buforów przez wymuszenie zapisania otwartych plików.\nInne możliwe warianty to:\n zwolnienie cache pliku stronicowania:  sync \u0026amp;\u0026amp; echo 1 \u0026gt; /proc/sys/vm/drop_caches  zwolnienie cache cache katalogów i inodów:  sync \u0026amp;\u0026amp; echo 2 \u0026gt; /proc/sys/vm/drop_caches Opcja ta dostępna jest w jajkach od wersji 2.6.16.\n","href":"/2011/09/wymuszenie-zwolnienia-pamieci-buforow-dyskowych-na-linuxie/","title":"Wymuszenie zwolnienia pamięci buforów dyskowych na Linux’ie"},{"content":"","href":"/tags/mod_antiloris/","title":"mod_antiloris"},{"content":"","href":"/tags/mod_reqtimeout/","title":"mod_reqtimeout"},{"content":"","href":"/tags/slowloris/","title":"Slowloris"},{"content":"Od jakiegoś czasu dostępny jest w sieci skrypt slowloris.pl pozwalający z pojedynczego komputera wykonać atak DOS na zdalny serwer WWW. Atak polega na uruchomieniu wielu równoczesnych sesji i bardzo wolnym wysyłaniu komunikatów HTTP. Atakujący udaje \u0026ldquo;klienta z wolnym łączem\u0026rdquo; równocześnie uruchamiając kolejne sesje by po pewnym czasie zająć wszystkie dostępne. Serwer WWW przestaje wtedy odpowiadać na zapytania od innych klientów. Dodatkowo na źle wyskalowanych serwerach duża liczba procesów Apachego może spowodować swapowanie i błędy braku pamięci.\nW zależności od wydajności atakowanej maszyny by doprowadzić do jej zablokowania potrzeba przeważnie od kilkunastu do kilkudziesięciu sekund. Tak przeprowadzony atak DOS nie wymaga botnetu czy super wydajnego sprzętu - zwykły lapciak w zupełności wystarczy.\nNa chwilę obecną są już co najmniej dwie metody ochrony Apachego przed takim atakiem: mod_antiloris i mod_reqtimeout. Pierwszy dostępny jako moduł do ręcznej kompilacji dla starszych wersji Apache (np. w Debianie Lennym). Drugi dostępny jest w standardzie od wersji 2.2.15 (np. w Debianie Squeeze).\nmod_antiloris pozwala na limitowanie ilości równoczesnych sesji dla zdalnego klienta. W przypadku przekroczenia dozwolonej liczby zrywane jest połączenie. Ma to swoje wady, np. gdy z naszego serwera WWW korzysta jakaś duża NAT\u0026rsquo;owana sieć to przy większej liczbie połączeń zostaną zablokowani \u0026ldquo;dobrzy\u0026rdquo; klienci. Trudne jest też właściwe ustawienie maksymalnej liczby połączeń - domyślnie ustawiona jest wartość 5. Ale niektóre przeglądarki (lub wtyczki do nich) podnoszą limit równoczesnych połączeń per serwer do 8.\nmod_reqtimeout pozwala na określenie po jakim czasie przerwać połączenie w przypadku nie utrzymywania wystarczającego przepływu danych. Skracając - pozwala precyzyjnie wyciąć \u0026ldquo;wolnych\u0026rdquo; klientów. Moduł ten monitoruje każdą sesję z osobna przez co nie ma zagrożenia blokowania sieci NAT czy \u0026ldquo;agresywnie\u0026rdquo; ustawionych przeglądarek.\nInstalacja i konfiguracja mod_antiloris (Apache do wersji 2.2.14) Najpierw musimy pobrać potrzebne zależności:\napt-get install gcc apache2-prefork-dev Później pobieramy mod\u0026rsquo;a i rozpakowujemy:\nwget \u0026#34;ftp://ftp.monshouwer.eu/pub/linux/mod_antiloris/mod_antiloris-0.4.tar.bz2\u0026#34; tar xvf mod_antiloris-0.4.tar.bz2 cd mod_antiloris-0.4 Jeżeli mamy taką potrzebę możemy wyedytować plik mod_antiloris.c i podnieść limit połączeń:\n#define antiloris_MAX_PER_IP 5 Do kompilacji modułu wykorzystamy narzędzie apxs2, które skompiluje moduł jako dynamicznie ładowalny:\napxs2 -c mod_antiloris.c Teraz możemy skopiować moduł do katalogu z innymi modułami:\nsudo cp .libs/mod_antiloris.so /usr/lib/apache2/modules/mod_antiloris.so Musimy też utworzyć plik konfiguracyjny, który będzie ładować mod\u0026rsquo;a:\nsudo su -c \u0026#34;echo \u0026#39;LoadModule antiloris_module /usr/lib/apache2/modules/mod_antiloris.so\u0026#39; \u0026gt; /etc/apache2/mods-available/antiloris.load\u0026#34; Włączamy moduł:\nsudo a2enmod antiloris Na koniec musimy przeładować Apache\u0026rsquo;go:\nsudo service apache2 reload Instalacja i konfiguracja mod_reqtimeout (Apache od wersji 2.2.15) Ponieważ moduł jest dostępny wystarczy go uaktywnić i przeładować Apachego:\nsudo a2enmod reqtimeout sudo service apache2 reload Domyślnie w Squeeze dostępny jest plik konfiguracyjny z poniższymi wartościami:\nRequestReadTimeout header=20-40,minrate=500 RequestReadTimeout body=10,minrate=500 Pierwsza opcja oznacza: zezwalaj na wysyłanie zapytania przez co najmniej 20 sekund i zwiększaj limit do maksymalnie 40 sekund po 1 sekundzie za każde przesłane 500 bajtów.\nDruga opcja oznacza: zezwalaj na pobieranie przez co najmniej 10 sekund i zwiększaj limit czasu w nieskończoność po 1 sekundzie za każde pobrane 500 bajtów.\nDomyślne ustawienia są sensowne i powinny wystarczyć w większości przypadków. Dodatkowe dopieszczenie tych opcji może być potrzebne na serwerach wysyłających bądź odbierających dość duże pliki lub w przypadku \u0026ldquo;spersonalizowanych\u0026rdquo; ataków DOS.\nTest działania Skoro mamy już \u0026ldquo;tarczę\u0026rdquo; warto sprawdzić czy działa. W tym celu pobieramy slowlorisa:\nwget \u0026#34;http://ha.ckers.org/slowloris/slowloris.pl\u0026#34; chmod +x slowloris.pl I możemy uruchomić test:\nperl slowloris.pl -dns twojserwer.pl -port 80 -timeout 1 -num 300 -cache Jeżeli po dwóch minutach (w zależności od konfiguracji sprzętowej serwera) strona odpowiada i można się na nią bez problemów dostać to znaczy że nasz wysiłek się opłacił.\nDalszym krokami wartymi podjęcia może być analiza logów np. z użyciem fail2ban i wycinanie na firewallu bardziej uciążliwych gości.\n","href":"/2011/09/zabezpieczenie-apachego-na-debianie-przed-slowlorisem/","title":"Zabezpieczenie Apachego na Debianie przed slowloris’em"},{"content":"","href":"/tags/io-wait/","title":"IO Wait"},{"content":"","href":"/tags/iostat/","title":"iostat"},{"content":"","href":"/tags/iotop/","title":"iotop"},{"content":"Na jednym z serwerów zauważyłem dziwny wzrost obciążenia. Tzw. LOAD od kilku dni po woli rósł. top pokazywał że dwa rdzenie CPU czekają na dane z dysku - tzw. io wait na poziomie 80~90% ale żaden proces w znaczącym stopniu nie obciążał CPU.\nJest kilka narzędzi (iostat, wmstat), które pozwalają monitorować obciążenie dysków ale ja nie szukałem informacji czy i w jakim stopniu dyski są obciążone - wiedziałem że są. Chciałem dowiedzieć się który proces generuje to obciążenie - by móc go ubić 😃\nPrzydatny okazał się programik iotop - który działa jak top ale sortuje procesy w zależności od generowanego przez nie obciążenia dysków - właśnie tego szukałem:Program jest w standardowych repozytoriach Debiana i można go zainstalować w ten sposób:\napt-get install iotop ","href":"/2011/09/sprawdzenie-ktory-proces-obciaza-dyski/","title":"Sprawdzenie który proces obciąża dyski"},{"content":"","href":"/tags/vmstat/","title":"vmstat"},{"content":"","href":"/tags/awstats/","title":"AWStats"},{"content":"","href":"/tags/cgi/","title":"CGI"},{"content":"","href":"/tags/jpgraph/","title":"JPGraph"},{"content":"Onego czasu próbowałem znaleźć coś co ułatwiłoby mi rysowanie prostych wykresów w PHP\u0026rsquo;ie inaczej niż z palca w GD. Kumpel polecił mi JPGraph.\nJPGraph to świetna sprawa, do generowania statystyk jak chociażby na mojej stronie, ale biblioteka potrafi dużo więcej\u0026hellip;\nZałóżmy, że ze stronki zbieramy do bazy takie rzeczy jak: datę, adres IP, ilość połączeń z tego adresu. Prosta tabela (przykład w Postgre SQL\u0026rsquo;u):\nCREATE TABLE wizyty ( pid serial NOT NULL, \u0026#34;data\u0026#34; date NOT NULL DEFAULT (\u0026#39;now\u0026#39;::text)::date, odslony integer NOT NULL DEFAULT 1, CONSTRAINT visits_pkey PRIMARY KEY (id) ); Dane z takiej tabeli można łatwo wyciągnąć jednym select\u0026rsquo;em:\nSELECT date_part(\u0026#39;day\u0026#39;, \u0026#34;data\u0026#34;) AS x, sum(odslony) AS y FROM wizyty WHERE date \u0026gt; (\u0026#39;today\u0026#39;::date - \u0026#39;30 days\u0026#39;::interval) GROUP BY \u0026#34;data\u0026#34; ORDER BY \u0026#34;data\u0026#34; LIMIT 30 Powyższe zapytanie wyciągnie nam z kolumny z datą tylko dzień miesiąca, który posłuży za etykietę (oś X) oraz sumę odsłon z danego dnia (oś Y), posortowane według daty i tylko z ostanich 30 dni.\nW zależności od sposobu dostępu i pobierania wyników, trzeba je ładnie zapisać w dwóch tablicach. Ja to robię w prostej pętli:\nforeach ($query-\u0026gt;result() as $row) { $x[] = $row-\u0026gt;x; $y[] = $row-\u0026gt;y; } Mając dane została zabawa z ustawieniem oczekiwanych opcji wykresu:\n/* bez tego ani rusz */ include_once \u0026#39;jpgraph/jpgraph.php\u0026#39;; include_once \u0026#39;jpgraph/jpgraph_bar.php\u0026#39;; /* szerokość */ $width = 400; /* wysokość */ $height = 300; $graph = new Graph($width, $height); /* oś X tekst, oś Y wartości całkowite */ $graph-\u0026gt;SetScale(\u0026#34;textint\u0026#34;); /* kolorowa ramka */ $graph-\u0026gt;SetFrame(true, \u0026#39;#222222\u0026#39;); $graph-\u0026gt;SetMarginColor(\u0026#39;#222222\u0026#39;); /* tytuł wykresu, trochę konfiguracji fontów i kolorów */ $graph-\u0026gt;title-\u0026gt;Set(\u0026#39;Sumaryczna liczba odsłon\u0026#39;); $graph-\u0026gt;title-\u0026gt;SetFont(FF_VERDANA,FS_BOLD,10); $graph-\u0026gt;title-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* podobnież jak powyżej dla osi X */ $graph-\u0026gt;xaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;xaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;yaxis-\u0026gt;SetFont(FF_VERDANA,FS_NORMAL,8); $graph-\u0026gt;yaxis-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); /* dodatkowe ozdobniki ;) */ $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SupressZeroLabel(false); $graph-\u0026gt;yscale-\u0026gt;ticks-\u0026gt;SetColor(\u0026#39;#999999\u0026#39;); $graph-\u0026gt;xaxis-\u0026gt;SetTickLabels($x); /* genrujemy właściwy wykres, w tym przypadku słupkowy */ $bplot = new BarPlot($y); $bplot-\u0026gt;SetWidth(0.6); $bplot-\u0026gt;SetFillColor(\u0026#39;#D01A71@0.25\u0026#39;); $bplot-\u0026gt;SetColor(\u0026#39;gray\u0026#39;); /* dodajemy wykres do obiektu $graph (który to może pomieścić i wyświetlić wiele wykresów, co z resztą pokazane jest poniżej) */ $graph-\u0026gt;Add($bplot); /* teraz wykres liniowy */ $lplot = new LinePlot($y); /* z wypełnieniem pod wykresem */ $lplot-\u0026gt;SetFillColor(\u0026#39;skyblue@0.5\u0026#39;); $lplot-\u0026gt;SetColor(\u0026#39;navy@0.7\u0026#39;); $lplot-\u0026gt;SetBarCenter(); /* typ znacznika na łącznikach pomiędzy kolejnymi słupkami */ $lplot-\u0026gt;mark-\u0026gt;SetType(MARK_SQUARE); $lplot-\u0026gt;mark-\u0026gt;SetColor(\u0026#39;blue@0.5\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetFillColor(\u0026#39;lightblue\u0026#39;); $lplot-\u0026gt;mark-\u0026gt;SetSize(6); /* wrzucamy kolejny wykres do obrazu */ $graph-\u0026gt;Add($lplot); /* a teraz magia... */ $graph-\u0026gt;Stroke(); Co to robi\u0026hellip; Ano dokładnie taki wykres jak ten poniżej:\nCo prawda ten wykres nie odpowiada w 100% podanemu źródłu. To statystyka z mojej strony, która niebieską linią zaznacza volumen pobranych danych - przykład jest nieco uproszczony.\nNa pierwszy rzut oka powyższy skrypt wydaje się dość długi i jest tam dużo nie do końca jasnych opcji, ale najfajniejsze jest to, że tak na prawdę nie trzeba się tego uczyć. Biblioteka domyślnie dostarczana jest z masą przykładów. Wystarczy wybrać wykres podobny do tego, który sami chcemy zrobić - skopiować większość kodu. Znaleźć inne wykresy, których cechy chcemy powielić i po kilku próbach skleimy takiego Frankenstein\u0026rsquo;a jakiego świat nie widział wcześniej ;-D\nMój wykres akurat ma trochę zmieniony kolor tła ramki i fonty, tak aby lepiej wkomponował się w moją stronę - reszta bazuje na dwóch przykładach z dokumentacji. Na zrobienie pierwszego wykresu z wykorzystaniem JPGraph potrzebowałem ok. godziny. Teraz wystarcza mi 10 minut 😃\n","href":"/2011/08/jpgraph-wykresy-z-phpa/","title":"JPGraph, wykresy z PHP’a"},{"content":"Tak się składa, że Debian ze względu na stosunkowo rzadkie wydawanie kolejnych wersji szybko staje się niezbyt świeży a dostępne w nim pakiety często nie spełaniają naszych oczekiwań. Nie ma najnowszej wersji Subversion\u0026hellip; Nie ma mod_security itd, itp\u0026hellip;\nRozwiązaniem tego problemu może być instalacja pakietów z testowej gałęzi ale można polec na zależnościach. Można też kompilować ze źródeł\u0026hellip; Tak czy siak w obu przypadkach aktualizacja i utrzymanie tak zmodyfikowanego systemu byłoby jak wrzód na zadku.\nNa szczęście jest prostsze rozwiązanie. System backportów - czyli repozytorium dostarczające możliwie najnowsze wersje pakietów dla gałęzi stabilnej. Dodając jedno źródło można zainstalować subversion, mod_security i inne, a przy tym równocześnie nie rozwalić sobie systemu.\nKonfiguracja - Squeeze Najpierw trzeba dodać dodatkowe źródło pakietów:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ squeeze-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Konfiguracja - Lenny Najpierw trzeba dodać dodatkowe źródło pakietów:\necho \u0026#34;deb http://backports.debian.org/debian-backports \\ lenny-backports main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list Ponadto w Lennym dostępne jest drugie repozytorium backportów tzw. lenny-backports-sloppy - to repozytorium nie gwarantuje bezproblemowej aktualizacji do Squeeze\u0026rsquo;a ale powinny się tam znaleźć nowsze wersje pakietów niż w przypadku podstawowego repo.\nDodatkowym krokiem w przypadku Lennego, aczkolwiek zalecanym jest ustawienie tzw. pinningu dla backportów, aby pakiety zainstalowane z nich były aktualizowane. Robimy to wklepując:\necho \u0026#34;Package: *\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin: release a=lenny-backports\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences echo \u0026#34;Pin-Priority: 200\u0026#34; \u0026gt;\u0026gt; /etc/apt/preferences Odświeżanie repozytoriów Teraz odświeżamy repozytoria:\napt-get update Instalacja pakietów z backportów Instalacja pakietów z backportów wymaga wymuszenia ich użycia, dzięki czemu jedynie wybrane przez nas pakiety zostaną zainstalowane w nowszych wersjach. Robimy to przykładowo tak:\napt-get -t squeeze-backports install subversion lub\napt-get -t lenny-backports install subversion To tyle. Możemy korzystać z aktualnych wersji paczek.\n","href":"/2011/08/konfiguracja-backportow-na-debianie/","title":"Konfiguracja backportów na Debianie"},{"content":"","href":"/tags/lenny/","title":"Lenny"},{"content":"Przy okazji wykonywania kilku drobnych optymalizacji swojej stronki natknąłem się na eAccelerator\u0026rsquo;a. Ciekawy projekt, który w sposobie działania przypomina Zend Optimizer\u0026rsquo;a ale ma jedną zasadniczą zaletę - jest darmowy 😃\nNiestety nie ma go w repozytoriach Debiana, więc trzeba go sobie skompilować - cały proces jest dość prosty. Zaczynamy od pobrania najświeższej paczki, obecnie jest to wersja 0.9.5.3:\nPobierz eAccelerator (ostatnio miałem problem z tym linkiem więc proponuję pogooglać)\nPobieramy i rozpakowujemy pliki:\ntar xvfj eaccelerator-0.9.5.3.tar.bz2 cd eaccelerator-0.9.5.3 Do kompilacji eAccelerator\u0026rsquo;a potrzebujemy paru paczek, które możemy zainstalować tak:\napt-get install build-essential php5-dev W katalogu ze źródłami klepiemy (prawie standardowo):\nphpize ./configure make make install No i eAccelerator jest już zainstalowany w naszym systemie. Pozostało wygenerowanie konfiguracji PHP aby moduł był automatycznie ładowany oraz ustawienie podstawowych parametrów konfiguracyjnych.\nW przypadku Debiana domyślna konfiguracja modułów PHP przechowywana jest w katalogu/etc/php5/conf.d tam też zapiszemy naszą konfigurację jako eaccelerator.ini. Tworzymy plik naszym ulubionym edytorem:\nvim /etc/php5/conf.d/eaccelerator.ini W pliku wpisujemy:\nextension=\u0026#34;eaccelerator.so\u0026#34; eaccelerator.shm_size=\u0026#34;128\u0026#34; eaccelerator.cache_dir=\u0026#34;/tmp/eaccelerator\u0026#34; eaccelerator.enable=\u0026#34;1\u0026#34; eaccelerator.optimizer=\u0026#34;1\u0026#34; eaccelerator.check_mtime=\u0026#34;1\u0026#34; eaccelerator.debug=\u0026#34;0\u0026#34; eaccelerator.filter=\u0026#34;\u0026#34; eaccelerator.shm_max=\u0026#34;0\u0026#34; eaccelerator.shm_ttl=\u0026#34;0\u0026#34; eaccelerator.shm_prune_period=\u0026#34;0\u0026#34; eaccelerator.shm_only=\u0026#34;0\u0026#34; eaccelerator.compress=\u0026#34;1\u0026#34; eaccelerator.compress_level=\u0026#34;9\u0026#34; extension - określa plik modułu, czasami może być konieczne podanie pełnej ścieżki,\neaccelerator.shm_size - określa ilość pamięci współdzielonej, którą eAccelerator rezerwuje do przechowywani skompilowanych skryptów. Ja dałem 128MB bo mój serwerek ma 2GB RAM-u, ale przypuszczam że dla mniej obciążonych maszyn wystarczy 16~32MB. Nadając temu parametrowi wartość 0 ufamy programistom 😉\neaccelerator.cache_dir - ustawiamy katalog, w którym będzie zapisywane to co nie będzie się już mieścić w pamięci współdzielonej. Ja mam akurat sporą osobną partycję na /tmp, ale równie dobrze nadaje się /var/cache/eaccelerator czy /var/tmp/eaccelerator - ustawiamy jak nam wygodnie,\nSkoro już piszę o katalogu na skrypty to warto go utworzyć i przypisać mu odpowiednie uprawnienia (ja ustawiam je tak aby tylko Apache miał dostęp):\nmkdir /tmp/eaccelerator chown -R www-data:www-data /tmp/eaccelerator chmod 0770 /tmp/eaccelerator No to jeszcze restart Apache\u0026rsquo;a i możemy sprawdzać jak działa:\ninvoke-rc.d apache2 restart Najszybszym sposobem sprawdzenia czy moduł się ładuje jest sprawdzenie wyniku poleceniaphp -v - powinno to wyglądać jak poniżej:\nphp -v Zend Engine v2.2.0, Copyright (c) 1998-2008 Zend Technologies with eAccelerator v0.9.5.3, Copyright (c) 2004-2006 eAccelerator, \\  by eAccelerator Podobny wynik można uzyskać z pomocą funkcji phpinfo() i prostego skryput PHP do jej wywołania.\nCzy warto? Na koniec trochę benchmarków aby sprawdzić czy cała zabawa jest warta świeczki 😃\nWykorzystam standardowe narzędzie dostępne z Apache\u0026rsquo;m: ab (Apache benchmark):\nab -n 1000 -c 10 http://mojastrona.pl/ U mnie dało to następujące wyniki - z wyłączonym eAccelerator\u0026rsquo;em:\nConcurrency Level: 10 Time taken for tests: 8.648 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12836000 bytes HTML transferred: 12612000 bytes Requests per second: 115.64 [#/sec] (mean) Time per request: 86.477 [ms] (mean) Time per request: 8.648 [ms] (mean, across all concurrent requests) Transfer rate: 1449.54 [Kbytes/sec] received Oraz z włączonym:\nConcurrency Level: 10 Time taken for tests: 3.663 seconds Complete requests: 1000 Failed requests: 0 Write errors: 0 Total transferred: 12840344 bytes HTML transferred: 12616120 bytes Requests per second: 272.97 [#/sec] (mean) Time per request: 36.634 [ms] (mean) Time per request: 3.663 [ms] (mean, across all concurrent requests) Transfer rate: 3422.90 [Kbytes/sec] received Wynik jest całkiem niezły, ponad dwa razy szybciej. A w niektórych przypadkach udaje się wyciągnąć więcej (nawet 5~10 krotnie). Tak, czy siak - warto!\n","href":"/2011/08/optymalizacja-php-z-eacceleratorem/","title":"Optymalizacja PHP z eAccelerator’em"},{"content":"","href":"/tags/postgresql/","title":"PostgreSQL"},{"content":"","href":"/tags/squeeze/","title":"Squeeze"},{"content":"Co prawda na swojej stronie zrobiłem kilka podstawowych statystyk i coś tam sobie loguję do bazy danych, ale gdyby się chwilę zastanowić to przecież to samo robi serwer www - wrzuca do logów każde zapytanie HTTP, kod błędu, nazwę agenta, itd. Dublowanie tych danych nie jest najbardziej optymalne.\nStąd też chwilę pogooglałem i znalazłem świetny Open Source\u0026rsquo;owy projekt: AWStats, który jest webowym analizatorem logów dla serwerów HTTP, FTP i SMTP.\nInstalacja i konfiguracja Najpierw instalacja, na moim Debianie leci to tak:\nsudo apt-get install awstats Teraz trzeba się chwilę zastanowić nad konfiguracją serwera i celem, który chcemy osiągnąć:\n czy staty będą dostępne publicznie? czy tylko dla ograniczonego grona zainsteresowanych (np. w pewnej sieci)? a może zabezpieczenie hasłem?  Wiedząc, że AWStats działają jako skrypt CGI wystawianie takiego serwisu \u0026ldquo;na świat\u0026rdquo; nie wydaje mi się bezpiecznym rozwiązaniem. Wolę np. skonfigurować serwis tak aby był dostępny tylko w LAN\u0026rsquo;ie, gdzie mam większą władzę i szybciej poradzę sobie z namierzeniem i zablokowaniem ewentualnego napastnika 😉\nOpiszę tylko dwa pierwsze przypadki (jak ktoś chce hasło to szybko znajdzie jak je ustawić) - pierwsza dla leniwych, druga dla ambitnych 😉\nPrzygotowania Bez względu na wybraną metodę konfiguracji (leniwą, bądź nie) do działania serwisu potrzebny jest włączony w Apache moduł CGI. Jest tak w domyślnej konfiguracji ale jeśli nie masz pewności to odpal:\na2enmod cgi Musi też być zdefiniowany katalog ze skryptami CGI z uaktywnioną interpretacją CGI - domyślnie w pliku /etc/apache2/sites-available w pliku default jest poniższa konfiguracja:\nScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; Na potrzeby metody leniwej jest to wystarczająca konfiguracja, dla ambitnych zalecam trochę inne umiejscowienie tego kodu.\nMetoda dla leniwych Ponieważ pakiet AWStats instaluje się przykładową konfiguracją można bardzo szybko uruchomić staty, wystarczy skopiować jeden plik:\ncp /usr/share/doc/awstats/examples/apache.conf /etc/apache2/conf.d/awstats No i tyle 😉\nMetoda dla ambitnych Ambitnym zalecam nieco inną konfigurację: z ograniczeniem dostępu do statystyk wyłącznie z LAN\u0026rsquo;u i tak samo z dostępem do skryptów CGI. W moim przypadku żaden z wystawianych przezemnie serwisów nie korzysta z CGI, więc udostępnianie tych skryptów wszystkim \u0026ldquo;zainteresowanym\u0026rdquo; nie ma sensu.\nProponuję wykorzystać taki lub podobny plik konfiguracyjny dla hosta serwującego statystyki:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName staty.domena.pl ServerAdmin webmaster@domena.pl DocumentRoot /var/www/stats/ \u0026lt;Directory /\u0026gt; Options FollowSymLinks AllowOverride None \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/www/stats/\u0026gt; Options -Indexes FollowSymLinks MultiViews AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ \u0026lt;Directory \u0026#34;/usr/lib/cgi-bin\u0026#34;\u0026gt; AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; \u0026lt;Directory /var/lib/awstats\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; Alias /awstats-icon/ /usr/share/awstats/icon/ \u0026lt;Directory /usr/share/awstats/icon\u0026gt; Options None AllowOverride None Order Deny,Allow Deny from all Allow from 192.168.1.0/24 \u0026lt;/Directory\u0026gt; ErrorLog /var/log/apache2/error.log LogLevel warn CustomLog /var/log/apache2/access.log combined \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On RewriteCond $1 !^$ RewriteCond %{REQUEST_URI} !.*cgi-bin RewriteCond %{REQUEST_URI} !.*awstats.pl RewriteRule /(.*)/? /cgi-bin/awstats.pl?config=$1 [PT] RewriteRule ^/awstats.pl(.*?) /cgi-bin/awstats.pl$1 [QSA,R,L] \u0026lt;/IfModule\u0026gt; \u0026lt;/VirtualHost\u0026gt; Z ważnych rzeczy do personalizacji:\n ServerName - wpisz swoją nazwę serwisu, Allow from 192.168.1.0/24 - zamień na adres/adresy, które Tobie odpowiadają, mod_rewrite - ostatnich kilka linijek wykorzystuje mod_rewrite do uproszczenia odwołań do statystyk, wystarczy wtedy wpisać adres np. tak: http://staty.domena.pl/nazwa.domeny.ktora.nas.interesuje.pl plik zapisujemy jako /etc/apache2/sites-available/awstats  Teraz zostało nam uaktywnienie site\u0026rsquo;a:\na2ensite awstats Część wspólna konfiguracji Teraz tworzymy katalog na skrypt, który wypisze nam dostępne statystyki:\nmkdir /var/www/stats chown -R www-data:www-data /var/www/stats/ Właściwa konfiguracja AWStats Pliki konfiguracyjne AWStats znajdują się w katalogu /etc/awstats. Jest ich dokładnie 2 szt.:\n awstats.conf awstats.conf.local  Plik awstats.conf zawiera przykładową konfigurację wystarczającą do odpalenia statystyk dla pojedynczego hosta. Z kolei plik awstats.conf.local jest miejscem gdzie można wrzucić wspólną konfigurację dla kilku plików hostów.\nJeżeli mamy wiele hostów (a taki przypadek tutaj omawiam) to wygodniej będzie nam wrzucić cały plik awstats.conf do awstats.conf.local i w kolejnych plikach konfiguracyjnych zmieniać tylko parametry rozróżniające poszczególne hosty. Robimy więc tak:\nmv /etc/awstats/awstats.conf.local /etc/awstats/awstats.conf.local.orig mv /etc/awstats/awstats.conf /etc/awstats/awstats.conf.local Teraz musimy zmienić kilka linijek w pliku awstats.conf.local:\n# musimy odszukać i zakomentować poniższe linie LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; SiteDomain=\u0026#34;\u0026#34; HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # w ten sposób #LogFile=\u0026#34;/var/log/apache/access.log\u0026#34; #SiteDomain=\u0026#34;\u0026#34; #HostAliases=\u0026#34;localhost 127.0.0.1\u0026#34; #Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; # dodatkowo odszukujemy linię LogFormat=4 # i zamieniamy na LogFormat=1 Teraz możemy utworzyć pliki konfiguracyjne dla naszych vhostów raptem w kilku linijkach, np.:\nLogFile=\u0026#34;/var/log/apache2/access.log\u0026#34; SiteDomain=\u0026#34;domena.pl\u0026#34; HostAliases=\u0026#34;www.domena.pl\u0026#34; Include \u0026#34;/etc/awstats/awstats.conf.local\u0026#34; Oczywiście trzeba wpisać własną lokalizację pliku access.log. W powyższym przypadku jest to lokalizacja domyślna, wspólna dla wszystkich vhostów - rozróżnienie ruchu do poszczególnych vhostów następuje dzięki podaniu parametrów SiteDomain (podstawowej domeny danej strony) oraz HostAliases (innych domen wskazujących na tego samego vhosta).\nOstatnim elementem jest załadowanie pliku ze wspólną konfiguracją.\nZmiana uprawnień do logów Aby umożliwić dostęp AWStats do logów serwera Apache musimy wykonać dwie czynności. Na początek zmiana atrybutu dla aktualnego pliku log:\nchmod o+r /var/log/apache2/access.log Później musimy zadbać aby logi po rotacji przez logrotate również zachowywały atrybuty, oraz aby przed rotacją AWStats wygenerowało statystyki, których nie zebrało wcześniej. W tym celu zmieniamy plik /etc/logrotate.d/apache2 tak by wyglądał jak poniżej:\n/var/log/apache2/*.log { weekly missingok rotate 52 compress delaycompress notifempty create 644 root adm sharedscripts prerotate /usr/share/doc/awstats/examples/awstats-update endscript postrotate if [ -f /var/run/apache2.pid ]; then /etc/init.d/apache2 restart \u0026gt; /dev/null fi endscript } Przeładowanie konfiguracji Apache Po tych wszystkich zmianach w konfiguracji musimy zrestartować Apache:\ninvoke-rc.d apache2 restart Testy konfiguracji Aby sprawdzić konfigurację AWStats spróbujemy wejść na stronę:http://staty.domena.pl/cgi-bin/awstats.pl?config=domena.pl\nJeżeli na żadnym z etapów nie popełniliśmy błędu to naszym oczom powinny ukazać się \u0026ldquo;wspaniałe i upragnione statystyki\u0026rdquo; 😃\nJedyną delikatną wadą awstats jest \u0026ldquo;brzydki\u0026rdquo; i długi link z cgi w środku\u0026hellip; Nieco mnie to irytowało, więc przysiadłem chwilę przy mod_rewrite i przygotowałem regułki (były podane w konfiguracji dla ambitnych), które pozwalają rozpocząć przeglądanie statystyk z uproszczonego linku postaci:\nhttp://staty.domena.pl/domena.pl\nProste, czyste i klarowne, bez zbędnych śmieci.\nCo prawda koniec tutora, ale nie koniec samej konfiguracji - proponuję aby przejrzeć przykładowy plik z konfiguracją i zapoznać się z zawartymi tam opcjami.\n","href":"/2011/08/statystyki-odwiedzin-dla-wielu-serwisow-z-awstats/","title":"Statystyki odwiedzin dla wielu serwisów z AWStats"},{"content":"","href":"/tags/ca/","title":"CA"},{"content":"Certyfikaty oparte o SSL stanowią obecnie podstawę bezpieczeństwa wielu usług sieciowych zaczynając od HTTP, przez POPS, IMAPS, itd\u0026hellip; Niestety zakupienie certyfikatu w organizacjach jak VeriSgin czy Thawte jest dość kosztowe, a jeżeli potrzebujemy kilka certyfikatów to często na lokalne potrzeby jest to po prostu nie opłacalne.\nPostaram się przedstawić wersję \u0026ldquo;ekonomiczną\u0026rdquo; certyfikacji 😃\nGenerowanie Certificate Signing Request Pierwszym etapem generowania certyfikatu jest przygotowanie Certificate Signing Request, czyli czegoś w rodzaju \u0026ldquo;prośby\u0026rdquo; o certyfikat. Nasza \u0026ldquo;prośba\u0026rdquo; po podpisaniu przez centrum autoryzacyjne stanie się certyfikatem.\nDo wygenerowania Request\u0026rsquo;u potrzebny jest najpierw klucz prywatny, który generujemy np. tak:\nopenssl genrsa -aes256 -out priv.key 4096 Powyższe polecenie poprosi nas dwa razy o hasło, które trzeba zapamiętać (albo zapisać i zamknąć w sejfie) - zalecam wykorzystanie pseudolosowego ciągu znaków o długości minimum 10 znaków.\nPo genrsa możemy użyć kilku opcji wybierając w ten sposób algorytm szyfrujący - do wyboru są: des, des3, aes128, aes192, aes256. Ja wybrałem 256-bitowego AES\u0026rsquo;a - najmocniejszy z tych algorytmów.\nOstatni parametr do długość klucza. Można użyć innej wartości np. 1024. Klucz wygeneruje się szybciej ale będzie prostszy do złamania. Z moich doświadczeń wynika, że niektóre aplikacje mogą mieć problem z obsługą długiego klucza (np. jak użyty w tym przykładzie), wtedy użycie mniejszej wartości może być konieczne (lepsze słabsze zabezpieczenie niż żadne).\nWarto ograniczyć uprawnienia do wygenerowanego klucza prywatnego tak by tylko właściciel miał do niego dostęp:\nchmod 400 priv.key Przy okazji tworzenia klucza prywatnego warto wspomnieć, że jeżeli będziemy chcieli go użyć w tak przygotowanej postaci np. w Apache\u0026rsquo;m to przy każdym starcie Apache będzie nas prosić o podanie hasła odbezpieczającego klucz prywatny. Niby to bezpieczne ale z drugiej strony przez takie zabezpieczenie Apache nie podniesie się samodzielnie np. po awarii. Rozwiązaniem jest przygotowanie odszyfrowanej wersji klucza prywatnego, z której będzie korzystał Apache. Robi się to tak:\nopenssl rsa -in priv.key -out priv.unsecure.key chmod 400 priv.unsecure.key Klucz priv.unsecure.key podany w konfiguracji programu nie będzie wymagał hasła. Koniecznie należy uniemożliwić dostęp do tego pliku wszystkim z wyjątkiem root\u0026rsquo;a!\nopenssl req -new -key priv.key -out request.csr Co dalej? Ok. Mamy już request\u0026rsquo;a, warto w tym miejscu wspomnieć co możemy z nim zrobić:\n Możemy taki plik przesłać do centrum autoryzacji (CA) celem podpisania i wygenerowania certyfiaktu. Klucze publiczne CA takich jak Thawte, VeriSign, itp są dołączane do przeglądarek internetowych, OS\u0026rsquo;ów, etc. dzięki temu strony internetowe (bądź inne usługi) zabezpieczone takimi certyfikatami weryfikują się bez żadnej dodatkowej akcji ze strony użytkownika. Warto wziąć pod uwagę to rozwiązanie, jeżeli np. chcemy zabezpieczyć sklep internetowy - my odpuścimy to rozwiązanie ze względu na koszty 😃 Drugą opcją jest możliwość samodzielnego podpisania certyfikatu tworząc tzw. SelfSigned Certificate. To zalecana opcja jeżeli mamy tylko jedną stronę/usługę. Użytkownik raz doda sobie nasz certyfikat jako zaufany i będzie mógł korzystać z szyfrowania do woli. Metoda ta robi się problematyczna gdy trzeba zarządzać większą liczbą certyfikatów. Ostatnia opcja to utworzenie własnego CA (czyli specjalnego certyfikatu), którym będziemy podpisywać wygenerowane przez nas Requesty. Ma to tę zaletę, że wystarczy rozdystrybuować klucz publiczny CA i będzie to wystarczające do weryfikacji wszystkich naszych certyfikatów.  Tworzenie certyfikatów SelfSigned Zaczniemy od prostszej wersji czyli podpiszemy nasz request wygenerowanym przez nas wcześniej kluczem prywatnym tworzac tzw. SelfSigned Certificate. Aby to zrobić potrzebne jest takie polecenie:\nopenssl x509 -req -days 365 -in request.csr \\ -signkey priv.key -out certificate.crt Tak oto wygenerowaliśmy certyfikat certificate.crt ważny przez 365 dni (tutaj dowolność ustawień ale 1 rok to sensowny okres).\nWłaściwości certyfikatu można sprawdzić np. tak:\nopenssl x509 -noout -text -in certificate.crt Tworzenie własnego CA Najpierw należy utworzyć klucz prywatny naszego CA, robi się to dokładnie tak samo jak w przypadku generowanego wcześniej klucza prywatnego dla serwera. Później musimy wygenerować requesta (analogicznie jak w przypadku certyfikatu SelfSigned). Czyli dwa polecenia:\nopenssl genrsa -aes256 -out ca.key 4096 openssl req -new -x509 -days 365 \\ -key ca.key -out ca.crt Skoro mamy już certyfikat naszego CA, możemy podpisać wcześniej wygenerowany Request dla serwera:\nopenssl x509 -req -days 365 -in request.csr -CA ca.crt \\ -CAkey ca.key -set_serial 01 -out certificate.crt Ogólnie podpisywanie przebiega podobnie jak przy certyfikatach SelfSigned ale jak widać wykorzystywane są klucze naszego CA i pojawia się nowy parametr: set_serial, którego wartość musi być inna dla każdego podpisanego przez to CA certyfikatu. Najprościej aby serial przyjmował numer kolejno wygenerowanego certyfikatu.\nJeżeli wiemy, że będziemy generować więcej tego typu certyfikatów warto przygotować sobie skrypt, który zadba o prawidłową wartość pola serial.\nPodsumowanie Certyfikaty przygotowane w ten sposób można wrzucić do Apache, postfix\u0026rsquo;a, itd\u0026hellip; i w ten sposób szyfrując ruch.\n","href":"/2011/08/certyfikaty-selfsigned/","title":"Certyfikaty SelfSigned"},{"content":"Mój serwer pocztowy działa od jakiegoś czasu na dynamicznym IP (dobre bo tanie\u0026hellip;) i przeważnie nie ma z tym problemów. Postarałem się jak mogłem ustawiając SPF\u0026rsquo;a i DomainKeys aby uwiarygodnić go u większych dostawców poczty.\nNiestety wszystko to diabli biorą w momencie gdy wygasa mi leasse DHCP i dostaję nowe IP po jakimś spamerze/zombiaku. Wisi takie w 2-3 większych RBL\u0026rsquo;ach i o dostarczaniu poczty można zapomnieć. Miło gdy jeszcze zdalny MTA zechce odesłać zwrotkę \u0026ldquo;zróbta coś bo wisisz w RBL\u0026rsquo;u takim a takim\u0026hellip;\u0026rdquo;, ale zdecydowania niefajnie gdy wysyłasz pocztę a ona od razu leci do /dev/null rblcheck Poszperałem trochę i znalazłem fajne narzędzie aka rblcheck, które sprawdza domyślnie kilka RBL\u0026rsquo;i. Można też dodać kolejne jako parametry. Wygląda to mniej więcej tak:\n$ rblcheck 89.76.116.114 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net Jak widać IP wisi w jednym z RBL\u0026rsquo;i.\nMożna odpytać RBL\u0026rsquo;e o konkretny powód znalezienia się na liście (o ile funkcja taka jest obsługiwana):\n$ rblcheck 89.76.116.114 -t 89.76.116.114 not listed by sbl.spamhaus.org 89.76.116.114 not listed by xbl.spamhaus.org 89.76.116.114 not listed by pbl.spamhaus.org 89.76.116.114 not listed by bl.spamcop.net 89.76.116.114 not listed by list.dsbl.org 89.76.116.114 not listed by dnsbl.njabl.org 89.76.116.114 listed by dul.dnsbl.sorbs.net: \\ Dynamic IP Addresses See: http://www.sorbs.net/lookup.shtml?89.76.116.114 W tym przypadku nie było się czym przejmować. Ta lista zawiera wszystkie IP dynamiczne, więc nic dziwnego że nasze dynamiczne też tam jest. Nawet jeżeli ktoś będzie korzystać z tej listy to raczej nie na zasadzie odcinania się od dynamicznych IP, ale w ramach punktowania wiarygodności danego adresu. Z naszej strony za bardzo nie jesteśmy w stanie nic z tym zrobić (bo nie zamierzamy dopłacić za statyczne IP), więc tak musi być.\nAutomatyzacja Najpierw zainstalowałem rblcheck‘a:\nsudo apt-get install rblcheck Później w /usr/local/sbin/check_rbls.sh utworzyłem skrypt sprawdzający kilka interesujących mnie RBL\u0026rsquo;i:\n#!/bin/bash  IP=`host mojadomena.pl | head -n1 | awk \u0026#39;{print $4}\u0026#39;` RBLCHECK=\u0026#34;rblcheck -t -s dul.dnsbl.sorbs.net \\ -s abuse.rfc-ignorant.org \\ -s postmaster.rfc-ignorant.org \\ -s dsn.rfc-ignorant.org \\ -s ix.dnsbl.manitu.net \\ -s rhsbl.ahbl.org\u0026#34; $RBLCHECK $IP | awk \u0026#39;{if($2 != \u0026#34;not\u0026#34;) print $0 }\u0026#39; Skrypt ten podlinkowałem aby uruchamiał się co godzinę:\nln -s /usr/local/sbin/check_rbls.sh /etc/cron.hourly/ Dzięki temu prostemu skryptowie jeżeli dostanę IP wiszące w tych kilku RBL\u0026rsquo;ach to stosunkowo szybko (maksymalnie w ciągu godziny) się o tym dowiem. Jeżeli natomiast IP będzie czyste to nie będę dostawać zbędnych maili. Do tego listę sprawdzanych RBL\u0026rsquo;i można bardzo łatwo powiększyć o kolejne w razie takiej potrzeby.\n","href":"/2011/08/dynamiczne-ip-i-rble/","title":"Dynamiczne IP i RBL’e"},{"content":"","href":"/tags/ftp/","title":"FTP"},{"content":"","href":"/tags/grub/","title":"GRUB"},{"content":"Klastrowanie to może zbyt dumnie powiedziane. Rozwiązanie to wyszukałem gdy chcąc skonfigurować dwa serwery apache do współpracy na rzecz jednego serwisu okazało się, że sejse trzymane są tylko przez jeden serwer a drugi nic o nich nie wie. To oczywiście nie pozwalało na prawidłowe działanie jakiegokolwiek serwisu korzystającego z sesji.\nPomysł jest taki, że zastępujemy domyśny mechanizm przechowywania sesji w plikach na dysku mechanizmem memcache. Ponieważ memcached działa jako usługa sieciowa, różne serwery mogą się odwoływać do puli memcached i odczytywać zapisane w niej dane. W przypadku sesji - nie jest ważne, kto ją utworzył - bo po jej wysłaniu do puli memcached staje się dostępna dla wszystkich klientów php z niej korzystających.\nJednym z pierwszych pytań nasuwających się do takiej kofiguracji jest: a co jeśli serwer memcached padnie? W chwili gdy wiele serwerów apache zależy od jednego serwera memcached jego awaria unieruchamia kaskadowo wszystkie.\nDlatego wykorzystałem konfigurację z dwoma serwerami memcached. Gdy php zapisuje dane w puli memcached dane są wysyłane do wszystkich podanych serwerów (w tym przypadku dwóch). A odczytywanie polega na odpytaniu pierwszego podanego serwera, a jeśli to się nie uda to drugiego. Układ nie jest idealny (jak mamy dwa działające serwery to aż się prosi o loadbalancing) ale zmniejsza prawdopodobieństwo, że awaria pojedynczego elementu położy wszystko.\nZ moim problemem moża było sobie poradzić też inaczej, np. zmieniając mechanizm sesji na stronie na taki, który korzysta z bazy danych. O ile w przypadku jednego serwisu nie jest to duży kłopot, to już przy kilku/kilkunastu byłoby to już spore przedsięwzięcie.\nWażną zaletą tego rozwiązanie jest fakt, że nie są wymagane żadne zmiany w istniejących serwisach. Po zmianie mechanizmu w konfiguracji php wszystko powinno działać bez zmian.\nInstalacja/Konfiguracja Najpierw trzeba zainstalować i uruchomić memcached oraz rozszerzenie php5-memcachedo php\u0026rsquo;a, które da nam możliwość korzystania z niego. U mnie robi się to tak:\napt-get install memcached php5-memcache Teraz trzeba by wyedytować konfigurację /etc/memcached.conf. Poniżej wycinek z tego pliku z opcjami, które należy ustawić:\n# Maksymalna wartość pamięci w MB jaka może być # wykorzystana przez demona. # Warto dostosować do swoich potrzeb. -m 128 # Interfejs, na którym nasłuchiwać będzie usługa. # Ja dla wygody wybiorę wszystkie :) -l 0.0.0.0 # można też dostosować port do nasłuchiwania -p 11211 # użytkownika nieuprzywilejowanego # (memcached domyślnie startuje jako root) -u nobody Wprowadzamy i zapisujemy zmiany. Trzeba zrestartować serwer memcached uruchamiając:\ninvoke-rc.d memcached restart Do tego momentu musimy powtórzyć konfigurację na drugiej maszynie (bo inaczej po co klastrować sesje).\nTeraz trzeba skonfiguraować php\u0026rsquo;a aby zamiast używać sesji zapisywanych do plików, korzystał z memcached. Edytujemy php.ini, u mnie akurat w lokalizacji:/etc/php5/apache2/php.ini. Odszukujemy opcje:\nsession.save_handler = files ;session.save_path = /var/lib/php5 I zamieniamy na:\nsession.save_handler = memcache ; adresy oczywiście należy dostosować do własnych ustawień session.save_path = \u0026#34;tcp://localhost:11211, tcp://remotehost:11211\u0026#34; W kolejnym kroku edytujemy plik konfiguracyjny rozszerzenia php\u0026rsquo;a dla memcache w /etc/php5/conf.d/memcache.ini dodając takie ustawienia:\nextension=memcache.so [memcache] memcache.dbpath=\u0026#34;/var/lib/memcache\u0026#34; memcache.maxreclevel=0 memcache.maxfiles=0 memcache.archivememlim=0 memcache.maxfilesize=0 memcache.maxratio=0 ; to jedyna wymagana opcja - resztę można dostosować pod siebie ; albo zostawić domyślnie memcache.allow_failover=1 memcache.max_failover_attempts=20 memcache.default_port=11211 memcache.chunk_size=8192 memcache.hash_strategy=standard memcache.hash_function=\u0026#34;crc32\u0026#34; Więcej po poszczególnych opcjach można się dowiedzieć z dokumentacji php.\nTest Jeżeli zrobiliśmy wszystko jak trzeba to sesje powinny zapisywać się z pamięci memcachedi dystrybuować na wszystkie wpisane w polu save_path serwery. Możemy to sprawdzić wykorzystując np. taki skrypt:\n\u0026lt;?php session_start(); print \u0026#34;Opcja save_handler: \u0026#34; . ini_get(\u0026#34;session.save_handler\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; print \u0026#34;Opcja save_path: \u0026#34; . ini_get(\u0026#34;session.save_path\u0026#34;) . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; if(isset($_SESSION[\u0026#39;testowa\u0026#39;])) { print \u0026#34;Testowa sesja jest już ustawiona: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } else { $_SESSION[\u0026#39;testowa\u0026#39;] = \u0026#34;i wygląda, że działa dobrze\u0026#34;; print \u0026#34;Ustawiamy testową sesją wartością: \u0026#34; . $_SESSION[\u0026#39;testowa\u0026#39;] . \u0026#34;\u0026lt;br\u0026gt;\u0026#34;; } ?\u0026gt;Wystarczy odświeżyć skrypt kilka razy. Za pierwszym razem sesja zostanie ustawiona a kolejne odświeżenia będą już zwracały jej wartość.\nProponuję też wyłączyć jeden z serwerów memcached aby sprawdzić czy php poprawnie odwoła się do drugiego serwera.\n","href":"/2011/08/klastrowanie-sesji-php-z-memcached/","title":"Klastrowanie sesji PHP z memcached"},{"content":"Instalacja serwera MySQL na Debianie jest niezwykle prosta i sprowadza się do jednego polecenia:\nsudo apt-get install mysql-server Polecenie to zainstaluje i uruchomi usługę serwerową MySQL. W czasie instalacji będziemy proszeni o podanie hasła dla root\u0026rsquo;a (które oczywiście dobrze jest zapamiętać bądź zapisać).\nTak zainstalowana baza nasłuchuje na lokalnym porcie (localhost:3306) umożliwiająć dostęp wyłącznie root\u0026rsquo;owi. Jest to bardzo bezpieczna konfiguracja\u0026hellip; Ale jeśli nie mamy zamiaru na tej samej maszynie instalować oprogramowania zarządzającego to nie zawsze jest to wygodne, tym bardziej gdy przykładowo mamy działającego phpmyadmin\u0026rsquo;a na jakimś serwerze www. W takim przypadku pierwszą rzeczą, którą robię jest udostępnienie dostępu zdalnego dla root\u0026rsquo;a. Warto zaznaczyć że uprawnienia root\u0026rsquo;a można nadać dowolnemu użytkownikowi (np. romanowi) co jest dużo bezpieczniejszą konfiguracją niż działanie bezpośrednio na koncie root\u0026rsquo;a (którego nazwa jest powszechnie znana).\nDostęp zdalny dla root\u0026rsquo;a Aby umożliwić zdalne zalogowanie się do bazy z uprawnieniami root\u0026rsquo;a trzeba ustawić odpowiednie GRANT\u0026rsquo;y, robimy to tak:\nmysql -u root -p mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;roman\u0026#39;@\u0026#39;%\u0026#39; \u0026gt; IDENTIFIED BY \u0026#39;haslo dla zdalnego roota\u0026#39; WITH GRANT OPTION; mysql\u0026gt; FLUSH PRIVILEGES; mysql\u0026gt; exit Pierwsze polecenie połączy nas z bazą prosząc o hasło podane w czasie instalacji.\nKolejne to polecenia SQL\u0026rsquo;owe, które pozwalają użytkownikowi ‘roman' łączącemu się z hosta ‘%' (dowolnego) identyfikującemu się hasłem ‘haslo dla zdalnego roota'. Jeżeli chcemy ograniczyć dostęp do tylko jednego zdalnego adresu to zamiast znaku procenta wpisujemy ten adres IP. Tak dodany użytkownik ma też prawo nadawania uprawnień (GRANT OPTION). Polecenie FLUSH PRIVILEGES przeładowuje uprawnienia - umożliwiając logowanie z podanymi wcześniej uprawnieniami.\nPozostało nam zmienić ustawienia serwera tak aby nasłuchiwał nie tylko na localhoście. W tym celu edytujemy plik /etc/mysql/my.cnf:\nsudo vim /etc/mysql/my.cnf Odszukujemy następującą linię:\nbind-address = 127.0.0.1 Linię tę możemy zakomentować co będzie skutkować nasłuchiwaniem przez serwer na wszystkich skonfigurowanych adresach IP (taki sam efekt da wpisanie w polu adresu 0.0.0.0). Można też wpisać tylko jeden adres IP w przypadku gdy na serwerze jest ich kilka i nie chcemy aby serwer był dostępny na wszystkich.\nOstatnim krokiem jest zrestartowania serwera MySQL aby zadziałały wprowadzone w pliku konfiguracyjnym zmiany. Można to zrobić tak:\ninvoke-rc.d mysql restart Jeżeli właśnie założyłeś i udostępniłeś nowy serwer bazodanowy MySQL to oszczędź sobie pracy w przyszłości i od razu ustaw przechowywanie tabel InnoDB w osobnych plikach.\n","href":"/2011/08/mysql-na-szybko/","title":"MySQL - dostęp zdalny na szybko"},{"content":"","href":"/tags/quota/","title":"quota"},{"content":"Większość systemów plików w linuksie pozwala na ustawienie quoty na dwóch poziomach: na użytkownika lub na grupę użytkowników. W wielu przypadkach taki podział jest sensowny i wystarczający. Ale zdarzają się scenariusze, w których to za mało.\nDobrym przykładem jest serwer FTP z wirtualnymi kontami użytkowników. Czyli usługa serwera działa jako pewien nieuprzywilejowany użytkownik systemowy (przeważnie ftp) przypisany do nieuprzywilejowanej grupy (np. nogroup). Konta użytkowników serwera FTP są zdefiniowane w bazie danych lub serwerze LDAP. Takie konta nazywa się wirtualnymi ponieważ po autoryzacji w pewnym systemie (bazie danych, LDAP\u0026rsquo;ie) działają z uprawnieniami pewnego systemowego konta (w tym przypadku ftp) - nie ma więc odzwierciedlenia pomiędzy użytkownikami korzystającymi z ftp a kontami systemowymi.\nProblemem w takim układzie jest to, że nie można rozróżnić poszczególnych użytkowników wirtualnych, ponieważ wszystkie operacje są wykonywane przez systemowego użytkownika ftp i to on jest właścicielem wszystich plików utworzonych przez użytkowników wirtualnych.\nRozwiązania są dwa:\n sewer FTP ma zaimplementowaną obsługę quot dla użytkowników wirtualnych, z czego możemy skorzystać, tworzymy quotę na katalog domowy użytkownika wirtualnego.  Mnie bardziej odpowiadała druga metoda.\nO ile mi wiadomo w ext3 nie można tworzyć quoty per katalog, na moje szczęscie taką możliwość oferuje xfs, z którego korzystałem.\nJeżeli jeszcze nie masz dysku sformatowanego jako xfs to jest to dobry moment aby to zrobić:\n# sdaX zastap nazwa urzadzenia ktore TY chcesz sformatowac mkfs.xfs /dev/sdaX # aby dzialala quoata per katalog trzeba zamonowac # dysk z opcja prjquota mount /dev/sdaX /mnt/ftp -o prjquota Logika quot per katalog w xfs\u0026rsquo;ie jest taka, że najpierw tworzymy projekt z powiązanym z nim id, a później wiążemy poprzez id projekt z katalogiem. Przez co quota nałożona na projekt ma zastosowanie do katalogu. Konfiguracja projektów i przypisanych im katalogów znajduje się w dwóch plikach: /etc/projid i /etc/projects.\nUtwórzymy teraz projekt i konfigurację dla niego:\n# nazwa projektu i jego id - w naszym przypadku # konto uzytkownika romana :) echo ftproman:10 \u0026gt;\u0026gt; /etc/projid # projektowi z id 10 przypisujemy katalog /mnt/ftp/roman echo 10:/mnt/ftp/roman \u0026gt;\u0026gt; /etc/projects Pozostało uruchomić quotę wpisująć komendy:\n# wiazemy projekt z punktem montowania xfs_quota -x -c \u0026#39;poject -s ftproman\u0026#39; /mnt/ftp # ustawiamy wlasiwa quote 1 gigabajt dla projektu xfs_quota -x -c \u0026#39;limit -p bhard=1g ftproman\u0026#39; /mnt/ftp To tyle - quota na katalog jest założona i działa. Aby zobaczyć jaka część miejsca jest już wykorzystana wystarczy uruchomić polecenie:\nxfs_quota -x -c \u0026#39;report -h /mnt/ftp\u0026#39; Project quota on /mnt/ftp (/dev/sda3) Blocks Project ID Used Soft Hard Warn/Grace ---------- --------------------------------- ftproman 1,5M 0 1G 00 [------] Dla kolejnych użytkowników musimy tworzyć kolejne wpisy.\n","href":"/2011/08/quota-na-katalog-w-xfsie/","title":"Quota na katalog w XFS’ie"},{"content":"","href":"/tags/rbl/","title":"RBL"},{"content":"","href":"/tags/rblcheck/","title":"rblcheck"},{"content":"","href":"/tags/selfsigned/","title":"SelfSigned"},{"content":"Kiedyś potrzebowałem w ramach testu obciążeniowego wysłać dużo wiadomości z załącznikami. Chciałem to zrobić na szybko z shell\u0026rsquo;a i tutaj chwilę musiałem pogooglać aby znaleźć działające polecenie. To co znalazłem wygląda tak:\n(echo \u0026#34;testowa wiadomosc\u0026#34;; uuencode test.zip test.zip) \\ | mail -s \u0026#34;Test\u0026#34; testowy@mail.pl Wiedząc już jak wysyłać maile z załącznikami, mały mail bombing mogłem zrobić tak:\nfor i in `seq 1 100`; do (cat tekst.txt; uuencode test.zip test.zip) \\  | mail -s \u0026#34;Test $i\u0026#34; testowy@mail.pl; done ","href":"/2011/08/wysylanie-zalacznikow-poleceniem-mail/","title":"Wysyłanie załączników poleceniem mail"},{"content":"Jeżeli tu zaglądasz pewnie zdarzyło Ci się kiedyś, że przykładowo wygrzebujesz jakiś stary serwer i nie masz pojęcia co na nim było, ani do czego służyło, czy jeszcze działa\u0026hellip; Albo jeszcze inaczej - serwer działał tak długo, że wszystkie osoby znające hasło na root\u0026rsquo;a przeszły na emeryturę lub zmarły\u0026hellip; Nieistotne 😃\nJest pewna prosta sztuczka, pozwalająca wbić się na konto root\u0026rsquo;a nie znając hasła - dając nam możliwość jego zmiany. Potrzebne dwa restarty ale za to nie trzeba korzystać z żadnychlive cd.\n Na początek zmuszamy serwer do restartu - mieć nadzieję, że maszyna obsługuje ACPI i delikatne wciśnięcie przycisku power subtelnie ją wyłączy. Jeśli to nie zadziała to kojarzą mi się tylko brzydkie rzeczy 😃 Gdy po restarcie załaduje się grub na domyślnej opcji bootowania wybieramy edycję wciskając \u0026ldquo;e\u0026rdquo;. Wybieramy linię zaczynającą się od kernel i znów wybieramy edycję wciskając \u0026ldquo;e\u0026rdquo;. Jeżeli znajduje się tam parametr ro to zastępujemy go rw i dopisujemy na końcu init=/bin/bash Wbijamy \u0026ldquo;enter\u0026rdquo; zapisując zmieniony wiersz. Bootujemy się z tak zmienionej konfiguracji wciskając \u0026ldquo;b\u0026rdquo;. Po chwili system zamiast wystartować init\u0026rsquo;a i uruchamiać usługi, ląduje w bash\u0026rsquo;u z uprawnieniami root\u0026rsquo;a. A skoro mamy root\u0026rsquo;a to możemy wpisać passwd i zmienić rootowi hasło 😃 Teraz już tylko reboot i startujemy system normalnie - hasło root\u0026rsquo;a powinno działać.  Niestety ta prosta sztuczka nie działa na wszystkich linux\u0026rsquo;ach - szczególnie tych wykorzystujących initramfs-tools. Na tych systemach trzeba ciut więcej pokombinować ale przynajmniej ma się jakiś punkt wyjścia.\n","href":"/2011/08/wlam-na-lokalne-konto-roota/","title":"Włam na lokalne konto root’a"},{"content":"My name is Tomasz Gągor, for some people known as TiMoR.\nI\u0026rsquo;m working as sys admin/DevOps. I enjoy using Linux (especially Debian and clones but not only), virtualization, high available systems, web optimization, programming in Python/Perl/Ruby. I spend a lot of time solving problems with systems integration, optimization, etc. and this is the place where I want to keep things that I\u0026rsquo;ve done well (or I think I\u0026rsquo;ve done it well) for future use. Second use of this site is to get confirmation from other people about my solutions.\nWith your help I hope to become the best DevOps I can be.\nThis work is licensed under CC BY-SA 4.0\n","href":"/about/","title":"About me"},{"content":"","href":"/search/","title":"Search"}]